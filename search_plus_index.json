{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction 概述 文章涉及内容：本文是我学习linux的笔记和梳理，还有工作中的案例分析。 本文编写宗旨：会使用大量举例截图和详尽的过程演示，所以复习起来效果会更好。 项目维护备忘 目录生成的方法： 打开pycharm里的windows/summary/list/directory_generate.py，运行 将运行结果去头掐尾后，复制到nodepad过滤一遍格式，然后再复制到SUMMARY.md中即可。 然后再gitbook init初始化目录结构。 github 创建项目： …or create a new repository on the command line git init git add README.md git commit -m \"first commit\" git branch -M main git remote add origin https://github.com/oneyearice/oneyearice.github.io.git git push -u origin main …or push an existing repository from the command line git remote add origin https://github.com/oneyearice/oneyearice.github.io.git git branch -M main git push -u origin main …or import code from another repository You can initialize this repository with code from a Subversion, Mercurial, or TFS project. 多终端pull和push注意点： 1、首先pull下来，得到最新的版本，如果是第一次git clone即可 2、复制oneyearice.github.io并重命名为gitbook；如果是git clone的就复制文件夹里的内容到gitbook下，选择替换原文件，得到最新的版本。 注意gitbook是本地编辑目录，oneyearice.gitbhu.io是pull和push目录 3、进入gitbook下运行gitbook install安装插件 3、在gitbook里编辑md文件，也就是主要工作内容 4、运行脚本自动上传 1、进入D盘 git clone https://github.com/oneyearice/oneyearice.github.io.git 如果有Oneyearice.github.io文件夹，进去后git pull 2、将oneyearice.github.io文件夹复制，并改名为gitbook 3、进入gitbook，删除node_module文件夹，cmd在gitbook文件夹下运行gitbook install ---开始编写md文章---完了就👇--- 4、我的笔记本电脑需要注释掉book.json里的\"-anchor-navigation-ex\"👈这样注释，运行脚本自动push--如果push失败，看报错，一般就是需要先git pull一下然后再运行脚本，因为可能最近的一次push是别的终端push的。这是合理的机制。 若优化，pull就上面的1 2 3，push就是4 使用nvm来管理nodejs两个版本 1、我要用gitbook，所以要用nodejs v12 2、我要用ES6 Module 好像nodejs v12 也行的。。。哈哈 玩gitbook要注意两个点一个就是js的修改 C:\\Users\\oneye\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs 还有一个就是nodes 的版本要低的，所以采用nvm来管理。 参考 https://www.cnblogs.com/eternalnight/p/15192585.html https://www.cnblogs.com/hacv/p/14311409.html Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 11:58:01 "},"序.html":{"url":"序.html","title":"序","keywords":"","body":"一声叹息解千愁 风中再无少年游 华发霜鬓难回首 万般滋味在心头 仰天长笑忆往昔 终究哪般得自由 还有一首👇，我原来的打油诗不记得了，只记得多年前误打误撞加了个群，发了一首打油诗给人家老师，然后老师给我修改了，就变成这首了，应该说提升了意境，重写了词汇，哈哈哈，可惜时过境迁原来的qq也没了，唉... 但这首诗我却一直没有忘记，每当想起它，当时的生活情景和内心的状态都不自禁地涌入心头 悠悠戚戚恨离愁，夜至寒窗静欲求， 灵犀巧带裁云月，化作扁舟任自流。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:04 "},"1-基础知识介绍/1-基础知识介绍.html":{"url":"1-基础知识介绍/1-基础知识介绍.html","title":"第一章 基础知识介绍","keywords":"","body":"第一章 基础知识介绍 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:23 "},"1-基础知识介绍/1-计算机基础.html":{"url":"1-基础知识介绍/1-计算机基础.html","title":"第1节 计算机基础","keywords":"","body":"第1节. 计算机基础 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:23 "},"1-基础知识介绍/2-操作系统基础.html":{"url":"1-基础知识介绍/2-操作系统基础.html","title":"第2节 操作系统基础","keywords":"","body":"第2节. 操作系统基础 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:23 "},"1-基础知识介绍/3-linux介绍.html":{"url":"1-基础知识介绍/3-linux介绍.html","title":"第3节 linux介绍","keywords":"","body":"第3节. linux介绍 https://www.kernel.org/ slackware:SUSE debian:ubuntu redhat:REHEL、CentOS [11:02:17 root@pyConsole ~]#uname -r 4.18.0-193.el8.x86_64 linux只是一个内核，加上GNU工具、附加软件和软件包管理组成的操作系统才是发行版。 CentOS https://wiki.centos.org/Download http://mirrors.aliyun.com http://mirrors.sohu.com http://mirrors.163.com https://mirrors.tuna.tsinghua.edu.cn/centos/ Ubuntu http://cdimage.ubuntu.com/releases/18.04.1/release/?_ga=2.56783850.1533668672.1544323446-1412352718.1543052421 tt Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 13:06:25 "},"2-linux基础和帮助/2-linux基础和帮助.html":{"url":"2-linux基础和帮助/2-linux基础和帮助.html","title":"第二章 linux基础和帮助","keywords":"","body":"第二章 linux基础和帮助 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"2-linux基础和帮助/1-linux安装前准备.html":{"url":"2-linux基础和帮助/1-linux安装前准备.html","title":"第1节 linux安装前准备","keywords":"","body":"第1节. linux安装前准备 1、 在windows是怎么安装的，类比一下， 分区、C盘、D盘，系统、数据文件。 2、 在linux同样类似思路，逻辑卷、raid， 大多数企业选择的是分区方式，而不是逻辑卷和raid。 3、 linux一切皆文件，不同类型的资源都命名为文件了 linux的目录结构 windows的C盘，D盘，里面格式有很多文件夹 C盘的顶级目录 在windows里有几个分区，就有几个顶级目录，或者叫根目录 在linux只有一个根root，而且用的是正斜线，linux不管你是几个分区，它的目录结构都是不变的。 /usr 类似于windows里的C盘下的windows目录，都是存放操作系统文件的。 /root或者/home/xx 类似windows里的C盘下的users目录，都是存放用户自己的文件的。 此外在linux还有/boot目录，存放启动文件，比如linux内核就是放在该目录下的。 日志信息在/var/log，/var本身就是变化的意思。 /tmp存放临时数据的。 /proc是个假目录，是映射成内存数据，就是看到的是内存里的数据。正常的数据是放在磁盘上的，真正的目录都是对应磁盘上的文件夹，里面的数据都是放在磁盘上。这就是一切皆文件。 /etc就是windows里的注册表，注册表并不是单一的某一个文件，是二进制的若干个文件，不是存在在磁盘的某一处的。/etc就是各种配置文件也是文本文件。 linux分区和目录结构无关 linux有个目录叫/dev，存放设备的，包括，硬盘、光盘、u盘等。都是在dev下。比如机器上多块硬盘，linxu自动会出现多个文件。命名方式为，命名硬盘的方式sda,sdb,sdc这是centos6和7的命名方式， 在硬盘上还可划分分区，涉及分区类型，在windows上的分区是用盘符来命名的。分区还有扩展分区和逻辑分区。 分区类型： 1、 分区跟你的硬盘分区策略有关，GPT只支持主分区，暂不做讨论 2、 MBR是传统的分区法，支持主分区、扩展分区、逻辑分区（逻辑驱动器） 主分区，在单块一个硬盘上只能最多四个。如果有两块硬盘，其中一块可以没有主分区。在windows里主分区通常可以安装操作系统。不仅仅存放数据，如果有多个主分区，就有且仅有一个激活的主分区，OS启动的时候会去寻找激活的主分区，一个硬盘上最多有一个。 扩展分区：仅仅是主分区的话最多是4个，所以还需要扩展分区。在一个硬盘上最多一个扩展分区。不能直接存放数据，必须先将其划分成更小的分区-逻辑分区， 逻辑分区，扩展分区里更小的单位，这个小分区才能存数据，逻辑分区的个数可以很多。 3、 一块硬盘，主分区、主分区、大的扩展分区（里面分成若干个小的逻辑分区） 4、 linux一切皆文件，硬盘是有文件的，分区同样也是。分区是有编号的，主分区是1-4，扩展分区也是1-4，主分区+扩展分区一共最多4个，因此都是用1-4来表示的。 主分区的文件名：/dev/sda1 /dev/sda2，这个就是a硬盘上的第一个分区，第2个分区。这是主分区，扩展呢，一样也是1-4，比如/dev/sda3就可能是扩展分区。 5、 逻辑分区的编号，是从5开始编号的，/dev/sda5 /dev/sda6 dev/sda7，这三个就是在/dev/sda3上面的分区的。这个逻辑的序号是自动分配的，不能像主分区和逻辑分区那样可以人为的命名。 6、 扩展分区删除，意味着里面的逻辑分区也没了。 7、 存在不同分区的同名文件。讨论分区和目录的关系。windows是C盘下的test文件，D盘下的test文件。linux呢？是从根下面开始描述的。此时就需要把分区和某一个文件夹做关联，将来这个test就是这个关联好的目录下的文件了。 8、 将第一个硬盘分区和boot关联，boot就是对应/dev/sda1，所以要访问第一个硬盘的第一个分区就访问/boot就行了。此时第一个分区里的test就在/boot/test。第二个分区要想访问，就得先把他映射成一个目录比如叫/data，把第二个主分区/dev/sda2挂载到/data下，第二个test就在/data/test 9、 这种挂载在windows里是存在的，windows的分区也可以挂载到文件夹的。windows的e盘的盘符可以删了，此时这块空间和目录结构就没有关联没有映射了，磁盘管理就看不到e盘了，但数据还在，再加回去，可以叫其他F盘之类，还可以挂载到NFS文件夹中的这就跟linux的挂载文件一样了。 10、 没有独立出来挂载分区的文件夹，都是跟在根下的，都在根所在的分区里，有些是不能独立挂载的，必须和根在一起，比如/etc /dev ， /proc是虚拟文件夹内存来着更加不能独立了。 11、 理论上一个分区也是阔以的，但是肯定不安全，一个分区挂了，就完了。 12、 一般分区这样 /dev/sda 200g硬盘的推荐分区： /dev/sda1 mount /boot 1G 这是引导目录不需要太多空间200M的实际占用，也不是给你存放数据的，你的数据也别扔这里面，1G空间足足有余了。这个目标文件夹就叫mount point挂载点。 /dev/sda2 mount / 100G 根上，根下存放的数据就比较多了，如果linux安装不是最小化安装，光是系统本身就要几个G的数据，如果是最小安装，至少1g。 /dev/sda3 mount /data 50G 测试练习用的文件夹，学习用的，工作中，用户会用来存放数据库单独占一个分区。 /dev/sda4 swap 4G，这里不能叫mount挂载，因为swap不是个文件夹，它是分区，不能叫挂载，挂载都是设备往文件里挂。 如果这样划分的话，我们知道一个硬盘上最多4个主分区，意味着200G剩下50G的空间用free，将来不能再分区了，因为4个分区满了，逻辑分区是在扩展分区里分的，扩展分区是占主分区的1-4这个编号的，现在没地儿了。所以上面的分区得改。 /dev/sda4 extend 剩下的所有空间（除了上面分的所有空间剩下的45G） /dev/sda5 逻辑分区 swap 4G sda5就是在sda4上面分的了 还可以继续分小的逻辑分区。比如/dev/sda6等。 13、 swap 交换 早期机器内存小2G 4G swap就是4g 8g的分配， 现在服务器你的内存都很大256g 512g，swap肯定不能乘以2了，swap一般就是8g 16g就足够了。 14、 linux的swap和windows里的pagefile.sys文件是一回事 15、 GPT不支持扩展分区和逻辑分区 4、 在使用vmware worksation安装镜像的时候，光盘需要最后挂载，不然系统自动安装不会让自己分区的，而且还是最小化安装。 5、 os下载，可以到阿里云上下载， 6、 vmware的lck缓存文件注意一下，突然断电关机了，lck可能需要手动删除才能保证VM正常开机。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"2-linux基础和帮助/2-centos7和rocky_linux安装详解.html":{"url":"2-linux基础和帮助/2-centos7和rocky_linux安装详解.html","title":"第2节 centos7和rocky_linux安装详解","keywords":"","body":"第2节 centos7和rocky_linux安装详解 1、安装前最好校验一下，防止iso损坏。 2、密码不符合安全要求，需要点击两次done。 3、文件格式化，linux centos6默认是ext4，也支持windows里的vfat，也支持centos7里的xfs。就用默认的就行 4、分区的sda1或sda2对应/还是/boot，这个序号对应关系不大，只要空间够就行。 5、ctrl + alt f2切到命令行界面，f6切回图形界面，cat /proc/meminfo 便于你在装机阶段查看内存大小。shift pageUp往上翻，memTotoal可见总内存大小。当然你要说 | less | more ，我也没办法。 6、在分区的时候，扩展分区是自动给你分的，你只要知道你在划分sda5的时候，会自动给你划分sda4—扩展分区就行了。 7、cat /proc/partition 可见当前只有一个sda 8、ls /dev/sda* 可见只有一个以sda开头的文件 9、等你的分区，确定格式化，werite chages to disk或done的时候，再去看ls /dev/sda*去看就看到分区开始实施了， cat /proc/partitions 也有了。 10、boot loader环节后面再说，这跳过 11、工作中一般都是minimal最小化安装，节约资源。学习选择Desktop或Server with GUI。 12、desktop，图形默认是GNOME，还有一种是KDE，一般不用KDE的（选择customize now—Desktops—KDE Desktop）。 13、centos7的安装注意： 1、内存2G，1G会导致系统安装报错-内存不够。 2、分区一样的，选择I will configure partitioning -Done-进入分区界面 单位可以输入比如1G 3、KDUMP别管，内核分析记录用的，系统崩溃查看用的，不是一般人玩的。默认是enabled，建议关掉，反正用不到。 4、关于ens33将来要改成eth1的 5、安装后，做好快照 14、ubuntu-18.04.1的安装 略，开发的，我看到的 智能小车，图像识别什么的用的就是ubuntu系统。 15、在安装过程中，可以 ctrl + alt f1 f2去切命令行界面 如果需要的话 16、centos允许你用root登陆，ubuntu不允许使用root，可以改passwd使能root的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"2-linux基础和帮助/3-linux入门操作和基础命令.html":{"url":"2-linux基础和帮助/3-linux入门操作和基础命令.html","title":"第3节 linux入门操作和基础命令","keywords":"","body":"第3节. linux入门操作和基础命令 入门操作 1、看版本 cat [root@localhost ~]# cat /etc/os-release NAME=\"Rocky Linux\" VERSION=\"8.5 (Green Obsidian)\" ID=\"rocky\" ID_LIKE=\"rhel centos fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Rocky Linux 8.5 (Green Obsidian)\" ANSI_COLOR=\"0;32\" CPE_NAME=\"cpe:/o:rocky:rocky:8.5:GA\" HOME_URL=\"https://rockylinux.org/\" BUG_REPORT_URL=\"https://bugs.rockylinux.org/\" ROCKY_SUPPORT_PRODUCT=\"Rocky Linux\" ROCKY_SUPPORT_PRODUCT_VERSION=\"8\" [root@localhost ~]# cat /etc/centos-release Rocky Linux release 8.5 (Green Obsidian) [root@localhost ~]# cat /etc/redhat-release Rocky Linux release 8.5 (Green Obsidian) [root@localhost ~]# cat /etc/rocky-release Rocky Linux release 8.5 (Green Obsidian) [18:27:16 root@pyConsole ~]#lsb_release -a bash: lsb_release: command not found... Install package 'redhat-lsb-core' to provide command 'lsb_release'? [N/y] y * Waiting in queue... * Loading list of packages.... Failed to install packages: Could not depsolve transaction; 1 problem detected: Problem: conflicting requests - nothing provides ncurses-compat-libs(x86-64) needed by redhat-lsb-core-4.1-47.el8.x86_64 [18:27:22 root@pyConsole ~]#cat /etc/centos-release CentOS Linux release 8.2.2004 (Core) 2、看内核 uname -r 3、关闭GUI，free可见 700M+的内存使用量，关闭前后的内存使用对比，init 3 关闭GUI，进入纯字符界面。再来看free 还剩200M+，一下500M的量省了。linux的图形界面相当于linux的一个软件可以开关。 5、runlevel 可见 5 3，说明之前是5模式切换到3的。 6、init 5如果你有图形的话，可以切回去。 7、who 不仅看当前登入的人 who -r可见这用户用的哪个运行模式，如果上一次是其他模式，也有last关键字看到 基础cli-1 1、root也不一定就是管理员，这是由UID的设置来影响的，UID=0就是超级用户 也就是管理员。 2、id 就能看到，id -u 显示当前用户的uid，id -u xx 显示xx用户的uid（从1000开始的） 3、centos6 新建用户默认从500开始，centos7和ubuntu默认冲1000开始 4、1000或500以内是特殊保留的数字 5、xx账号的uid改成0，xx就是超级用户了。 6、ll /dev/console 救援模式用的就是console终端，一般用不到 7、cat /etc/shells 可见目前支持的shell类型，在ubuntu上支持的并不相同。 8、/sbin/nologin 是一个特殊shell，禁止登入，当用户使用这种类型的shell的时候，就是不能登入的，软件运行的时候是后台运行的，但是不需要用户登入就能运行的。nologin其实就是无需登入就能运行的意思。 9、主机名：bj-yz-k8s-node1-100-10.XXX.com，北京亦庄k8snode1节点X.X.100.10.域名 10、echo $PS1 可见默认值，\\u就是用户名 \\h主机名 \\w当前目录 ，可以加上颜色 11、字体颜色 31-37 7种颜色，背景颜色 41-47 也是7种颜色 ， 1和5 就是亮色和闪烁。 PS1=\"[\\u@\\h \\W]$\" 这是默认值 PS1=\"[\\e[1;5;41;33m][\\u@\\h \\W]\\$[\\e[0m]\" PS1=\"[\\e[1;32m][[\\e[0m]\\t [\\e[1;33m]\\u[\\e[36m]@\\h[\\e[1;31m] \\W[\\e[1;32m]][\\e[0m]\\$\" 12、配置文件のPS1，ls /etc/profile.d/xx.sh 名字无所谓后缀要求是sh。 13、ubuntu切root, sudo -i 输入当前用户的口令就行了。 14、centos的PS1配置文件可以放在/etc/profile.d/xx.sh 或者和ubuntu一样放到/etc/profile文件下，unbutnu实在~/.profile，在每个账号的家目录下。 15、/etc/profile是统一的配置文件，这个文件影响范围大，配置要小心。 16、sleep 10，然后看pstree -p可以看到bash下面有一个sleep 17、还有很多程序不依赖于bash，不需要和人进行交互，后台直接运行了。 18、shell自身提供的内部命令、非shell自身提供的，磁盘上其他程序 19、bash里面集成了很多工具，就是内部命令，bash运行了，这些内部命令是加载到了内存中的。 20、cat /etc/profile.d/env.sh，这个cat就是bin下的cat cat就不是bash下内部命令了。是独立的二进制程序，这就是外部命令。 21、外部命令需要找到磁盘的存放路径，内部命令不需要会开机加载到内存中的 22、type用来查看命令是内部还是外部。 23、内部命令是集成在shell中的，而shell用户一登入就加载到内存里了；而外部命令表现为磁盘上的某个文件，所以内部命令速度更快。 24、如果有一个命令即使内部命令又是外部命令，那么内部命令优先生效，比如echo 当执行echo命令的时候，系统自动选择第一个内部命令。 25、为啥有一个内部命令了，还需要一个外部命令存在呢？因为内部命令是存在某个特定shell里的，echo在bash里，但是不一定在csh等其他shell里，所以需要外部命令来保证命令的普遍适用性。 26、切换shell 27、help可以列出所有内部命令的帮助。不多 28、外部命令就多了，表现为磁盘文件，ls /bin/ 或ls/sbin/ 29、enable会列出所有内部命令的列表 enable -n echo禁用echo后，echo就只有外部命令了，就表现为磁盘文件了，所以如上图。 禁用后，就看不到echo了 help里还可以看到一个*号 enable echo就再次启用了 30、enable -n enable enable enable不可能了，因为enable已经禁用了，退出重进就行了，或新开一个终端。 31、which专用用来查看文件的路径，自然可以查看外部命令了（外部命令就是磁盘的一个二进制bin文件） 32、bc了解一下 obase=2输出为二进制或10进制 ibase=2输入为二进制，默认都是obase=10，ibase=10. 32、whereis不仅仅显示文件（外部命令）的存放路径，还显示了相关文档帮助也显示出来了， 这是man帮助 33、外部命令，系统是怎么找到的，PATH变量了解一下 PATH变量就是存放了一个个文件夹， 1、当你输入一个命令，系统首先判断是否有别名，是执行，不是继续。 2、如果是内部命令，执行，如果不是，继续 3、如果是外部命令，就搜索PATH变量里的路径。前面的目录找到了，后面就不会找了。 4、其实也不是每次执行外部命令，都搜一遍PATH变量的路径的。为了加快访问速度，比如有一个文件我经常访问，linux就会将其缓存在内存中，因为内存中已经有了，就直接在内存中访问就行了。这也是linux的一个经常的思路。这就是所谓的缓存技术。 5、第一次执行hostname的时候，会按PATH变量里的路径来搜，一旦找到后，就会把hostname的路径缓存在内存里HASHE。下次执行hostname的时候，先从内存的HASH表里去搜索，如果查到内存中有这个路径，就不按照PATH变量搜索了，直接按HASH的记录的上次缓存的路径直接去到那个路径找到hostname文件去执行。 6、这里面还有一个细节，就是除了HASH，其实还有一个HASH对应的明文（路径的明文），这个一般不会给你讲这么细，网工会有这个思路可能。 7、这样的一个漏洞或缺点就是，如果外部命令hostname被移动了，那么HASH缓存的路径就不对了，这样命令执行就会报错。如下图 图示为hash记录的上一次hostname的路径。 再移回去就可以了， 疑点：我明明移动的是/bin/hostname而不是/user/bin/hostname，为啥一个效果！ 因为bin就是/usr/bin的快捷方式-软连接，所以一回事了。 再来一遍完整的： 8、上面的也可以不将外部命令移回去，可以清一下缓存就行了。 hash -d hostname 就行了 hash -r 全删 删掉了后，由于缓存没了，所以就会重新搜索，然后再次hash缓存到内存中了。 9、上面就意味着，自己做的程序，就要放到PATH变量里的路径或加一个新路径。 基础cli-2 1、内部命令和外部命令的本质区别，首先都会放入内存中的，本质区别是，内部命令在shell（bin/bash）二进制文件中；外部命令不在二进制文件里，是独立的文件。还一个内存方面一个是登入加载，一个是首次运行加载。 内外之分在于是否在/bin/bash文件里，在就是内部，不在就是外部。 2、问题：内部命令放在/bin/bash下，那么外部命令放在哪？首先放哪都行，关键是外部命令要运行，就得保证PATH变量里有该路径，然后规范行为是，外部命令放到PATH变量下的路径里去。所以外部命令一般来讲就在PATH下。 3、除了内部命令和外部命令，还有别名。 alias cdnet=\"cd /etc/sysconfig/network-scripts/\" 退出后失效，要想存住，就要将其放到文件里，别名的文件在家目录里的.bashrc里 重新登入后依然有效 4、alias列出所有别名，unalias cdnet可以临时删掉，但由于之前写在了配置文件里，所以重新登入后，还是没删掉 还在。所以配置文件的需要进配置文件删除 5、如果有一个字符串，既是 别名、又是内部命令、还是外部命令，那么执行的顺序是什么，这就是命令的执行优先级问题。 以echo（这个即使内部又是外部命令）为例，将其定义成别名，进行测试 说明，别名优先 总结：命令的执行顺序： ①首先判断是否是别名，如果是别名，别名是在内存中定义的，所以直接就执行了。所谓直接就是指已经在内存中了，不像外部命令那样首次执行还需要在PATH变量里进行查找。 助记词alias别名 ②其次，如果不是别名，判读是否是内部命令，如果是，直接执行内部命令（因为内部命令是内置在shell中的，用户登入就已经加载到内存中了）， 助记词 内部命令 ③最后，如果既不是别名也不是内部命令，那就按外部命令处理，就会看HASH表（表里记录了已经被执行过了外部命令的路径），如果HASH表里有该命令，就按表内记录的路径去搜索该外部命令去执行；如果HASH表里没有，就在PATH变量里查找，找到后执行。当然所谓执行也是加载到内存中执行的。对于首次运行的外部命令，也会产生的新的HASH表项。 助记词 外部命令（hash $PATH变量） ④如果找不着，就报错，此命令不存在。 PS：缓存为王，如果想提供一个慢速设备上（比如硬盘）的数据的执行效率，就把它放到内存里，下次从内存访问，速度就提升了。外部命令就是该逻辑思想。后面还有很多次这种套路。 6、加别名用~/.bashrc，这是只针对当前用户有效，家目录嘛，肯定的了。 对所有用户有效是编辑/etc/bashrc 7、别名修改后使之生效的方法，这也是很多配置文件修改后使其生效的通用方法： source /path/to/config_file #就是source 后跟你的配置文件路径 . /path/to/config_file # 就是. 后跟配置文件全路径 比如 . ~/.bashrc 比如： 8、之前的echo既是别名又是内部命令还是外部命令，如何不执行默认的别名优先呢， \\ 和 ‘ 以及 “ 或者 路径 再次command都是可以的 9、命令的格式，COMMAND [OPTIONS…] [ARGUMENTS…] 这点可以联系网络设备的cli 以及python argparse 自定义命令的格式或者规范问题。 -c 这种短选项，以及bsd风格的只有c没有-的用法，freeBSD这种好像cisco的wsa esa底层是这个。 ls -l 这个l就没有长格式 很多命令使用风格已经变了 可以理解成多层子命令的嵌套 10、ctrl+d 是正常退出 sleep 100就不能ctrl+d正常退出，得用ctrl+c强行退出。 11、ctrl+z 12、多个命令写在一行里用分号隔开 上图就是命令太长后认为换行用的。比如pycharm里面也是这么玩的。不过pycharm后来新版本直接回车也没有\\了，也能实现一套命令认为换行的效果。 13、date 系统时间：有软件操系统内核维护，通过CPU的工作频率维护的，date查看 硬件时间：主板上的CMOS，有块小电池（银币状）可供电5年。clock查看 timedatectl 看的最全 clock -s 将system time改一下，改成硬件时间 clock -w 将hardware time改一下，改成系统时间。 date -s ‘20200101 12:02:01’ 系统时间和硬件时间 或者date 010101012008.01 缺点就是看着乱七八糟，优点就是不用写引号便于python调用时的字符串拼接入库啥的。 上图GMT+8 14、时间其实内部一般用NTP去同步的 这里我先停掉ntpd服务，再去同步时间就好了 注意，ntp只会同步系统时间，不会同步硬件时间。 如果硬件时间不对，就先ntp保证系统时间准确后，再clock -w让硬件时间去同步系统时间就可以了。 15、ntp后面细讲，如果企业里时间不同步，涉及加密、集群就会出问题。 16、查看隐藏文件的方法 推荐第一种 l. -l ll -ad .* ll -a |grep -E \" \\.\" a alias b basename bc c cal 9 1752 chkconfig iptables off cd command clock(hwclock) cat /etc/rehat-release /proc/maminfo /proc/partitions d dir类似ls dirname df du date e enable enable -n exit echo f free -h g getenforce disabled h hexdump -C halt history hostname help hash i iptables -vnL init 3字符模式 5图形 0是关机 6是重启 info ifconfig id j k l lsblk logout ls lsb_release -a lshw m mount /dev/sr0 /mnt挂光盘到/mnt下 man mv mandb n ntpdate o p ps pwd poweroff ping pstree q r rpm -ivh rm rz runlevel查看当前运行模式的 s systemctl disable firewalld stat shutdown screen sleep source(.) sz sudo -i t touch tty 看在哪个终端里 type u uname -r unalias v vdir类似ll w which whereis whoami who -r whatis x xxd 等价于hexdump -C y z rz后再按esc可以产生如下图效果，并排两个提示符😶 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"2-linux基础和帮助/4-linux帮助用法.html":{"url":"2-linux基础和帮助/4-linux帮助用法.html","title":"第4节 linux帮助用法","keywords":"","body":"第4节. linux帮助用法 查看帮助相关整理 帮助有这些 whatis command –help -h man and info /usr/share/doc Red Hat documentation 其他网站和搜索 whatis是外部命令，是用来告诉你去找man手册第几页的 以及 该命令的简要说明 如果系统是刚刚装好，whatis是没有结果的，就会出现如下提示，我就不还原系统了。 whatis数据库存放的帮助的简要说明，在刚装好操作系统是没有的，是需要过一会自动生成。如果你不想等，只需要执行 centos6上 makewhatis centos7上 mandb 这样就能生成whatis数据库了。 whatis只是简单说明，详细用法还得参考其他帮助 -----------↓-----Unicode字符集和UTF-8编码--------↓----------- 1、有关文件类型 cat是查看文本文件的，二进制文件看不了，常见的二进制文件有：图片、视频、可执行文件，bash也是 2、二进制的怎么看，二进制太长了，一般就是以16进制形式查看。 hexdump /bin/bash hexdump -C /bin/bash 在linux保存处理都是二进制的，上图的A就是41这个十六进制对应的二进制进行存放在磁盘上的。 阔以的，这排版都得倒一下的~ 3、这些英文或者汉字对应的都有二进制表示的，这种就需要编码表来实现。ASCII就是早期著名的编码表。ASCII（American Standard Code for Information Interchange） 4、ASCII只能表达128个字符，只适用于欧美国家，后来就有了GB2312、GBK我国自己的编码表，以及还有韩国自己的。每个国家的编码都不一样。可能存在都是55这个十六进制，代表的字符不一样，这就产生了冲突。为了统一，就有了统一命令的Unicode， 5、Unicode只是一个规范，定义了全球所有文字或者叫字符和二进制的对应关系。它并没有明确下来这个二进制在磁盘上保存到底是什么形式（用哪种风格）并没有确定下来，比如具体这个二进制在磁盘上占几位，Unicode就没有明确下来，为了具体明确下来（磁盘上存放某个字符占多少位、在网络上传输的时候转成二进制占多少位）就推出了编码的具体实现—比如UTF-8，UTF-8就是Unicode的具体实现。 6、Unicode属于字符集（字符和二进制的对应关系）、UTF-8是编码方式（站位的明确定义）。还有UTF-16和UTF-32，UTF-8使用的最多。 UTF-16用2个或4个字节来表示某个字符。工作中，用得少 UTF-32，所有的文字全都占4个字节。工作中，用的少 UTF-8，使用1-4个字节来表示字符。比如UTF-8兼容ASCII码就是一个字节就够了。生僻字3个或4个字节，常用文字占1-2个字节。 7、Unicode（UTF-8）处理的逻辑机制是这样的，我们vim或nano处理某个文件的时候，在编辑的时候，vim是个软件，自然会在内存中运行这个vim程序。vim打开比如xx.txt这个文件，就会把xx.txt里的字符就转化成了Unicode。你在xx.txt里就可以编辑修改，此时注意这些字符都是在内存中以Unicode形式存在的，当然显示的时候是转换成人类看懂的文字的（这是计算机内部处理的）。 然后你一旦保存，就是存到磁盘上去了，保存到磁盘上就表现为具体编码形式UTF-8了。 后面你再次读xx.txt文件的时候，就是读入内存中，那么此文件在内存中又是以Unicode形式存在的。 8、在网络传输的过程中，PC----请求----Server。比如PC请求http://www.xx.com/index.html, 这个文件在server的磁盘上保存时UTF-8形式，读取到server自己的内存中就是unicode，在网络上传输就是UTF-8，读取到PC处的内存就是unicode，存到PC本地的磁盘就是UTF-8了。当然我记得还有一个BASE64，也需要了解一下。在加密数据的时候就有先使用对称加密后在使用BASE64进行网络传输的，图片在网络传输也是使用BASE64的。 ---------------------↑---------帮助的用法-------------↑------------------------ 9、命令帮助，内部命令和外部命令的help是不一样的 内部命令：help COMMAND 或 man bash 只要是内部命令，都可以通过man bash查看，（当然man也可以看外部命令） 因为/bin/bash文件里集成了所有内部命令，所有man bash就能看到所有内部命令的详细帮助。 bash的man手册有4000多行，有人对其进行了翻译，就是普通学员做了这个事。当然网上也有人汉化也可以参考。 外部命令帮助用法： ①COMMAND -help -h ②man COMMAND ③info COMMAND ④README INSTALL ChangeLog这些程序自身的帮助文档 ⑤程序官方文档 也就是官网的Documentation ⑥发行版的官方文档 ⑦Google bing baidu等搜索引擎 [OPTIONS]...就是选项可以有多个，FORMAT就是在下面列出了很多的FORMAT 10、date 1970-1-1是Unix诞生日 11、显示前天是周几 date 010210102020.300 unix的诞生日，很多时间计算都是从这个点开始的，比如 显示前天周几 12、man手册 manual的意思，利用man可以查看很多外部命令的帮助 上图whereis列出了man帮助的文档。gz压缩文档，不用解压后查看，直接用man命令查看即可。 13、man对应的文档基本都是放在/usr/share/man下的。 man1：linux命令基本都在第1章，是我们常常需要的； man2：系统调用，OS对 外部APP提供内核调用的接口，开发用的；就是应用程序需要和操作系统内核打交道，就得系统调用来完成，在第二章里。man socket 一般看到的就是man 2，属于网络方面的系统调用。不过为啥我的腾讯云上的VPS默认是man3的socket。另外就算是开发也不会直接进行这种底层调用，一般也是通过C库或者python库去调用。 man3：C库调用 man4：设备文件及特殊文件 man5：配置文件格式，也是我们关心的，linux好多配置文件，配置文件有很多格式，这些说明就在第5章里。 这个issu文件怎么配置，他的帮助就在第5章里。然而VPS上并没有 换成我自己的vm虚机就有了 应该VPS是最小化安装，当然也不是都没有，最小化安装man ls，man bash都有的。 man6：游戏 man7：杂项 man8：管理类的命令，管理员，root身份进行一些管理型的命令。 man9：和开发相关的linux内核API。 14、man使用注意点 man xx 默认看的是man1，1以外的章节 需要特别指定第几章才行。比如passwd这个文件配置说明。 /etc/passwd是个文件，/usr/bin/passwd是个命令，这两个passwd不是同一个东西。 显然whatis查看的不仅仅是命令，而type只是查看命令是内部 还是外部 亦或是alias。 man passwd看的是第1章，看的是passwd命令（/usr/bin/passwd） 如果要/etc/passwd配置文件帮助，就需要：man 5 passwd 总之，先whatis xxx看一下在那一章节，然后man n xx去看 man也是个外部命令，可以用whatis去看一下有哪些章。 1p都是和开发相关的。 man man可见 15、man搜索和vim搜索一样 或者 ”?second”，n 和 N是下一个或者上一个。方向键上下，一样调用历史记录。 man -a 查看所有，q+enter进入下一章 man -k passwd查看包含passwd的帮助类似whatis passwd，但明显要比whatis passwd多得多。 man -f passwd 等价于whatis passwd man -w date查看帮助文件在磁盘的路径，类似whereis date，区别在于man只是看man手册在哪，而whereis date还显示命令（文件）的路径。 16、man举例 \\S就是OS版本 Kernel就是Kernel \\r内部版本 \\m X86架构 这个issue就定了用户登入提示信息。 现在需要：显示用户在哪个终端登入上来的，（原本是tty查看的），还要显示时间、显示主机名，此时就需要查看帮助 说明了issue只有第5章有，是预登入和标识文件。 直接man issue就行了，因为只有1个 第5章 上图是man issue的所有信息了（就这么多行，到底了），其中并没有看到什么\\S \\r \\m的解释 不过有一个SEE ALSO可以参考 于是man motd 我们issue是登入之前，而motd是登入之后，不是此时需要的帮助，换一个 man 8 agetty 都找到了 再来对比一下 都找到出处了, welcome…是自己加的 修改为： ​ 17、linux的语言默认是英文，不建议转成中文，但是可以转，如下 有些地方就是中文了 但是man里面还是英文的， 还需要安装如下的中文的包 才能在man手册中显示为中文。 安装的话，我准备使用本地安装光盘里找一找相关软件包，所以不适用VPS了，换成本地CENTOS7 上图表示现在光盘没有挂载，可是我已经在WmwareWorkstation上勾选了connect了。此时只需要在GUI界面上使用和cli同样的账号，此时是root，登入一下就行了 进到光盘的路径下，里面就是所有的安装的软件文件，而且后缀都是.rpm。找到man开头的 发现了zh-CN的中文包了，这个就是可以修改man手册里的中文的包。 使用rpm -ivh 安装，注意ls man按tab补全后ctrl a切换到头将ls改为rpm -ivh，因为rpm 不带自动补全功能 此时在修改一下LANG，localectl set-locale LANG=zh_CN.UTF-8，退出再登入，然后就可以看到man手册里的中文了 但是man 1 passwd就是命令的帮助手册还是英文的 说明中文支持的还不是非常全。不过只是了解一下怎么切中文，一般也不会切的。 所以，切回英文 localectl set-locale LANG=en_US.UTF-8。 18、info 命令一般不用，不过info里面的都是一个个链接，更像是一个网页，是*号开头的，按回车就会跳转，挺有意思的，了解一下，比如info ls 光标停在*号行，按回车，就会跳转 19、man帮助使用较多，info基本不用，此外还有一些不怎么用的帮助，了解一下 GUI里的Applications\\help里面点开可以查看的，这是CentOS7的。CentOS6实在GUI的system/help下。 20、不怎么用帮助之/usr/share/doc 每一个安装好的软件包，都有一个对应的文件夹放在/usr/share/doc下，你可以进去查看软件的说明。 这些文档大部分都是文本，PDF\\HTML\\TXT 都能打开看基本上。 21、linxu只是一个操作系统，常规操作掌握后，更多精力是放在linux系统之上的应用程序—这些第三方软件（apache、nginx、mariadb等），这些软件就要去官方网站查看文档。 第三方应用官方文档举例 http://httpd.apache.org http://www.nginx.org https://mariadb.com/kb/en https://dev.mysql.com/doc http://tomcat.apache.org http://www.python.org 点击documentation后，可以看到 这个软件有很多moduels模块组成，其中有core模块，点击进去可以看看该模块里的各个指令directives。 点击root命令 要学会看懂这里面的说明，因为官方文档才是最权威的一手资料。 22、现在还没进入都linux上层应用的学习阶段，现在还是在学习操作系统本身，系统本身也有官方网站以及documentation的。 红帽知识库和官方在线文档 http://kbase.redhat.com http://www.redhat.com/docs http://access.redhat.com https://help.ubuntu.com/lts/serverguide/index.html 比如现在需要看centos8的安装指南 http://www.redhat.com 如图就找到了CentOS8的安装手册，有升级的、基本安装、定制化、高级安装（kickstat自动化安装），而且右边栏还可以选择查看的文档格式（pdf or web） 23、搜索引擎 http://tldp.org http://www.slideshare.net # 这网站是很多国外的人在研究技术的时候写成了PPT，可以拿下来改吧改吧。 http://www.google.com 搜索的技巧 https://segmentfault.com/a/1190000038432191 https://funletu.com/12851/.html openstack filetype:pdf rhca site:redhat.com/docs #这种站内搜索，受限于对方的安全措施，应该叫反爬机制 https://www.ibm.com/developerworks/cn/linux/index.html 要知道IBM已经收购红帽了，所以该网站也是阔以的。 24、帮助举例 ASCII 字符集 在编码的时候使用 八进制、十进制、十六进制。 然后，man ascii ASCII码总共128个字符，每个字符对应的八进制、十进制、十六进制分别是什么，分两列展示。 想用echo一下ascii，不知道怎么玩，可以man echo在搜oct就行了 所以echo -e “\\0xxx”还有echo-e “\\x21” 25、ascii查看举例 \\x0a是十六进制的，是不分大小写的 26、\\r和\\n 这是\\r的回车return的效果，没有换行，所以还在本行，就会把之前的xxx覆盖了前两位。 这是换行+回车的效果，linux的\\n就是微软的\\r\\n linux也认\\r是回车的意思，所以\\r就是回车、\\n就是换行在回车，这里两个回车了，效果还是把车停到了最左边，就是100个\\r也就是1个\\r的效果，一个\\r也被集成在\\n里了。 所以，在python使用paramiko非交互模式获取H3C的dis cu inter | I inter后，详情如下 如果上图re.split处这么写：portPer1.split(‘\\n’)，在print(i)的时候你会看到显示都是OK的，但是一旦将这些i传入list里后，就会出现xxx\\r\\r如下情况： 此处需补图，回公司才有环境，还得自己整一个网络环境出来，用eve吧明天搞。 所以H3C的display 看到的分行，其实里面是xxx\\r\\r\\n，我猜它这么做为了，为了个屁，就是强迫症，多次回车保证车必然停在最右边，然后\\n换行，所以我改成了re.plit(xxx)如上图。 27、对比一下MS和Linxu的\\r\\n 我们已经知道\\r\\n是MS的换行，\\n是linux的换行，这里说换行自然就包含了回车了。 下面看实验 传到桌面，在打开 可见linux的\\n到MS里是没问题的，结论MS兼容\\n 反过来，来一波 在windows新建f2.txt 使用rz传到linux里或直接拖进去 carriage return简写了 结论 MS的\\r\\n到linux下linux只需要\\n就能进行换行回车，所以多了一个\\r，当然cat的时候是看不到的，或者说python处理的时候可能print也是看不到的，但是\\r确实留在了文本里面，确实会为后面的数据处理带来麻烦的。 相对的，linux下的\\n到了MS里，MS处理的就很好，奇了怪了。我明明记得很多txt文本在windows里看到的都是xxxx\\nxxxx\\nxxxx\\n不换行的格式错乱啊，难道是sz工具对其进行处理了？ 尝试不使用sz工具，而是用sftp进行linxu->ms的文件传输，结果一样还是MS里显示OK。 尝试不用echo -e \"ABC\\nabc\" > f1.txt的方法创建内容，而是使用vim 依然在MS里显示OK，好了此问题不研究了。MS你优秀~ 我明明小写abc后面在linux里就没有换行，传过来结果换行了。 方法论：在文本处理的时候，文本从linux->MS，\\n前加上\\r；从ms->linux，\\r\\n去掉\\r。 以上就说明了：二进制在磁盘上保存机制不同，不可见的符号是看不到的，但是在磁盘上保存确实有的。 xxd和hexdump -C一样的效果，专门看不可见字符。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"2-linux基础和帮助/5-linux入门命令.html":{"url":"2-linux基础和帮助/5-linux入门命令.html","title":"第5节 linux入门命令","keywords":"","body":"第5节. linux入门命令 1、localectl list-locales查看所支持的语言 [11:45:53 root@localhost ~]#localectl list-locales | grep ^en_US en_US en_US.iso88591 en_US.iso885915 en_US.utf8 [11:46:02 root@localhost ~]# [11:46:02 root@localhost ~]#localectl set-locale LANG=en_US.utf8 这是界面风格是英文，不是说不支持中文，因为UTF-8全球语言都支持，之前的文章也讲过UTF-8是unicode全球文职字符集的编码格式。 2、时区文件/etc/localtime 这个时区文件Shanghai也是个二进制文件 3、timedatectl list-timezones 4、cal显示日历 -h看一下就好 [11:49:22 root@localhost ~]#cal 9 1752 September 1752 Su Mo Tu We Th Fr Sa 1 2 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 5、halt，poweroff，reboot，shutdown 关机：halt,poweroff 重启：reboot ​ -f: 强制，不调用shutdown ​ -p: 切断电源 关机或重启：shutdown shutdown [OPTION]... [TIME] [MESSAGE] -r: reboot -h: halt -c: cancel TIME: 不指定，默认就是+1（CentOS7），相对当前时间过一分钟后执行相关动作 ​ now: 立刻关机，等于+0; ​ +m: 例如+3，表示3分钟后； ​ hh:mm: 绝对时间表示，特定时间执行相关动作。 哈哈哈，shtudown -h查看帮助？想当然了，小心误操作哦。 [11:50:25 root@localhost ~]#shutdown Shutdown scheduled for Fri 2022-01-07 11:53:15 CST, use 'shutdown -c' to cancel. [11:52:15 root@localhost ~]#shutdown -c [11:52:22 root@localhost ~]# [11:52:36 root@localhost ~]#shutdown -h Shutdown scheduled for Fri 2022-01-07 11:53:38 CST, use 'shutdown -c' to cancel. [11:52:38 root@localhost ~]#shutdown -c [11:52:41 root@localhost ~]# 6、windows也有shutdown命令 1小时后关机 取消 还支持远程关机 7、who 和 who am I 以及w who，当前有哪些用户登入上来 who am I 或who x x只看当前用户 w 显示谁在登入，运行了那些程序 有点监控的意思了,并没有。 8、screen的用法 虚拟多个屏幕用 首先找到光盘进行安装，当然也用直接yum VmwareWorkstation 连接光盘后，使用非GUI界面相同账号登入(这里是root)，即可完成挂载光盘。哈哈，就这么挂，怎么滴~，当然我现在是mini没GUI。 rpm有tab自动补齐啊，上次怎么没有呢，不管了。 举例1：防止ssh断开导致ping或tftp断开 如果ping着的时候，关闭ssh窗口，ping就停了，证明方法ps aux |grep ping 问题就来了，如果我在备份数据的时候，ssh断了，那备份就失败了，所以screen有用武之地了。 screen -S ming # 创建ming命名的新的屏幕 screen -ls # 查看开启的屏幕有哪些 screen -r ming 进入ming命名的屏幕 举例2：屏幕协助 screen -S ming # A开启一个屏幕 screen -x ming # B进入这个屏幕，此时两个人就共享一个屏幕了。 screen -x ming # C同样可以可以进入该屏幕，此时就三个人共享一个屏幕了，那么问题来了上限是多少人呢？没兴趣知道。哈哈 ctrl+a+d # 注意手法，不能先按ctrl+d，因为ctrl+d是退出的快捷键（等价于exit）；临时剥离该屏幕，但是其他共享屏幕的人看不到的，此时可以干一些不想共享给别人知道的事，然后完了再screen -x ming切回去，就有成了大家共享一个屏幕的情况了。 exit # 一人退出共享屏幕，全部菜单都退出了。 9、echo，回显用 尝试使用帮助看echo，①type echo 发下是内部命令，②所以使用help echo看帮助 [root@centos7 ~]# type echo echo is a shell builtin [root@centos7 ~]# help echo echo: echo [-neE] [arg ...] Write arguments to the standard output. Display the ARGs on the standard output followed by a newline. Options: -n do not append a newline -e enable interpretation of the following backslash escapes -E explicitly suppress interpretation of backslash escapes `echo' interprets the following backslash-escaped characters: \\a alert (bell) \\b backspace \\c suppress further output \\e escape character \\f form feed \\n new line \\r carriage return \\t horizontal tab \\v vertical tab \\\\ backslash \\0nnn the character whose ASCII code is NNN (octal). NNN can be 0 to 3 octal digits \\xHH the eight-bit character whose value is HH (hexadecimal). HH can be one or two hex digits Exit Status: Returns success unless a write error occurs. 功能：显示字符 语法：echo [-neE][字符串] 说明：echo会将输入的字符串送往标准输出，输出的字符串间以空白字符隔开，并在最后加上换行符。 选项： ​ -E （默认）不支持\\解释功能，也就是\\的转义功能； ​ -e 启动\\解释功能，也就是启动转义； ​ -n 不自动换行，类似python的print(‘xxx’,end=’’) 显示变量的值 ​ echo “$VAR_NAME” # 显示变量的值 ​ echo ‘$VAR_NAME’ # 这里讲的其实不是echo的事情了，是引号的用法，单引号就是里面都是原封不动的字符串，不会给你查找变量或命令的。 单引号，最笨，里面是什么就是什么 双引号，普通，里面的变量能够解释出来，里面的命令不识别 原样输出； 反向单引号，最聪明，里面的变量、命令，统统给你识别并解释运行个结果出来。 echo只是结合echo来回显而已。 举个栗子 echo -e “\\a” # 可以发出声音，一般用在代码跑完后滴一声提示。 注意这个linux是在远端PC上，不是本地，但是声音却是本地声音 10、echo显示颜色 echo -e '\\033[43;31;5mICE\\e[0m' 前景颜色43、背景颜色31、5是闪烁‘ 注意43和31位置互换效果一样的，看的其实是4X就是背景色、3X就是字体色 注意\\e也就是\\033前后都有。颜色数字顺序不重要，然后上图是大家喜爱的红配绿。 1m的1是亮色 注意\\033等价于\\e linux里很多信息都是带颜色的，比如 我们自己也可以做出该效果 比如echo： 上图是1是加亮，下图5是闪烁 将来shell脚本，需要屏幕上显示一些东西带颜色，就这么玩。 11、一些编码转换和查询 之前一篇说过字符集和编码也就是unicode和UTF-8的事情，下面是工具网站 http://www.chi2ko.com/tool/CJK.htm https://javawind.net/tools/native2ascii.jsp?action=transform http://tool.oschina.net/encode 类似的网站 12、命令行扩展、被括起来的集合 很多时候会用到3种引号 ‘’ 单引号 “” 双引号 `` 反向单引号 等价于 $() 注意凡是在word或execl中的引号不能直接复制到linux或python里运行，不管你是否是英文的，复制过去就是不对，基本上需要重新键入引号。后面才知道word本身可以设置引号为英文的，这样就可以统一了。 花括号 echo file {1,3,5} rm -f file{1,3,5} echo {1..10} echo {a..z} echo {000..20..2} 花括号里面的就是选择或者递增的关系，花括号外面的_是必然有的。 批量创建文件、用户等。 双引号、单引号、反向单引号，针对不同的场景，作用不同， 针对echo的，针对shell编程的，不同应用地方的作用是不同的。 1、引号在echo处的作用 单引号：里面全是字符串，他大舅他二舅都是他舅。 反向单引号：能识别里面的命令和变量。 双引号：不能识别里面的命令，只能识别里面的变量。 2、一个命令调用另一个命令的结构的时候，经常使用反向单引号。 问题来了 data后面的要引起来，所改成双引号，但是问题如下 结合下图 结论date ‘+%F %T’得到的是两个值，所以touch的时候才会创建两个文件 都是空格惹的祸 所以最终的方法如下 结论：touch创建一个文件的时候不能带空格，有了就是两个文件了。上面的’+%F_T’可以不用引号了，因为本身就是一个整体了。 但文件名应该可以带空格的，虽然不太好，windows就是 3、反向单引号和$()是等价的 结论：反向单引号作为一个单元在其他引号内部出现，不影响效果。 所以上面的也可以这么写 4、每晚12点01分执行备份日志等操作，并保存为前一天的时间 5、tab补全 命令补全（命令的option也是可以补全的，按两下tab会出来一推） 路径补全 文件补全 虽然没神马用。 6、命令行历史 linux输入的每个命令默认都是有历史记录的，除非在键入命令的时候加入特别选项（也很方便做到）。 这些输入的命令记录会放在内存的缓存区里。内存里有一个历史列表，存放了输入的这些命令。 突然断电，或者直接关闭xshell，这些记录就有可能没了，可能就没写到.bash_history文件里。 一般系统会自动保存到文件里， history的ctrl r快速搜 上图的操作为：!然后按ctrl r，输入echo，就会从history里找到最近的一次包含echo的命令，我经常用来做snmpwalk -v -2c xxx 10.1.1.1 .1.3.6.1 x.x.x.x.x 这种历史的调用。 这是包含，下面的水以什么开头的最近的一次历史命令 下面是包含什么的最近的一次历史命令 10、把上一条命令的前面的换掉 11、上面的ctrl r修正一下 不需要输入history在按ctrl r直接ctrl r就行ctrl g是退出 12、非常实用的命令,把前一个命令的最后一个参数调出来 上面的!$调用比较方便，还有交互式下的快捷键可用来替代!$，比如 按esc松开不松开都可以再按. 或者按alt . 考虑到xshell的默认快捷键冲突，所以建议改一改，这么改就行 需要更加多样的调用，实际上!$是上一个命令的最后一个参数，!^是就是第一个参数，其他还有很多类似用法，但是我觉得没必要了，其他的不实用。 13、history命令选项用法继续 ①history会默认记录命令，现在考虑安全，可以清除历史 这个history -c是清楚的内存中的历史，而历史命令不仅仅是内存中有，还有磁盘文件也有。 但是这个文件放的是以前的历史命令。不是现在的几条。退出重进，发现echo passwordxxx确实不在，其实就是趁着内存中的命令还没自动放进./.bash_history里history -c直接就清掉了。 但是腾讯云上显然不是这样，应该是有了优化（内存中的命令会立刻存到.bash_history文件中的，如下图。） ①history会默认记录命令，现在考虑安全，可以清除历史 这个history -c是清楚的内存中的历史，而历史命令不仅仅是内存中有，还有磁盘文件也有。 而且腾讯云的VPS，内存里的命令清了，立即退出，重进，会发现命令还在的，说明内存和.bash_history磁盘文件是实时同步的。 为了确认一下，可以cat看一下 注意#1587828143这些是时间应该 上面写错了，不是腾讯云的优化，是这个原因： 我把历史命令前面的时间格式取消后，发现内存的命令不会自动同步进.bash_history了。 果然又再次秒同步了。 最后再验证一下 懵逼了， 终于知道什么因果关系了：将将将将~ 一般情况内存的命令不会实时同步进.bash_history文件里的。 想要实时同步，可以这么做，在history显示行首加上时间格式就能促使命令的实时存盘。 但是需要注意的事 即使注释了这行，还是会实时同步的，如下图， 但是如果你使用；去注释改行，那么 export HISTTIMEFORMAT=\"%F %T \" ①没有做时间格式的历史记录，但是命令实时同步进.bash_history ②做了时间格式，命令也是实时同步的 ③用#注释时间格式，命令还是实时同步的 ④用;号注释时间格式，命令就不再实时同步了。 什么鬼。。。睡觉 反正记着有办法让内存的命令实时同步就行了。方法之一就有上面的思路。 上面瞎折腾，靠谱的还是参考下面人家的 https://developer.aliyun.com/article/637427 14、history默认是1000条最近的记录 也可以在/etc/profile里写 退出重登，发现还是3000的历史记录，说明etc/bashrc优先 验证，去etc/bashrc下注释掉那行，退出重进，发现此时是10条记录了 history -d 36就是清第36条 怎么删除一个范围？ -a ，追加经磁盘文件 -r , 将历史文件的记录读到内存中的history记录下，默认用户登入的时候就会读取，执行该动作。 -w , 将当前的history内存记录存到指定文件中，比-a多了个路径，-a是默认的.bash_history 所以 -p 是不存在历史内存列表中，而且是将命令按空格展开成多行。 -s 是制造虚假的历史命令，实际未执行。 15、history命令历史的相关环境变量 HSITSIZE：命令历史的记录条数是内存中的记录条数 HISTFILE：指定历史文件，默认为~/.bash_history HISTFILESIZE：命令历史文件记录历史的条数，这个其实用HISTSIZE控制内存中的记录条数，就能控制文件的条数了，对了，默认HISTFILESIZE多大？也是1000条。 HISTTIMEFORMAT=”%F %T ”，显示时间，指定格式，也可以写到/etc/profile.d/env.sh下。PS1就是写在这个下面的 HISTIGNORE=”str1:str2*:...” 忽略str1命令，str2开头的历史命令。用法如下 安全敏感的不记录 HISTCONTROL：控制命令历史的记录方式，该环境变量的值如下： ​ ignoredups 默认值，忽略重复的命令，连续且相同的为“重复”.不过腾讯云主机是unset没有默认值的： 如图，不赘述 ​ ignorespace 忽略以空白开头的命令（类似HISIGNORE=”str1:str2*...”），如下图 ​ ignoreboth 相当于ignoredups,ignorespace的组合 ​ ​ erasedups 删除重复命令，不同于ignoredups(连续的相同命令只留一条随机的？)。erasedups是不连续的也删。 这些变量的赋值，上图是直接HISTCONTORL=XXX，这种不会保存在配置文件里，退出用户丢失。可以保存在/etc/profile或/etc/profile.d/env.sh或~/.bash_profile或/etc/bashrc，可以看看云主机的一些环境变量的保存路径，初步总结下来，只要大的路径对了，就行了，比如tab.vimrc不一定非要独立的文件的。感觉腾讯云这么做也无所谓规范不规范的。 16、快捷键 在xshell里没问题，但是在摸粑粑里有的不灵 ctrl + l 等价于clear清屏 ctrl + o 执行键入的命令，并重新显示出来，这玩意有延迟的，需要将命令打在屏幕上等一会，在按ctrl + o 才对，不让出来的命令是之前的 ctrl + s 锁屏，用来盲敲的 ctrl + q 解锁，恢复输入可见 crtl + c 强制退出 ctrl + d 规范退出，正常退出 ctrl + z 挂起命令 bg 恢复后就停不下来了，除非退出xshell。 fg 比较好，可以退出ctrl c ctrl z在挂起都行 17、一些有用的快捷键 ctrl+w，往前删除，一段一段的删，就是遇到空格就停下了 ctrl+k 和 ctrl+u相反，光标处删到行尾 alr+r 删除整行 和xshell冲突 ctrl + xx 光标在行首和当前位置切换 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"3-文件管理/3-文件管理.html":{"url":"3-文件管理/3-文件管理.html","title":"第三章 文件管理","keywords":"","body":"第三章 文件管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"3-文件管理/1-文件系统.html":{"url":"3-文件管理/1-文件系统.html","title":"第1节 文件系统","keywords":"","body":"第1节. 文件系统 1、文件系统结构元素 就是文件目录结构， /boot 启动相关文件，例如内核 /etc 配置文件，比如之前的/etc/issue /etc/profile.d/ /etc/bashrc等 /dev 硬件设备 b开头，表示块， c开头，字符文件， 都是设备文件 tty，字符设备，是一个个字符来进行输入输出的 块设备，光盘、硬盘，是以一块一块为单位，而一块代表N个字节，比如4096Byte表示一块。如果是块设备，每次输入输出就是4Kbyte。一下子读入4K或写入4K。 比如硬盘分区都是BLOCK为单位， 比如issue该文件里面存放了点数据，如图才346byte，但是由于磁盘分区上保存数据是以块为单位的，哪怕你只是修改该文件1个字节，实际上读取的也是以块为单位的，所以 假设4KB一个block，那么，读动作 就是一下子读取4kbyte到内存中，改完了写入磁盘分区也是也是 4kbyte的空间站位。 总之，实际的数据量虽然不大，可能把这个数据写入、写出的时候，实际会大很多，就是这个意思。 2、/bin 给普通用户用的 /sbin给系统管理员用的 还有/usr/tmp 都是软连接， ll的细节注意一下 ls /bin和ls /bin/效果一样，都是现实/bin下的所有文件 # 这里还是要用ll -d去看文件夹，反正有些py模块里，文件夹就要用XX/来标识。实际操作需要验证一下的。（带不带/，要不要-d，之类的） ll /bin 和 ll /bin/不一样，前者是显示文件夹本身，后缀是显示该文件夹下的文件。 # 这话也是错的，因为/bin是个软连接，本身是文件。 ll /bin -d 和 ll /bin/ -d 不一样，虽然都是显示文件夹metaDate。前者是显示软连接的元数据，后缀是显示源文件的medaDate。 这两个都是等价的 这两个显然不同。 所以总结，看文件夹的信息，就是ll /bin -d，这样最靠谱。 靠谱的原因就是-d看起来是看文件夹，其实对文件一样有效。所以看文件本身不进入子层，正确的使用方法就是ll /bin -d ll / -d这种了。 上面仔细看图就好，其实还是软连接造成的操作上的细微区别。 3、/usr 下很多文件夹和/很像， /usr 像是个二层根。 结构是有规范的无论是什么linxu版本，都基本符合这一套目录结构。 FHS：Filesystem Hierarchy Standard http://www.pathname.com/fhs 文件要存放规矩 4、两个特殊目录/proc 和 /sys 这两个目录大小为0 虽然是0，但是在/proc下是可以看到数据的。 说明proc下有很多数据的，但是proc文件夹的大小就是0。这是因为proc看到的是内存中的数据，内存数据不占磁盘空间，所以ll /proc -d显示的是磁盘空间占用大小。 /sys 是映射的硬件信息 /proc是放进程process相关信息的 5、以/sys为例，可以用里面的一些文件来管理硬件 这里换本地的VM来做实验—增加新的硬盘 当前只有一块硬盘和一个光盘 现在要实现加硬盘不重启的效果，以VmwareWorkstation为例，直接在VM里添加即可(略)。 但是添加完了，一般需要重启才行。不过可以这样： 在/sys 此时就新加的硬盘就出现了，也不需要重启系统。 一般就是host2或host0就行。 可以考虑将上面的两条命令定义成别名 现在再加一块硬盘，就可以利用别名快捷实现了 6、linux文件名最长为255个字节，验证如下 创建一个256字符（一个字符对应一个字节？），利用ALT + NUMBER 在输入字符即可 按住alt不动，紧接着输入256，然后松开后输入x，这样就完成256个x的输入。 一个x字符对应的就是一个字节，UTF-8格式规定的。不信可以这么检查 vim test，里面写一个x，然后hexdump test -C 看一下： 再man一下ascii找到小写的x 确实是16进制的78，而16进制78就是一个字节的空间。 所以，touch xxxx...xx 256个x就是256个字节的长度了，验证方法有效。 7、包含路径在内文件名最长4095个字节。 蓝色-目录 绿色-可执行文件 红色-压缩文件 其实就是看后缀，系统一看后缀是.gz的就给你打上红色了 天蓝色-链接文件 灰色-其他文件 黄色-设备文件，有的是b块设备、有的是c字符设备。 ​ b的单位是块，是随机读写，不是顺序的，是随机的放在磁盘的某些位置。而c的单位是字符，是顺序一个个字符进行输入输出的。块设备通常是有缓存的，硬盘有缓存，而字符设备是没有缓存的，就按照顺序进行访问就行了。 粉色-socke文件，套接字文件，s开头的，是为了实现网络通讯的。后面讲mysql会用到。 /run下面又很多粉红色文件 棕色-管道文件，p开头的，是实现进程间通信的，就是同一台PC上的不同APP互访，用的不多，用socket用的比较多。 文件的颜色和后缀的关系，实在/etc/DIR_COLORS下定义的 看下一个pip40,33确实是棕色 试一下上图DIR_COLORS的效果 01,31就是红色没跑了 然后在试一下exe文件，默认是注释了的， 现在打开 还是没有变，不急，执行一下DIR_COLORS文件，执行不了，退出重进就行了 搞不懂为什么.bashrc可以. ~/bashrc直接跑一遍，是修改的配置生效，无需退出重进。 而. /etc/DIR_COLORS却不能这样。 8、文件名规则 1、上面说了255个字节的文件名 2、说了4095个字节带路径的文件名 3、说了颜色 4、还有，除了斜杠和NULL，所有字符都可以用来作为文件名，但是使用特殊字符的目录名和文件名不推荐， 5、标准Linux文件系统（如ext4），文件名称大小写敏感，如果是linux挂载了fat的硬盘（ntfs，需要额外装软件，才能挂到linux下），则给硬盘下大小写不敏感。总之文件名称的大小写是跟着文件系统走的，而文件系统就是你格式化硬盘分区所选择的xfs、fat32、ntfs这些。 验证方法：linux关机，添加硬盘-使用现有的物理磁盘-选择磁盘1（假设你的fat分区在1下，这里看到的0和1就是物理硬盘的编号）-分区2（假设fat格式的是分区2），启动centos 这里的0就是硬盘0，1就是你电脑的第二块磁盘。我就一块0. 确定即可 但是我的实验不能加载物理硬盘 没什么意义，有时间可以换个机器试试，成功开机后，然后接着下面操作： lsblk -f 可见是这个硬盘是vfat格式 文件系统，需要挂载到一个目录才能使用 所以mount /dev/sdd2 /mnt df 可见sdd2已挂载 cd /mnt ls 可见各种windows下的格式， 此时该/mnt下的文件就不再区分大小写了。 这个实验就是说，标准linux文件系统(如ext4)，文件名称大小写敏感。 然而你可以挂载fat32硬盘上去，这个就不区分大小写了。 理论上可以，但直接创建是失败的。可以这么做： 删除一样， 或者 带上路径就行了 9、文件类型 - 普通文件 d 目录文件 b 块设备 c 字符设备 l 符号链接文件 p 管道文件pipe s 套接字文件socket 共7种类型，联系上文 除了-普通文件，其他的文件操作都要小心。特性不一样 p和s 主要是为了两个应用程序之间互相通信用的临时文件。比如两个软件交换数据，一个往pipe里写，另一个从对应的pipe里读。 10、CentOS 7的bin和usr/bin实际是同一个东西了 同样的，lib和/usr/lib， 这些在早期的centos6里不是这样的，都是独立。就是很相似，所以干脆合在一起了。 lib是放库文件的。 11、pwd -P 显示原文件路径 -L 显示的快捷方式就是软链接文件的路径 pwd默认带-L 同样需要注意的是，ll -d 看到的情况也要验证一下，是否是软链接的还是原文件的。 12、有些场合下，相对路径不是相对当前的cwd（current work direction）当前工作目录 比如前文提到的软连接，以及练习-2里也有提到， 就是相对于你要存放软连接的路径的 相对路径 13、basename和dirname， 创建和之前文件相同目录下的另一个文件，可以将下图中的/data/dir1/通过其他方式取出来，比如py里的os.gold、os.walk还是os.list都有方法的。或者你直接将/data/dir1作为变量。 14、cd备忘 cd ~ 等于 cd 进入当前用户的home目录 cd ~user1就回到了user1的家目录 cd – 上一次的路径，效果就是当前和上一次路径返回切换，原理就是OLDPWD这个变量里保存了上一次目录 之所以回得去，就是因为有一个变量保存了上一次的pwd信息。 15、ls备忘 ls -a 包含隐藏 ls -l 显示metadata ls -R 递归，应该有用，os.walk估计还没这个ls -R原生的优秀呢 ls -d 虽然是directory，但是ll -d通常用来看单个文件或文件夹都可以的 ls -1 文件分行显示？啥意思 ls -S 按从大到小排序，这个好 ls -t 按mtime排序 ls -u 配合-t选项，显示并按atime从新到旧排序 ls -U 按目录存放顺序显示 ls -X 按文件后缀排序 ls其实现在也是alias别名了，想用原始的ls只需要\\ls就行了 关于atime 所以正如练习-2里提到过的，atime并不是实时更新的 Access：最近访问时间acces time (atime)，这个不是实时更新的，为了防止大量的写accesstime这个操作，节省资源 但是当你当前读取的时间比上次atime超过1天了都，所以肯定给你立马更新了。最小差值多少，这个可以测一下。 由此可见，还正就是日期从25号变成了26号， 且时间跨度有一个值大概在12小时， 总结一下，hours在12小时，day + 1，基本就会cat后立刻更新时间了。当然可能12小时都嫑。我只是无聊，至于到底几个小时，who care。 clock -s 记得还原 16、ctime 文件的属性发生改变的时间 属性就是：一行里的各种数据，包括 文件的权限、inode数量（硬链接个数，注意软连接不算在meta data里）、所有者、所属组、大小。 这些通通都是元数据 上图cli写错了，直接stat /etc/motd就行了，不要time stat meta data发生改变，ctime就变了 注意atime 各种time不属于元数据。文件名是属于元数据的 17、selinux和防火墙 先不管，关闭即可 上图是selinux 防火墙也是启用的 关闭selinux 原来是enforceing，改成disabled 改完后，需要重启才能生效，不过可以结合cli方式临时关闭selinux就不用重启了。 不过可惜，disabled没有对应的值，0-permissive，1-enforcing。所以我还是老老实实重启吧。 systemctl disabled firewalld.service # 开启不启动 systemctl stop firewalld # 关闭防火墙 最后这样： Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"3-文件管理/2-文件管理.html":{"url":"3-文件管理/2-文件管理.html","title":"第2节 文件管理","keywords":"","body":"第2节. 文件管理 1、centos6以前的版本禁用selinux vi /etc/selinux/config SELINUX=disabled chkconfig iptables off service firewalld stop # 应该是这个吧，如果不是，上面一条重启不自启动就行了 2、文件通配符 ★文件通配符，顾名思义，就是匹配文件名称的，别想多了。 * 匹配零个或多个字符 ? 匹配任何单个字符 ~ 当前用户家目录 ~haiwang 用户haiwang家目录 ~+ 当前工作目录 ~- 前一个工作目录 # 等同于cd -，但是ls 不能ls -这么用，要ls ~-这样 [0-9] 匹配任意一个数字 [a-z] 任意小写字母一个 [A-Z] 任意一个大写字母 [haiwang] 匹配列表中的任何的一个字符 yingxiong 匹配列表中的所有字符以外的字符 y开头的 包含x的 .txt结尾的，但是不包括.txt，因为*不会包含隐藏文件。 *在文件名通配符领域里，表示该位置有一个或多个，或者没有，都可以 要在非隐藏文件里找需要的文件名，就可以用ls *来做 而要在隐藏文件里找，就用正则就行了 而查看隐藏的我现在想到的就可以用正则来做，后来又发现还有l.（列出当前所有的隐藏文件和文件夹）。 查看隐藏文件 l.只能看当前文件夹下的隐藏文件/文件夹，如果是看其他路径，就需要参考l.这个alias里的原来语法： ls -d /data/.* 只看文件夹 ll |grep \"^d\" ll -d */ ls ??? 表示就看3个字符的文件 汉字unicode，一个汉字也是一个字符。只是一个汉字这一个字符 在磁盘上保存不是占一个字节。 unicode 汉字，可能占2-4个字节。 在通配符里面没有^[xxx]这种写法，和正则regex相似又不一样 文件里面过滤字符串，这是不是通配符的活，通配符是匹配文件名称，文件内容交给regex 注意事项 ls /data/f[a-c].html表示啥 [a-c]代表aAbBc，这个regex又不同了 [A-C]等价于AbBcC 见下图 如果就是想要小写或大写，可以这么写 [:digit:] 任意数字，相当于0-9 [:lower:] 任意小写字母 [:upppere:] 任意大写字母 [:alpha:] 任意 大 小 写字母 [:alnum:] 任意数字或字母 [:blank:] 水平空白字符 [:space:] 水平或垂直空白字符，垂直空白字符是啥？回车？还是↓ [:punct:] 标点符号 [:print:] 可打印字符 [:cntrl:] 控制（非打印）字符 [:graph:] 图形字符 [:xdigit:] 十六进制字符 注意两个[[:lower:]] 方括号的意思，里面的[:lower:]是一个整体表示一个小写字符，外面的表示任意一个字母。 等价于正则里的[a-z]写法。 只看隐藏文件的方法，和上面的对比一下 l.的缺陷，只能看当前文件夹，下图就是，明明cli里写的ls -d /data/ .*但是看得还是当前目录的 上图有一个思路对了，手残敲错了，应该如下 只看文件夹的方法 一个是看非隐藏，一个是看隐藏的文件夹 通配符，在py的os.xx模块里，好像就不是regex而是通配符 按理说练习应该放在外面，但是这是课堂视频里的练习，不是作业，就不放在外面单独文章了 1、显示/var目录下所有以l开头，以一个小写字母结尾，且中间出现至少一位数字的文件或目录 ls /var l*[0-9]*[[:lower:]] 2、显示/etc目录下任意一位数字开头，且以非数字结尾的文件或目录 ll -d /etc/[0-9]*[^0-9] 这里可以考虑吧-d去掉，为的就是遍历一下/etc下文件夹下的文件或更深层次 3、显示/etc目录下以非字母开头，后面跟了一个字母及其他任意长度任意字符的文件或目录 ll -d /etc/[^[:alpha:]][[:alpha:]]* 注意些通配符的时候，有根弦--通配符不是正则。然后一个字母要有大小写[a-zA-Z] 4、显示/etc/目录下所有以rc开头，并后面是0-6之间的数字，其他为任意字符的文件或目录 ls -d /etc/rc[0-6]* 5、显示/etc目录下，所有以.d结尾的文件或目录 ls -d /etc/*.d # 注意.d文件也算是.d结尾的，这样就看不到了 ls -d /etc/*.d;ls -d /etc/.d 6、显示/etc目下，所有.conf结尾的，且以m.n,r,p开头的文件或目录 ls -d /etc/[mnrp]*.conf 7、只显示/root下的隐藏文件和目录 ls -d /root/.* 8、只显示/etc下的非隐藏目录 ls -d /etc/*/ 3、touch 1、创建文件 2、如果文件存在，只是修改时间（atime、ctime、mtime）都给你改当前时间了 补充： 默认ll的时间是mtime 3、总结，touch是安全的创建文件的方法 还有个创建文件的方法 > echo >> f5.txt 追加内容也是一个道理 以上的> 或 >> 是依赖于shell的， 上图换成csh就不行了 > 、>>不是命令，其实是重定向。 > 常用来快速给文件清空，无论文件有多大，都给你快速清空。据说灰常好的小功能。 touch [OPTION]... FILE... -a 仅改变atime和ctime -m 仅改变mtime和ctime -t [[CC]YY]MMDDhhmm[ss] 指定atime和mtime的时间戳 -c 如果文件不存在，则不予创建，这个一看就不错 4、保持日志为前一天 生成昨天日期作为文件名，上图是错误的写法，会自己坑自己 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"3-文件管理/3-cp和mv.html":{"url":"3-文件管理/3-cp和mv.html","title":"第3节 cp和mv","keywords":"","body":"第3节. cp和mv 1、cp命令 cp的三种语法 前两种用的多， cp [OPTION]... [-T] SOURCE DEST # 复制并改名 cp [OPTION]... SOURCE... DIRECTORY # 复制多个源文件到一个文件夹中 cp [OPTION]... -t DIRECTORY SOURCE # 同上，多了个-t 其实第三种用的也很多，因为通常会将rm，alisa成mv，此时就需要将目的文件夹放到前面，如下图： cp 复制文件的meta data会改变的 cp复制文件的时候，可能有一些信息会丢失（时间） cp复制文件的时候，可能有一些信息会丢失（时间，所属用户 用户组） 文件拷过来了，但是所属者、所属组，也包括时间都变了。 这就是信息丢失了，很可能导致文件复制过来不可用了。 cp赋值普通文件是可以的，除了上面的说的问题。但是cp不能复制特殊文件 之前讲过7种类型的文件 ll看最前面的符号就是类型了 zero也是一个常用的字符文件 cp复制文件的时候，cp复制问题-文件内容变了，拷的就不是你要的文件 下图注意复制的其实不是软连接，而是真是的文件被复制的。 cp复制文件的时候，cp了一个/dev/zero设备文件，如下图，什么都不一样了（文件类型、权限、大小）都变了 其实zero好像通常用来产生固定大小的文件的，一般你测试网速，就可以用zero产生一个大文件提供下载。 所以 cp主要针对普通文件的。要复制特殊文件，需要加一些选项，就算普通文件，如果需要保留原来的时间也需要cp加选项。 2、复制文件夹的注意事项 复制到文件夹，不改名字 复制多个文件，复制到文件夹下 如果是复制文件夹 需要递归选项 如果文件夹不存在，自动给你创建 如果文件夹存在，会覆盖？！不是，看清楚下图，①sysconfigbak文件存在，②所以会把sysconfig文件夹的内容全都复制到sysconfigbak文件夹下。③而再次cp -r的时候由于sysconfigbak下已经有sysconfig整个文件夹的内容了，所以会问你是否覆盖。 这就是传说中的：幂等性 多次重复执行一个命令，效果一样，这就叫做幂等性。 所以cp命令不具有幂等性的特点。 源 目标 不存在 存在且为文件 存在且为目录 一个文件 新建DEST，并将SRC中内容填充至DEST中 将SRC中的内容覆盖至DEST中 注意数据丢失风险！ 建议用-i 选项 在DEST下新建与原文件同名的文件，并将SRC中内容填充至新文件中 多个文件 提示错误 提示错误 在DEST下新建与原文件同名的文件，并将原文件内容复制进新文件中 目录 须使用-r选项 创建指定DEST同名目录，复制SRC目录中所有文件至DEST下 提示错误 在DEST下新建与原目录同名的目录，并将SRC中的内容复制到目录中 3、CP常用选项 -i 覆盖前提示，默认就有 需要注意 这是因为root账号有自己的alias别名定义，user1没有定义，别名的定义在家目录的.bashrc里写的。在root账号的家目录里有定义的别名，这里 -n 不覆盖，注意两者顺序 -r, -R 递归复制目录及内容 -a 归档，相当于-dR –perserv=all -d --no-dereference –preserv=links 不复制原文件，只复制链接名 --preserv[=ATTR_LIST] ​ mode:权限 ​ ownership:属主属组 ​ timestamp: ​ links ​ xattr ​ connext ​ all 4、cp的技巧 cp通常是需要加上-i ，cp – i 作为alias别名存在，但是存在下图情况，一个个问就很烦了。也不能取消别名的安全措施 所以可以利用\\前缀来还原成原始的命令，不用别名 不要想当然以为是cp -f，并不是这样的 -n 不覆盖 -d 不复制原文件，只复制链接名 默认是复制原始文件，而不是软链接本身。 --preserv[=ATTR_LIST] ​ mode:权限 ​ ownership:属主属组 ​ timestamp: ​ links ​ xattr ​ connext ​ all !*等价于上一次命令的后面所有参数，不仅仅是下图表示的两个，上图就出现了三个 如果我们希望保留时间属性，就可以 cp xx xx –preserv=timestamp 这会时间就保留住了 如果所有的都保留住 -p 等同于 –preserv=mode,ownership,timestamp #mode是权限、owership所有者所属组、timestamp就是时间了。 -a 前文有，能保留的属性都保留了，最全了。相当于-dR --preserv=all # 这个其实是help里这么写的，但是你不觉得很奇怪吗，r = R, -d只是--preserv=links，所以-a应该是-r 和 --preverv=all这样表示才对。很明显帮助里多了个-d。作为-dr --preserv=all，写法应该就是cli的是偶带了多个参数。所以自然也是and的关系。 -v --verbose # 复制的时候看到过程，如果文件很大，就需要这个直观显示，防止有人以为卡主不动了。 所以工作中推荐av经典组合 -f --force 演示过程中的错误注意事项： ​ root用户将某文件复制到user1用户的家目录下 正确写法是~user1，经常写错的原因是因为cd ~/切到自身的家目录这里是可以有/的。 现在user1家目录下的fstab的所属用和用户组是root的 于是如图所示，不能覆盖了，但是我自己的家目录，我还不能改吗？！-f就是强制措施 -f的思路，就是如果覆盖不了，实际上先删掉后 重新创建新的文件。当然如果删不了就肯定不行了！ 谁复制的，就变成谁的↑，但这话又不全对↓ -f 此时是删了再创建的，所以用户和用户组都是user1。 切到root用户下，cp /etc/fstab ~user1，覆盖掉，发现用户和组还是user1。 -u --update 只复制源比目标更新文件或目标不存在的文件 复制的时候存在一个覆盖的问题，一般都是更新的数据整个文件夹，cp -u 到服务器上的数据，这样就只做 增量更新。 -b 目标存在，覆盖前先备份，形式为filename~ --backup=numbered目标存在，覆盖前先备份加数字后缀 ★工作中，可以做个alias bak=’cp -a --backup=numbered’ 这样就小整合了一下。 所以-a 经常用来做备份的效果，①保留了所有能保留住的属性，②本身-a就集成了-d和递归的功能。 PS：之所以说保留了能保留的，原因见上图，至少有一个ctime是实时的。 简化写法示例 ll grub2.cfg{,.bak} # {}里面被,逗号分隔成两个部分，,号前面是空，后面是.bak，所以就是 ll grub2.cfg grub2.cfg.bak 这个了。用echo可以直观的看 据说这还是常用的备份方法，搞不懂，秀技术吗？ 练习： 1、每天将/etc/目录下的所有文件，备份到/data独立的子目录下，并要求子目录格式为backupYYYY-mm-dd，备份过程可见 cp -av /etc /data/bakcup`data +%F` 2、创建/data/rootdir目录，并复制/root下所有文件到该目录内，要求保留原有权限。 mkdir /data/rootdir;cp -a /root /data/rootdir cp -r --perserv=mode /root /data/rootdir cp -rp /root /data/rootdir cp -a /root /data/rootdir 5、mv 移动和改名 ★可以用mv替代rm，方法就是alias rm=mv ... mv [OPTION]... [-T] SOURCE DEST mv [OPTION]... SOURCE... DIRECTORY mv [OPTION]... -t DIRECTROY SOURCE 常用选项： -i 交互式 -f 强制 -b 目标存在，覆盖前先备份 6、rm删除 rm [OPTION]... FILE... 常用选项： ​ -i 交互式 ​ -f 强制删除 ​ -r 递归 带文件夹一般都带r ​ -no-preserve-root 删除/ 示例： ​ rm -rf /* 有的rm会被alias成rm -i，所以如果需要关闭提示，就用\\rm f1 f2 f3 当然也可以使用-f选项 ★rm -rf / data #这就完蛋了，你带了空格，就是把/下面全删了 rm -rf --no-preserve-root / 在windows里正在使用的文件是不能删除的，但是在linux里没有这个概念。 有些是删不了的，比如media光盘、proc、sys内存、/home /misc /net有些是特殊情况，确实不能删，其他都能。 /删掉后，pwd，cd都能用，原因就是这些都是内部命令，内部命令都已经加载到了内存里。 你删掉的是磁盘文件，内存里的东西都还在。 但是 内部命令依赖的/bin/bash文件已经没了，下次重启后，这些命令就没了。 外部命令那些本次开机后还没有使用过的就不行了，因为外部命令第一次使用后才会加载到内存中，那些本次开机后没有用过的，还都是磁盘文件呢。所以文件没了，就不能使用这些命令了。 rm -rf /* 这个命令误操作的可能性不太大，但是下一个命令就不行了 ★上图就把/data和 /*下的文件全删了。 工作中rm就别用了，别名成mv，其中涉及mv覆盖同名的文件的解决思路 思路就是，rm改成mv mv的时候考虑同名文件，就事前创建一个以当前时间（精确到秒单位,这样只要你的rm命令频率在1s以外，都没有问题）为文件夹名称。然后将要删除的文件移动到该文件夹里。 所以最终的mv替代rm的方法就是： 待填空 7、tree 显示目录树 ​ -d：只显示目录 ​ -L level：指定显示多少层 ​ -P pattern：只显示由指定pattern匹配到的路径，pattern涉及一些正则表达式 mkdir d1/d2/d3/d4 -pv 竟然不是-r,-v 就是建立的过程 -m MODE：创建目时，直接指定权限 rmdir a1/a2/a3/a4 这是删了a4，且a4是空文件夹，rmdir用的不多。 -p：递归删除父空目录 -v：显示详细信息 rm也不是都能删的，报错资源忙，忙的原因是因为/data是个设备挂载点 rm -rf /data确实会把里面的文件都删了，但是当删/data这个文件夹的时候（注意rm -rf /data是删了整个/data文件夹的）由于data是个分区挂载点，所以报错忙。 8、关于磁盘利用率的释放 cp /dev/zero /boot/bigfile # 时间越久，产生的文件越大 ll /boot/bigfile -h >该实验第1遍 现在删除bigfile，该磁盘利用率是否会立刻降下来呢？不一定。这个实验是立即降下来的。 rm -f /boot/bigfile >该实验第2遍 同样上面的实验，现在再rm删除bigfile该文件之前，先用另一个ssh登入打开它。然后在尝试删除观察磁盘利用率是否下降。 然后删除该文件 工作中，很多企业会遇到类似的场景，有些分区要满了，硬盘如果要满了，数据写不进去，就会造成很严重的结果，系统会崩溃，对外服务就挂了。 可能一些log日志文件，就删了不能立刻释放，存在这种情况。 现在关闭之前vim打开的bigfile的窗口 此时空间就释放了 下面 >该实验第3遍 恢复bigfile被占用的情形，就是rm -rf bigfile后磁盘空间不会得到释放的。 推荐的方法为：> fileName ，就是将文件清空 然后再删除该文件就可以了，整个过程完整截图如下 面试题：发现文件删了，空间没释放，正确应该怎么做，面试常见的答案就是上图。 rmdir 删除空目录 ​ -p：递归删除父空目录，就是从内层外外层删，当删除一个子目录后发现父目录也空了，就把父目录也删了。一直删到根。 # 和mkdir -p相反，mkdir -p是先创建父目录，再创建子目录，删除就自然反着删了。 9、rename：改文件名，mv如何改多个文件 rename --help 这样就改了，改文件名称，不仅仅只知道mv，还要知道rename。 再改回去（将.bak删掉） rename .bak \"\" * 练习 第一题的思路，存在一个组合，会想到是大括号的组合用法 ②第二题 ③第三题 这样也可以 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"3-文件管理/4-文件节点表.html":{"url":"3-文件管理/4-文件节点表.html","title":"第4节 文件节点表","keywords":"","body":"第4节. 文件节点表 文件的存放有inode表格和data表格 上图可见，创建一个文件夹后，inode就占掉一个。 touch命令本身不支持这么多个参数，所以换种方式 echo f{1..524288} |xargs touch 节点编号 上图可见分区的空间大小，以及节点数最大值。sda1的inodes最大值为65536。 超出节点编号就不行了 echo f{1..65539} |xargs touch 所以看到没有设备空间不一定是真的，可能是节点编号满了。 删除rm *是不行的，可以删除所在文件夹rm -rf /boot/testdir 或者这么删就行了啊： echo f{1..524288} |xargs rm -rf 节点编号和软连接和硬链接密切相关 ext文件系统的架构 直接指针是12个 ①数据量低于48K，直接指针就可以搞定。 ②超出48K--4M，使用间接指针表示。 ③4M-4GB的采用二级指针 现在centos7是XFT文件系统和ext的文件系统不一样。 不管什么系统都是类似的机制。 对于文件夹来讲他的内容放的是什么 文件名是属于文件夹的内容DATA。是放在数据块空间的。 明白这一点，rm f1本质上是删除他的节点表，指正指向的数据块就没人用了，该空间标记为空闲free状态，但是不会删除数据。如果你新建一个文件可能会覆盖掉的。 此外dir1/下的F1的数据就清了。 所以删除f1是需要有f1所在文件夹的权限就行了。 硬连接，本身就是同一个文件 跨分区了，不同分区肯定不是一个文件了，所以肯定不支持 硬链接不能针对文件夹创建。据说是防止循环现象。 删a1的操作等价上图的示例 备注： 👉以下是vim一个文件，然后echo 然后vim vim 发现indoe在两个数字跳来跳去的(估计和vim打开的时候会自动创建一个.xxx.swp有关，可能是这个原因，也不是swp文件的inode不在那两个反复替换的inode里面，反正vim该文件inode是变的，而且是2个inode数字来回变)。然后echo不会。 [10:54:28 root@localhost data]#echo inode_echo >> test [10:54:43 root@localhost data]#stat test File: test Size: 19 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 33577448 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:default_t:s0 Access: 2022-01-29 10:54:20.066021237 +0800 Modify: 2022-01-29 10:54:43.345023132 +0800 Change: 2022-01-29 10:54:43.345023132 +0800 Birth: 2022-01-29 10:54:20.066021237 +0800 [10:54:44 root@localhost data]#ll -i total 8 51325766 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir 373349 drwxr-xr-x. 2 user1 g12 6 Jan 19 19:58 dir2 33577410 -rw-rw-r--+ 1 root root 7 Jan 29 10:53 f1 33577446 -rw-r--r--. 1 user1 g12 0 Jan 19 19:58 f2 33577448 -rw-r--r--. 1 root root 19 Jan 29 10:54 test [10:54:47 root@localhost data]# [10:54:50 root@localhost data]#vim test [10:55:00 root@localhost data]# [10:55:00 root@localhost data]#vim test [10:55:03 root@localhost data]#ll -i total 8 51325766 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir 373349 drwxr-xr-x. 2 user1 g12 6 Jan 19 19:58 dir2 33577410 -rw-rw-r--+ 1 root root 7 Jan 29 10:53 f1 33577446 -rw-r--r--. 1 user1 g12 0 Jan 19 19:58 f2 33577450 -rw-r--r--. 1 root root 28 Jan 29 10:54 test [10:55:04 root@localhost data]#stat test File: test Size: 28 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 33577450 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:default_t:s0 Access: 2022-01-29 10:55:01.373024600 +0800 Modify: 2022-01-29 10:54:59.985024487 +0800 Change: 2022-01-29 10:54:59.987024487 +0800 Birth: 2022-01-29 10:54:59.985024487 +0800 [10:55:09 root@localhost data]#vim test [10:55:25 root@localhost data]# [10:55:25 root@localhost data]# [10:55:25 root@localhost data]#stat test File: test Size: 39 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 33577448 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:default_t:s0 Access: 2022-01-29 10:55:25.695026580 +0800 Modify: 2022-01-29 10:55:25.695026580 +0800 Change: 2022-01-29 10:55:25.697026580 +0800 Birth: 2022-01-29 10:55:25.695026580 +0800 [10:55:27 root@localhost data]# [10:55:35 root@localhost data]# [10:55:35 root@localhost data]#vim test [10:55:44 root@localhost data]# [10:55:44 root@localhost data]#stat test File: test Size: 50 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 33577450 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:default_t:s0 Access: 2022-01-29 10:55:44.687028126 +0800 Modify: 2022-01-29 10:55:44.687028126 +0800 Change: 2022-01-29 10:55:44.688028126 +0800 Birth: 2022-01-29 10:55:44.687028126 +0800 [10:55:45 root@localhost data]# [10:57:23 root@localhost data]#stat test File: test Size: 55 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 33577448 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:default_t:s0 Access: 2022-01-29 10:57:23.128036141 +0800 Modify: 2022-01-29 10:57:23.128036141 +0800 Change: 2022-01-29 10:57:23.129036141 +0800 Birth: 2022-01-29 10:57:23.128036141 +0800 [10:57:23 root@localhost data]#echo 12 >> test [10:57:29 root@localhost data]#stat test File: test Size: 58 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 33577448 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:default_t:s0 Access: 2022-01-29 10:57:23.128036141 +0800 Modify: 2022-01-29 10:57:29.437036654 +0800 Change: 2022-01-29 10:57:29.437036654 +0800 Birth: 2022-01-29 10:57:23.128036141 +0800 [10:57:30 root@localhost data]#echo 333 >> test [10:57:35 root@localhost data]#stat test File: test Size: 62 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 33577448 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:default_t:s0 Access: 2022-01-29 10:57:23.128036141 +0800 Modify: 2022-01-29 10:57:35.603037157 +0800 Change: 2022-01-29 10:57:35.603037157 +0800 Birth: 2022-01-29 10:57:23.128036141 +0800 [10:57:36 root@localhost data]# total 20 33577460 drwxrwxrwx. 4 root root 78 Jan 29 10:58 . 128 dr-xr-xr-x. 18 root root 236 Jan 10 18:13 .. 51325766 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir 373349 drwxr-xr-x. 2 user1 g12 6 Jan 19 19:58 dir2 33577410 -rw-rw-r--+ 1 root root 7 Jan 29 10:53 f1 33577446 -rw-r--r--. 1 user1 g12 0 Jan 19 19:58 f2 33577448 -rw-r--r--. 1 root root 62 Jan 29 10:57 test 33577447 -rw-r--r--. 1 root root 12288 Jan 29 10:58 .test.swp Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"3-文件管理/5-文件链接.html":{"url":"3-文件管理/5-文件链接.html","title":"第5节 文件链接","keywords":"","body":"第5节. 文件链接 硬链接，在同一个分区的不同目录下 不能针对文件夹 不能跨分区 data有3个inode ID的原因，是多一个子文件夹就多一个硬连接。 ll -i查看的连接数是硬连接数，与软连接无关。 硬连接如果使用相对路径是，相对的当前操作的路径。 软连接文件的大小是指向路径的长度决定的 软连接的原始文件要写相对于你要创建的连接文件的路径。 写相对路径，你直接复制/data复制重命名其他的，你的软连接一样使用OK。 软连接可以针对文件夹创建 删除软连接是个危险活~ 因为rm -rf d1.你一个tab键补全就是rm -rf d1.link/ ， 然后你删除的就是d1.link链接的那个源文件夹里的所有东西，而d1.link这个软链接本身并没有删除。 软连接、硬链接区别 1、本质上：硬链接-本质上是同一个文件多个名字；软连接-本质上是不同文件； 2、跨分区：硬链接不支持，软连接支持 3、目录创建：硬链接不支持；软连接支持 4、相互关系：硬链接是相互平等；软连接原始文件删除软连接失效 5、inode编号：硬链接是相同的；软连接不同 6、连接数：硬链接的创建删除会影响连接数；软连接删了这个文件就没了不存在连接数多个的问题。 7、路径问题：原始文件路径：硬链接创建是相对当前工作目录，软连接是相对于要创建的软连接的相对路径。 8、文件类型：软连接时l表示软连接；硬链接就是文件本身是啥类型就是啥。 9、颜色：软连接是蓝色、硬链接看原文件 10、命令实现不同：ln -s和ln linux的文件格式，不存在后缀一说，可以通过file xxx去判断该文件类型。 file -b # 只显示文件名本身 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"4-文本编辑工具vim/4-文本编辑工具vim.html":{"url":"4-文本编辑工具vim/4-文本编辑工具vim.html","title":"第四章 文本编辑工具vim","keywords":"","body":"第四章 文本编辑工具vim Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"4-文本编辑工具vim/1-vim.html":{"url":"4-文本编辑工具vim/1-vim.html","title":"第1节 vim","keywords":"","body":"第1节. vim 大篇幅编辑使用软件如vscode的plugin弄就行了。 会用到的备忘： :set nu 删除100dd,dGG,dgg ^$切换行尾行首w光标移动一个空格 vim +10 xxx # 打开就进入了第10行 vim 颜色也不是都加的，/etc/passwd，复制到/data下再vim就会发现颜色没了。/etc/下的属于系统配置文件，所以给你加颜色了。 vim -m 只读打开 /XXX搜索，nN u撤销 U 改行的修改全部撤销 s/要查找的内容/替换的内容/修饰符 这是正则，后面到了正则再说 整个文件的内容vim里替换用%s :%s/XXX/YYY/g /可以替换的 :%s#/dev#/tmp#g 0的ASCI码 cat也看不到 可以这么看二进制文件 00000000这是位置，后面00 00 00是内容 批量注释会有用 复制到vim里，空行格式错位，可以试试 :set paste 这个比较好用， 跳行同学的福音 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"5-标准I&O和管道/5-标准I&O和管道.html":{"url":"5-标准I&O和管道/5-标准I&O和管道.html","title":"第五章 标准I&O和管道","keywords":"","body":"第五章 标准I&O和管道 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"5-标准I&O和管道/1-标准输入输出和重定向.html":{"url":"5-标准I&O和管道/1-标准输入输出和重定向.html","title":"第1节 标准输入输出和重定向","keywords":"","body":"第1节. 标准输入输出和重定向 三种I/O设备 把I/O重定向到文件 使用管道 标准输入 每打开一个文件，系统都会分配一个数字编号对应该文件， 可见一个文件会有4个描述符与之对应，退出tail后，这里的对应关系就没了。 可以看到tail -f .bashrc，系统分配了一个3的文件描述符-软连接指向。 而0 1 2是输入输出信息对应的设备文件描述符，什么意思，就是你对.bashrc文件进行操作，会存在各种交互信息，正常的，错误的，等等从键盘输入的，打印到屏幕的。 关于输出重定向的小例子 > # 这是标准输出的重定向 以下命令特别的一个：C ls /data /xxx 2> all.log 1>&2 ls /data /xxx &> all.log ls /data /xxx 2>&1 all.log # 打印到屏幕上去了 ls /data /xxx > all.log 2>&1 以上是标准输出 标准输入 tr的一些用法 tr abcde 123 tr -t abcde 123 tr [:lower:] [:upper:] tr -d '135' tr -s 'ace' tr就可以和标准输入结合 所以转换的话，也可以用tr来做 tr可以转换、压缩、删除，也方便了。 上图是CTRL+D结束才会看到结果。是除了a、b、c以外的都删了。 单行重定向举例 此时多开一个窗口可见aaa已近写进去了 多行重定向 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"5-标准I&O和管道/2-管道实现重定向.html":{"url":"5-标准I&O和管道/2-管道实现重定向.html","title":"第2节 管道实现重定向","keywords":"","body":"第2节. 管道实现重定向 如何对错误信息进行管道符传递 上图：管道符|只能处理标准输出，而标准错误无法传递，不过可以下图做法： 上图有两种写法，最后的|&是相对2>&1晚一些时间出来的写法。 换种邮件正文的写法 bc的灵活用法 tee的意义 tee会覆盖 tee的追加效果 tee的意义 计算1+2+3+ ... +100 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"6-用户、用户组和权限/6-用户、用户组和权限.html":{"url":"6-用户、用户组和权限/6-用户、用户组和权限.html","title":"第六章 用户、用户组和权限","keywords":"","body":"第六章 用户、用户组和权限 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"6-用户、用户组和权限/1-用户和组的增删改查.html":{"url":"6-用户、用户组和权限/1-用户和组的增删改查.html","title":"第1节 用户和组的增删改查","keywords":"","body":"第1节. 用户和组的增删改查 AAA authentication、authorization、accouting|audition UID windows看用户和组 windows里user和group 不能同名，但是在linux里是正常情况。 早期密码是放在/etc/passwd里的，后买放到shaow里，可以回归早期的情况 pwunconv # 密码放到/etc/passwd里 pwconv # 密码放到/etc/shadow里 UID才是关键，将root的UID改成1000，它就不是管理员了。 如果没有一个user的UID=0，重启就起不来了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"6-用户、用户组和权限/2-用户和组的权限管理.html":{"url":"6-用户、用户组和权限/2-用户和组的权限管理.html","title":"第2节 用户和组的权限管理","keywords":"","body":"第2节. 用户和组的权限管理 /etc/shaow字段说明 useradd -r ming [09:19:15 root@localhost ~]#echo cisco | passwd --stdin ming Changing password for user ming. passwd: all authentication tokens updated successfully. ming:$6$AUsIFTgTuY/hWn8Y$0PgiuhWLxBGeGRtbd/Niz5R1EsMRvV3gdSRT45jnXMyZisgBzcCybETswhJzuuUQeIPkO/gMzo3rKyXvRAE6h.:19008:::::: ---上面是rocke-linux复制过来的---下面是centos----- ming:$6$nfkcZ5x7$Le5WQnLSUiJbw2tdXiilhvVZFGy69iuzKQq2XoA84jrHtrOp8fwQgMyunGIt1wQimPf37xdUL7B6rCOvpSaDE0:19008:0:99999:7::: [root@centos7 ~]# 这些字段的帮助信息可通过man 5 shadow获得 $6 ：表示sha512 $1 ：表示md5 👇注意第一个时间字段=0的特殊功效，就是首次登入修改密码 你改口令的时间距离1970-1-1号的时间 date of last password change The date of the last password change, expressed as the number of days since Jan 1, 1970. The value 0 has a special meaning, which is that the user should change her pasword the next time she will log in the system. An empty field means that password aging features are disabled. # 这句其实不对，该字段为空，后面设置了超时，实际还是超的 [09:25:26 root@localhost ~]#echo `date +%s`/86400 |bc 19008 这就是上图ming用户的19008的由来 minimum password age 用户必须等多久才能修改口令，0就是可以立即修改密码，以天为单位，centos8里默认是0，rocke-linux默认为空 maximum password age 用户不改密码，密码多久到期，以天为单位，centos8默认99999，rocke-linux/centos7默认为空 password warning period 上面的最大密码时间意味着过期，而过期前提前7天会提醒你修改密码，但是实际情况是提前6天，因为那个提前的第7天是也可继续用的，而且时间是当天8点0分0秒作为天数24小时去算的，可能是GTM+8估计。估计这2个小点就可以问住一拨人 [root@centos7 ~]# date -s \"+5 day\" # 这个必须root才能修改成功 Fri Jan 21 09:40:55 CST 2022 [root@centos7 ~]# date 然后再将本地时间延后10天，此时在用ming登入看看 [root@centos7 ~]# date -s \"+5 day\" Wed Jan 26 09:44:25 CST 2022 [root@centos7 ~]# date -s \"+5 day\" Mon Jan 31 09:45:03 CST 2022 [root@centos7 ~]# password inactivity period 围绕着密码过期时间--maximum password age，如果超过这个时间X天就禁用该账号，这个X就是password inactivity period的意思。 ming:$6$nfkcZ5x7$Le5WQnLSUiJbw2tdXiilhvVZFGy69iuzKQq2XoA84jrHtrOp8fwQgMyunGIt1wQimPf37xdUL7B6rCOvpSaDE0:19008:0:10:7:5:: account expiration date 账户有效期，前面都是针对密码过期的，这个参数是针对账号的 注意这里和前一张图的区别，已经不再说authentication的事了，直接说的事账号挂了。 /etc/group和/etc/gshadow dbus:x:81: polkitd:x:998: ssh_keys:x:997: sshd:x:74: postdrop:x:90: postfix:x:89: user1:x:1000: ming:x:1001: [root@centos7 ~]# cat /etc/group systemd-journal:!:: systemd-network:!:: dbus:!:: polkitd:!:: ssh_keys:!:: sshd:!:: postdrop:!:: postfix:!:: user1:!:: ming:!:: [root@centos7 ~]# cat /etc/gshadow 组设置口令是给普通用户加组的权限， 附加组显示在/etc/group里的行最后一个字段 ming:x:1001:user1 user1用户就加入进了ming这个组，ming就是user1的附加组 /etc/gshadow 存放组口令的文件 ming:!!::user1,user2,user3 !!组密码禁用的，不能通过组口令来往里加成员，只能是root管理了 ::里放的是管理员账号，每个组可以设置管理员,用来添加删除组成员，默认为空就只有root管理 user1,user2,user3就是和/etc/group一样，加入该组的成员就罗列在这里 随机口令的产生 [root@centos7 ~]# openssl rand -base64 9 rvgumQ+4U67t [root@centos7 ~]# openssl rand -base64 9 328culZ3wpV1 [root@centos7 ~]# yum -y install expect 查看man手册： FLAGS The -l flag defines the length of the password. The default is 9. The following example creates a 20 character password. mkpasswd -l 20 The -d flag defines the minimum number of digits that must be in the password. The default is 2. The following example creates a password with at least 3 digits. mkpasswd -d 3 The -c flag defines the minimum number of lowercase alphabetic characters that must be in the password. The default is 2. The -C flag defines the minimum number of uppercase alphabetic characters that must be in the password. The default is 2. EXAMPLE The following example creates a 15-character password that contains at least 3 digits and 5 uppercase characters. mkpasswd -l 15 -d 3 -C 5 [root@centos7 ~]# mkpasswd -l 15 -d 3 -C 5 \\Dpbel2VZa8Dv9W [root@centos7 ~]# mkpasswd -l 15 -d 3 -C 5 m0hsZaXZ*O1Dap9 [root@centos7 ~]# mkpasswd -l 15 -d 3 -C 5 zBfuS0evQP6x1H/ C:\\Users\\MingYi>net accounts 强制用户在时间到期之后多久必须注销?: 从不 密码最短使用期限(天): 0 密码最长使用期限(天): 42 密码长度最小值: 0 保持的密码历史记录长度: None 锁定阈值: 从不 锁定持续时间(分): 30 锁定观测窗口(分): 30 计算机角色: WORKSTATION 命令成功完成。 真要改这个时间，不推荐上文的直接修改/etc/shadow，而是用命令去改 [12:33:30 root@localhost ~]#chage ming Changing the aging information for ming Enter the new value, or press ENTER for the default Minimum Password Age [-1]: 2 Maximum Password Age [-1]: 33 Last Password Change (YYYY-MM-DD) [2022-01-16]: Password Expiration Warning [-1]: 7 Password Inactive [-1]: Account Expiration Date (YYYY-MM-DD) [-1]: 2023-01-16 [12:34:26 root@localhost ~]# ----改时间---- 这里rockety-linux还弄出个-1出来，呵呵，反正估计也是不限制的意思 [12:36:06 root@localhost ~]#getent shadow ming ming:$6$AUsIFTgTuY/hWn8Y$0PgiuhWLxBGeGRtbd/Niz5R1EsMRvV3gdSRT45jnXMyZisgBzcCybETswhJzuuUQeIPkO/gMzo3rKyXvRAE6h.:19008:2:33:7::19373: [12:36:13 root@localhost ~]# [12:36:26 root@localhost ~]#getent passwd ming ming:x:992:988::/home/ming:/bin/bash [12:36:34 root@localhost ~]# [12:36:36 root@localhost ~]#getent group ming ming:x:988: [12:36:48 root@localhost ~]#getent gshadow ming ming:!:: [12:36:53 root@localhost ~]# [12:36:54 root@localhost ~]#getent passwd ming root ming:x:992:988::/home/ming:/bin/bash root:x:0:0:root:/root:/bin/bash [12:37:02 root@localhost ~]# vipw和vigr 编辑passwd和group的推荐命令 pwck和grpck 检查passwd和group的命令 [12:38:19 root@localhost ~]#pwck [user 'cockpit-ws': directory '/nonexisting' does not exist user 'cockpit-wsinstance': directory '/nonexisting' does not exist user 'ming': directory '/home/ming' does not exist pwck: no changes [12:39:02 root@localhost ~]#grpck [12:39:11 root@localhost ~]#ll /home/ total 0 groupadd 创建组 创建组 [13:39:41 root@localhost ~]#groupadd admins [13:39:50 root@localhost ~]#getent group admins admins:x:1000: [13:40:00 root@localhost ~]# 创建系统组 [13:40:46 root@localhost ~]#groupadd -r mysql [13:40:50 root@localhost ~]# [13:40:52 root@localhost ~]#getent group mysql mysql:x:987: [13:40:55 root@localhost ~]# 修改组名 [13:42:45 root@localhost ~]#getent group admins admins:x:1000: [13:42:50 root@localhost ~]#groupmod -n mgmt admins [13:42:56 root@localhost ~]#getent group mgmt mgmt:x:1000: 删除组 [13:44:06 root@localhost ~]#getent group mgmt mgmt:x:1000: [13:44:08 root@localhost ~]#getent group mysql mysql:x:987: [13:44:10 root@localhost ~]#groupdel mgmt [13:44:18 root@localhost ~]#groupdel mysql [13:44:20 root@localhost ~]#getent group mysql [13:44:23 root@localhost ~]#getent group mgmt 删不掉组的原因 [13:49:33 root@localhost ~]#groupdel ming groupdel: cannot remove the primary group of user 'ming' 是因为有用户将ming作为主组，这个用户就是ming自己。是useradd创建ming的时候自动生成的主组。 [13:49:43 root@localhost ~]#useradd ming2 [13:51:05 root@localhost ~]#getent group ming2 ming2:x:1000: [13:51:23 root@localhost ~]#groupdel ming2 groupdel: cannot remove the primary group of user 'ming2' [13:51:32 root@localhost ~]#userdel ming2 [13:51:39 root@localhost ~]#getent group ming2 [13:51:45 root@localhost ~]#ll /home/ total 0 drwx------. 2 1000 1000 62 Jan 16 13:51 ming2 userdel 删除用户连带组，但不会连带家目录，所以关于创建用户和删除用户的时候要注意家目录是否连带生成和删除 man useradd -r, --system Create a system account. System users will be created with no aging information in /etc/shadow, and their numeric identifiers are chosen in the SYS_UID_MIN-SYS_UID_MAX range, defined in /etc/login.defs, instead of UID_MIN-UID_MAX (and their GID counterparts for the creation of groups). Note that useradd will not create a home directory for such a user, regardless of the default setting in /etc/login.defs (CREATE_HOME). You have to specify the -m options if you want a home directory for a system account to be created. man userdel -f, --force This option forces the removal of the user account, even if the user is still logged in. It also forces userdel to remove the user's home directory and mail spool, even if another user uses the same home directory or if the mail spool is not owned by the specified user. If USERGROUPS_ENAB is defined to yes in /etc/login.defs and if a group exists with the same name as the deleted user, then this group will be removed, even if it is still the primary group of another user. Note: This option is dangerous and may leave your system in an inconsistent state. -h, --help Display help message and exit. -r, --remove Files in the user's home directory will be removed along with the home directory itself and the user's mail spool. Files located in other file systems will have to be searched for and deleted manually. The mail spool is defined by the MAIL_DIR variable in the login.defs file. [14:01:12 root@localhost ~]#useradd ming2 useradd: warning: the home directory already exists. Not copying any file from skel directory into it. Creating mailbox file: File exists [14:01:18 root@localhost ~]# [14:01:18 root@localhost ~]# [14:01:18 root@localhost ~]#getent passwd ming2 ming2:x:1000:1000::/home/ming2:/bin/bash [14:01:22 root@localhost ~]# [14:01:23 root@localhost ~]#ll /home/ total 0 drwx------. 2 ming2 ming2 62 Jan 16 13:51 ming2 [14:01:25 root@localhost ~]# [14:01:26 root@localhost ~]#userdel -r ming2 [14:01:30 root@localhost ~]#ll /home/ total 0 [14:01:32 root@localhost ~]#getent passwd ming2 [14:01:39 root@localhost ~]#getent group ming2 [14:01:43 root@localhost ~]# 用户创建管理 [14:03:53 root@localhost ~]#rpm -q --scripts postfix preinstall scriptlet (using /bin/sh): # Add user and groups if necessary /usr/sbin/groupadd -g 90 -r postdrop 2>/dev/null /usr/sbin/groupadd -g 89 -r postfix 2>/dev/null /usr/sbin/groupadd -g 12 -r mail 2>/dev/null /usr/sbin/useradd -d /var/spool/postfix -s /sbin/nologin -g postfix -G mail -M -r -u 89 postfix 2>/dev/null ------------------------- -g 90 gid -r 指定为系统组 useradd的选项学习 /usr/sbin/useradd -d /var/spool/postfix -s /sbin/nologin -g postfix -G mail -M -r -u 89 postfix 2>/dev/null -u 89 ： 指定用户UID为89 -s : 指定shell类型 -o ： 忽略uid唯一性的检查 [14:12:58 root@localhost ~]#getent passwd root root:x:0:0:root:/root:/bin/bash [14:13:09 root@localhost ~]#useradd -u 0 ming3 useradd: UID 0 is not unique [14:13:20 root@localhost ~]#getent passwd ming3 [14:13:36 root@localhost ~]#useradd -u 0 -o ming3 [14:13:42 root@localhost ~]#getent passwd ming3 ming3:x:0:1000::/home/ming3:/bin/bash [14:13:43 root@localhost ~]#getent passwd root root:x:0:0:root:/root:/bin/bash [14:14:47 root@localhost ~]#id root uid=0(root) gid=0(root) groups=0(root) [14:14:49 root@localhost ~]#id ming3 uid=0(root) gid=0(root) groups=0(root) [14:14:51 root@localhost ~]# 创建系统服务， [14:18:18 root@localhost ~]#useradd -r -s /sbin/nologin mysql [14:18:26 root@localhost ~]#getent passwd mysql mysql:x:991:987::/home/mysql:/sbin/nologin 默认行为是useradd不指定主组，就会自动创建和用户名同名的主组 [14:21:35 root@localhost ~]#useradd alice [14:21:48 root@localhost ~]#id alice uid=1000(alice) gid=1000(alice) groups=1000(alice) 通过-g指定主组，就不会自动创建主组了，指定的组要事先存在 [14:22:33 root@localhost ~]#useradd tom -g alice [14:22:58 root@localhost ~]#id tom uid=1001(tom) gid=1000(alice) groups=1000(alice) -G 是附加组 [14:24:10 root@localhost ~]#groupadd g1 [14:24:13 root@localhost ~]#groupadd g2 [14:24:21 root@localhost ~]#groupadd g3 [14:24:23 root@localhost ~]# [14:24:24 root@localhost ~]#useradd jack -g alice -G g1,g2 [14:24:44 root@localhost ~]#id jack uid=1002(jack) gid=1000(alice) groups=1000(alice),1001(g1),1002(g2) [14:26:44 root@localhost ~]#getent group | grep jack g1:x:1001:jack g2:x:1002:jack [14:26:49 root@localhost ~]#getent gshadow | grep jack g1:!::jack g2:!::jack [14:26:55 root@localhost ~]# -N 不创建同名的主组，也不指定，就默认创建一个users [14:28:06 root@localhost ~]#useradd rose -N [14:28:15 root@localhost ~]#id rose uid=1003(rose) gid=100(users) groups=100(users) [14:28:18 root@localhost ~]# 看下windows的创建用户的默认行为，默认就是和linux的useradd -N一样的，都是将新建用户放到users组下面。 C:\\WINDOWS\\system32>net user test001 passwd001 /add 命令成功完成。 C:\\WINDOWS\\system32>net user \\\\DESKTOP-5T7A4A1 的用户帐户 ------------------------------------------------------------------------------- ___VMware_Conv_SA___ admin Administrator ciscoacvpnuser DefaultAccount Guest MingYi named test001 WDAGUtilityAccount 命令成功完成。 C:\\WINDOWS\\system32>net localgroup users 别名 users 注释 防止用户进行有意或无意的系统范围的更改，但是可以运行大部分应用程序 成员 ------------------------------------------------------------------------------- NT AUTHORITY\\Authenticated Users NT AUTHORITY\\INTERACTIVE test001 命令成功完成。 C:\\WINDOWS\\system32>net user \\\\DESKTOP-5T7A4A1 的用户帐户 ------------------------------------------------------------------------------- ___VMware_Conv_SA___ admin Administrator ciscoacvpnuser DefaultAccount Guest MingYi named test001 WDAGUtilityAccount 命令成功完成。 C:\\WINDOWS\\system32>net user test001 /del 命令成功完成。 C:\\WINDOWS\\system32>net user \\\\DESKTOP-5T7A4A1 的用户帐户 ------------------------------------------------------------------------------- ___VMware_Conv_SA___ admin Administrator ciscoacvpnuser DefaultAccount Guest MingYi named WDAGUtilityAccount 命令成功完成。 关于家目录 不带家目录的方式，useradd -r -s /sbin/nologin mysql [14:35:33 root@localhost ~]#getent passwd | tail -5 mysql:x:991:987::/home/mysql:/sbin/nologin alice:x:1000:1000::/home/alice:/bin/bash tom:x:1001:1000::/home/tom:/bin/bash jack:x:1002:1000::/home/jack:/bin/bash rose:x:1003:100::/home/rose:/bin/bash [14:35:38 root@localhost ~]#ll /home/ total 0 drwx------. 2 alice alice 62 Jan 16 14:21 alice drwx------. 2 jack alice 62 Jan 16 14:24 jack drwx------. 2 rose users 62 Jan 16 14:28 rose drwx------. 2 tom alice 62 Jan 16 14:22 tom [14:35:43 root@localhost ~]# 指定创建家目录 [14:43:13 root@localhost ~]#useradd -d /data/jerryhome jerry [14:43:18 root@localhost ~]#ll /data/jerryhome/ -d drwx------. 2 jerry jerry 62 Jan 16 14:43 /data/jerryhome/ [14:43:29 root@localhost ~]#id jerry uid=1004(jerry) gid=1004(jerry) groups=1004(jerry) [14:43:32 root@localhost ~]# 有个奇怪的行为，就是创建用户的时候指定家目录，但是并不创建 [14:46:22 root@localhost ~]#useradd -d /data/xiaohong -M xiaohong [14:46:36 root@localhost ~]#id xiaohong uid=1005(xiaohong) gid=1005(xiaohong) groups=1005(xiaohong) [14:46:38 root@localhost ~]#ll /data/xiao* ls: cannot access '/data/xiao*': No such file or directory 还有与之相反的思路，useradd -r是系统用户不会创建家目录，-m就是会创建了 [14:47:50 root@localhost ~]#useradd -r zhangsan [14:48:09 root@localhost ~]#id zhangsan uid=990(zhangsan) gid=986(zhangsan) groups=986(zhangsan) [14:48:10 root@localhost ~]#ll /home/ total 0 drwx------. 2 alice alice 62 Jan 16 14:21 alice drwx------. 2 jack alice 62 Jan 16 14:24 jack drwx------. 2 rose users 62 Jan 16 14:28 rose drwx------. 2 tom alice 62 Jan 16 14:22 tom [14:48:14 root@localhost ~]# [14:48:14 root@localhost ~]#useradd -r lisi -m [14:48:33 root@localhost ~]#ll /home/lisi/ -d drwx------. 2 lisi lisi 62 Jan 16 14:48 /home/lisi/ [14:48:38 root@localhost ~]# [14:49:16 root@localhost ~]#useradd -r -m -d /data/ada ada [14:49:20 root@localhost ~]#ll /data/ada -d drwx------. 2 ada ada 62 Jan 16 14:49 /data/ada [14:49:24 root@localhost ~]#id ada uid=988(ada) gid=988(ada) groups=988(ada) [14:49:26 root@localhost ~]# -c : 描述信息，有点用的，讲究人士的专用 [14:51:48 root@localhost ~]#useradd -c \"sbZhuanYong\" sb001 [14:52:21 root@localhost ~]#getent passwd sb001 sb001:x:1006:1006:sbZhuanYong:/home/sb001:/bin/bash [14:52:26 root@localhost ~]# 如果是centos可以yum -y install finger然后查看用户描述信息，rokey-linux好像yum不了finger，yum源rocky的里面貌似没有finger 的rpm包。 [root@centos7 ~]# useradd -c 'dalaozhuanyong' dalao001 [root@centos7 ~]# getent passwd dalao001 dalao001:x:1002:1002:dalaozhuanyong:/home/dalao001:/bin/bash [root@centos7 ~]# [root@centos7 ~]# finger dalao001 Login: dalao001 Name: dalaozhuanyong Directory: /home/dalao001 Shell: /bin/bash Never logged in. No mail. No Plan. [root@centos7 ~]# -----改描述------desc--------- [root@centos7 ~]# chfn dalao001 Changing finger information for dalao001. Name [dalaozhuanyong]: Office []: !wgame Office Phone []: 110 Home Phone []: 110 Finger information changed. [root@centos7 ~]# finger dalao001 Login: dalao001 Name: dalaozhuanyong Directory: /home/dalao001 Shell: /bin/bash Office: !wgame, 110 Home Phone: 110 Never logged in. No mail. No Plan. [root@centos7 ~]# getent passwd dalao001 dalao001:x:1002:1002:dalaozhuanyong,!wgame,110,110:/home/dalao001:/bin/bash [root@centos7 ~]# 所以人家postfix的安装后或者前，跑的脚本里的useradd就能理解了 [root@centos7 ~]# rpm -q --scripts postfix preinstall scriptlet (using /bin/sh): # Add user and groups if necessary /usr/sbin/groupadd -g 90 -r postdrop 2>/dev/null /usr/sbin/groupadd -g 89 -r postfix 2>/dev/null /usr/sbin/groupadd -g 12 -r mail 2>/dev/null /usr/sbin/useradd -d /var/spool/postfix -s /sbin/nologin -g postfix -G mail -M -r -u 89 postfix 2>/dev/null [15:08:55 root@localhost yum.repos.d]#yum -y remove postfix [15:10:09 root@localhost yum.repos.d]#groupadd -r postfix [15:11:58 root@localhost yum.repos.d]#groupadd postfix-1 [15:12:02 root@localhost yum.repos.d]# [15:12:03 root@localhost yum.repos.d]#getent group postfix postfix:x:984: [15:12:09 root@localhost yum.repos.d]#getent group postfix-1 postfix-1:x:1007: [15:12:12 root@localhost yum.repos.d]# [15:12:16 root@localhost yum.repos.d]#id postfix id: ‘postfix’: no such user [15:12:19 root@localhost yum.repos.d]#/usr/sbin/useradd -d /var/spool/postfix -s /sbin/nologin -g postfix -G mail -M -r -u 89 postfix [15:12:23 root@localhost yum.repos.d]#id postfix uid=89(postfix) gid=984(postfix) groups=984(postfix),12(mail) [15:12:28 root@localhost yum.repos.d]#ll /home/pos* ls: cannot access '/home/pos*': No such file or directory [15:12:33 root@localhost yum.repos.d]#ll /var/spool/pos* ls: cannot access '/var/spool/pos*': No such file or directory [15:12:43 root@localhost yum.repos.d]# 其实-M没有意义，就是保险，-r本身就不会创建家目录。 注意下，不管是不是需要userdel -r 加不加r都要去确认下家目录是否真的删除，因为我操作时候发现有时候不加-r，好像也是把家目录删了。这个是在rockey-linux上操作的。 默认useradd的行为有文件定义的 [15:25:22 root@localhost ~]#cat /etc/default/useradd # useradd defaults file GROUP=100 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes [15:25:27 root@localhost ~]#getent group users users:x:100: [15:26:08 root@localhost ~]# GROUP = 100 默认useradd -N 不创建主组的时候，自动加到users主组里，这个行为就是GROUP=100设置的。 INACTIVE=-1 默认用密码过期后 是否给宽限期，默认是永远宽限。 HOME=/home 默认家目录在哪 CREATE_MAIL_SPOOL=yes 默认用户创建的时候创建它的邮箱， [15:26:08 root@localhost ~]#ll /var/spool/mail/ total 0 -rw-rw----. 1 alice mail 0 Jan 16 14:21 alice -rw-rw----. 1 jack mail 0 Jan 16 14:24 jack -rw-rw----. 1 jerry mail 0 Jan 16 14:43 jerry -rw-rw----. 1 rose mail 0 Jan 16 14:28 rose -rw-rw----. 1 sb001 mail 0 Jan 16 14:52 sb001 -rw-rw----. 1 tom mail 0 Jan 16 14:22 tom -rw-rw----. 1 xiaohong mail 0 Jan 16 14:46 xiaohong SKEL=/etc/skel 默认创建家目录里的隐藏文件的由来 [15:33:50 root@localhost ~]#ls -a /etc/skel/ . .. .bash_logout .bash_profile .bashrc [15:33:53 root@localhost ~]# 所以如果需要创建用户，生成的家目录里自动带上什么文件，就有办法了 [15:36:42 root@localhost ~]#ls -a /etc/skel . .. .bash_logout .bash_profile .bashrc [15:36:51 root@localhost ~]# [15:37:01 root@localhost ~]#ls -a /home/ alice/ jack/ lisi/ rose/ sb001/ tom/ [15:37:01 root@localhost ~]#ls -a /home/alice/ . .. .bash_logout .bash_profile .bashrc [15:37:13 root@localhost ~]#touch /etc/skel/.vimrc [15:37:22 root@localhost ~]#ls -a /etc/skel/ . .. .bash_logout .bash_profile .bashrc .vimrc [15:37:32 root@localhost ~]#useradd test-1 [15:37:43 root@localhost ~]#ls -a /home/test-1/ . .. .bash_logout .bash_profile .bashrc .vimrc [15:37:49 root@localhost ~]# 还有个默认行为文件 [15:43:09 root@localhost ~]#cat /etc/login.defs | grep -Ev \"^#|^$\" MAIL_DIR /var/spool/mail UMASK 022 HOME_MODE 0700 PASS_MAX_DAYS 99999 # 口令最大有效期 PASS_MIN_DAYS 0 # 口令修改无需等待直接改 PASS_MIN_LEN 5 # 口令最短5个 PASS_WARN_AGE 7 UID_MIN 1000 # 默认普通用户UID从1000开始，就是这里设置的 UID_MAX 60000 # 这里的1000和60000都是自动的范围，手动除外 SYS_UID_MIN 201 # 系统UID自动范围 SYS_UID_MAX 999 # 系统UID自动范围 GID_MIN 1000 GID_MAX 60000 SYS_GID_MIN 201 SYS_GID_MAX 999 CREATE_HOME yes USERGROUPS_ENAB yes ENCRYPT_METHOD SHA512 # 默认的哈希算法，/etc/passwd里的$6 [15:43:16 root@localhost ~]# root 不受上述配置的限制 所以默认新建用户的相关文件如下 [15:51:36 root@localhost ~]#ll /etc/default/useradd -d -rw-r--r--. 1 root root 119 Aug 19 03:04 /etc/default/useradd [15:51:40 root@localhost ~]#ll /etc/skel -d drwxr-xr-x. 2 root root 76 Jan 16 15:37 /etc/skel [15:51:44 root@localhost ~]#ll /etc/login.defs -d -rw-r--r--. 1 root root 2512 Aug 19 03:04 /etc/login.defs /etc/default/useradd也可以用useradd -D查看 [15:53:09 root@localhost ~]#useradd -D GROUP=100 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes [15:54:10 root@localhost ~]#cat /etc/default/useradd # useradd defaults file GROUP=100 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes 1、直接修改文件 2、通过useradd修改 useradd -D -s SHELL类型 useradd -D -b BASE_DIR/home useradd -D -g GROUP默认useradd -N所带的组 [15:54:10 root@localhost ~]#cat /etc/default/useradd # useradd defaults file GROUP=100 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes [15:54:22 root@localhost ~]#useradd -D -g 1000 [16:01:36 root@localhost ~]#cat /etc/default/useradd # useradd defaults file GROUP=1000 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes [16:01:37 root@localhost ~]# 批量操作 newusers # passwd格式文件 批量创建用户 chpasswd # 批量修改用户口令 将/etc/passwd文件里的格式复制出来，放入单个文件，然后到别的机器上newusers xxx即可创建 user1:x:1008:1009::/home/user1:/bin/bash user2:x:1009:1010::/home/user2:/bin/bash user3:x:1010:1011::/home/user3:/bin/bash user4:x:1011:1012::/home/user4:/sbin/nologin [13:58:02 root@localhost ~]#getent passwd -----看下👇创建的具体过程---------- [root@centos7 ~]# cat addusers user1:x:1008:1009::/home/user1:/bin/bash user2:x:1009:1010::/home/user2:/bin/bash user3:x:1010:1011::/home/user3:/bin/bash user4:x:1011:1012::/home/user4:/sbin/nologin [root@centos7 ~]# newusers addusers newusers: line 5: invalid line newusers: line 6: invalid line newusers: error detected, changes ignored [root@centos7 ~]# cat addusers -n 1 user1:x:1008:1009::/home/user1:/bin/bash 2 user2:x:1009:1010::/home/user2:/bin/bash 3 user3:x:1010:1011::/home/user3:/bin/bash 4 user4:x:1011:1012::/home/user4:/sbin/nologin 5 6 [root@centos7 ~]# ---------👆可惜报错鸟------------ 删掉多余的空行后，尝试👇----------- [root@centos7 ~]# cat addusers user1:x:1008:1009::/home/user1:/bin/bash user2:x:1009:1010::/home/user2:/bin/bash user3:x:1010:1011::/home/user3:/bin/bash user4:x:1011:1012::/home/user4:/sbin/nologin [root@centos7 ~]# [root@centos7 ~]# newusers addusers [root@centos7 ~]# getent passwd |grep user* user1:x:1000:1000::/home/user1:/bin/bash user2:x:1009:1010::/home/user2:/bin/bash user3:x:1010:1011::/home/user3:/bin/bash user4:x:1011:1012::/home/user4:/sbin/nologin [root@centos7 ~]# -------------然后再批量改口令方法1👇-------------- [root@centos7 ~]# chpasswd user1:cisco # ctrl d安全退出才能生效 [root@centos7 ~]# getent shadow user1 user1:$6$dR6ZP/lQ5aA$V6xJwgibZELgZh2NKOVDSez8CTzw6h05NX.n/Ft4ZpGtlVNfNEpkpTFRdZqkSQWKKgeZ/BxKgfSm2BRcFaMgo0:19029:0:99999:7::: [root@centos7 ~]# chpasswd user1:huawei ^C # 不能ctrl c强制退出 [root@centos7 ~]# getent shadow user1 user1:$6$dR6ZP/lQ5aA$V6xJwgibZELgZh2NKOVDSez8CTzw6h05NX.n/Ft4ZpGtlVNfNEpkpTFRdZqkSQWKKgeZ/BxKgfSm2BRcFaMgo0:19029:0:99999:7::: [root@centos7 ~]# ------👆注意没截图就是方便后面搜索，但是要小心失真丢东西，这里是ctrl C强制退出，所以没改成功，很多这种交互式的配置都需要ctrl +d 退出。--------- ----非交互式配置方式👇------ [root@centos7 ~]# echo user1:lianxiang |chpasswd [root@centos7 ~]# getent shadow user1 user1:$6$A55IfCFmc$aJPxuWvGRvpTzNocXonzz/gEZTEjV7y3qcHSWEPvxZg1IfA0EUrXMMBpOsw9DXodx4KQ1yCa8SZCTiQtvDYu50:19029:0:99999:7::: [root@centos7 ~]# ---------批量改的方法👇---改口令1---------- [root@centos7 ~]# vi p.set [root@centos7 ~]# cat p.set user1:centos user2:cisco user3:huawei [root@centos7 ~]# cat p.set |chpasswd [root@centos7 ~]# getent shadow user1 user1:$6$PFoqG/41wd3x$PDCFFjFD84xNc2t4je5119lP.ifsTyspYRGnbP4Bx0QpP/9XRd4s9vUFICbEdoDv3pOd7y/7PBLuBsE6EXhwu/:19029:0:99999:7::: [root@centos7 ~]# getent shadow user2 user2:$6$0ilWD/oW7CN$IdC6Gz0.eJKdPBWuGx4KJR00GBrjoxE8KWtCp9lurmP1TaCQGcUra5.VscBTQZ5Um0lKYZO.qb6/fNyYiey0s1:19029:0:99999:7::: [root@centos7 ~]# getent shadow user3 user3:$6$Uv41MY/y9$Q1251b9f9CPX5/sQ1aDhITVsl9pbKEXJspkV4uib/ugaCAlMfg9/Xy4WJBdyq56SJF4k5YuIc0muouxLpi61T0:19029:0:99999:7::: [root@centos7 ~]# 查看id [root@centos7 ~]# id user1 uid=1000(user1) gid=1000(user1) groups=1000(user1) [root@centos7 ~]# id -u user1 1000 [root@centos7 ~]# id -g user1 1000 [root@centos7 ~]# id -G user1 1000 [root@centos7 ~]# id -n user1 id: cannot print only names or real IDs in default format [root@centos7 ~]# id -ng user1 user1 [root@centos7 ~]# id -ngG user1 id: cannot print \"only\" of more than one choice [root@centos7 ~]# id -nG user1 user1 usermod修改用户 [16:04:46 root@localhost ~]#id jack uid=1002(jack) gid=1000(alice) groups=1000(alice),1001(g1),1002(g2) [16:04:48 root@localhost ~]#usermod -g sb001 jack # 修改主组 [16:04:56 root@localhost ~]#id jack uid=1002(jack) gid=1006(sb001) groups=1006(sb001),1001(g1),1002(g2) [16:04:58 root@localhost ~]#usermod -G root jack # -G附加组要注意是覆盖性操作 [16:05:11 root@localhost ~]#id jack uid=1002(jack) gid=1006(sb001) groups=1006(sb001),0(root) [16:05:13 root@localhost ~]# 需要用到-aG,-a只能配合G用，因为其他属性不存在多个值。 [16:05:11 root@localhost ~]#id jack uid=1002(jack) gid=1006(sb001) groups=1006(sb001),0(root) [16:06:01 root@localhost ~]#usermod -aG g1,g2,g3 jack [16:06:12 root@localhost ~]#id jack uid=1002(jack) gid=1006(sb001) groups=1006(sb001),1001(g1),1002(g2),1003(g3),0(root) 空口令登入方法 关联用户锁定，就是不需要输入密码进行登入 修改/etc/shadow里的两个!!，没设置密码，就是两个!! [11:38:54 root@localhost ~]#cat /etc/shadow |grep lisi lisi:!!:19008:::::: [11:38:59 root@localhost ~]#vi /etc/shadow ][11:39:14 root@localhost ~]#cat /etc/shadow |grep lisi lisi::19008:::::: 修改/etc/passwd里的x拿掉，效果一样，无需密码，x就是占位，表示密码放在了/etc/shadow里。 [11:47:43 root@localhost ~]#cat /etc/passwd |grep sb001 sb001:x:1006:1006:sbZhuanYong:/home/sb001:/bin/bash [11:47:44 root@localhost ~]#vi /etc/passwd [11:48:00 root@localhost ~]#cat /etc/passwd |grep sb001 sb001::1006:1006:sbZhuanYong:/home/sb001:/bin/bash 注意这个好像没用了 锁定用户 [root@centos7 ~]# useradd test [root@centos7 ~]# getent passwd test test:x:1003:1003::/home/test:/bin/bash [root@centos7 ~]# getent shadow test test:!!:19029:0:99999:7::: # 新创建的用户是被锁定的 [root@centos7 ~]# getent group test test:x:1003: [root@centos7 ~]# getent gshadow test test:!:: -------↓--------设置密码后的变化------↓--------- [root@centos7 ~]# echo cisco |passwd --stdin test Changing password for user test. passwd: all authentication tokens updated successfully. [root@centos7 ~]# getent passwd test test:x:1003:1003::/home/test:/bin/bash [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7::: [root@centos7 ~]# getent group test test:x:1003: [root@centos7 ~]# getent gshadow test test:!:: -------👇------加锁后的变化---------- [root@centos7 ~]# usermod -L test [root@centos7 ~]# getent passwd test test:x:1003:1003::/home/test:/bin/bash [root@centos7 ~]# getent shadow test test:!$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7::: # 锁是加在shadow里 [root@centos7 ~]# getent group test test:x:1003: [root@centos7 ~]# getent gshadow test test:!:: #gshadow这里一直都是这样的 [root@centos7 ~]# ------👇----解锁------ [root@centos7 ~]# usermod -U test [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7::: [root@centos7 ~]# -------👇如果是空口令是不给你解锁的---- [root@centos7 ~]# useradd ceshi [root@centos7 ~]# getent passwd ceshi ceshi:x:1004:1004::/home/ceshi:/bin/bash [root@centos7 ~]# getent shadow ceshi ceshi:!!:19029:0:99999:7::: [root@centos7 ~]# usermod -U ceshi usermod: unlocking the user's password would result in a passwordless account. You should set a password with usermod -p to unlock this user's password. 至于两个！！和一个！没啥区别，一来都是锁定。二来只要没有设置密码都不能-U解锁 不过可以vi进去解锁。 当然这个!可以加在passwd里的--通过vi手动加，usermod -L -U都是针对shadow操作，而且要比加在shadow里优先。 其实针对这个 echo \"cisco\" | passwd --sdtin test 这个非交互是的修改密码，其实我可以这样做 亲测有效稳定。 修改账号有效期 chage比它好 就是shadow文件里单行，倒数第二个字段--账号有效期，倒数第一字段保留字段。 [root@centos7 ~]# getent shadow test test:!$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7::: [root@centos7 ~]# usermod -U test [root@centos7 ~]# usermod -e 2023-12-12 test [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7::19703: 设置密码宽限期 chage比它好 在最大超时时间到期后，你还能用几天 [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7::19703: [root@centos7 ~]# usermod -f 3 test [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7:3:19703: [root@centos7 ~]# 改时间还是用chage 交互式和非交互是都有 [root@centos7 ~]# chage test Changing the aging information for test Enter the new value, or press ENTER for the default Minimum Password Age [0]: Maximum Password Age [99999]: Last Password Change (YYYY-MM-DD) [2022-02-06]: Password Expiration Warning [7]: Password Inactive [3]: Account Expiration Date (YYYY-MM-DD) [2023-12-12]: [root@centos7 ~]# [root@centos7 ~]# chage -h Usage: chage [options] LOGIN Options: -d, --lastday LAST_DAY set date of last password change to LAST_DAY -E, --expiredate EXPIRE_DATE set account expiration date to EXPIRE_DATE -h, --help display this help message and exit -I, --inactive INACTIVE set password inactive after expiration to INACTIVE -l, --list show account aging information -m, --mindays MIN_DAYS set minimum number of days before password change to MIN_DAYS -M, --maxdays MAX_DAYS set maximum number of days before password change to MAX_DAYS -R, --root CHROOT_DIR directory to chroot into -W, --warndays WARN_DAYS set expiration warning days to WARN_DAYS [root@centos7 ~]# ------注意-E选项时间格式有点坑---------👇--人工写成YYYY-MM-DD----- [root@centos7 ~]# chage -E 10 test [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7:3:10: [root@centos7 ~]# chage -E 2022-12-12 test [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7:3:19338: [root@centos7 ~]# chage -I 2 test [root@centos7 ~]# getent shadow test test:$6$QexkrOWQ$3e85TAtRmiYEMioZOU8QJ7qaD0xCP2lW4Dposctg7evJMRZy3kseNQXD.C21lKtXtqFnzGEhCnbiFq7yI4c5H.:19029:0:99999:7:2:19338: [root@centos7 ~]# userdel删除用户 [root@centos7 ~]# ll /home/ total 0 drwx------. 2 ceshi ceshi 83 Feb 6 12:08 ceshi drwx------. 2 dalao001 dalao001 62 Feb 5 14:54 dalao001 drwx------. 2 ming ming 83 Jan 26 09:44 ming drwx------. 2 test test 83 Feb 6 12:08 test drwx------. 2 998 996 62 Feb 5 15:20 test001 drwx------. 2 user1 user1 96 Jan 10 14:16 user1 [root@centos7 ~]# [root@centos7 ~]# [root@centos7 ~]# ll /var/spool/mail/ total 0 -rw-rw----. 1 ceshi mail 0 Feb 6 12:06 ceshi -rw-rw----. 1 dalao001 mail 0 Feb 5 14:54 dalao001 -rw-rw----. 1 ming mail 0 Jan 16 09:28 ming -rw-rw----. 1 test mail 0 Feb 6 11:57 test -rw-rw----. 1 user1 mail 0 Jan 10 11:12 user1 [root@centos7 ~]# [root@centos7 ~]# userdel test [root@centos7 ~]# ll /home/test -d drwx------. 2 1003 1003 83 Feb 6 12:08 /home/test [root@centos7 ~]# ll /var/spool/mail/test -d -rw-rw----. 1 1003 mail 0 Feb 6 11:57 /var/spool/mail/test # 除了家目录，邮箱也要注意是否删除 [root@centos7 ~]# ------不要以为重新创建test用户能继续关联之前没有删除的家目录和邮箱---那是不可能的👇--因为此用户非彼用户，正所谓去年今日此门中，人面桃花相映红，人面不知何处去，桃花依旧笑春风一句话就是uid变了，要是uid没变还是可以对接回去的，或者你人工修改新建用户名的uid为之前的id就可以对接上了-- [root@centos7 ~]# useradd test useradd: warning: the home directory already exists. Not copying any file from skel directory into it. Creating mailbox file: File exists [root@centos7 ~]# [root@centos7 ~]# [root@centos7 ~]# ll /var/spool/mail/test -d -rw-rw----. 1 1003 mail 0 Feb 6 11:57 /var/spool/mail/test [root@centos7 ~]# ll /home/test -d drwx------. 2 1003 1003 83 Feb 6 12:08 /home/test [root@centos7 ~]# userdel -r test userdel: /var/spool/mail/test not owned by test, not removing userdel: /home/test not owned by test, not removing [root@centos7 ~]# ll /var/spool/mail/test -d -rw-rw----. 1 1003 mail 0 Feb 6 11:57 /var/spool/mail/test [root@centos7 ~]# ll /home/test test/ test001/ [root@centos7 ~]# ll /home/test -d drwx------. 2 1003 1003 83 Feb 6 12:08 /home/test [root@centos7 ~]# userdel -r test userdel: user 'test' does not exist -----选项 -r ----👇能够删除用户家目录和邮箱------- [root@centos7 ~]# useradd test2 [root@centos7 ~]# ll /home/test2 -d drwx------. 2 test2 test2 62 Feb 6 13:36 /home/test2 [root@centos7 ~]# ll /var/spool/mail/test2 -d -rw-rw----. 1 test2 mail 0 Feb 6 13:36 /var/spool/mail/test2 [root@centos7 ~]# userdel -r test2 [root@centos7 ~]# ll /home/test2 -d ls: cannot access /home/test2: No such file or directory [root@centos7 ~]# ll /var/spool/mail/test2 -d ls: cannot access /var/spool/mail/test2: No such file or directory [root@centos7 ~]# 附加，group如果是组里没有其他人，userdel 也会删除组的。 所以这里userdel，要注意 组、家目录、邮箱 信息是否有变化。user没了，group、家目录、邮箱如果在，那么这些文件的属性里的user id都会变成该用户的uid--数字，而不再是原来的用户名。而且1005也只是个空数字，并没有任何用户与其对应。 PATH内容随用户而变 变量变量，它是变的，root和user1的PATH变量的内容是不一样 [root@centos7 ~]# echo $PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin [user1@centos7 ~]$ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user1/.local/bin:/home/user1/bin [user1@centos7 ~]$ su 切换 su - user1 # 这种切换用户环境一并切换 su # 原地切换，pwd的所在路径都不变 ------------完全切换👇---------------- [root@centos7 data]# pwd /data [root@centos7 data]# su - user1 Last login: Sun Feb 6 16:10:51 CST 2022 on pts/0 [user1@centos7 ~]$ pwd /home/user1 [user1@centos7 ~]$ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user1/.local/bin:/home/user1/bin [user1@centos7 ~]$ --------------原地切换👇-pwd和$path都不变的-------------- [root@centos7 data]# su user1 [user1@centos7 data]$ pwd /data [user1@centos7 data]$ echo $PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin [user1@centos7 data]$ --------换个用户执行命令，本身不切换过去----👇-------------- [user1@centos7 data]$ su root -c \"getent shadow root\" Password: root:$6$Kih8C.5/adh8TNjx$wNC09CUb7KsECwLH1LFfAagv8N47OAEqSMHLlOuU.vB24ZzU/H5p6DLpNV31zLKlJioqNmXkIkQEaAUf0H74Z0::0:99999:7::: [user1@centos7 data]$ su活su - 直接回车就是切root，root参数默认就有 root 切换 普通用户 无需密码 普通用户 切换 root 需要密码 ubuntu默认不让root登入的情况 ubuntu默认root等不了，su 切换要密码，但是没设置过root密码默认就 root的口令情况，你非root用户也看不全 这个时候就的使用sudo命令咯 sudo -i 提示出入的是当前普通用户的口令---👇-- 不是每个账号都能sudo 切换到root的，之所以可以是因为系统安装的时候授予了wang账号可以切换。 所以ubuntu这里的！去掉，root就直接无口令登入了 ----而且上面知识运行了root本地登入👆-----远程root还是无法登入ubuntu-------- -----👇一般只有系统安装时设置的默认第一个账户才能sudo -i 切成root用户------ passwd的有用选项 [root@centos7 data]# type passwd # 外部命令，就是用--help去查看帮助，当然也可以man passwd is hashed (/usr/bin/passwd) [root@centos7 data]# passwd --help Usage: passwd [OPTION...] -k, --keep-tokens keep non-expired authentication tokens -d, --delete delete the password for the named account (root only) -l, --lock lock the password for the named account (root only) -u, --unlock unlock the password for the named account (root only) -e, --expire expire the password for the named account (root only) -f, --force force operation -x, --maximum=DAYS maximum password lifetime (root only) -n, --minimum=DAYS minimum password lifetime (root only) -w, --warning=DAYS number of days warning users receives before password expiration (root only) -i, --inactive=DAYS number of days after password expiration when an account becomes disabled (root only) -S, --status report password status on the named account (root only) --stdin read new tokens from stdin (root only) Help options: -?, --help Show this help message --usage Display brief usage message -d ： 删除密码 -l : 这个和usermode -L一样 -u: 这个和usermode -U一样 -e: 👈这个好，典型应用按理，强制用户首次登入修改密码的。 -n -x -w 这些和chage以及usermod差不多，都可以改，推荐chage或者passwd。 改口令的方法2 echo cisco | passwd --stdin user1 &> /dev/null 这个passwd怎么批量啊？密码和用户都是变量，密码可以放到文件里，cat file |重定向给passwd，问题时用户怎么弄呢？好像没有chpasswd方面呢~注意此处划重点敲黑板👉呢字带尾音~。其他技术问题忽略即可~ [root@centos7 data]# vi change_passwd.sh [root@centos7 data]# [root@centos7 data]# . change_passwd.sh # 这里也是个点，放到后面，就是脚本执行的N种方法 Changing password for user user1. passwd: all authentication tokens updated successfully. Changing password for user user2. passwd: all authentication tokens updated successfully. Changing password for user user3. passwd: all authentication tokens updated successfully. [root@centos7 data]# [root@centos7 data]# cat change_passwd.sh #!/bin/bash echo cisco |passwd --stdin user1 echo huawei |passwd --stdin user2 echo juniper |passwd --stdin user3 [root@centos7 data]# --------👇优化输出----------- [root@centos7 data]# . change_passwd.sh [root@centos7 data]# [root@centos7 data]# cat change_passwd.sh #!/bin/bash echo cisco |passwd --stdin user1 > /dev/null echo huawei |passwd --stdin user2 > /dev/null echo juniper |passwd --stdin user3 > /dev/null [root@centos7 data]# 修改shell类型 [root@centos7 data]# getent passwd user1 user1:x:1000:1000::/home/user1:/bin/bash [root@centos7 data]# chsh -s /sbin/nologin user1 Changing shell for user1. chsh: Warning: \"/sbin/nologin\" is not listed in /etc/shells. Shell changed. [root@centos7 data]# getent passwd user1 user1:x:1000:1000::/home/user1:/sbin/nologin [root@centos7 data]# cat /etc/shells /bin/sh /bin/bash /usr/bin/sh /usr/bin/bash [root@centos7 data]# -------- 等价于usermod -s --------- [root@centos7 data]# getent passwd user2 user2:x:1009:1010::/home/user2:/bin/bash [root@centos7 data]# usermod -s /sbin/nolgin user2 [root@centos7 data]# getent passwd user2 user2:x:1009:1010::/home/user2:/sbin/nolgin [root@centos7 data]# 组操作补充 附加组的操作 [root@centos7 data]# id user1 uid=1000(user1) gid=1000(user1) groups=1000(user1) [root@centos7 data]# usermod -G g11,g12 user1 usermod: group 'g11' does not exist usermod: group 'g12' does not exist [root@centos7 data]# groupadd g11 [root@centos7 data]# groupadd g12 [root@centos7 data]# usermod -G g11,g12 user1 [root@centos7 data]# id user1 uid=1000(user1) gid=1000(user1) groups=1000(user1),1013(g11),1014(g12) [root@centos7 data]# -------------删除附加组👇----------- [root@centos7 data]# usermod -G \"\" user1 [root@centos7 data]# id user1 uid=1000(user1) gid=1000(user1) groups=1000(user1) [root@centos7 data]# 查看组信息的其他方法 [root@centos7 data]# usermod -G g11,g12 user1 [root@centos7 data]# groups user1 user1 : user1 g11 g12 [root@centos7 data]# [root@centos7 data]# gpasswd --help Usage: gpasswd [option] GROUP Options: -a, --add USER add USER to GROUP -d, --delete USER remove USER from GROUP -h, --help display this help message and exit -Q, --root CHROOT_DIR directory to chroot into -r, --delete-password remove the GROUP's password -R, --restrict restrict access to GROUP to its members -M, --members USER,... set the list of members of GROUP -A, --administrators ADMIN,... set the list of administrators for GROUP Except for the -A and -M options, the options cannot be combined. [root@centos7 data]# gpasswd -a user1 root Adding user user1 to group root [root@centos7 data]# id user1 uid=1000(user1) gid=1000(user1) groups=1000(user1),0(root),1013(g11),1014(g12) [root@centos7 data]# 有时候发现直接图片更好看一些，要不还是放弃cli复制了，直接图片吧，补充关键字就好。 给组设置口令 某个普通用户要加入某个组，就用到了组密码 id user1 su - user1 touch file1 ll file1 希望创建的文件主组时g12，可修改user1的主组为g12， newgrp g12 user1 -------👆 上图的newgrp时临时有效的，exit后就退出来临时的主组了，👇见下图------ root一样可以 永久的修改就用usermod -g 一些操作排查踩坑记录 因为user3创建之前就有同名的家目录，所以带来一些问题：这也是su user3为什么显示-bash-4.2$ 的原因 查看附加组 [root@centos7 ~]# id ming uid=1001(ming) gid=1001(ming) groups=1001(ming) [root@centos7 ~]# [root@centos7 ~]# [root@centos7 ~]# groupmems --help Usage: groupmems [options] [action] Options: -g, --group groupname change groupname instead of the user's group (root only) -R, --root CHROOT_DIR directory to chroot into Actions: -a, --add username add username to the members of the group -d, --delete username remove username from the members of the group -h, --help display this help message and exit -p, --purge purge all members from the group -l, --list list the members of the group [root@centos7 ~]# groupmems -l -g ming [root@centos7 ~]# groupadd g1 [root@centos7 ~]# groupadd g2 [root@centos7 ~]# groupadd g3 [root@centos7 ~]# usermod -G ming g1,g2,g3 usermod: user 'g1,g2,g3' does not exist [root@centos7 ~]# usermod -G ming g1 g2 g3 [root@centos7 ~]# usermod -G g1,g2,g3 ming # 注意次序 [root@centos7 ~]# id ming uid=1001(ming) gid=1001(ming) groups=1001(ming),1015(g1),1016(g2),1017(g3) [root@centos7 ~]# groupmes -l -g ming -bash: groupmes: command not found [root@centos7 ~]# groupmems -l -g ming [root@centos7 ~]# groupmems -l -g g1 # 这是以某个组为线索看谁把它作为附加组了 ming [root@centos7 ~]# groupmems -l -g g2 ming [root@centos7 ~]# groupmems -l -g g3 ming [root@centos7 ~]# -----如果本身就是group的附加组，newgrp直接切成主组 无需密码-----👇---- [11:41:27 root@localhost ~]#usermod -G g12 user1 [11:41:43 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12) [11:41:47 root@localhost ~]#id uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 [11:41:48 root@localhost ~]#su - user1 Last login: Mon Jan 17 20:48:42 CST 2022 on pts/0 [11:41:55 user1@localhost ~]$id uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 [11:42:01 user1@localhost ~]$id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12) [11:42:03 user1@localhost ~]$newgrp g12 [11:42:09 user1@localhost ~]$id uid=1008(user1) gid=1013(g12) groups=1013(g12),1009(user1) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 [11:42:10 user1@localhost ~]$exit exit [11:42:25 user1@localhost ~]$exit logout [11:42:29 root@localhost ~]#groupmems -l -g g12 user1 [11:42:35 root@localhost ~]# 删除附加组成员 [11:42:29 root@localhost ~]#groupmems -l -g g12 user1 [11:45:06 root@localhost ~]#groupmems -l -g g12 user1 [11:45:08 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12) [11:45:24 root@localhost ~]#groupmems -d user1 -g g12 [11:45:30 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1) [11:45:32 root@localhost ~]# 这个只是附加组里的成员 [11:47:07 root@localhost ~]#groupmems -d user1 -g user1 groupmems: user 'user1' is not a member of 'user1' [11:47:12 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1) 其实usermod就挺好，方法有点多，哈哈 [11:49:06 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1014(g13) [11:49:08 root@localhost ~]#usermod -G g12,g13 user1 [11:49:19 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12),1014(g13) [11:49:21 root@localhost ~]#usermod -G g12 user1 [11:49:46 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12) [11:49:48 root@localhost ~]# 清空组中所有成员 工具各有所长 [11:51:22 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12) [11:51:24 root@localhost ~]#usermod -G g12 user2 [11:51:44 root@localhost ~]#usermod -G g12 user3 [11:51:46 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1),1013(g12) [11:51:48 root@localhost ~]#id user2 uid=1009(user2) gid=1010(user2) groups=1010(user2),1013(g12) [11:51:48 root@localhost ~]#id user3 uid=1010(user3) gid=1011(user3) groups=1011(user3),1013(g12) [11:51:49 root@localhost ~]# [11:51:49 root@localhost ~]#groupmems -l -g g12 user1 user2 user3 [11:51:57 root@localhost ~]# 情况附加组成员就用groupmems [11:51:49 root@localhost ~]#groupmems -l -g g12 user1 user2 user3 [11:51:57 root@localhost ~]# [11:52:28 root@localhost ~]#groupmems -p -g g12 [11:52:53 root@localhost ~]#groupmems -l -g g12 [11:52:56 root@localhost ~]#id user1 uid=1008(user1) gid=1009(user1) groups=1009(user1) [11:52:59 root@localhost ~]#id user2 uid=1009(user2) gid=1010(user2) groups=1010(user2) [11:53:02 root@localhost ~]#id user3 uid=1010(user3) gid=1011(user3) groups=1011(user3) [11:53:02 root@localhost ~]# 不知道主组成员能不能清 [11:53:55 root@localhost ~]#usermod -g g12 user1 [11:54:03 root@localhost ~]#id user1 uid=1008(user1) gid=1013(g12) groups=1013(g12) [11:54:05 root@localhost ~]#groupmems -l -g g12 [11:54:18 root@localhost ~]#groupmems -p -g g12 [11:54:35 root@localhost ~]#groupmems -l -g g12 [11:54:35 root@localhost ~]# --------👆肯定不能了，主组是不归groupmems管的----------- useradd\\usermod\\userdel基本上这些事都能做， groupmems这个命令有问题啊 [root@centos7 ~]# id user1 uid=1000(user1) gid=1000(user1) groups=1000(user1),1014(g12) [root@centos7 ~]# whatis groupmems groupmems (8) - administer members of a user's primary group [root@centos7 ~]# groupmems -l -g user1 [root@centos7 ~]# groupmems -l -g g12 user1 [root@centos7 ~]# ---------毛的primary group，它就是个附加组管理工具------------👆------------ 练习 创建用户cacti，附加组为bin和root，默认shell为/bin/csh，注释信息为\"i am a cacti\" useradd -s /bin/csh -G bin,root -c \"i am a cacti\" cacti [root@centos7 ~]# useradd -s /bin/csh -G bin,root -c \"i am a cacti\" cacti [root@centos7 ~]# id cacti uid=1007(cacti) gid=1007(cacti) groups=1007(cacti),0(root),1(bin) [root@centos7 ~]# getent passwd cacti cacti:x:1007:1007:i am a cacti:/home/cacti:/bin/csh [root@centos7 ~]# finger cacti Login: cacti Name: i am a cacti Directory: /home/cacti Shell: /bin/csh Never logged in. No mail. No Plan. [root@centos7 ~]# 创建下面的用户、组和组成员关系， 名字为webs的组， 用户nginx，使用webs作为附加组 用户varnish，使用webs作为附加组 用户mysql，不可交互登入西路，且不是webs的成员 nginx,varnish,mysql密码都是cisco groupadd webs useradd -G webs nginx useradd -G webs varnish useradd -s /sbin/nologin mysql cat p.set nginx:cisco varnish:cisco mysql:cisco EOF cat p.set |chpasswd ---------------------------👇检查下，效果杠杠的----------------------- --------👆上面讲了用户和组，👇下面开始整理文件针对这些用户和组的权限------- QoS， diff serv （打标\\分类+后面的管制、限速、队列）也是这个道理，区别对待，上面的用户和组就是区别，下面针对这些人设置对应文件的访问就是对待。 文件权限 chown修改文件所属 [root@centos7 ~]# touch /data/f1 [root@centos7 ~]# su - user1 Last login: Mon Feb 7 12:33:30 CST 2022 on pts/0 [user1@centos7 ~]$ ll /data/f1 -rw-r--r--. 1 root root 0 Feb 7 12:33 /data/f1 [user1@centos7 ~]$ cat /data/f1 [user1@centos7 ~]$ echo 111 > /data/f1 -bash: /data/f1: Permission denied -------👆user1作为other没有f1的写权限----------- -------👇chown就可以修改文件的所有者和所属组，好像也用不到chgrp------- [user1@centos7 ~]$ [user1@centos7 ~]$ exit logout [root@centos7 ~]# chown user1 /data/f1 [root@centos7 ~]# ll /data/f1 -rw-r--r--. 1 user1 root 0 Feb 7 12:33 /data/f1 [root@centos7 ~]# chown :g12 /data/f1 [root@centos7 ~]# ll /data/f1 -rw-r--r--. 1 user1 g12 0 Feb 7 12:33 /data/f1 [root@centos7 ~]# ll /data/f1 -rw-r--r--. 1 user1 g12 0 Feb 7 12:33 /data/f1 [root@centos7 ~]# chown root:g13 /data/f1 chown: invalid group: ‘root:g13’ [root@centos7 ~]# chown root:g1 /data/f1 [root@centos7 ~]# ll /data/f1 -rw-r--r--. 1 root g1 0 Feb 7 12:33 /data/f1 [root@centos7 ~]# chown user1.g12 /data/f1 [root@centos7 ~]# ll /data/f1 -rw-r--r--. 1 user1 g12 0 Feb 7 12:33 /data/f1 --------👇chgrp就是文件属组---------- [root@centos7 ~]# chgrp g2 /data/f1 [root@centos7 ~]# ll /data/f1 -rw-r--r--. 1 user1 g2 0 Feb 7 12:33 /data/f1 [root@centos7 ~]# chmod修改文件权限 模式法： chmod who opt per file who: u,g,o,a opt: +,-,= per: r,w,x 去掉文件的所有者r权限：chmod u-r file [user1@centos7 ~]$ ll total 4 -rw-rw-r--. 1 user1 user1 0 Feb 6 20:23 file1 [user1@centos7 ~]$ chmod u-r file1 [user1@centos7 ~]$ ll total 4 --w-rw-r--. 1 user1 user1 0 Feb 6 20:23 file1 [user1@centos7 ~]$ ---------此时再看👇user1用户对file1的权限的情况----------- [user1@centos7 ~]$ cat file1 cat: file1: Permission denied [user1@centos7 ~]$ echo xx >> file1 [user1@centos7 ~]$ ll total 8 --w-rw-r--. 1 user1 user1 3 Feb 7 13:43 file1 -rwxrwxrwx. 1 root root 839 Jan 10 14:20 fstab [user1@centos7 ~]$ -----------可见👆权限是所属者适用就只看所属者了--------------- ---------------user>group>other-----3个权限只一个有效--------- [root@centos7 ~]# chmod u=-,g=r,o=rwx /data/f1 [root@centos7 ~]# ll /data/f1 ----r--rwx. 1 user1 g2 3 Feb 7 13:48 /data/f1 [root@centos7 ~]# su user1 [user1@centos7 root]$ cat /data/f1 cat: /data/f1: Permission denied [user1@centos7 root]$ echo xx > /data/f1 bash: /data/f1: Permission denied [user1@centos7 root]$ exit exit [root@centos7 ~]# su user2 [user2@centos7 root]$ cat /data/f1 11 [user2@centos7 root]$ echo xx > /data/f1 [user2@centos7 root]$ cat /data/f1 xx [user2@centos7 root]$ --------------user1的文件权限user1自然可以加回去-------👇--- [user1@centos7 root]$ ll /data/f1 ----r--rwx. 1 user1 g2 3 Feb 7 13:52 /data/f1 [user1@centos7 root]$ chmod u=rwx /data/f1 [user1@centos7 root]$ ll /data/f1 -rwxr--rwx. 1 user1 g2 3 Feb 7 13:52 /data/f1 [user1@centos7 root]$ cat /data/f1 xx [user1@centos7 root]$ echo yy >> /data/f1 [user1@centos7 root]$ cat /data/f1 xx yy [user1@centos7 root]$ ----------非文件拥有者自然不能修改该文件的属性👇------------- [root@centos7 ~]# su user2 [user2@centos7 root]$ ll /data/f1 -rwxr--rwx. 1 user1 g2 6 Feb 7 13:53 /data/f1 [user2@centos7 root]$ chmod u=rx /data/f1 chmod: changing permissions of ‘/data/f1’: Operation not permitted [user2@centos7 root]$ chmod g=- /data/f1 chmod: changing permissions of ‘/data/f1’: Operation not permitted [user2@centos7 root]$ [root@centos7 ~]# chmod a=rwx /data/f1 [root@centos7 ~]# ll /data/f1 -rwxrwxrwx. 1 user1 g2 6 Feb 7 13:53 /data/f1 [root@centos7 ~]# chmod a=- /data/f1 [root@centos7 ~]# ll /data/f1 ----------. 1 user1 g2 6 Feb 7 13:53 /data/f1 [root@centos7 ~]# chmod a=r /data/f1 [root@centos7 ~]# ll /data/f1 -r--r--r--. 1 user1 g2 6 Feb 7 13:53 /data/f1 [root@centos7 ~]# chmod a= /data/f1 [root@centos7 ~]# ll /data/f1 ----------. 1 user1 g2 6 Feb 7 13:53 /data/f1 [root@centos7 ~]# ------------谁都不行，root还行👇------root超脱🐟权限除了x执行权限----- [root@centos7 ~]# chown root.root /data/f1 [root@centos7 ~]# ll /data/f1 ----------. 1 root root 9 Feb 7 13:59 /data/f1 [root@centos7 ~]# cat /data/f1 xx yy zz [root@centos7 ~]# echo ee >> /data/f1 [root@centos7 ~]# cat /data/f1 xx yy zz ee [root@centos7 ~]# ---------👇--执行权限root要是没有的，也不行，root也就rw读写不受权限影响----- [root@centos7 ~]# ll /bin/cat -rwxr-xr-x. 1 root root 54080 Aug 20 2019 /bin/cat [root@centos7 ~]# [root@centos7 ~]# chow a-x /bin/cat -bash: chow: command not found [root@centos7 ~]# chmod a-x /bin/cat [root@centos7 ~]# ll /bin/cat -rw-r--r--. 1 root root 54080 Aug 20 2019 /bin/cat [root@centos7 ~]# cat /data/f1 -bash: /usr/bin/cat: Permission denied [root@centos7 ~]# /bin/cat /data/f1 -bash: /bin/cat: Permission denied [root@centos7 ~]# chmod +x /bin/cat # 这里等价于a+x [root@centos7 ~]# cat /data/f1 xx yy zz ee -------👇----root比较牛逼，只要u、g、o里一个角色有执行权限，那他就有权限了--------- [root@centos7 ~]# ll /bin/cat -rwxr-xr-x. 1 root root 54080 Aug 20 2019 /bin/cat [root@centos7 ~]# chmod u-x,g-x /bin/cat [root@centos7 ~]# ll /bin/cat -rw-r--r-x. 1 root root 54080 Aug 20 2019 /bin/cat [root@centos7 ~]# cat /data/f1 xx yy zz ee [root@centos7 ~]# 修改目录下所有文件和子目录的权限，这个R特别的坑和rm也差不多了 [root@centos7 ~]# ll /data/ total 8 -rw-r--r--. 1 root root 151 Feb 6 19:31 change_passwd.sh ----------. 1 root root 12 Feb 7 14:19 f1 -rw-r--r--. 1 root root 0 Feb 7 14:19 f10 -rw-r--r--. 1 root root 0 Feb 7 14:19 f11 -rw-r--r--. 1 root root 0 Feb 7 14:19 f12 -rw-r--r--. 1 root root 0 Feb 7 14:19 f13 -rw-r--r--. 1 root root 0 Feb 7 14:19 f14 -rw-r--r--. 1 root root 0 Feb 7 14:19 f15 -rw-r--r--. 1 root root 0 Feb 7 14:19 f16 -rw-r--r--. 1 root root 0 Feb 7 14:19 f17 -rw-r--r--. 1 root root 0 Feb 7 14:19 f18 -rw-r--r--. 1 root root 0 Feb 7 14:19 f19 -rw-r--r--. 1 root root 0 Feb 7 14:19 f2 -rw-r--r--. 1 root root 0 Feb 7 14:19 f20 -rw-r--r--. 1 root root 0 Feb 7 14:19 f3 -rw-r--r--. 1 root root 0 Feb 7 14:19 f4 -rw-r--r--. 1 root root 0 Feb 7 14:19 f5 -rw-r--r--. 1 root root 0 Feb 7 14:19 f6 -rw-r--r--. 1 root root 0 Feb 7 14:19 f7 -rw-r--r--. 1 root root 0 Feb 7 14:19 f8 -rw-r--r--. 1 root root 0 Feb 7 14:19 f9 [root@centos7 ~]# chmod a+x -R /data/ [root@centos7 ~]# ll /data/ -d drwxr-xr-x. 2 root root 241 Feb 7 14:19 /data/ [root@centos7 ~]# ll /data/ total 8 -rwxr-xr-x. 1 root root 151 Feb 6 19:31 change_passwd.sh ---x--x--x. 1 root root 12 Feb 7 14:19 f1 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f10 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f11 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f12 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f13 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f14 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f15 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f16 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f17 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f18 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f19 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f2 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f20 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f3 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f4 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f5 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f6 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f7 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f8 -rwxr-xr-x. 1 root root 0 Feb 7 14:19 f9 [root@centos7 ~]# [root@centos7 ~]# ll /data/ total 8 -rw-r--r--. 1 root root 151 Feb 6 19:31 change_passwd.sh ----------. 1 root root 12 Feb 7 14:19 f1 -rw-r--r--. 1 root root 0 Feb 7 14:19 f10 -rw-r--r--. 1 root root 0 Feb 7 14:19 f11 -rw-r--r--. 1 root root 0 Feb 7 14:19 f12 -rw-r--r--. 1 root root 0 Feb 7 14:19 f13 -rw-r--r--. 1 root root 0 Feb 7 14:19 f14 -rw-r--r--. 1 root root 0 Feb 7 14:19 f15 -rw-r--r--. 1 root root 0 Feb 7 14:19 f16 -rw-r--r--. 1 root root 0 Feb 7 14:19 f17 -rw-r--r--. 1 root root 0 Feb 7 14:19 f18 -rw-r--r--. 1 root root 0 Feb 7 14:19 f19 -rw-r--r--. 1 root root 0 Feb 7 14:19 f2 -rw-r--r--. 1 root root 0 Feb 7 14:19 f20 -rw-r--r--. 1 root root 0 Feb 7 14:19 f3 -rw-r--r--. 1 root root 0 Feb 7 14:19 f4 -rw-r--r--. 1 root root 0 Feb 7 14:19 f5 -rw-r--r--. 1 root root 0 Feb 7 14:19 f6 -rw-r--r--. 1 root root 0 Feb 7 14:19 f7 -rw-r--r--. 1 root root 0 Feb 7 14:19 f8 -rw-r--r--. 1 root root 0 Feb 7 14:19 f9 [root@centos7 ~]# chown -R user1 /data/ [root@centos7 ~]# ll /data/ total 8 -rw-r--r--. 1 user1 root 151 Feb 6 19:31 change_passwd.sh ----------. 1 user1 root 12 Feb 7 14:19 f1 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f10 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f11 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f12 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f13 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f14 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f15 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f16 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f17 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f18 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f19 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f2 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f20 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f3 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f4 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f5 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f6 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f7 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f8 -rw-r--r--. 1 user1 root 0 Feb 7 14:19 f9 [root@centos7 ~]# [root@centos7 ~]# ll /data/ -d drw-r--r--. 2 user1 root 241 Feb 7 14:19 /data/ -------坑在这里👇----- rm -rf / data #小手一抖，空格全没有 chmod -R a=rwx / data #小手再都，大妈食堂有 ------👆你把/根下所有的文件夹和子文件权限都弄了，更狠的来了👇---- chown -R user1 / data #/根下所有文件夹和文件所有者都变成了user1了 参考别的文件设置同样的用户和组，以及权限 [root@centos7 ~]# ll /etc/fstab -rw-r--r--. 1 root root 595 Jan 5 17:41 /etc/fstab [root@centos7 ~]# ll /data/f1 ----------. 1 user1 root 12 Feb 7 14:19 /data/f1 [root@centos7 ~]# chown --reference /etc/fstab /data/f1 [root@centos7 ~]# ll /data/f1 ----------. 1 root root 12 Feb 7 14:19 /data/f1 [root@centos7 ~]# chmod --reference /etc/fstab /data/f1 [root@centos7 ~]# ll /data/f1 -rw-r--r--. 1 root root 12 Feb 7 14:19 /data/f1 [root@centos7 ~]# Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"6-用户、用户组和权限/3-文件和特殊权限.html":{"url":"6-用户、用户组和权限/3-文件和特殊权限.html","title":"第3节 文件和特殊权限","keywords":"","body":"第3节. 文件和特殊权限 接上文，模式法 chmod who opt per file who: u,g,o,a opt: +,-,= per: r,w,x 数字法 rwxrw-r-- f1 111110100 👉 111 110 100 = 7 6 4 r 4 w 2 x 1 文件夹的权限 w：针对文件夹，就是创建、删除其中的文件。而修改文件涉及的是文件本身的写权限。 ----------👇--------------------------------- [user1@centos7 dir]$ ll ./ -d drwxr-xr-x. 2 root root 16 Feb 8 09:23 ./ # 文件夹dir的权限 [root@centos7 dir]# su user1 [user1@centos7 dir]$ ll total 0 -rw-r--rw-. 1 root root 0 Feb 8 09:23 f1 [user1@centos7 dir]$ touch 11 touch: cannot touch ‘11’: Permission denied [user1@centos7 dir]$ vi f1 [user1@centos7 dir]$ cat f1 111 [user1@centos7 dir]$ rm f1 rm: cannot remove ‘f1’: Permission denied [user1@centos7 dir]$ echo xxx > f1 [user1@centos7 dir]$ cat f1 xxx [user1@centos7 dir]$ r: 读权限针对文件夹就是看不到文件夹下面的内容，但是如果你知道某个文件的名称，是可以直接看该文件内容的。 [root@centos7 data]# chmod o-r dir/ [root@centos7 data]# ll total 0 drwxr-x--x. 2 root root 16 Feb 8 09:46 dir [root@centos7 data]# su user1 [user1@centos7 data]$ cat dir/f1 xx [user1@centos7 data]$ cd dir [user1@centos7 dir]$ ls ls: cannot open directory .: Permission denied [user1@centos7 dir]$ cat f1 xx [user1@centos7 dir]$ ll -d f1 -rw-r--r--. 1 root root 3 Feb 8 09:48 f1 [user1@centos7 dir]$ ll ls: cannot open directory .: Permission denied [user1@centos7 dir]$ x：执行权限针对文件夹就是进入咯，这个执行一旦取消，关系就大了，你连文件夹都进不去，那么文件夹下的文件就看不到 dir/f1 bash: dir/f1: Permission denied [user1@centos7 data]$ ll dir/ ls: cannot open directory dir/: Permission denied [user1@centos7 data]$ 👇-----给上面的dir补一个r读权限---- [root@centos7 data]# chmod o=r dir [root@centos7 data]# ll total 0 drwxr-xr--. 2 root root 16 Feb 8 09:46 dir [root@centos7 data]# ll dir/f1 -rw-r--rw-. 1 root root 3 Feb 8 09:48 dir/f1 [root@centos7 data]# [root@centos7 data]# su user1 [user1@centos7 data]$ ll dir ls: cannot access dir/f1: Permission denied total 0 -????????? ? ? ? ? ? f1 # 👈元数据看不到，文件名字倒是可以的 [user1@centos7 data]$ cd dir bash: cd: dir: Permission denied [user1@centos7 data]$ ll dir/f1 ls: cannot access dir/f1: Permission denied [user1@centos7 data]$ 文件夹来讲： 目录存放的数据块里的内容是各个文件名和其对应的节点信息 文件存放的数据块里的内容是文件的内容 读：可以列出该文件夹下的文件名，拿掉后，如果知道文件夹下的文件名，也能通过/dir/file去直接cat（这取决于文件本身的r权限）。 执行：可以进入目录，可以访问目录里的文件内容(依赖于文件本身的r权限)。 写：决定是否可以在目录里面创建和删除文件。文件本身的权限还得看文件自己的。 注意，w需要x加持~如果文件夹的执行权限取消，及时有写权限，由于进不到该目录下，所以也就没法去写文件的。 说下大X，后面再将st [root@centos7 data]# ll total 0 drwxr-xr--. 2 root root 16 Feb 8 09:46 dir -rw-r--r--. 1 root root 0 Feb 8 11:31 f1 -rw-r--r--. 1 root root 0 Feb 8 11:31 f2 [root@centos7 data]# chmod -R a+x /data [root@centos7 data]# ll /data total 0 drwxr-xr-x. 2 root root 16 Feb 8 09:46 dir -rwxr-xr-x. 1 root root 0 Feb 8 11:31 f1 -rwxr-xr-x. 1 root root 0 Feb 8 11:31 f2 [root@centos7 data]# ll /data -d drwxr-xr-x. 3 root root 37 Feb 8 11:31 /data ------👆通过-R可以使文件夹和其下所有文件的权限都递归改掉----- 但是如果遇到文件你给它一个x执行权限，往往存在安全风险---所以-R 配合大X就可以过滤文件的权限修改 👇 [root@centos7 data]# chmod -R a-x /data/ [root@centos7 data]# ll /data/ total 0 drw-r--r--. 2 root root 16 Feb 8 09:46 dir -rw-r--r--. 1 root root 0 Feb 8 11:31 f1 -rw-r--r--. 1 root root 0 Feb 8 11:31 f2 [root@centos7 data]# chmod -R a+X /data/ [root@centos7 data]# ll /data/ total 0 drwxr-xr-x. 2 root root 16 Feb 8 09:46 dir -rw-r--r--. 1 root root 0 Feb 8 11:31 f1 -rw-r--r--. 1 root root 0 Feb 8 11:31 f2 [root@centos7 data]# 练习： 误删了用户ming的家目录，进行恢复 三条命令 cp -r /etc/skel /home/ming chown -R ming.ming /home/ming chmod 700 /home/ming # 里面的几个隐藏文件都是从/etc/skel下复制过来的，属性不变就好。 ---误删除了用户ming家目录下的文件，但是目录还在，进行恢复👇 [root@centos7 ~]# rm -rf /home/ming/.* rm: refusing to remove ‘.’ or ‘..’ directory: skipping ‘/home/ming/.’ rm: refusing to remove ‘.’ or ‘..’ directory: skipping ‘/home/ming/..’ [root@centos7 ~]# ll -a /home/ming total 0 drwx------. 2 ming ming 6 Feb 8 13:43 . drwxr-xr-x. 16 root root 192 Feb 8 12:01 .. --👇注意，此时通过.*复制/etc/skel/下的所有文件--包含隐藏和非隐藏，会有大问题🐕-- [root@centos7 home]# cp -r /etc/skel/.* /home/ming cp: will not create hard link ‘/home/ming/skel’ to directory ‘/home/ming/.’ cp: overwrite ‘/home/ming/.bash_logout’? ^C [root@centos7 home]# ll /home/ming total 1036 -rw-r--r--. 1 root root 16 Feb 8 13:44 adjtime -rw-r--r--. 1 root root 1529 Feb 8 13:44 aliases -rw-r--r--. 1 root root 12288 Feb 8 13:44 aliases.db drwxr-xr-x. 2 root root 236 Feb 8 13:44 alternatives -rw-------. 1 root root 541 Feb 8 13:44 anacrontab -rw-r--r--. 1 root root 55 Feb 8 13:44 asound.conf drwxr-x---. 3 root root 43 Feb 8 13:44 audisp drwxr-x---. 3 root root 83 Feb 8 13:44 audit drwxr-xr-x. 2 root root 22 Feb 8 13:44 bash_completion.d -rw-r--r--. 1 root root 2853 Feb 8 13:44 bashrc drwxr-xr-x. 2 root root 6 Feb 8 13:44 binfmt.d -rw-r--r--. 1 root root 37 Feb 8 13:44 centos-release -rw-r--r--. 1 root root 51 Feb 8 13:44 centos-release-upstream drwxr-xr-x. 2 root root 6 Feb 8 13:44 chkconfig.d drwxr-xr-x. 2 root root 21 Feb 8 13:44 cron.d drwxr-xr-x. 2 root root 42 Feb 8 13:44 cron.daily -rw-------. 1 root root 0 Feb 8 13:44 cron.deny drwxr-xr-x. 2 root root 22 Feb 8 13:44 cron.hourly drwxr-xr-x. 2 root root 6 Feb 8 13:44 cron.monthly -rw-r--r--. 1 root root 451 Feb 8 13:44 crontab drwxr-xr-x. 2 root root 6 Feb 8 13:44 cron.weekly -rw-------. 1 root root 0 Feb 8 13:44 crypttab -rw-r--r--. 1 root root 1620 Feb 8 13:44 csh.cshrc -rw-r--r--. 1 root root 1103 Feb 8 13:44 csh.login 👆发现复制很N多文件过来了，原因是因为.*通配符它代表.xxx还有..xxx所以复制.*意味着你不仅仅复制了当前目录下的所有文件，也复制了上级目录下的所有文件。 推荐👇👉使用cp -r /etc/skel/. /home/ming这种方式复制所有文件含隐藏文件 drwx------. 2 root root 6 Feb 8 13:52 ming [root@centos7 home]# cp -r /etc/skel/. /home/ming [root@centos7 home]# ll -a ming total 12 drwx------. 2 root root 72 Feb 8 13:53 . drwxr-xr-x. 16 root root 192 Feb 8 13:52 .. -rw-r--r--. 1 root root 18 Feb 8 13:53 .bash_logout -rw-r--r--. 1 root root 193 Feb 8 13:53 .bash_profile -rw-r--r--. 1 root root 231 Feb 8 13:53 .bashrc -rw-r--r--. 1 root root 0 Feb 8 13:53 f1 [root@centos7 home]# ------👇这样也行，就是通过.[^.]*来表示所有隐藏文件，和*来表示所有非隐藏文件---- [root@centos7 home]# cp -r /etc/skel/.[^.]* /home/ming [root@centos7 home]# ll /home/ming -a total 12 drwx------. 2 root root 62 Feb 8 13:55 . drwxr-xr-x. 16 root root 192 Feb 8 13:52 .. -rw-r--r--. 1 root root 18 Feb 8 13:55 .bash_logout -rw-r--r--. 1 root root 193 Feb 8 13:55 .bash_profile -rw-r--r--. 1 root root 231 Feb 8 13:55 .bashrc [root@centos7 home]# cp -r /etc/skel/* /home/ming [root@centos7 home]# ll /home/ming -a total 12 drwx------. 2 root root 72 Feb 8 13:55 . drwxr-xr-x. 16 root root 192 Feb 8 13:52 .. -rw-r--r--. 1 root root 18 Feb 8 13:55 .bash_logout -rw-r--r--. 1 root root 193 Feb 8 13:55 .bash_profile -rw-r--r--. 1 root root 231 Feb 8 13:55 .bashrc -rw-r--r--. 1 root root 0 Feb 8 13:55 f1 [root@centos7 home]# mkdir创建文件夹的时候可以设置权限 [root@centos7 home]# mkdir --help Usage: mkdir [OPTION]... DIRECTORY... Create the DIRECTORY(ies), if they do not already exist. Mandatory arguments to long options are mandatory for short options too. -m, --mode=MODE set file mode (as in chmod), not a=rwx - umask -p, --parents no error if existing, make parent directories as needed -v, --verbose print a message for each created directory -Z set SELinux security context of each created directory to the default type --context[=CTX] like -Z, or if CTX is specified then set the SELinux or SMACK security context to CTX --help display this help and exit --version output version information and exit GNU coreutils online help: For complete documentation, run: info coreutils 'mkdir invocation' [root@centos7 home]# [root@centos7 ~]# mkdir -m 000 /home/sb001 [root@centos7 ~]# ll /home/sb001 -d d---------. 2 root root 6 Feb 8 14:00 /home/sb001 [root@centos7 ~]# 文件的特殊权限 /etc/shaow这个文件普通用户没有权限对其修改，但是可以通过passwd命令对其进行修改的，因为改自身密码本质上就是修改了shadow文件。 [root@centos7 ~]# ll /etc/shadow ----------. 1 root root 1366 Feb 8 14:06 /etc/shadow [root@centos7 ~]# su user1 [user1@centos7 root]$ cat /etc/shadow cat: /etc/shadow: Permission denied [user1@centos7 root]$ echo xx /etc/shadow xx /etc/shadow [user1@centos7 root]$ echo xx >> /etc/shadow bash: /etc/shadow: Permission denied [user1@centos7 root]$ passwd Changing password for user user1. Changing password for user1. (current) UNIX password: passwd: Authentication token manipulation error [user1@centos7 root]$ passwd Changing password for user user1. Changing password for user1. (current) UNIX password: New password: Retype new password: passwd: all authentication tokens updated successfully. [user1@centos7 root] 这是因为passwd命令-也就是这/bin/passwd这个执行文件用户属性位上有s位。 [user1@centos7 root]$ ll /bin/passwd suid当用户使用该程序/命令访问某个文件的时候，原则上是使用这个用户的权限去访问文件。 一旦有了suid，不管谁运行这个程序，通过这个程序访问文件，就是获得这个程序所有者的权限。上图只要你运行passwd，你的身份就转换为root了。suid全称就是set owner user id up to execution在执行时设置所有者用户ID。 第二点，suid一定是作用在二进制的可执行的文件上(对shell脚本无效)，否则没有意义了就。所以大S没有意义-去掉x后就是大S [root@centos7 ~]# ll /usr/bin/vi -rwsr-xr-x. 1 root root 928056 Oct 14 2020 /usr/bin/vi [root@centos7 ~]# su user su: user user does not exist [root@centos7 ~]# su user1 [user1@centos7 root]$ vi /etc/shadow # 此时就可以vi进去修改并保持了 [user1@centos7 root]$ echo xxx >> /etc/shadow # echo不行肯定的啊你suid的是vim啊 bash: /etc/shadow: Permission denied ----------这个，，，尝试将echo变成suid权限，发现还是不行，可能要该重定向文件咯呵呵---- [root@centos7 ~]# which echo /usr/bin/echo [root@centos7 ~]# ll /usr/bin/echo -rwxr-xr-x. 1 root root 33088 Aug 20 2019 /usr/bin/echo [root@centos7 ~]# chmod u+s /usr/bin/echo [root@centos7 ~]# ll /usr/bin/echo -rwsr-xr-x. 1 root root 33088 Aug 20 2019 /usr/bin/echo [root@centos7 ~]# su user1 [user1@centos7 root]$ echo xx >> /etc/shadow bash: /etc/shadow: Permission denied [user1@centos7 root]$ exit exit [root@centos7 ~]# which >> -bash: syntax error near unexpected token `newline' [root@centos7 ~]# 数字法修改suid： 4是单独算的 [root@centos7 ~]# ll /usr/bin/echo -rwsr-xr-x. 1 root root 33088 Aug 20 2019 /usr/bin/echo [root@centos7 ~]# chmod 755 /usr/bin/echo [root@centos7 ~]# ll /usr/bin/echo -rwxr-xr-x. 1 root root 33088 Aug 20 2019 /usr/bin/echo [root@centos7 ~]# chmod 4755 /usr/bin/echo [root@centos7 ~]# ll /usr/bin/echo -rwsr-xr-x. 1 root root 33088 Aug 20 2019 /usr/bin/echo [root@centos7 ~]# sgid 1、如果某个用户运行cat程序也即是使用cat去访问文件，就会继承所属组的权限。或者说就会将用户原本的所属组提升至该程序的所属组。 同样数字法 755前面补一个2，sgid 755前面补一个4，sguid 755前面补一个6，suid+sgid 粘滞位 针对文件夹的属性 非用户本人，无法删除文件夹下的文件 粘滞位对文件不生效 [root@centos7 dir]# su user1 [user1@centos7 dir]$ ll total 0 -rw-r--r--. 1 root root 0 Feb 8 16:19 f1 -rw-rw-r-T. 1 user1 user1 0 Feb 8 16:22 f2 [user1@centos7 dir]$ rm -rf f2 [user1@centos7 dir]$ ll total 0 -rw-r--r--. 1 root root 0 Feb 8 16:19 f1 [user1@centos7 dir]$ 总结 suid： ​ 作用于可执行的二进制的程序，权限4，功能：用户执行此程序时，将继承此程序所有者的权限。 sguid： ​ 作用于可执行的二进制的程序，权限2，功能：用户执行此程序时，将集成此程序所属组的权限。 ​ 作用于目录，权限2，功能，新建的文件，将自动集成该目录的所属组。 sticky： ​ 作用于目录，权限1，功能：只能删除自己的文件，root不受限。 创建一个目录可以让有限的几个用户使用 [16:49:55 root@localhost data]#mkdir -m 770 testdir [16:50:10 root@localhost data]#ll total 1 drwxrwx---. 2 root root 6 Jan 19 16:50 testdir [16:52:38 root@localhost data]#chown .grp001 testdir [16:52:50 root@localhost data]#ll total 1 drwxrwx---. 2 root grp001 6 Jan 19 16:50 testdir [16:53:01 root@localhost data]#usermod -G grp001 user1 [16:54:31 root@localhost data]#usermod -G grp001 user2 [16:55:01 root@localhost data]#groupmems -l -g grp001 user1 user2 [16:55:14 root@localhost data]#su user1 [16:55:19 user1@localhost data]$cd testdir/ [16:55:28 user1@localhost testdir]$touch f1 [16:55:30 user1@localhost testdir]$echo 11 >> f1 [16:55:34 user1@localhost testdir]$exit exit [16:55:36 root@localhost data]#su user2 [16:55:43 user2@localhost data]$cd testdir/ [16:55:54 user2@localhost testdir]$echo xx >> f2 [16:56:06 user2@localhost testdir]$cat f2 xx 貌似centos7.9和7.6这里有个不一样的点👇 就是user1可以修改user2创建的文件，如果时之前的观点，就会再设置文件夹的sguid来使目录下的文件创建的时候自动集成父目录的所属组。 当然最好还是设置一下文件夹的sgid [root@centos7 data]# chmod g+s renyue/ [root@centos7 data]# ll total 0 drwxrws---. 2 root renyue-group 26 Feb 8 17:12 renyue [root@centos7 data]# [root@centos7 data]# su client1 [client1@centos7 data]$ ll total 0 drwxrws---. 2 root renyue-group 26 Feb 8 17:12 renyue [client1@centos7 data]$ cd renyue/ [client1@centos7 renyue]$ touch f3 [client1@centos7 renyue]$ ll total 4 -rw-rw-r--. 1 client1 client1 2 Feb 8 17:12 f1 -rw-rw-r--. 1 client2 client2 0 Feb 8 17:12 f2 -rw-rw-r--. 1 client1 renyue-group 0 Feb 8 17:35 f3 [client1@centos7 renyue]$ cp /etc/fstab /data/dir/ 普通需要什么权限？ cp 命令的执行权限 /etc文件夹的执行 fstab文件的读 /data文件夹的执行 /dir文件夹得执行和写 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"6-用户、用户组和权限/4-umask权限.html":{"url":"6-用户、用户组和权限/4-umask权限.html","title":"第4节 umask权限","keywords":"","body":"第4节. umask权限 chattr i :文件不会被修改、删除、包含所在目录也不能被删除。 [19:40:28 root@localhost data]#chattr +i f1 [19:40:37 root@localhost data]#ll total 0 -rw-r--r--. 1 root root 0 Jan 19 19:40 f1 [19:40:38 root@localhost data]#lsattr f1 ----i--------------- f1 [19:41:22 root@localhost data]#rm -rf f1 rm: cannot remove 'f1': Operation not permitted [19:41:29 root@localhost data]#echo xx > f1 -bash: f1: Operation not permitted [19:41:32 root@localhost data]#mv f1 f2 mv: cannot move 'f1' to 'f2': Operation not permitted [19:41:36 root@localhost data]#cd .. [19:41:38 root@localhost /]#rm -rf /data/ rm: cannot remove '/data/f1': Operation not permitted [19:41:44 root@localhost /]# a:文件仅可以添加，同样所在目录不能被删除 [19:45:29 root@localhost data]#chattr +a f1 [19:45:36 root@localhost data]#lsattr f1 ----ia-------------- f1 [19:45:39 root@localhost data]#echo xx > f1 -bash: f1: Operation not permitted [19:45:45 root@localhost data]#echo xx >> f1 -bash: f1: Operation not permitted [19:45:48 root@localhost data]#chattr -i f1 [19:46:05 root@localhost data]#lsattr f1 -----a-------------- f1 [19:46:09 root@localhost data]#echo xx > f1 -bash: f1: Operation not permitted [19:46:13 root@localhost data]#echo xx >> f1 [19:46:16 root@localhost data]#cat f1 xx [19:46:19 root@localhost data]#rm -rf f1 rm: cannot remove 'f1': Operation not permitted [19:46:22 root@localhost data]#exit logout [19:46:54 root@localhost /]#rm -rf /data/ rm: cannot remove '/data/f1': Operation not permitted ---但是👇vi进去后在最后一行添加这种操作，系统判定不出来你是不是追加所以这种追加时不行的。 umask 1、root用户新建文件和文件夹可发现默认的权限分别时644和755 [19:57:42 root@localhost data]#ll total 0 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir -rw-r--r--. 1 root root 0 Jan 19 19:57 f1 [19:57:42 root@localhost data]# 2、换普通用户创建文件和文件夹权限就不一样 [putong@centos7 data]$ ll |grep putong drwxrwxr-x. 2 putong putong 6 Feb 8 20:02 dir2 -rw-rw-r--. 1 putong putong 0 Feb 8 20:02 f2 [putong@centos7 data]$ [putong@centos7 data]$ type umask umask is a shell builtin [putong@centos7 data]$ umask 0002 [putong@centos7 data]$ exit exit [root@centos7 data]# umask 0022 [root@centos7 data]# umask+default_value=777目录|666文件 000+default=777，假设文件也用777总和，会导致文件可能就带上了x执行权限，带来一定的风险。 022+default=777|666，文件夹default=755，文件default=644 👆上面的公式不正确 umask，mask时掩码，user的mask就是用户的掩码的意思。 umask ugo 也分user group other umask的功能，将777或者666中对应的权限去掉，得出默认权限。 777 022 ↓转成二进制,0锁死，1放开，就是通配符或者反掩码的算法. 111 111 111 000 010 010 ------------ 111 101 101 = 755 这就是文件夹创建默认权限 666 022 110 110 110 000 010 010 ------------ 110 100 100 = 644 这就是文件创建默认权限 ----👇修改umask值再来看，发现777|666-umask就不准确了---- [root@centos7 data]# umask 123 [root@centos7 data]# touch f1 [root@centos7 data]# mkdir dir [root@centos7 data]# ll total 0 drw-r-xr--. 2 root root 6 Feb 8 20:18 dir -rw-r--r--. 1 root root 0 Feb 8 20:18 f1 [root@centos7 data]# ------- 分析： 777 123 111 111 111 001 010 011 -------------- 110 101 100 => 掩出来的文件夹默认值为：654，这个确实就是777-123=654 如果是文件 666 123 110 110 110 001 010 011 -------------- 110 100 100 => 得到：644，这个就不是666-123=543，使用奇数+1的规律=644，所以速算法就是 543里面带上了执行权限了，肯定不可能的。 👇 如果是文件夹777-umask=default 如果是文件666-umask=default(3个数，如果是奇数就+1，偶数不变) umask退出后丢失，可以写到.bashrc或者/etc/bashrc里，这里也有个点就是几个文件里配置环境变量等，谁优先的原则，涉及文件有/etc/profile bashrc等，这个后面讲。 [root@centos7 ~]# cat /etc/bashrc | tail -3 fi # vim:ts=4:sw=4 umask 123 [root@centos7 ~]# [root@centos7 ~]# umask 0022 [root@centos7 ~]# umask -p umask 0022 [root@centos7 ~]# umask -p >> .bashrc # 将当前umaks值写入配置文件里 [root@centos7 ~]# tail -1 .bashrc umask 0022 [root@centos7 ~]# FACL 解决一些特殊需求，普通权限解决不了，比如 user1不能访问f1，user2能对f1完全控制，user3只能读f1，user4只能写f1 此时ugo三个角色，user、group、other，用户权限超过3个，就需要ACL了。 setfacl -m u:user1:0 f1 setfacl -m u:user1:- f1 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"6-用户、用户组和权限/5-FACL实现权限的灵活控制.html":{"url":"6-用户、用户组和权限/5-FACL实现权限的灵活控制.html","title":"第5节 FACL实现权限的灵活控制","keywords":"","body":"第5节. FACL实现权限的灵活控制 解决一些特殊需求，普通权限解决不了，比如 user1不能访问f1，user2能对f1完全控制，user3只能写f1，user4只能读f1 此时ugo三个角色，user、group、other，用户权限超过3个，就需要ACL了。 setfacl -m u:user1:0 f1 👈表示啥权限都没有 setfacl -m u:user1:- f1 👈表示啥权限都没有，等价于0 👉user1不能访问f1 [10:03:19 root@localhost data]#ll f1 -rw-r--r--+ 1 root root 4 Jan 29 10:03 f1 [10:04:59 root@localhost data]#su user2 -c \"cat f1\"👈切换用户输入cli后直接退出来 123 [10:05:00 root@localhost data]#su user1 -c \"cat f1\" cat: f1: Permission denied 👉user2完全控制f1 [10:20:21 root@localhost data]#su user2 -c 'cat f1' 123 [10:20:39 root@localhost data]#su user2 -c 'echo 123 > f1' bash: f1: Permission denied [10:20:43 root@localhost data]#setfacl -m u:user2:rw f1 [10:20:59 root@localhost data]#su user2 -c 'echo 321 > f1' [10:21:05 root@localhost data]#su user2 -c 'cat f1' 321 [10:21:11 root@localhost data]# 👉user3只能写f1 [10:24:33 root@localhost data]#setfacl -m u:user3:w f1 [10:25:44 root@localhost data]#su user3 -c 'cat f1' cat: f1: Permission denied [10:25:52 root@localhost data]#su user3 -c 'echo aaa >> f1' [10:26:01 root@localhost data]#su user3 -c 'cat f1' cat: f1: Permission denied [10:26:03 root@localhost data]#cat f1 321 aaa [10:26:15 root@localhost data]# 👉user4只能读f1,就归到other整体权限去，无需修改 👉查看facl [10:26:15 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user1:--- user:user2:rw- user:user3:-w- group::r-- mask::rw- other::r-- 针对group设置facl [10:28:34 root@localhost data]#setfacl -m g:g1:rw f1 [10:36:27 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user1:--- user:user2:rw- user:user3:-w- group::r-- group:g1:rw- 👈g1有了rw mask::rw- other::r-- [10:38:33 root@localhost data]#su user5 -c \"echo 123 > f1\" bash: f1: Permission denied [10:38:37 root@localhost data]#usermod -G g1 user5 👈user5加入g1扩展组 [10:39:09 root@localhost data]#id user5 uid=1012(user5) gid=1016(user5) groups=1016(user5),1001(g1) [10:39:11 root@localhost data]# [10:39:15 root@localhost data]#su user5 -c \"echo aaa > f1\" [10:39:28 root@localhost data]#cat f1 aaa 针对user1 同时设置facl的user和group权限，user优先。 [11:03:16 root@localhost data]#setfacl -m u:user1:- f1 [11:03:37 root@localhost data]#su user1 -c 'cat f1' cat: f1: Permission denied [11:03:46 root@localhost data]#su user1 -c 'echo 123 > f1' bash: f1: Permission denied [11:03:49 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user1:--- user:user2:rw- user:user3:-w- group::r-- group:g1:rw- mask::rw- other::r-- [11:03:55 root@localhost data]#id user1 uid=1008(user1) gid=1013(g12) groups=1013(g12),1015(grp001) [11:03:59 root@localhost data]#usermod -G g1 user1 [11:04:08 root@localhost data]#id user1 uid=1008(user1) gid=1013(g12) groups=1013(g12),1001(g1) [11:04:46 root@localhost data]#su user1 -c 'echo 123 > f1' bash: f1: Permission denied [11:05:06 root@localhost data]#su user1 -c 'cat f1' cat: f1: Permission denied 所有文件的权限判定规则：从上往下优先，先中先得 1、先看所有者 2、看针对user的FACL 3、看所属组 4、看针对group的FACL 5、看other 交换机的acl 、linux 路由表 ip roue show (metric小的自动放到上面) ，都是从上到下匹配的， ssg 的policy 也是从上到下匹配，linux的shell脚本、python的主程序都是从上到下，所以此乃天地法则🤮 👇判断所有者优于facl的user [11:12:11 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user1:--- user:user2:rw- user:user3:-w- group::r-- group:g1:rw- mask::rw- other::r-- [11:12:18 root@localhost data]#ll f1 -rw-rw-r--+ 1 root root 7 Jan 29 10:53 f1 [11:12:21 root@localhost data]#chown user1 f1 [11:12:35 root@localhost data]#ll f1 -rw-rw-r--+ 1 user1 root 7 Jan 29 10:53 f1 [11:12:36 root@localhost data]#su user1 -c 'cat f1' aaa bb [11:15:58 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user2:rw- user:user3:-w- group::r-- group:g1:rw- mask::rw- other::r-- 👇判断facl的group 优先于本来的group，这里错啦，group没命令中，自然不起作用啦 [11:16:01 root@localhost data]#su user1 -c 'catf1' bash: catf1: command not found [11:16:10 root@localhost data]#su user1 -c 'echo aa > f1' [11:16:16 root@localhost data]#su user1 -c 'cat f1' aa [11:16:21 root@localhost data]#setfacl -x g:g1 f1 [11:16:48 root@localhost data]#su user1 -c 'cat f1' aa [11:16:54 root@localhost data]#su user1 -c 'echo 11 >> f1' bash: f1: Permission denied [11:16:59 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user2:rw- user:user3:-w- group::r-- mask::rw- other::r-- [11:17:04 root@localhost data]# 删除facl两种方法 -x删一个 -b全删 -R -b dir 递归删除文件夹下所有的acl，据说相当有用 [11:19:58 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user1:--- user:user2:rw- user:user3:-w- group::r-- group:g1:rw- mask::rw- other::r-- [11:28:31 root@localhost data]#setfacl -x u:user1 f1 [11:28:41 root@localhost data]#setfacl -x g:g1 f1 [11:28:46 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user2:rw- user:user3:-w- group::r-- mask::rw- other::r-- [11:28:48 root@localhost data]#setfacl -b f1 [11:28:54 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- group::r-- other::r-- [11:28:57 root@localhost data]# 经典案例 我们复制文件夹的时候总担心权限、所有者、所属组这些信息的丢失，cp -a 可以提供思路 我们修改文件夹的权限比如chmod -R 777 dir/ , 带来的问题是，以后想要回收权限，没有办法了，这个时候facl就提供了很好的思路。 [11:40:55 root@localhost ~]#setfacl -R -m u:user1:r data/ [11:41:02 root@localhost ~]#ll total 12 -rw-r--r--. 1 root root 3 Jan 12 16:53 1 -rw-------. 1 root root 1031 Jan 5 16:52 anaconda-ks.cfg drwxrwxrwx+ 4 root root 61 Jan 29 10:59 data -rw-r--r--. 1 root root 0 Jan 12 16:53 f1 -rw-r--r--. 1 root root 0 Jan 12 16:53 f2 -rw-r--r--. 1 root root 4 Jan 12 17:51 hello.txt [11:41:03 root@localhost ~]#cd data/ [11:41:04 root@localhost data]#ll total 8 drwxr-xr-x+ 2 root root 6 Jan 19 19:57 dir drwxr-xr-x+ 2 user1 g12 6 Jan 19 19:58 dir2 -rw-r--r--+ 1 root root 3 Jan 29 11:16 f1 -rw-r--r--+ 1 user1 g12 0 Jan 19 19:58 f2 -rw-r--r--+ 1 root root 65 Jan 29 10:59 test [11:41:05 root@localhost data]#getfacl f2 # file: f2 # owner: user1 # group: g12 user::rw- user:user1:r-- group::r-- mask::r-- other::r-- [11:41:31 root@localhost ~]#getfacl data # file: data # owner: root # group: root user::rwx user:user1:r-- group::rwx mask::rwx other::rwx [11:41:56 root@localhost ~]#setfacl -R -b data/ [11:42:08 root@localhost ~]# [11:42:09 root@localhost ~]#ll total 12 -rw-r--r--. 1 root root 3 Jan 12 16:53 1 -rw-------. 1 root root 1031 Jan 5 16:52 anaconda-ks.cfg drwxrwxrwx. 4 root root 61 Jan 29 10:59 data -rw-r--r--. 1 root root 0 Jan 12 16:53 f1 -rw-r--r--. 1 root root 0 Jan 12 16:53 f2 -rw-r--r--. 1 root root 4 Jan 12 17:51 hello.txt [11:42:11 root@localhost ~]#ll data/ total 8 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir drwxr-xr-x. 2 user1 g12 6 Jan 19 19:58 dir2 -rw-r--r--. 1 root root 3 Jan 29 11:16 f1 -rw-r--r--. 1 user1 g12 0 Jan 19 19:58 f2 -rw-r--r--. 1 root root 65 Jan 29 10:59 test [11:42:19 root@localhost ~]# 其他用法参考setfacl -h 比如 setfacl -m d:u:ming:rx dir👈意思是dir文件夹下创建的文件默认就带针对ming的rx权限，d设置的默认权限，删除用setfac -k dir来删 setfacl -X file.acl dir👈意思是file.acl里写好g:sales:rw这些facl的明细，这个比较好的。 setfacl -m u:user1:rwX dir👈X是只是针对文件夹设置，不过我用x一i杨的效果。要么是rocky-linux自带的，要吗是版本高的好处，不管。 getfacl file1 | setfacl --set-file=- file2👈参考chmod里的--reference一个效果，就是将f2的权限设置成f1一样的。 FACL里的mask mask就是设置一个最高权限，谁都不能超过 ll可见group的rwx3位现在用来填充mask的值了。 mask默认设置了facl后位rwx，手动修改后getfacl 可见#effective:rw-这种 mask只影响单个人，所有者和other不受影响 [11:59:54 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- group::r-- other::r-- [11:59:57 root@localhost data]#ll total 8 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir drwxr-xr-x. 2 user1 g12 6 Jan 19 19:58 dir2 -rw-r--r--. 1 root root 3 Jan 29 11:16 f1 -rw-r--r--. 1 user1 g12 0 Jan 19 19:58 f2 -rw-r--r--. 1 root root 65 Jan 29 10:59 test [12:00:03 root@localhost data]#setfacl -m u:user1:rw f1 [12:00:25 root@localhost data]#ll f1 -rw-rw-r--+ 1 root root 3 Jan 29 11:16 f1 [12:00:27 root@localhost data]#setfacl -m u:user2:rwx f1 [12:00:42 root@localhost data]#ll total 8 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir drwxr-xr-x. 2 user1 g12 6 Jan 19 19:58 dir2 -rw-rwxr--+ 1 root root 3 Jan 29 11:16 f1 -rw-r--r--. 1 user1 g12 0 Jan 19 19:58 f2 -rw-r--r--. 1 root root 65 Jan 29 10:59 test [12:00:44 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user1:rw- user:user2:rwx group::r-- mask::rwx 👈mask默认值 other::r-- [12:00:57 root@localhost data]#setfacl -m mask::r f1 [12:01:14 root@localhost data]#ll total 8 drwxr-xr-x. 2 root root 6 Jan 19 19:57 dir drwxr-xr-x. 2 user1 g12 6 Jan 19 19:58 dir2 -rw-r--r--+ 1 root root 3 Jan 29 11:16 f1 👈修改mask后group位的3位用来表示mask的3位 -rw-r--r--. 1 user1 g12 0 Jan 19 19:58 f2 -rw-r--r--. 1 root root 65 Jan 29 10:59 test [12:01:15 root@localhost data]#getfacl f1 # file: f1 # owner: root # group: root user::rw- user:user1:rw- #effective:r-- 👈注意mask影响了单个用户的权限上限 user:user2:rwx #effective:r-- group::r-- mask::r-- other::r-- [12:01:20 root@localhost data]# facl的备份 怎么备份和还原 getfacl -R /tmp/dir1 > acl.txt 👈备份到acl.txt setfacl -R -b /tmp/dir1 👈清空下，Centos8 -b一旦用了，组权限清空为---，8的BUG,centos7亲测没问题，放心用，读我这篇的你自己测你的版本啊。 setfacl -R --set-file=acl.txt /tmp/dir1 👈恢复方法1 setfacl --restore acl.txt 👈恢复方法2 getfacl -R /tmp/dir1 👈递归，也就是包含dir1及其下所有文件的facl cp -p 或-a就能备份facl，还有所有者权限等 mv 也支持facl这些的保留 tar不行，tar备份的时候facl就丢了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"7-文本三剑客1_grep和正则表达式/7-文本三剑客1_grep和正则表达式.html":{"url":"7-文本三剑客1_grep和正则表达式/7-文本三剑客1_grep和正则表达式.html","title":"第七章 文本三剑客1_grep和正则表达式","keywords":"","body":"第七章 文本三剑客1_grep和正则表达式 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"7-文本三剑客1_grep和正则表达式/1-常见文本处理工具.html":{"url":"7-文本三剑客1_grep和正则表达式/1-常见文本处理工具.html","title":"第1节 常见文本处理工具","keywords":"","body":"第1节. 常见文本处理工具 cat查看行结束符/行尾 cat -A查看回车、换行、TAB建 👆cat 空行不算行号 cat 压缩连续空行 倒着看tac和rev nl等价于cat -b 文本截取前后 head默认前10行 tail默认后10行 head一行内取前3个字节，密码生成方法2 前面有openssl还有一种，这里就是3中随机数的生成方法了。好像就一个靠谱，其他缺胳膊少腿的。 tail -f和-F跟踪是不同的，删除文件的效果 文件描述符时连接着inode的，删除文件后重新创建同名，其实inode变了。 文件名就是简单的只看名称了 当然文件描述符本身和inode也不是等价的 只要文件名恢复了，tail -F又继续跟踪了 只是理解一下各个用法，不一定这么用 这就是一个取某个网卡IP地址的固定语法咯，可以做成别名来用。 cut列截取 多个空格的压缩成1个边缘cut基于空格 进一步 [16:14:14 root@localhost ~]#df Filesystem 1K-blocks Used Available Use% Mounted on devtmpfs 897812 0 897812 0% /dev tmpfs 916616 0 916616 0% /dev/shm tmpfs 916616 8868 907748 1% /run tmpfs 916616 0 916616 0% /sys/fs/cgroup /dev/mapper/rl-root 17811456 2153364 15658092 13% / /dev/sda1 1038336 198012 840324 20% /boot tmpfs 183320 0 183320 0% /run/user/0 [16:14:16 root@localhost ~]# [16:14:17 root@localhost ~]#df |cut -c48-51 Use 0 0 1 0 13 20 0 [16:14:18 root@localhost ~]#df |cut -c48-51|tr -dc '[0-9\\n] ' 0 0 1 0 13 20 0 [16:14:50 root@localhost ~]#df |cut -c48-51|tr -dc '[0-9\\n]' 0 0 1 0 13 20 0 [16:14:55 root@localhost ~]# 👆这可以作为观察服务器的登入信息 👇看网站访问信息 linux的词汇量？ 起密码的时候，说明你这是一个单词不让你起，凭的就是这个words里的单词了吧 文件内容纵向合并 文件内容横向合并 两个文件的内容合并到一行 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"7-文本三剑客1_grep和正则表达式/2-文本三剑客1_grep和正则.html":{"url":"7-文本三剑客1_grep和正则表达式/2-文本三剑客1_grep和正则.html","title":"第2节 文本三剑客1_grep和正则","keywords":"","body":"第2节. 文本三剑客1_grep和正则 wc统计 显示最长一行的长度（单位字符？5个字符） sort排序 排名部分先后，可以用R 对第3列当作数字进行排序 对IP地址排序， 这个还是要用lambda去排的，比如接口和IP这种g1/0/1,192.16.10.1分段去比较的，这里仅仅是整体比一下，看下效果就好。 sort -u 去掉重复行 uniq删除上下连续的重复行 last -f wtmp就是last一个意思 [17:33:36 root@localhost data]#ll /var/log/*tmp -rw-rw----. 1 root utmp 2688 Jan 17 12:00 /var/log/btmp 👈lastb看的是这个文件，是密码输错了的记录 -rw-rw-r--. 1 root utmp 36480 Jan 29 11:15 /var/log/wtmp 👈last看的就是这个文件，是登入成功的记录 [17:33:41 root@localhost data]# 👆以什么样的用户猜密码的，安全加固的方式 uniqd -d 只显示重复的 uniq 的话如果不前置一个sort排序的话，就只是抓取连续的情况。 找出两个文件的相同行 👆上面的题目有BUG啊，如果a.txt里有两行z，那么就会误判咯。 [17:50:07 root@localhost data]#cat f1 z z a b c [17:50:10 root@localhost data]#cat f2 b c [17:50:11 root@localhost data]#cat f1 f2 | sort |uniq -d b c z [17:50:23 root@localhost data]# 可以这样优化👇，先各自去重后再cat结合再找出重复的就行了。 [17:51:20 root@localhost data]#cat f1 z z a b c [17:51:29 root@localhost data]#cat f2 b c [17:51:49 root@localhost data]#uniq f1 |cat - f2 z a b c b c [17:52:22 root@localhost data]#uniq f1 |cat - f2 | sort | uniq -d 👈这才是正解-d就是重复的 b c [17:52:29 root@localhost data]#uniq f1 |cat - f2 | sort | uniq -u 👈u就是取uniqu不一样的 a z 比较文件 -号代表第一个文件，+代表第二个文件,-号去掉+号加上 👆注意patch -b 选项是为了恢复之前先备份a.txt，因为patch的还原文件时直接将a.txt原文件覆盖掉的。 grep三剑客之一 grep不一定都带颜色，因为root的grep系统默认是别名 grep选项 grep -m匹配N次后停止 grep -i忽略大小写 grep -n命中第几行 grep -c匹配的行数 grep -o 命中多少个单行内多次也算 统计文本出现字符的次数-o出现一次单行列出来，再wc -l计算行数 grep -q 静默输出0找到1没找到0是true保持一惯的linux的真假标准 grep -A或-B或-C还是经常用的，但是cisco的show run | section router ospf显然更优化 此外还有-B -C nmap扫描、关于IP探测要总结一下好几种呢ping呢也是有灰常快速的方法的很赞的，当然ping肯定不可靠的。 👉最好是探测该IP上的几个常用端口，然后才能说这个IP是不是UP。他这个sP就是scan ping，聊胜于无，要用-Pn去扫 这招可以用来梳理IDC或者内网的HOST网段使用情况，选项不靠谱，仅作参考咯。 --- grep -E \"XX|YY|ZZ\"或的关系等价于grep -e xx -e yy -e zz 一样 grep 并且过滤 grep -w单词等价于grep的定界符grep \"\\\" grep -w 或grep \"\\\"的这个单词整体 判断的能力： 👆数字 字母 下划线 是一个单词，- 默认会当作分隔符的 ；同样也算作分隔符了 -w就是查找root单词，而root-er是当做root和er两个单词的。-不会被当做一个整体的。 不支持regex，这是什么需求？👇就是比如. *这玩意不做正则的时候，省的转义了 -F 或者fgrep就挺好，挺好~lizheng~tt tx sf sx grep -f 文件内容去匹配，这个玩意支持regex吗？支持的 用文件过滤文件，文件里也是可以写正则的 说明：以前学习的通配符是匹配文件名的，而grep里的正则是匹配文本内容的。 而且通配符的*和regex的\\，以及通配符的.和regex的.都不太一样。regex的.\\表示所有差点比如换行符？，而且regex的*不能独立存在，然是通配符的*就是自称一体表示所有。 1、regex的 .表示 除了 \\n以外任何一个字符，*表示前面的字符不出现或者出现N次。 2、通配符的.就表示. 而通配符的*表示除了.开头的文件名，其他都可以匹配(大概吧哈哈，有的文章说明什么路径中带/的，我就纳闷了通配符是抓文件名的，你文件名中能带/这玩意？呵呵所以这块理解的差不多得了，够了)，包括文件名中带.的(只要不是.开头的就好) 3、还有regex和通配符的其他区别，比如[a-z]在通配符中表示小写的a-z和大写的A-Y不到Z；regex显然没这么奇葩。 好，下面是工作中遇到的补充👇 -l, --files-with-matches Suppress normal output; instead print the name of each input file from which output would normally have been printed. The scanning will stop on the first match. 就是去重的效果咯，我只要知道在哪个文件里，不需要知道一个文件里出现了几次，所以加上-l。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"7-文本三剑客1_grep和正则表达式/3-基本和扩展正则.html":{"url":"7-文本三剑客1_grep和正则表达式/3-基本和扩展正则.html","title":"第3节 基本和扩展正则","keywords":"","body":"第3节 基本和扩展正则 举个例子 [^ming] 不是m、i、n、g的字符 [0-9] [[:lower:]] 任意一个小写字母 . 任意一个非\\n的字符 * 前一个字符出现0次或N次 a\\? a出现了0次或1次，可有可无的表达方式 a\\+ a出现1次及以上 a\\{10\\} a出现了10次 a\\{10,20\\} 10到20次 a\\{,20\\} 20次以下 a\\{10,\\} 10次以上 .* 所有但是不能匹配\\n，所以你用.*抓全文只能抓到换行符就结束了， .*等价于通配符里的* 位置锚定 [16:47:46 root@host1 ~]#grep -v \"^#\" /etc/fstab /dev/mapper/centos_host1-root / xfs defaults 0 0 UUID=e36eac36-1940-4883-8c19-a05f6b4bb4a6 /boot xfs defaults 0 0 /dev/mapper/centos_host1-swap swap swap defaults 0 0 [16:47:55 root@host1 ~]# [16:47:56 root@host1 ~]# [16:47:56 root@host1 ~]#grep ^[^#] /etc/fstab /dev/mapper/centos_host1-root / xfs defaults 0 0 UUID=e36eac36-1940-4883-8c19-a05f6b4bb4a6 /boot xfs defaults 0 0 /dev/mapper/centos_host1-swap swap swap defaults 0 0 [16:48:07 root@host1 ~]# 上图注意下，grep -v \"^#\" 和 grep \"^[^#]\"的区别，明显第二种也过滤空行。因为[^#]里面至少的又一个字符的。 这种[^#]写法是有问题的，不推荐这么写。 搜索shutdown行尾👆 [[:space:]]他不仅仅抓空格，还抓TAB。当然上图其实都是空格，因为做了4空格等1tab的设置，取消后再验证下 还是[[:space:]]能够抓到空格和TAB的。没问题。注意一个细节，cat -A 是能够区分TAB和空格的，但是如果你优化了tab=4空格，那么就自然都是空格了。-A看到的都是空格了就。 搜索空行👆 单词：在系统中，数字字母下划线都算单词的范畴。此外都不算单词。 空行是^$,空白行^[[:space:]]*$ 注意，写的思路： :%s///g :%s/(abc)(123)/1eradmin2/g :%s/(abc)(123)/\\1eradmin\\2/g 👆这个叫后向引用，在后面的sed搜索替代有关 nginx里也有后向引用的 [17:30:32 root@host1 ~]#echo rootrootxxroot |grep -E \"(root){2}\" rootrootxxroot 👆抓两连续的root 练习 4题 cat /etc/passwd |grep -E \"[0-9]{2,3}\" -o | grep -Ev ^0 👈这是错误的，因为4位数也会搜出来的比如65534这个数字也会当作655和34两个匹配结果的，需要词尾锚定 cat /etc/passwd |grep -E \"/\" -o | grep -Ev ^0 注意该方法由于是:xx:所以对于后面的数字是不匹配的。 方法一肯定只能是抓出第一个段数字， 方法二可以匹配所有数字 --- 词尾铆钉的必要性👇 扩展正则 grep -E还是有一些还是需要加\\的。 nginx的后向引用举例 这是nginx里的rewrite替换的正则写法 windows里也有正则 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"8-shell脚本编程基础/8-shell脚本编程基础.html":{"url":"8-shell脚本编程基础/8-shell脚本编程基础.html","title":"第八章 shell脚本编程基础","keywords":"","body":"第八章 shell脚本编程基础 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:02 "},"8-shell脚本编程基础/1-shell编程脚本基础.html":{"url":"8-shell脚本编程基础/1-shell编程脚本基础.html","title":"第1节 shell编程脚本基础","keywords":"","body":"第1节. shell编程脚本基础 编程基础 shell和py都是边解释边执行 gcc是个编译软件，可以把高级语言转换成机器代码 gcc就是编译器 在执行的时候有python解释器，会读到内存里翻译成机器码了。但是这个机器码是在内存里的，不是个放在硬盘里的文件。它是边执行边翻译。 编程基本概念 shell脚本基础 创建shell脚本 脚本规范 脚本的基本结构 vim的初始化 脚本执行的方法1:bash xxx 方法2，source xxx和. xxxx 方法3：添加执行权限 👆直接运行脚本，就是外部命令了，是要到PATH变量里找路径的，而当前目录是/root并不在PATH变量里，所以找不到。 添加到PATH变量 👆其实也可以用ln -s 软连接来实现path变量的 但是如果你以后很多脚本都统一放到/data/scipts下的话，还是加/data/scripts为PATH变量好一点 脚本运行方法4：传递给bash命令 evn.sh，只要是sh后缀就行了。 例子，写个脚本创建用户 让其口令立即过期 chage -d 0 test等价于passwd -e tezt都是修改date of last password chage这个值为0，意思就是登入后强制修改密码 语法错误检查方法 两种语法检查方法 删除if那行后 再次执行就OK了 举个例子 之前接触过%s/xx/yy/g，现在又看到了.,$s/XX/yy/g .点表示当前行号,逗号是一直到整个文件最后一行 u撤销后，改成 引号替换一下 变量 变量代表着内存空间 内存中的一个地址块放了magedu，而name就表示地址值。于是就是name中存放了magedu。 变量，值可变化，当然也有不可变 python和shell都不需要事先申明变量 变量起名规范 特殊变量 👆变量的正儿八经的写法，很重要 如果此时Y的值变成了30，问X的值是多少，这个在PYTHON里面叫变量赋值，如果是列表、字典是需要.copy()的 变量取消 上图替换语法存在错误 不用加g，%s/xx/yy/g，的g如果是每行只有一个不需要加g全局 一般脚本结束了变量也就没了。不过还是建议删掉。 把命令放到变量里 第一题答案就有了 cp -a 的a等价于-dR 文件夹不存在cp会直接创建的 第二题答案 nl 和 cat -b一个意思，不过不能列出空行行号 环境变量的查看 env和printenv是等价的 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"8-shell脚本编程基础/2-shell编程特殊位置变量.html":{"url":"8-shell脚本编程基础/2-shell编程特殊位置变量.html","title":"第2节 shell编程特殊位置变量","keywords":"","body":"第2节. shell编程特殊位置变量 初识变量生效范围 1、tty再开一个终端就没了 2、退出后就没了 3、再次运行/bin/bash后就没了👇 [23:15:39 root@host1 ~]#NAME=ming [23:15:43 root@host1 ~]#echo $NAME ming [23:16:21 root@host1 ~]#pstree |grep -A 2 bash |-sshd---sshd---bash-+-grep | `-pstree |-systemd-journal [23:16:34 root@host1 ~]#pstree -p |grep -A 2 bash |-sshd(1191)---sshd(2132)---bash(2140)-+-grep(2207) | `-pstree(2206) |-systemd-journal(502) [23:17:10 root@host1 ~]#/bin/bash [23:17:17 root@host1 ~]#pstree -p |grep -A 2 bash |-sshd(1191)---sshd(2132)---bash(2140)---bash(2208)-+-grep(2239) | `-pstree(2238) |-systemd-journal(502) [23:17:29 root@host1 ~]#echo $NAME [23:17:36 root@host1 ~]#echo $BASHPID 2208 [23:17:49 root@host1 ~]#exit exit [23:17:52 root@host1 ~]#echo $BASHPID 2140 [23:17:56 root@host1 ~]#echo $NAME ming [23:17:58 root@host1 ~]# 父进程的NAME变量并没有传给子进程。 每个账号都有个shell类型，比如👇，表示root账号一登入就会自动去运行/bin/bash [23:23:25 root@host1 ~]#getent passwd root root:x:0:0:root:/root:/bin/bash 父进程的NAME变量并没有传给子进程，如果需要传进去，就要使用环境变量。 其上👆这句话也不完全对，换个方式就能将局部变量传进去了，比如小括号 环境变量 查看上级父进程编号 查看环境变量 举例EDITOR=vim 默认编辑器是 vipw是调用的EDITOR编辑器这个变量，而EDITOR默认复制应该就是vi，如下： 现在将EDITOR改成vim，再看，发现还是黑底白字，只有将EDITOR提升为环境变量，才会出彩，也就是vipw调用的是环境变量EDITOR里的值，普通变量没有关系。 unset name普通变量和环境变量都适用 环境变量可以由父进程传给子进程，但是不能从子进程传给父进程，也就是说子进程里修改的环境变量只在子进程里有效，退出子进程后，在父进程中还是原来的值；但是子进程再次赋值可以影响身后的子进程 [23:49:22 root@host1 ~]#export name=ming [23:49:42 root@host1 ~]#echo $name ming [23:49:46 root@host1 ~]#bash [23:49:51 root@host1 ~]#echo $name ming [23:49:55 root@host1 ~]#name=yi [23:50:11 root@host1 ~]#echo $name yi [23:50:13 root@host1 ~]#export name=yi 👈多余动作，name早就是环境变量了，所以无需再次申明 [23:50:24 root@host1 ~]#echo $name yi [23:50:28 root@host1 ~]#exit exit [23:50:29 root@host1 ~]#echo $name ming [23:50:31 root@host1 ~]# shell的嵌套深度 上一次执行的命令 常量就是只读变量 可能父进程和子进程配合使用是有用的， 小括号就是开启子shell，一运行完，子shell就退出了 注意只针对内部命令和变量赋值 证明小括号就是开启了子shell 上图👆此时可以再开一个窗口pstree -p看到确实8542就是7703的子进程 大括号和小括号是不同的 上图说明，小括号开启子进程，大括号不开启子进程。 位置变量 这样的话，就可以将输入的固定位置的参数变量传到脚本里面。 对比下$*和$@ 引号引起来才是一个整体，不能去掉。 练习，rm=rm.sh，rm.sh里写mv f1.txt /tmp/当前日期精确到秒，开局设置里可以用 用mv替代rm 如果要使用原来的rm，则\\rm就行了，不过要自带-i了 其他注意事项 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:02 "},"8-shell脚本编程基础/3-算术逻辑运算和条件测试.html":{"url":"8-shell脚本编程基础/3-算术逻辑运算和条件测试.html","title":"第3节 算术逻辑运算和条件测试","keywords":"","body":"第3节. 算术逻辑运算和条件测试 退出状态 [00:16:02 root@host1 ~]#grep aaa /etc/passwd #找到找不到结果自然不同 [03:44:30 root@host1 ~]#echo $? 1 [03:44:35 root@host1 ~]#grep -q root /etc/passwd [03:44:59 root@host1 ~]#echo $? 0 [03:45:02 root@host1 ~]# 错误对照表：https://www.cnblogs.com/x_wukong/p/5148237.html 一个脚本里2条命令，最后一条执行成功了，返回的$0就是0 算术运算 图中的echo $[$RANDOM%50]里面的$可以去掉的，因为$[]里面会认为RANDOM就是变量 方法一：let 方法二：中括号 方法三：2个小括号 方法四：expr 方法五：declare申明强制运算 65取模是0-64 如果是0-65的随机数呢？哈哈，不好弄了吧。 /66啊，不就行了哦，笨哦。 颜色的取值范围是31-37，可以用RANDOM随机数产生，用7取模范围就是0-6，+31就可以了。 ++i和i++ let id+=5 就是 id=id+5 逻辑运算与或非 python里也学过与或非，来了解一下，哈哈哈 and是与，&也是与，两者截然不同，貌似相同又。举例 再来 再看 懂了吧~ 1、and和or是基于运算符两边的整体值来算的；而&又叫做位与是将运算符两边化作0101后再进行位与的哈哈，我在用名称解释名称咯，额。 2、然后and和or里的99 or 100 和99 and 100也挺有意思的。一句话做人呐or就行了，做研究呐可能需要and。 or就是已经是true的情况下就不会再继续比了。反正或的话，结果都是true。 and就是当前如果是真，就一定要看到最后一个元素，万一他是假，就全盘就是假了，所以要那最后一个元素。A(true) and B(true)也就取B了。 3、一句话，or和and是真假运算--基于表达式两边的整体，而&和|是二进制的与或运算--基于表达式两边的数值的二进制单个位来算的。 这里的短路与的真假，不要简单按上图0和1，去理解，0啊他这个图是假的意思。但是linux，true你去echo $?会发现是0，所以0代表的是真。所以这种运算是真假运算，不要用0和1区理解，除非你定死了01和真假的一一对应。 当然也可以不要理解短路与，而直接理解第一个cmd1执行ok了再执行cmd2：cmd1 && cmd2 true和false就是命令，专门产生真假的 还有yes就是专门产生y，不停的 其实不是y，而是yes后面的参数 两个变量值互换 方法二就是上图的A^B=C，C^A=B，C^B=A x=$[x^y]就是得出了中间值C赋值给了x，x此时就是中间值。然后拿中间值x去和y异或得到的就是原来的x，将x赋值给y。此时y里的值就变成了x。再拿中间值x去和现在的y--其实是原来的x异或就得到原来的y将此值赋给x，这样x里的值就变成了原来的y。 短路与 短路或 true是真，echo true本身也是真，同时打印出true，此时两个都是真，结果就是真，后面的就不执行了。 false是假，只要是假都是假，所以就不会执行后面的 echo true。 然后不会执行&&后面的内容，但是&&的结果还是假，所以就会执行||后面的内容，于是打印出false了就。 test比较表达式 这个和if else还不是一样的，因为A && B || C，不是if A成立就执行B，A不成立就执行C这么简单，还多一个A成立执行B，B执行失败，那么||前面的整体就是假，于是还是会执行C的。 除了字符串的比较，还有数字比较 printf是是格式化字符串的。类似python里的format 中括号代替test test $x -gt $y 案例 1、如何用pycharm使用这个项目 https://ziwei.pro/quick-start.html 2、pycharm里跑nodejs https://blog.csdn.net/pyzzd/article/details/108478746 ①安装nodejs，走官网 ②pycharm安装nodejs插件 3、nodes要高版本，才能运行ES6 Module nodes升上去了，看看我的gitbook是否正常，哈哈，好像之前node不能用高版本来着。。。 可以用，出现，就是有什么URL格式警告，暂时还能用 4、然后pycharm里用js npm install 要在项目目录下install，否则找不到库 这个报错，看起来是就该文件后缀为mjs，其实没必要，改了一样会报错，其实是npm安装的包找不到 在项目根目录下，新建package.json，然后同样在项目根目录下调用cmd运行npm install xxx ；这两部操作pycharm只需要新建package.json，pycharm就会自动给你安装了 编辑好package.json后👇一般就会自动弹窗让你安装 如果不弹出来，也可以右键安装 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 11:19:34 "},"8-shell脚本编程基础/4-条件判断.html":{"url":"8-shell脚本编程基础/4-条件判断.html","title":"第4节 条件判断","keywords":"","body":"第4节. 条件判断 判断是否为空 不是的，空格当然不为空，但是[ ]综括号的写法 里面空格再多就是不算的。你引号来表示空格试试 上图是help test出来的，内部命令的帮助用法help xx 0自然也不是空 文件夹存在就不创建的方法 存在就不会创建了。 本身touch就是这样的效果了，不过touch一个存在的文件，虽然内容不会清掉，但是3个时间统统刷新。如下 [03:22:10 root@host1 ~]#stat f1 File: ‘f1’ Size: 9 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 71287069 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:admin_home_t:s0 Access: 2022-02-02 18:49:12.966012106 +0800 Modify: 2022-02-02 18:49:11.773012078 +0800 Change: 2022-02-02 18:49:11.777012078 +0800 Birth: - [03:22:14 root@host1 ~]#cat f1 dd [03:22:17 root@host1 ~]#touch f1 [03:22:21 root@host1 ~]#ll f1 -rw-r--r--. 1 root root 9 Feb 6 03:22 f1 [03:22:24 root@host1 ~]#stat f1 File: ‘f1’ Size: 9 Blocks: 8 IO Block: 4096 regular file Device: fd00h/64768d Inode: 71287069 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Context: unconfined_u:object_r:admin_home_t:s0 Access: 2022-02-06 03:22:21.871014487 +0800 Modify: 2022-02-06 03:22:21.871014487 +0800 Change: 2022-02-06 03:22:21.871014487 +0800 Birth: - [03:22:26 root@host1 ~]#cat f1 dd -e -a后面不跟文件，单单一个-e或者-a就不认为是选项了，而是当作字符了，有字符就是非空了👆👇所以使用-e这种一定要后面用双引号引起来。 所以加上 双引号 可以避免严重逻辑错误， 判断是否位数字，可以用正则表达式👇 [03:41:04 root@host1 ~]#[ \"11\" -gt 10 ] && echo true true [03:41:09 root@host1 ~]#[ '11' -gt 10 ] && echo true true [03:41:16 root@host1 ~]#[ 'xx' -gt 10 ] && echo true -bash: [: xx: integer expression expected [03:41:22 root@host1 ~]# 说明linux里的引号，并不会改变里面值的类型，这里11引起来还是数字，不会变成字符串。这话不对， 其实是shell是动态编译语言--就是类型是随时改变的。 双综括号里面支持正则，需要这么写： 纯数字的判断👆 还是需要补上双引号👆，虽然不影响结果，一般在[]里变量都是推荐加上双引号。 判断是否位.sh后缀 判断合法IP 判断是否是一个合法IP的办法：其实还可以进一步到A B C 类地址以及私网地址。 下图第三行才是正解，没有^和$就是包含了。 注意上图命令是转行了 reg嵌入到字符串表达式里shell的写法。 并且的关系-方法1 [ xxx -a yyy] 并且的写法，不过它这里不支持正则了，正则得用短路与，不过不是普遍适用的，可能还需要第二种方式： 并且的关系-方法2 判断的是实际上的权限，不是表面上的文件权限。上图是root执行的命令，所以就是可读可写的。 但是执行权限不同，root也是需要文件的执行权限的。 补充个点，f1.sh要执行，用户要有f1.sh的r读权限，然后再/bin/bash f1.sh就能执行了，如果连f1.sh的读权限都没有，必然无法执行的。 [04:52:19 root@host1 ~]#su user1 [04:52:23 user1@host1 root]$bash /data/f1 bash: /data/f1: Permission denied [04:52:27 user1@host1 root]$ll /data/f1 ----------. 1 root root 23 Feb 6 04:49 /data/f1 [04:52:30 user1@host1 root]$exit exit [04:52:58 root@host1 ~]#chmod 444 /data/f1 [04:53:09 root@host1 ~]#ll /data/f1 -r--r--r--. 1 root root 23 Feb 6 04:49 /data/f1 [04:53:11 root@host1 ~]#su user1 [04:53:15 user1@host1 root]$bash /data/f1 xxx 需求： 写个脚本user10.sh创建用户，考虑用户存在以及user10.sh后没有跟参数的问题。 上图是第一版有问题啊不少 这版是OK了 这是对应的结果 可能$1为空要改一下，改成S# 好理解一下，一个参数都没有不就是$1为空嘛。对不对，所以上面无需修改。 但是还是有问题 添加花括号： 现在应该OK了 可以个屁，（ ）小括号是子shell，exit10是退出的子shell。 优化一下将Changing passowd for user tom和passwd：xxx隐藏。 如果你干了这事：将chmod的执行权限弄没了 利用facl修复权限 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:02 "},"8-shell脚本编程基础/5-算术运算和read.html":{"url":"8-shell脚本编程基础/5-算术运算和read.html","title":"第5节 算术运算和read","keywords":"","body":"第5节. 算术运算和read && || 和 || &&先后效果不一样 = 是比字符串 -eq是比整数的，小数不行。 上图的两个等号，一般一个就行了，双综括号里可以用两个等号，然后双综括号里一般用=~正则表达式。 read read varXX unset varXX 这两个后面跟的都是变量名，不需要加$xxx这样。就是变量了。 优化不换行 echo 不换行 再次优化，read的本身就自带提示语句 写个脚本实现鸡兔同笼算法 read一下赋值多个 失败案例 man bash可见 管道符后面是一个子进程，所以要括起来，你用小括号就是子进程后面再接一个子进程了。 花括号就是管道符-子进程后面直接一个整体。 总之作为一个整体就行了。 证明管道符确实开启了子进程 先来一个还阔以但是有点不太推荐的解法，因为短路与或用的太多了 if条件判断 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:02 "},"8-shell脚本编程基础/6-脚本条件分支和安全.html":{"url":"8-shell脚本编程基础/6-脚本条件分支和安全.html","title":"第6节 脚本条件分支和安全","keywords":"","body":"第6节. 脚本条件分支和安全 type xxx一般可见就是build in 这种内部命令或者外部命令或者alias，现在忽然发现type if不一样的显示 [17:19:51 root@host1 ~]#type echo echo is a shell builtin [17:19:59 root@host1 ~]#type help help is a shell builtin [17:20:07 root@host1 ~]#type ls ls is aliased to `ls --color=auto' [17:20:09 root@host1 ~]#type cp cp is aliased to `cp -i' [17:20:10 root@host1 ~]#type date date is /usr/bin/date [17:20:13 root@host1 ~]#type if if is a shell keyword 👈不能独立作为命令，是shell的关键字 [17:20:15 root@host1 ~]# 老王说得好，世界上最远的距离就是，一个在if下，另一个在else里。 if的shell格式 有个问题啊，上面的exit没有意义。下面的都是一样效果，# 怎么没有意义，那会的脑子是没带嘛我！ 有啥意义 你告诉我呢。我告诉你个二五仔，fi下面要是还有东西，你不加exit试试。要做靓仔啊，不要做二五仔啊~ 例子：ping一个主机通就算了，不通看下是否处于维护状态(维护的机器一般规范的话是放到一个文件里记着的)，如果不在维护 则认为机器是down的。 if不适合的情况 此时就需要case 👆上图注意关键字：变量引用。变量引用和变量是两码事，变量引用是要加$的，就是说case 和read 不同，read后面是直接写NAME就表示变量了，而case得写$NAME，引用一下。 注意：PAT1)是通配符，不是正则！ 变量引用和变量是两码事 if开头，fi结尾，case开头esac结尾。 下面是一些补充 上图的小括号是什么鬼？！，上图还差一个小括号没讲，小括号的优先级最高。 各种符号的优先级见👆上图： 小括号的分组优先级最高 1、命令行里先拆成单词， 2、然后看单词里有没有别名，别名要展开 3、然后看花括号，也要展开，{1..10}这种 4、如果有~表示家目录 5、然后再看$()和``表示里面放的命令 6、因为5所以再次将命令行拆成单词 7、展开通配符命令的文件名 8、接着再重定向 9、最后再运行命令 转义 [11:18:16 root@localhost ~]#echo \"ls\" 👈双引号防止扩展，就是转义的意思 ls 上图就是说双引号也可以转义，但是以下几个情况转不了。 [11:23:13 root@localhost ~]#echo \"$PATH\" /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin [11:23:20 root@localhost ~]# [11:23:24 root@localhost ~]#echo \"`ls`\" 1 anaconda-ks.cfg data f1 f2 hello.txt [11:23:50 root@localhost ~]#\"!1020\" \"ls\" 1 anaconda-ks.cfg data f1 f2 hello.txt [11:29:23 root@localhost ~]#echo \"\\\\n\" \\n [11:29:25 root@localhost ~]#echo '\\\\n' \\\\n [11:29:28 root@localhost ~]# 环境配置文件等到底放哪里 “个人配置”只影响家目录的这个用户，“全局” 所有用户生效。 交互登入就是：xshell或者直接终端登入，su - 完全切换是交互式登入。 非交互登入：su 这种不完全切换就是非交互式登入；图新界面里右键打开终端也是非交互登入；脚本登入； 👉非交互方式，看的配置文件就少了两个，所以推荐放到/etc/profile.d/*.sh这样，你脚本上来也是OK的。不过一般脚本上来也不太需要这些环境配置吧，比如vim 空格 退出不清屏 PS1颜色等脚本上来也不需要的啊。 演示1： [11:45:23 root@localhost ~]#vim /etc/profile 👈最后一行添加echo xxxxyyyy [11:46:01 root@localhost ~]# [11:46:02 root@localhost ~]# [11:46:02 root@localhost ~]#su - user1 👈完全切换的交互登入 Last login: Mon Feb 7 11:45:20 CST 2022 on pts/0 xxxxyyyy [11:46:09 user1@localhost ~]$exit logout [11:46:12 root@localhost ~]#su user1 👈不完全切换的非交互登入 [11:46:14 user1@localhost root]$exit exit [11:46:15 root@localhost ~]# 演示2： 👆仔细看上图，分析一下到底是~/.bash_profile优先还是~/.bashrc优先，对吧，脑经多走两步就出来了。.bash_profile是加载了.bashrc的，如果在fi xxx后面重新设置相同的变量，则就是.bash_profiile优先，如果是在if xxx之前设置，肯定是.bashrc优先嘛，所以上面的PPT也不一定的哦，同理FACL的优先级，owner肯定也没有FACL针对owner的优先。这些不要学的太细，树干+枝叶+框架比较合理。 下图👇是su - 完全切换 这是属于交互方式的真正原因，而不是什么终端，他这个终端明显是GUI里右键调出来--属于非交互式，但是又用了su - 所以最终还是交互式的。 profile是配置文件的意思 bashrc是bash和rc，bash是shell类型，rc是run config，run bash运行的时候的对应的配置文件。run command profile和bashrc分工不是很明确 一般认为profile用来定义环境变量和运行命令或脚本 bashrc用来定义别名和函数还有本地变量 profile和bashrc修改后生效的方法 其实就是把profie当作脚本执行一下 1、加执行权限 2、bash xxx 3、cat xxx | bash 这种有点问题 4、source xxx或. xxx sleep观察source和bash的区别 在文件里跑个slepp 100看看当前的bash，source和bash不同👇 而bash bash会开启子进程，一般运行脚本都会开启子进程。否则可能会影响当前变量的值。 source(.)运行脚本不推荐哈，否则👇；配置文件恰恰建议用source(.) 所以一般脚本不用source。而一般配置config文件就是要用source，因为配置文件就是希望改变当前环境的。 退出的时候执行点东西 set命令相关 $_是上条命令最后一个参数 $-又是啥 插入题外话 关闭**VIM后，屏幕唉显示之前的VIM里的内容：** 在.vimrc文件里加上配置语句： 在.vimrc中设置set t_ti= t_te= 方法二 回到原题 [13:56:43 root@localhost ~]#echo ${-#*i} 👈好神奇的表达式，意思就是$-的值 从左到右 看到i，就连i一起截除掉。其实搜索也没搜到，然后一想换个变量用$-试试，一下子就知道答案了。 mBHs [13:56:46 root@localhost ~]# 然后himBHs的s是这个意思 https://unix.stackexchange.com/questions/329682/what-is-an-s-inside 其他补充 解释下unmask为什么root时022，普通账户时002 uid大于199，并且 gid=uid，则umask=002，否则umask=022 同理 脚本安全 当使用一个没有定义的变量的时候，直接报错。 问题来了，如果上面的脚本写成如下错误 变量写错，$DIR为空，直接就灾难性的把根删了。 将上图略微修改一下（rm -rf $DIR/*.txt），然后👇测试结果如下： 如何避免 上图👆问题大了-如果没有set -u的话就是$DIR不存在直接把/根下面都删了，删库的100种方法你又学废了一种，恭喜恭喜。 还有一个 说明脚本也是在子进程里跑的。 脚本错误就不执行了的方法，因为不是语法错误时会继续执行的。 虽然出错了，但是还是继续执行了 处理方法如下 整一个这个还是不错的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:02 "},"9-文本查找和压缩/9-文本查找和压缩.html":{"url":"9-文本查找和压缩/9-文本查找和压缩.html","title":"第九章 文本查找和压缩","keywords":"","body":"第九章 文本查找和压缩 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:03 "},"9-文本查找和压缩/1-文件查找.html":{"url":"9-文本查找和压缩/1-文件查找.html","title":"第1节 文件查找","keywords":"","body":"第1节. 文件查找 章节目录 grep是过滤文本的行 cut是过滤文本的列 这里是磁盘上搜索文件 locate找文件非常快，依赖于mlocate.db 用updatedb创建/var/lib/mlocate/mlocate.db文件。如果工作中磁盘文件很多，就会占用磁盘IO，瞬间飙高，导致业务被波及。所以需要操作窗口。 默认模糊搜索 👇要updatedb(不能随便用，生产小心)更新一下数据库，就能利用locate查找了 locate支持基本正则 👆基本正则写起来还是比较麻烦，上图。 [15:27:53 root@localhost ~]#find / -name passwd |grep -Ei \"^/[a-z]+/passwd$\" /etc/passwd /Etc/passwd /a/passwd /A/passwd /Z/passwd /z/passwd [15:28:17 root@localhost ~]#find / -name passwd |grep -Ei \"^/[A-Z]+/passwd$\" /etc/passwd /Etc/passwd /a/passwd /A/passwd /Z/passwd /z/passwd [15:28:51 root@localhost ~]#find / -name passwd |grep -E \"^/[A-Z]+/passwd$\" /A/passwd /Z/passwd [15:28:57 root@localhost ~]#find / -name passwd |grep -E \"^/[a-z]+/passwd$\" /etc/passwd /a/passwd /z/passwd [15:29:03 root@localhost ~]#find / -name passwd |grep -E \"^/[a-Z]+/passwd$\" /etc/passwd /Etc/passwd /a/passwd /A/passwd /Z/passwd /z/passwd [15:29:09 root@localhost ~]#find / -name passwd |grep -E \"^/[A-z]+/passwd$\" grep: Invalid range end [15:29:17 root@localhost ~]#find / -name passwd |grep -E \"^/[A-z]+/passwd$\" grep: Invalid range end [15:29:22 root@localhost ~]# 👆通过上面的实验发现find后面的[A-Z][a-z][a-Z]这样写都有其特定的意义，但是没有[A-z]的写法。这点和通配符不同 [15:37:01 root@localhost ~]#locate -r \"^/[a-z]+/passwd$\" [15:37:06 root@localhost ~]#locate -r \"^/[A-Z]+/passwd$\" [15:37:10 root@localhost ~]#find / -name passwd |grep -E \"^/[a-z]+/passwd$\" /etc/passwd /a/passwd /z/passwd 👆可见locate -r只支持正则，所以+这种扩展正则表达式 是不支持的。 [15:38:24 root@localhost ~]#locate -r \"^/[A-Z]*/passwd$\" /A/passwd /Z/passwd [15:39:57 root@localhost ~]#locate -r \"^/[a-Z]*/passwd$\" /A/passwd /Etc/passwd /Z/passwd /a/passwd /etc/passwd /z/passwd [15:40:01 root@localhost ~]#locate -r \"^/[[:alnum:]]*/passwd$\" /A/passwd /Etc/passwd /Z/passwd /a/passwd /etc/passwd /z/passwd [15:40:45 root@localhost ~]#find / -name passwd |grep -E \"^/[[:alnum:]]+/passwd$\" /etc/passwd /Etc/passwd /a/passwd /A/passwd /Z/passwd /z/passwd find 实时查找，其实也可以利用xargs变得快些，同样占CPU，locate就是在机器上APP服务维护阶段执行updatedb会占CPU [15:36:15 root@pyConsole /]#time `ls --hide=proc | xargs -i -P 0 find /{} -name \"*i*\"` -bash: /bin: Is a directory real 0m1.101s user 0m1.206s sys 0m0.336s [15:36:17 root@pyConsole /]#time `ls --hide=proc | xargs -i -P 0 find /{} -name \"*i*\"` -bash: /bin: Is a directory real 0m1.391s user 0m1.496s sys 0m0.356s [15:36:22 root@pyConsole /]#time `ls --hide=proc | xargs -i -P 0 find /{} -name \"*i*\"` -bash: /bin: Is a directory real 0m1.123s user 0m1.186s sys 0m0.363s [15:36:24 root@pyConsole /]#time `find / -name \"*i*\"` find: ‘/proc/4065730’: No such file or directory find: ‘/proc/4065733’: No such file or directory -bash: /boot/efi: Is a directory real 0m1.677s user 0m1.380s sys 0m0.396s [15:36:28 root@pyConsole /]#time `find / -name \"*i*\"` -bash: /boot/efi: Is a directory real 0m1.721s user 0m1.400s sys 0m0.399s [15:36:31 root@pyConsole /]#time `find / -name \"*i*\"` -bash: /boot/efi: Is a directory real 0m1.739s user 0m1.407s sys 0m0.404s [15:36:34 root@pyConsole /]# locate和find一样存在一些普通用户没有某些文件夹权限，也会搜不到。 find默认就是递归查找，也就是会进到子目录里继续查找。 [处理动作]比较实用，搜出来删除之类。 find 指定搜索深度 [16:08:08 root@pyConsole /]#find / -name \"*i*\" -maxdepth 1 find: warning: you have specified the -maxdepth option after a non-option argument -name, but options are not positional (-maxdepth affects tests specified before it as well as those specified after it). Please specify options before other arguments. /bin /sbin /lib /lib64 /media /.bash_history /switch 更精准一些的搜索方法 [16:10:06 root@pyConsole /]#find / -name \"passwd\" /etc/pam.d/passwd /etc/passwd /var/lib/sss/mc/passwd /usr/bin/passwd /usr/share/licenses/passwd /usr/share/doc/passwd /usr/share/bash-completion/completions/passwd [16:10:11 root@pyConsole /]# [16:10:13 root@pyConsole /]# [16:10:13 root@pyConsole /]#find / -name \"passwd\" -maxdepth 2 find: warning: you have specified the -maxdepth option after a non-option argument -name, but options are not positional (-maxdepth affects tests specified before it as well as those specified after it). Please specify options before other arguments. /etc/passwd [16:10:18 root@pyConsole /]#find -maxdepth 2 / -name passwd find: paths must precede expression: / Usage: find [-H] [-L] [-P] [-Olevel] [-D help|tree|search|stat|rates|opt|exec] [path...] [expression] [16:10:59 root@pyConsole /]#find / -maxdepth 2 -name passwd /etc/passwd [16:11:11 root@pyConsole /]#find / -maxdepth 2 -mindepth 2 -name passwd /etc/passwd [16:11:22 root@pyConsole /]#find / -name passwd /etc/pam.d/passwd /etc/passwd /var/lib/sss/mc/passwd /usr/bin/passwd /usr/share/licenses/passwd /usr/share/doc/passwd /usr/share/bash-completion/completions/passwd [16:11:29 root@pyConsole /]# [16:11:32 root@pyConsole /]#find / -maxdepth 3 -mindepth 3 -name passwd /etc/pam.d/passwd /usr/bin/passwd [16:11:36 root@pyConsole /]# find -depth选项,这个有什么应用场景？想不出来 默认是先搜索目录本身，再进到每个目录再去搜索。 现在就是先处理文件，再处理文件夹。应用场景呢？ find自带的是通配符不是正则 find -iname 不分大小写 [16:32:18 root@localhost ~]#find / -name etc /run/initramfs/state/etc /etc /usr/share/factory/etc /usr/local/etc [16:32:25 root@localhost ~]#find / -iname etc 👈忽略大小写的方法，这也是个总结点，以后用▲来表示总结线索吧，▲忽略大小写2 /run/initramfs/state/etc /etc /root/ETc /usr/share/zoneinfo/Etc /usr/share/zoneinfo/posix/Etc /usr/share/zoneinfo/right/Etc /usr/share/factory/etc /usr/local/etc /Etc [16:32:32 root@localhost ~]# find -inum 根据inode编号来搜 [16:54:36 root@localhost ~]#find / -inum 70 /sys/kernel/tracing/events/raw_syscalls/sys_exit/filter /sys/kernel/debug/tracing/events/raw_syscalls/sys_exit/filter /sys/fs/cgroup/devices/system.slice/sys-kernel-tracing.mount/tasks /sys/fs/cgroup/memory/system.slice/system-systemd\\x2dhibernate\\x2dresume.slice/tasks /sys/fs/cgroup/pids/system.slice/systemd-journald-dev-log.socket/pids.current /sys/bus/memory/drivers_autoprobe [16:54:39 root@localhost ~]# [16:54:41 root@localhost ~]# [16:54:41 root@localhost ~]#find / -inum 70 -exec ls -il {} + 👈提前用一下exec看下效果呵呵 70 -rw-r--r--. 1 root root 4096 Feb 7 16:54 /sys/bus/memory/drivers_autoprobe 70 -rw-r--r--. 1 root root 0 Feb 7 16:54 /sys/fs/cgroup/devices/system.slice/sys-kernel-tracing.mount/tasks 70 -rw-r--r--. 1 root root 0 Feb 7 16:54 '/sys/fs/cgroup/memory/system.slice/system-systemd\\x2dhibernate\\x2dresume.slice/tasks' 70 -r--r--r--. 1 root root 0 Feb 7 16:54 /sys/fs/cgroup/pids/system.slice/systemd-journald-dev-log.socket/pids.current 70 -rw-r--r--. 1 root root 0 Jan 29 09:57 /sys/kernel/debug/tracing/events/raw_syscalls/sys_exit/filter 70 -rw-r--r--. 1 root root 0 Jan 29 09:57 /sys/kernel/tracing/events/raw_syscalls/sys_exit/filter [16:54:43 root@localhost ~]# 👆节点编号相同也不是同一个文件哈哈。 插入一个find -exec的用法细节 搜索inode节点编号，以及搜索inode相同的文件(硬的) find 的regex要匹配的是全路径，locate不需要 对比实验 17:07:14 root@localhost ~]#ll /usr/share/pixmaps/ total 92 -rw-r--r--. 1 root root 5459 Sep 9 13:25 cockpit.png -rw-r--r--. 1 root root 13071 Jun 28 2021 fedora-gdm-logo.png -rw-r--r--. 1 root root 21820 Jun 28 2021 fedora-logo.png -rw-r--r--. 1 root root 12760 Jun 28 2021 fedora-logo-small.png -rw-r--r--. 1 root root 6620 Jun 28 2021 fedora-logo-sprite.png -rw-r--r--. 1 root root 1442 Jun 28 2021 fedora-logo-sprite.svg -rw-r--r--. 1 root root 14493 Jun 28 2021 system-logo-white.png 👇这是 -name后跟通配符 [17:07:37 root@localhost ~]#find /usr/share/pixmaps -name *png /usr/share/pixmaps/fedora-gdm-logo.png /usr/share/pixmaps/fedora-logo-small.png /usr/share/pixmaps/fedora-logo-sprite.png /usr/share/pixmaps/fedora-logo.png /usr/share/pixmaps/system-logo-white.png /usr/share/pixmaps/cockpit.png 👇这是 -regex后跟正则(扩展正则，因为不用\\[xx\\]这样写，也支持+)，注意这里的正则匹配的是全路径 [17:08:21 root@localhost ~]#find /usr/share/pixmaps -regex \".*\\.png$\" /usr/share/pixmaps/fedora-gdm-logo.png /usr/share/pixmaps/fedora-logo-small.png /usr/share/pixmaps/fedora-logo-sprite.png /usr/share/pixmaps/fedora-logo.png /usr/share/pixmaps/system-logo-white.png /usr/share/pixmaps/cockpit.png [17:08:30 root@localhost ~]#find /usr/share/pixmaps -regex \"\\.png$\" [17:08:33 root@localhost ~]# [17:08:34 root@localhost ~]# 👇这是locate -r 后跟 正则 [17:09:12 root@localhost ~]#locate -r \"/usr/share/pixmaps/.*\\.png$\" /usr/share/pixmaps/cockpit.png /usr/share/pixmaps/fedora-gdm-logo.png /usr/share/pixmaps/fedora-logo-small.png /usr/share/pixmaps/fedora-logo-sprite.png /usr/share/pixmaps/fedora-logo.png /usr/share/pixmaps/system-logo-white.png 👇当然locate无需全路径 find 选项的PPT总结图 还有一个补充，要注意-name 后面要带上双引号的 [18:32:04 root@localhost ~]#find -name \"f*\" ./f2 ./data/f2 ./data/f1 ./f1~ ./fz~ ./f3 ./f1 ./f1.link [18:32:05 root@localhost ~]#find -name f* find: paths must precede expression: f1~ Usage: find [-H] [-L] [-P] [-Olevel] [-D help|tree|search|stat|rates|opt|exec] [path...] [expression] [18:32:11 root@localhost ~]# [18:32:12 root@localhost ~]# [18:32:12 root@localhost ~]# 按属主和属组查找 默认是print，这里换了个动作-ls 所以其实不用上面的-exec \"ls\" {} + 这么麻烦，简单的ls直接加就行了。 这样-nouser就体现出来了👆 根据文件类型查找 搜索所有文件夹 搜索所有块 搜索空文件或空文件夹 组合条件与或非 非空文件和文件夹 并且关系 或者关系 -a与的运算优先级要比-o或运算高，所以-type f -a -ls先进行运算了。 关键点来了，为什么-name \"f*\" -o -type f -a -ls 后面先算，结果就是t.txt了呢 [17:28:06 root@localhost data]#ll total 16 -rw-r--r--+ 1 root root 10 Jan 29 17:49 f1 -rw-r--r--+ 2 root root 4 Jan 29 17:49 f2 -rw-r--r--+ 2 root root 4 Jan 29 17:49 f2.link drwxr-xr-x. 2 root root 6 Feb 8 10:39 fdir -rw-r--r--. 1 root root 65 Jan 29 10:59 t.txt [17:28:08 root@localhost data]# [17:28:10 root@localhost data]#find ./ -name \"f*\" -o -type f -ls 👈不太好解释为什么变成了1行 33577450 4 -rw-r--r-- 1 root root 65 Jan 29 10:59 ./t.txt 👆上面的命令等价于下面两个的结果做了-o，但是-o显然不能用man里的用法来解释这个结果。 [17:28:12 root@localhost data]#find ./ -name \"f*\" ./f1 ./f2 ./fdir ./f2.link [17:28:16 root@localhost data]#find ./ -type f -ls 33577448 4 -rw-r--r-- 1 root root 10 Jan 29 17:49 ./f1 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2 33577450 4 -rw-r--r-- 1 root root 65 Jan 29 10:59 ./t.txt 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2.link [17:28:21 root@localhost data]# 👇倒是如果大家都是一个格式都是ls -l长格式就可以-o了 [17:32:00 root@localhost data]#find ./ -name \"f*\" -ls 33577448 4 -rw-r--r-- 1 root root 10 Jan 29 17:49 ./f1 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2 51325760 0 drwxr-xr-x 2 root root 6 Feb 8 10:39 ./fdir 👈 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2.link [17:32:02 root@localhost data]#find ./ -type f -ls 33577448 4 -rw-r--r-- 1 root root 10 Jan 29 17:49 ./f1 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2 33577450 4 -rw-r--r-- 1 root root 65 Jan 29 10:59 ./t.txt 👈 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2.link [17:32:08 root@localhost data]#find ./ -name \"f*\" -ls -o -type f -ls 33577448 4 -rw-r--r-- 1 root root 10 Jan 29 17:49 ./f1 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2 33577450 4 -rw-r--r-- 1 root root 65 Jan 29 10:59 ./t.txt 👈 51325760 0 drwxr-xr-x 2 root root 6 Feb 8 10:39 ./fdir 👈 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2.link ------------👇这段是符合-o的本来逻辑的，就是前面true就不算后面了---------- [17:33:24 root@localhost data]#find ./ -name \"f*\" -ls -o -type f 33577448 4 -rw-r--r-- 1 root root 10 Jan 29 17:49 ./f1 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2 51325760 0 drwxr-xr-x 2 root root 6 Feb 8 10:39 ./fdir 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2.link [17:33:28 root@localhost data]# [17:33:44 root@localhost data]#find ./ -name \"f*\" -ls 33577448 4 -rw-r--r-- 1 root root 10 Jan 29 17:49 ./f1 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2 51325760 0 drwxr-xr-x 2 root root 6 Feb 8 10:39 ./fdir 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2.link [17:33:59 root@localhost data]# [17:34:01 root@localhost data]#find ./ -type f ./f1 ./f2 ./t.txt ./f2.link ------------👇这段是不符合-o的本来\"表面\"逻辑的，看起来像是短格式在前(左)，长格式在(右),会变成左边的文件名去掩码右边的文件名，掩出来就剩下一个t.txt在用find里的ls(其实就是ls -dils)显示出来----------可能这里的-o就是异或运算了，相同出0就消掉了，不同也就是t.txt不同就出1也就保留了-----但显然上面的例子是逻辑或--▲linux逻辑混乱案例1-- [17:34:22 root@localhost data]#find ./ -name \"f*\" ./f1 ./f2 ./fdir ./f2.link [17:34:34 root@localhost data]#find ./ -type f -ls 33577448 4 -rw-r--r-- 1 root root 10 Jan 29 17:49 ./f1 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2 33577450 4 -rw-r--r-- 1 root root 65 Jan 29 10:59 ./t.txt 33577447 4 -rw-r--r-- 2 root root 4 Jan 29 17:49 ./f2.link [17:34:48 root@localhost data]# [17:34:56 root@localhost data]#find ./ -name \"f*\" -o -type f -ls 33577450 4 -rw-r--r-- 1 root root 65 Jan 29 10:59 ./t.txt [17:35:04 root@localhost data]# 总之，这东西要规范了用就是-a是默认存在的 优于 -o，然后要规范使用小阔话来实现正确的逻辑，否则结果很难解释。 德·摩根定律 locate -r xx是正则，find -name xx是通配，locate xx是包含就算，find -regex 是扩展正则，本章上文有总结过了。 如果find 后接 regex就需要是全路径匹配。 [18:04:14 root@localhost ~]#find /etc -name *.conf | head -10 /etc/dnf/dnf.conf /etc/dnf/plugins/kpatch.conf /etc/dnf/plugins/copr.conf /etc/dnf/plugins/debuginfo-install.conf /etc/dnf/protected.d/dnf.conf /etc/dnf/protected.d/setup.conf /etc/dnf/protected.d/systemd.conf /etc/dnf/protected.d/sudo.conf /etc/dnf/protected.d/yum.conf /etc/libreport/events.d/collect_dnf.conf [18:04:18 root@localhost ~]#touch x.conf [18:04:33 root@localhost ~]#find /etc -name *.conf | head -10 [18:04:36 root@localhost ~]#find /etc -name *.conf [18:04:39 root@localhost ~]#👇-name后面一定要加上引号，否则你看家目录里的文件竟然会影响/etc/下面的文件匹配，这个似乎很不合理，但是让我联想到了pycharm的init文件也是，你只要跑一个普通的xx.py文件，就会先跑一个init文件，但是也不至于像linux这种奇奇怪怪的问题。 都说linux稳定，今儿给大家看看逻辑不通的两个案例~▲linux逻辑混乱案例2 [18:04:39 root@localhost ~]#find /etc -name \"*.conf\" | head -10 /etc/dnf/dnf.conf /etc/dnf/plugins/kpatch.conf /etc/dnf/plugins/copr.conf /etc/dnf/plugins/debuginfo-install.conf /etc/dnf/protected.d/dnf.conf /etc/dnf/protected.d/setup.conf /etc/dnf/protected.d/systemd.conf /etc/dnf/protected.d/sudo.conf /etc/dnf/protected.d/yum.conf /etc/libreport/events.d/collect_dnf.conf ▲linux逻辑混乱案例2 [18:13:15 root@localhost ~]#cd /data/ ---进到/data下find -name 不带*看看，果然是当前目录会干掉实际搜索的目录文件，具体往下看👇--- [18:13:25 root@localhost data]# [18:13:25 root@localhost data]# [18:13:25 root@localhost data]# [18:13:25 root@localhost data]#find /etc -name *.conf | head -10 👈现在不带引号可以搜到 /etc/dnf/dnf.conf /etc/dnf/plugins/kpatch.conf /etc/dnf/plugins/copr.conf /etc/dnf/plugins/debuginfo-install.conf /etc/dnf/protected.d/dnf.conf /etc/dnf/protected.d/setup.conf /etc/dnf/protected.d/systemd.conf /etc/dnf/protected.d/sudo.conf /etc/dnf/protected.d/yum.conf /etc/libreport/events.d/collect_dnf.conf [18:13:28 root@localhost data]#ll total 16 -rw-r--r--+ 1 root root 10 Jan 29 17:49 f1 -rw-r--r--+ 2 root root 4 Jan 29 17:49 f2 -rw-r--r--+ 2 root root 4 Jan 29 17:49 f2.link drwxr-xr-x. 2 root root 6 Feb 8 10:39 fdir -rw-r--r--. 1 root root 65 Jan 29 10:59 t.txt [18:13:31 root@localhost data]#touch a.conf 👈只要在find的命令键入的当前目录下创建a.conf [18:13:48 root@localhost data]#find /etc -name *.conf | head -10 👈就搜不到了 [18:13:51 root@localhost data]# [18:13:53 root@localhost data]#touch /etc/b.conf [18:14:02 root@localhost data]#touch /etc/a.conf 👈然后在/etc/下面创建a.conf就搜索到了 [18:14:05 root@localhost data]# [18:14:05 root@localhost data]#find /etc -name *.conf | head -10 /etc/a.conf [18:14:07 root@localhost data]# [18:14:07 root@localhost data]#touch b.conf 👈再在/data下创建b.conf，就报错了哈哈。 [18:16:14 root@localhost data]#find /etc -name *.conf | head -10 find: paths must precede expression: b.conf Usage: find [-H] [-L] [-P] [-Olevel] [-D help|tree|search|stat|rates|opt|exec] [path...] [expression] [18:16:40 root@localhost data]#rm -rf b.conf [18:17:32 root@localhost data]#find /etc -name *.conf | head -10 /etc/a.conf [18:17:34 root@localhost data]# find的裁剪，让我想到了vtp的裁剪 用裁剪prune，它的裁剪语法是组合逻辑的，不是很简洁 题外话 搜arp协议文件 搜文件大小区间(] -size 写10K，实际上是(9k-10k] 👆既然是(9k-10k]之间，那么8192就不在里面了，自然就搜不到了。 搜文件时间区间[) -size 👆这个线索可以拎出来，▲判断某个动作或者事件 带来了哪些影响，①比如yum history是可以回滚yum的动作(包括依赖，这是remove做不到的)；②就是这里的时间，我这一分钟内完成了用户的创建，于是我查看1分钟来产生的新文件-mmin -1就这些，所以八九不离十的 就是useradd 产生的。 权限搜索-perm xxx -perm /222的意思是三个人(所有者、所属组、other)只要有一个角色有写权限就匹配出来，本质是222 和 / 的组合，010 或 010 或 010 ，0是不关心，1是固定住，/是或的关系。 -perm 222的意思是只找出 权限为222的文件或文件夹 -perm -222的意思是010 且 010 且 010，三者都必须有写权限就行。 助记： -少就是且 /多就是或 要知道/以前等价于+ ，就是说+222等价于/222，只不过+不推荐了。 下面👇的理解过程是不是有问题，是的~！ 总结下 当/XXX都不为0的时候好理解：就是xx or xx or xx或者的关系，比如/222就是只要三者一个有写 当/XXX为101 001 010 反正只要有0出现，就意味着0不看，比如/202就是u和o两个中的有一人有写 注意/333表示u,g,o三者一人有写和执行？请看下例👇 ★是拆成二进制然后bit位之间或的关系 ★是整体111111111 9个bit位之间是或的关系，而不是u,g,o之间的关系。 [18:06:26 root@pyConsole test]#find -perm /333 👈表示111111111都是或 . ./f1 ./f2 ./f3 ./f4 ./f5 [18:06:32 root@pyConsole test]#find -perm /303 👈表示011nulnulnul011都是或，不看g，只看u和o . ./f1 ./f2 ./f3 ./f5 [18:06:34 root@pyConsole test]#ll total 0 ---x--x-w- 1 root root 0 Feb 9 17:45 f1 -rw-rw--w- 1 root root 0 Feb 9 17:45 f2 -r--r---wx 1 root root 0 Feb 9 17:48 f3 ------x--- 1 root root 0 Feb 9 17:56 f4 ---x------ 1 root root 0 Feb 9 17:59 f5 [18:06:35 root@pyConsole test]# ----------------------上面这段其实是后写的，下面的是梳理过程中的截图，保留供参考---上面的结论OK的---------- [18:00:21 root@pyConsole test]#ll total 0 ---x--x-w- 1 root root 0 Feb 9 17:45 f1 -rw-rw--w- 1 root root 0 Feb 9 17:45 f2 -r--r---wx 1 root root 0 Feb 9 17:48 f3 ------x--- 1 root root 0 Feb 9 17:56 f4 ---x------ 1 root root 0 Feb 9 17:59 f5 [18:00:22 root@pyConsole test]#find -perm /100 . ./f1 ./f5 [18:00:25 root@pyConsole test]#find -perm -100 . ./f1 ./f5 [18:00:27 root@pyConsole test]#find -perm /101 . ./f1 ./f3 ./f5 [18:00:29 root@pyConsole test]#find -perm /111 . ./f1 ./f3 ./f4 ./f5 [18:00:31 root@pyConsole test]# 分析 find -perm /622 的意思👇 上图错了，/622，不是必须6，而是110里面有一个就行了 哈哈，上图是最最开始的笔记，那会我就发现啦，哈哈，给自己点个赞👍。不过没有这一次梳理的完整。 所以回到一开始的PPT ① xxx就是精确匹配，000就是u,g,r三者权限都是---的文件，精确匹配不存在什么或，不存在0表示不关注的说法；倒是存在bit位的并且哦，哈哈~ ② 然后接下来： 2.1 /表示或，人家说了一位就是bit位，看到没，哈哈 2.2 0表示不关注，这个0说的是/303，里的这个0是十位数的0，哈哈(不对，这个0依然是二进制的0)，好家伙，PPT果然言简意赅，结果大佬就讲错了。 2.3 同样-xxx 人家也说了0表示不关注，然后是bit位之间的且。 2.5 一句话总结：/ 和 - 都是展开bit二进制后，0不关注，然后/就是或，-就是与。 ③ 我牛逼的地方来了哦，以上总结OK了到位了，问题来了，如果我要find 权限是rw?---rw?也就是707,606,607,706的文件捏，哦你要输入 find -perm -606 没办法了吧，哈哈，还不如这样 find 后的处理 👆上面的用法很危险。 重定向的>等于-fls -ok 的交互 但是是交互式的。 -exec的非交互 案例，日志处理 找到大于10M的，移动到tmp/下。 还是不行，因为还有子文件夹，//没关系的，不影响效果。 👆上图存在同名冲突的问题的。那样怎么做呢？遗留问题用⚪这个吧哈哈。 文章标识有▲、⚪、了★预留。⚪是微软输入法yuan第5个。 找到符合文件\\文件夹，然后整体(带路径)搬到某个文件里去 将上面查找出来的符合条件：1天内修改过的 并且 名字不是 . 的--这就去除了当前目录。找出来后，带路径复制到目标文件夹处。 但是上图要注意cp -a --path 才行，-a就是保证属性不变。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:03 "},"9-文本查找和压缩/2-文件压缩和解压缩.html":{"url":"9-文本查找和压缩/2-文件压缩和解压缩.html","title":"第2节 文件压缩和解压缩","keywords":"","body":"第2节. 文件压缩和解压缩 参数太多，一个个传进去 touch rm 等命令后面的参数都有上限的。 touch f{1..100000000}应该就会报错， xargs可以解决该问题 没有标准输入，怎么传给他比如rm 上图 ls是没有标准输入的，管道符是传不过去的，所以说它是错误的，不是说：+号过时了用/替代。 参数太长怎么办 利用xargs获取stdin来传给rm -f作为参数 ▲xargs是用来传参数的。 注意默认行为，达到一定长度就换行了 上图不是一个个传给touch的，而是一堆堆，大概是2000+一次传给touch，一个个传如下图 就是说 touch f1 f2 f3 这种一下创建多个文件，xargs 一行传过去也没问题，但是useradd xx yy 的yy生效的，所以需要换行。 创建10个用户 /7000，0不关心，7就是111，针对3个特殊权限有一个有就行 上图因为没有xargs传过去，所以就仅执行了ls -SL，验证下👇 find 后删除 上面是表面上合理，但可能有问题的。 特殊情况案例：单个文件名称位\"fa b.txt\"中间带空格。 观察上面报错，find 传给xargs后xargs默认是空格作为分隔符，所以\"fa b.txt\"就被判定位fa和b.txt两个文件了。 print0 是啥呢？是ASCII码为0，可不是十进制的0哦。 验证下-print0的分隔符为null的效果👇 这是txt的t后买那就是00(print0) -print0就是为了避免文件名带特殊符号。 第一个示例，是原地目录备份提好的，好在如果备份到其他地方存在子，子子文件夹不存在的情况从而报错的。要么复制其他目录就别带文件夹了，统统放在一个层级下就是不带{}的意思，这样又会存在文件同名的问题。详情见上一章节的文末内容。 练习 打包压缩 gz bz2 xz 3个主流 压缩要消耗CPU的，看你需求，你的磁盘空间大，你需要节省CPU消耗，就不压。空间换性能(时间) compress默认压缩后原文件就没了。 👆上图就是compress比较聪明，不给你压缩.jgp文件。同样👇issue根本不给你压缩。 compress是标准输入的信息进行压缩，完了屏幕上打印ctrl+d退出可见，可能存在乱码。 zcat直接预览 gz后缀用的多 也是压完，原文件没了。 gzip -d 一样的也是gunzip解压 压缩比 gzip也是标准输入形式，不过要带上-f 其实有没有标准输入都可以传的，有就用|，没有就用xargs。 当一个命令的标准输出非常大，比如数据库特别大就(压缩一下放硬盘里)，就可以使用gzip配合生成文件。 bz2 压缩比跟高 bz还是比gz的9级，压缩度还要高。 所以 看下两个压缩算法： 上图的-r是不是写错了，应该是-R啊？⚪表示疑问。 👇解是能解，但是不能tab补全 👇这次都不能解了 后缀很关键啊 gzip、bz2 、xz 三种压缩，网上比较多。 zcat看.gz和.Z；bzcat看.bz2；xzcat看xz xz 压缩工具 所以linux内核就是用xz xz 压缩和解压缩速度慢，耗时长。 理论上xz要强，但是数据量不大的时候，还不如bz2。 观察 这里都是有tar 结论：都需要先用tar打个包，再用gz，bz2，xz去压缩（这些工具只能针对一个文件去压缩，不能针对文件夹）。 打完包就是一个文件了，再压就行了。 zip打包和压缩 这个勾选，你感觉不到什么变化，该怎么用还是怎么用。就是读取的时候会自动压缩解压缩。NTFS有这个压缩功能。 zip在windows和linux都是一样的软件 adding：下面的明细etc前头是没有/的，就是担心将来你解压缩导致覆盖了/etc。目的就是让你解压到当前文件夹下。👈压缩的时候把/etc/变成了etc/，把/根拿掉了，防止你加压缩把根下面的etc覆盖掉了，所以给你变成了相对路径。 👆上面的压完后再看：白压了，越压越大了，哈哈，找原因： 因为把内存里的文件带上了吧？搞不清⚪存在问题。 前面sysconfig是压缩后产生的文件，会自动打赏后缀.zip的。 管道符的使用▲ 线索总结 这是解压后重定向到文件中。 原来是436K压缩后为93K 解压的风险，存在撑爆磁盘的可能 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:03 "},"9-文本查找和压缩/3-文件打包和解包.html":{"url":"9-文本查找和压缩/3-文件打包和解包.html","title":"第3节 文件打包和解包","keywords":"","body":"第3节. 文件打包和解包 传统公司还有用磁带保存数据的tap -cpvf 这个只是打包，没有压缩 c就是创建 v详情显示 f指定创建的文件名 p是保留权限，但是保留不住acl (联想到cp -p，不过tar的-p保留不住facl) 之前讲过备份acl的方法，，，恩，cp也能备份facl的--“第5节. FACL实现权限的灵活控制--文末”。 tar -cvf 的-可有可无，bsd unix风格 打包不会压缩什么的 t是预览 x是解包 也可以解压到指定文件夹 r是追加 打包且压缩 传统打包压缩 也可以合并起来 解压和解包 压缩格式是自动识别，J不用加 102M ---解开后大小--->930M 查看C代码一共多少行 到底花了多久呢 如此大的差距啊 sys不是系统空间是内核空间。 一些常见后缀比如tgz .**tar.gz 就等于 .tgz** 排除以及filelist 要打包的文件名统一放到一个文件里，↑ split切割文件 合并的方法 cpio不常用，老文件可能采用该格式 cpio也是个打包的，类似tar 预览可见，这其实就是一个小linux系统-虚拟的小文件系统。 i就是解包，d就是自动创建文件夹，解开后就是一个完整的操作系统 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:03 "},"10-文本三剑客2_sed/10-文本三剑客2_sed.html":{"url":"10-文本三剑客2_sed/10-文本三剑客2_sed.html","title":"第十章 文本三剑客2_sed","keywords":"","body":"第十章 文本三剑客2_sed Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:30 "},"10-文本三剑客2_sed/1-文本处理三剑客2_sed.html":{"url":"10-文本三剑客2_sed/1-文本处理三剑客2_sed.html","title":"第1节 文本处理三剑客2_sed","keywords":"","body":"第1节. 文本处理三剑客2_sed sed内置行为 Stream EDitor, 行编辑器 sed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（ pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。然后读入下行，执行下一个循环。如果没有使诸如‘D’ 的特殊命令，那会在两个循环之间清空模式空间，但不会清空保留空间。这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 功能：主要用来自动编辑一个或多个文件,简化对文件的反复操作,编写转换程序等 参考： http://www.gnu.org/software/sed/manual/sed.html sed就是用来解决我多地方dns文件的最佳实践，好好学。 sed命令内置特性，内置循环，内置一行行，和grep也是一样一行一行处理 内存空间在这里就叫模式空间 一行处理完 print，删除，第二行读进来，处理，print，删除，第三行继续 sed 语法 地址+命令， 地址就是哪行， sed '' passwd 👈这个就是内置行为的证明了，就是每行打印一遍 下面是脱裤子放屁的命令，是不是很吊~👇 2P- 只打印第2行 sed是读取标准输入处理的👇，有这玩意--标准输入，就可以利用管道；没有就利用xargs一样传！ 最后一行 tail -n1就行，sed 10 | tail -1或seq 10 | sed -n '$p' 在最后一行添加文本 sed '$ a xxx' /etc/passwd sed '10 a xxx' /etc/passwd 就是在第10行下面插入，$在这个位置就是最后一行的行号 sed '/nginx/s@xxx@zzz@' /etc/passwd 这就是查找nginx关键字的那一行进行替换，不同于上面的 第几行 定位。 下图最后一样是cli，上面的内容是cli的结果 正则匹配 基本上还是grep的等价命令。还没有体现sed的自己的活。 行的范围 显示3-5行 👆上图可以用来过滤出你想要的日志--几点几分到几点几分。 步进行-实现打印奇偶行 sed编辑命令-这个只是print的时候修改下，不修改原文件 d 删除模式空间匹配的行，并立即启用下一轮循环 两个sed👆其实可以合成一个sed来做👇，两次操作合并▲ 但是要注意下这个是不是真的就是两个sed合在一起，看下图👇就知道明显不是。 我觉得正确解释就是，针对处理内容，进行分号前后的两个命令的执行。这也是还原了最基本的逻辑。①👆针对/etc/fstab执行去注释;和去空行的动作②👇针对/etc/passwd执行了打印10行和打印20行的动作。你看有的就是两个sed合并，有的就不是，哈哈。神奇嘛，其实不神奇，就是一个常规操作在不同场景(一个描述在不同语境)里的不同解释。 一个是d删除操作，一个是p选择操作；删除自然两个动作可以用|管道符两个sed作为前后传参，说~哦，我一个文件删除这个再删除那个；而挑选的动作就是我针对这个文件挑选这个，再挑选那个。一个圆圈挖掉两个洞和一个圆圈取出两个洞，都是针对圈圈这个实体，但是挖掉两个洞和先挖一个后的结果作为后挖动作的输入是一致的；而一个圆圈取出两个洞，就不能说我取出的那个洞作为后取得输入参数，这就是逻辑上好玩地方，我希望我自己能把这些看似不好理解，但是实际是一会事得东西啊，有时间又机会琢磨透，世人常说转牛角尖，我很小的时候就想过很小很小10岁，小学吧好像，就说钻出来不就行了，哈哈，其实底层知识逻辑就是哲学了，换了个名字，世人就认了，世人是愚昧的。但是这样的人当前社会很难成功，因为给他的时间不够，他也要玩啊，哈哈哈哈哈~ 问：下面的[\\]是啥东东，答：答NM，是转义，举例 同样对原文件未做修改 如果想要改 sed修改原文件-i，为了安全推荐-i.bak 👆/^Listen/i listen 8080，这里你几个空格都没用，如果你要插入的字符前面带空额，就需要上图的 ​ /^Listen/i\\ listen 8080，转义下 注意c是找到行后，整行替换 sed另存为，之前是修改原文件或是备份，这个直接是另存为 👆注意d;w用;分开来，两次操作合并▲ a是追加文本，r是追加整个文件的内容，注意sed -r和上图的r不是一个东西哦。 其实就是位置+动作，位置就是/正则或者行号之类/ ，都工作就是这里的a也好c也好。 包含root行的行号 查找替换 这里和vim里面很像 sed -e 的用法-等价于上面的两次操作合并▲ 例子，找出IP地址 sed -r 是表示后面使用扩展正则，而sed /基本正则/ ▲正则 这里用到了分组()的概念 这个和py里的format字符串格式化一样。再合并一下整成一个sed命令👇 这个其实就等价于py里的rematch，而不是refind，而refind不用写全。rematch还需要写全。确实要用正则匹配全了，证明： 优化下 牛逼的是👆(())这种写法它能识别好。 例子，取消注释 先定位位置 上图是系统判定为/的用法，至少查找的//是占用了的。不行你猫猫看。 批量取消注释▲，这个vim里也有操作比如ctrl+v ,I, # 两下esc注意是vim不是vi。 例子，sed实现dirname和basename 例子，修改网卡名称为eth0，可能不对 思考这么对不对 sed -rn '/ .*linux/s/$/net.ifnames=0$/' /boot/grub2/grub.cfg 上面的显然不对，而且还多了一个$ 上图infname写错了改成ifname &就表示前面搜索出来的内容。注意下面的p参数拿掉，不然会将目标行复制2遍。 成功案例 [root@centos7 ~]# sed -rn 's/^[[:space:]]+linux16.*/& net.ifnames=0/p' /boot/grub2/grub.cfg linux16 /vmlinuz-3.10.0-1160.el7.x86_64 root=UUID=db575dcc-512a-4240-ab49-f3d41bc3e372 ro crashkernel=auto rhgb quiet LANG=en_US.UTF-8 net.ifnames=0 linux16 /vmlinuz-0-rescue-8173b565dc324c7180303567796b941c root=UUID=db575dcc-512a-4240-ab49-f3d41bc3e372 ro crashkernel=auto rhgb quiet net.ifnames=0 [root@centos7 ~]# [root@centos7 ~]# sed -ri.org 's/^[[:space:]]+linux16.*/& net.ifnames=0/' /boot/grub2/grub.cfg [root@centos7 ~]# ll /boot/grub2/grub* -rw-r--r--. 1 root root 4267 Feb 11 14:47 /boot/grub2/grub.cfg -rw-r--r--. 1 root root 4239 Jan 5 17:45 /boot/grub2/grub.cfg.bak -rw-r--r--. 1 root root 4239 Jan 5 17:45 /boot/grub2/grub.cfg.org -rw-r--r--. 1 root root 1024 Jan 5 17:45 /boot/grub2/grubenv [root@centos7 ~]# cat /boot/grub2/grub.cfg |grep ifname linux16 /vmlinuz-3.10.0-1160.el7.x86_64 root=UUID=db575dcc-512a-4240-ab49-f3d41bc3e372 ro crashkernel=auto rhgb quiet LANG=en_US.UTF-8 net.ifnames=0 linux16 /vmlinuz-0-rescue-8173b565dc324c7180303567796b941c root=UUID=db575dcc-512a-4240-ab49-f3d41bc3e372 ro crashkernel=auto rhgb quiet net.ifnames=0 重启后 [root@centos7 ~]# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:b9:89:eb brd ff:ff:ff:ff:ff:ff inet 192.168.25.44/24 brd 192.168.25.255 scope global noprefixroute dynamic eth0 valid_lft 64705sec preferred_lft 64705sec inet6 fe80::4efd:3be2:da5a:12cb/64 scope link noprefixroute valid_lft forever preferred_lft forever [root@centos7 ~]# [root@centos7 ~]# [root@centos7 ~]# 然后再补一个这个：也不知道是不是上面需求擦头必须的： 或者 [root@centos7 ~]# cat /etc/default/grub GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=auto rhgb quiet\" GRUB_DISABLE_RECOVERY=\"true\" [root@centos7 ~]# [root@centos7 ~]# [root@centos7 ~]# sed -rn 's/(.*CMD.*)\"$/\\1 net.ifnames=0\"/p' /etc/default/grub GRUB_CMDLINE_LINUX=\"crashkernel=auto rhgb quiet net.ifnames=0\" 这样👇更好，我找到不直接s///，而是先//再s///也就是/zz/s#xx#yy#m，因为/CMD/是找出这一行然后再查找部分字符进行替换，而s///可能就匹配的过多，理由不充分哈哈~还没没找到 [root@centos7 ~]# sed -rn '/CMD/s/\"$/net.ifnames=0\"/p' /etc/default/grub GRUB_CMDLINE_LINUX=\"crashkernel=auto rhgb quietnet.ifnames=0\" [root@centos7 ~]# 👇如果是直接s@@@，会全部行都直接换了 “部分” 字符比如s@CMD@zzz@， [root@centos7 ~]# sed -rn 's/\"$/net.ifnames=0\"/p' /etc/default/grub GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)net.ifnames=0\" GRUB_TERMINAL_OUTPUT=\"consolenet.ifnames=0\" GRUB_CMDLINE_LINUX=\"crashkernel=auto rhgb quietnet.ifnames=0\" GRUB_DISABLE_RECOVERY=\"truenet.ifnames=0\" [root@centos7 ~]# sed -rn 's/CMD/net.ifnames=0\"/p' /etc/default/grub GRUB_net.ifnames=0\"LINE_LINUX=\"crashkernel=auto rhgb quiet\" [root@centos7 ~]# [root@centos7 ~]# sed -rn 's/CMD//p' /etc/default/grub GRUB_LINE_LINUX=\"crashkernel=auto rhgb quiet\" [root@centos7 ~]# sed -rn 's/CMD/&/p' /etc/default/grub GRUB_CMDLINE_LINUX=\"crashkernel=auto rhgb quiet\" [root@centos7 ~]# s///是基于行去进行字符替换，//是找到这一行。 不加g，就是只处理每行第一个命中的，g加上就是行内所有都替换。 --------------- sed 中的变量要注意双引号的基本常识 sed自己可以👇这样，在单引号里使用变量 这是sed的自己的用法，比较少见。 存在这种情况 'xx'''$var'''xx\"xxx'，这样可以使用变量了，然后可不可以这样 \"xx$varxx\\\"xx\" 使用转义可以不，可以的吧，可以的 所以要啥自行车~，注意哦，下图三引号不是在sed里用的，是我在echo玩的，别搞混了，本段讨论的是sed里如何使用变量以及转义的双引号，哈哈。 然后上图的另一个点就是和sed一样，也支持单引号里表达变量 sed高级命令-多了一个空间 体现在sed内置的行为就是一个模式空间，高级就高级在多了一个保持空间 hold空间就是临时存一下 sed的强大之处还体现在高级命令及其保持空间。 hold住保持一会，待会取回来用。 模式空间里是可以放好几行的，D是删除一行，后续行就不删了。 sed高级用法的的例子 分析sed -n 'n;p' FILE ​ ①初始 ​ ②打印第2行 ​ ③读入第3行 ​ ④同理 ​ ⑤试试： 看sed '1!G;h;$!d' FILE 不是第一行就G，不是最后一样就删除，中间有G和h追加和覆盖操作，涉及两个模式空间 真要倒着写，tac就行了， 例子sed 'N;D' FILE 补充 linux每行结尾只有换行“\\n”， Windows每行结尾是换行+回车“\\r\\n” Mac OS 为 “\\r”。 用dos2unix file 活unix2dos file命令直接转换，有时候是最小化安装，所以vim里的方法也是要会的。 利用Linux下的vim，去除^M，去之前file看一下 vi xxx 然后 :set ff 用于查看当前文件是dos格式还是unix格式，显示如下： 切换为unix格式，然后保存即可： :set ff=unix #👈转换为unix格式 :wq 如果上图的格式不对，直接./xxx.py是找不到文件的，只能用python xxx变通运行，这里改成unix换行符后，就可以直接./xxx.py运行了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:30 "},"11-软件包管理/11-软件包管理.html":{"url":"11-软件包管理/11-软件包管理.html","title":"第十一章 软件包管理","keywords":"","body":"第十一章 软件包管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:31 "},"11-软件包管理/1-软件管理基础.html":{"url":"11-软件包管理/1-软件管理基础.html","title":"第1节 软件管理基础","keywords":"","body":"第1节. 软件管理基础 ABI 应用程序二进制接口，API是开发接口 WINE，让linux上跑windows的软件 Cygwin，windows上跑linux的软件 预处理：去掉注释、打上行号、引用的文件(include代码)等放进来，等 编译：语法分析，错误，转成汇编代码 汇编：汇编代码还是文本，汇编之后就是机器码了，还要把库连接起来。这些库有静态和动态之分 静态：把库和二进制结合在一起，合成一个文件； 动态：程序运行的时候再寻找依赖库；不合并。 .so是shareobject 共享对象，好多程序会共用这个库 ldd可以看用的哪些库，不仅仅cat依赖这个库，ls也依赖。共同依赖的库 共用的库 .c源代码C语言写的---用gcc预处理成.i后缀---compilation编译成汇编语言assembly----汇编成二进制.o对象文件----link链接.a静态库----生成最终的一个文件。 动静优缺点 应用案例：注意通常我们都是动态链接的库，所以把一个二进制程序复制过去别的电脑上需要考虑把依赖的库文件也一并找出来复制过去。 JAVA号称一次编译，到处运行的底层思路： 1、存在本质上的两次编译 2、第1次编译，是编译成标准字节码文件 3、然后再各个系统上面预装了JVM java虚拟机，当字节码在这些JVM上运行的时候 就会再次编译一次。 需求产生了，降低使用开源软件的难度，直接给C，编译困难，就出来了debian版本 三大主流linux分支debian、redhat、slackware。 debian就率先考虑的需求，就帮你编译好，打好包，让你使用的时候自动的解包，dpkg debian package管理器，Ubuntu就是debian咯。 RPM GNU自己定义自己~八错八错~ linux-5.1.4 5是大版本号，1是次版本号，4是小版本号；大5-就是版本大变动，次1-就是小变化，小4-就是打个补丁。大概这么个意思。 rpm就是什么，就是开源软件，红帽拿过来打包整合的rpm包。所以包名的意思是 bash-4.3.46-19.el7.x86_64.rpm 包名和人家包自己的版本 19是红帽拿过来编译的次数 el7是红帽企业版7 x86_64CPU架构 神奇的文件夹/misc， 明明里面没东西，强行进入/misc/cd下就生成了cd文件夹，并自动挂载了光盘 后面补一个这个神奇的文件夹是什么来的，6上面可能自带的。7上面要想这么用，就执行systemctl start autofs systemctl enable autofs ;-安装方法要对--应该是server的套餐吧，反正最小化安装是没有这东西的。 在centos6和7上进到这Packages文件夹下，注意5和8都不叫这么名字好像，不管了8也不用了 用sed来弄 问题来了，什么用-n什么时候用p，怎么这里不用-n 和p呢，结果是对的。它这个上图的命令是已经对print的结果进行编辑过了修改过了，所以无需-n p，当然你加上-n 和p应该也是OK的 1、-n就是不打印了呗，全都不打印 2、xxxp就是你处理后的结果也要打印出来，如果此时前头没有-n就是说默认打印+处理后的打印 3、-n xxxp就是只打印处理后的结果 4、-n和xxxp 两个都不带，就是处理后的全文显示。 5、上图为啥-n xxxp和 都不带 效果一样呢，因为匹配了所有都能匹配到的，所以就一样咯。 ================================================================================ rpm包el6\\el7\\x86_64\\noarch\\i686 大软件 一般会拆包，挑着安装就可以比较方便。 这就是拆包分类 rpm安装的依赖只是直接的依赖包，间接依赖包不会显示，所以一眼看不见 --------------------------- lib64库文件 思考是不是cat命令就是用的/bin/cat文件，还是后面调用了C语言的库呢？ 再者so就是share object 很多二进制文件都是依赖一些共同的so库 例子-移动一个库及修复 可惜mv也是依赖这个库 然后图形界面也死掉了 修复要么快照，或者借助光盘 上面的关闭客户机这些本质上还是调系统的命令，下面的重置才是按电源重启。 重启也起不来了 重试救援光盘来解决 1、插入光盘，连着的 2、进度条出来后果断esc 3、选择3 CD-ROM启动 4、选择troubleshooting 5、救援系统 就是类似windows的pe，交换机路由器的RMON，类似这种最小操作系统。 6、进入后界面 因为是从光盘启动，所根不再是/了，而是/mnt/sysimage/ 你的系统被挂载到了/mnt/sysimage。你看到的文件系统是/mnt/sysimage这套 上图system写错了改成sysimage。 而我们要找的文件就在： 思考此时mv能用吗？ 因为用的是光盘里的mv，不是硬盘里的mv。所以是可以用的。 然后exit退出，自动重启就修复好了。 以上就是mv修复，下面是rm修复： 应该就在上面救援模式的/lib64下面有的。 lib64，库现在都是64位的了 学习包管理 一个RPM包里可能包含的东西比较多： 脚本的意义在于，你安装程序前先给你创建好用户，这样你才有所有者，所有组啊。诸如此类的信息。 前两行都有： 第三行的脚本不一定每个RPM包里都有。 RPM包不是安装好了就行了，还需要处理文件的属性信息存放便于后续查找。 数据库简单说就是文件夹，里面存放了安装rpm包的很多信息。如果没有/var/lib/rpm，你都不知道你安装了那些程序。也不知道哪些文件来自哪个包。 /var/lib/rpm的意义 比如安装软件，如果已经安装过了，就不会安装了，这就是到这里查找的。 有些官网提供了编译好的，有些就是提供源码给你自己编译。 操作路径 ppc是powerpc的，不用管 EPEL Fedora是红帽的上游测试版，dnf在Fedora 18版就有了。 然后现在centos-stream要取代centos8变成rehat的上游了。 ------- 配置 > /dev/null就可以实现静默安装。 一般warning看下，问题不大，是光盘里的，但是这里的告警是说验证来源是没有验证的。 signature是来源，其实就是公钥的验证。就是windows里的受信任的证书。 --- 选项放前面，这里习惯不好。 软件脚本安装的思路 之前装过了，这里查询的话只要写名称就行了 没装是这个样子的 我们-e卸载后再试试 ====== 或者查询某个包里包含哪些文件 是不是重新安装一些tree就行了 因为那个var/lib/rpm里有安装过的信息了，所以不会给你安装了，你说rpm -e卸载呢，不推荐，因为说不定之前都做过一些配置优化了。 办法也许可以，但通常不行，因为会全部覆盖的。方法还在下面往下看 replacepkgs和replacesfiles，就是只覆盖有的，没有的就安装。 replacefiles是只覆盖有冲突的文件。一般用在两个包一样但是版本不同，第二个包安装覆盖掉冲突的就是相同的文件就能正常安装第二个包了，同时第一个包的不同版本的软件可以自然可以使用同名的文件。 bash这个包基本就是随着操作系统安装而安装的，所以通过rpm -qi bash也能看到何时安装的系统 查看软件包的信息 上图中的官方站点可以去获取最新的版本 tgz就是tar.gz 如何只修复其中一个文件呢 利用cpio来解开rpm 利用cpio -tv查看文件列表 不过这样cpio解开的rpm再mv过去，有个问题，文件属性可能还需要注意一下。 据说类似于菜市场猪肉上面的蓝章。不过我想起来好像有红的。 默认就不具备完整性校验 先看下包里有没有脚本 -q --scripts 查看已安装的包 -qp --scripts 查未安装的包 将来如果是源码安装，就需要参考这些rpm安装的脚本来自己创建这些用户和组。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:31 "},"11-软件包管理/2-rpm管理软件.html":{"url":"11-软件包管理/2-rpm管理软件.html","title":"第2节 rpm管理软件","keywords":"","body":"第2节. rpm管理软件 rpm包升级 感觉，不会单单升某一个包吧，除非比较清楚依赖版本的限制情况。 -U也就是--upgrade，包没有就给你安装的，有就给你升级。 -F也就是--freshen，包没有就不安装，仅仅升级。 类似--replacekgs还有一个force，一个效果。 思考能不能安装两个版本的软件 windows的比如foxmail可以安装在不同的文件夹里，同时使用，无需关注注册表。 rpm包的话，除非名称都改一改，不然是不能共存的，很多都不能共存。 内核倒是可以的 可见kernel安装的文件绝大多数是带版本号的，所以可以并存的。 个别文件是冲突的，替代掉： 包查询 上图PPT有问题啊，cpio 的-i是解包，你都解包了，还和t预览个屁啊。 名字记不住，可以这样： -qa 加过滤的方法grep的regex和直接的通配符 两个不同版本的内核就有了 下面开始安装 强制安装 机器上存在两套内核， 计算机启动后可选择其中一套 如果要卸载就要注意 卸载哪个都可以，即使是当前使用的内核也可以的，因为linux加载到内存里了已经，而内核作为文件存在是可以删的。 重启后就剩一个内核了 查询文件来自哪个包 删除文件依然可以查，因为/var/lib/rpm下面有安装tree的文件信息的 如果卸载了就自然这些信息就没了 记一次VmwareWorkstation的光盘挂载问题和重启解决的经过 重新挂载光盘依然还是8.8G，还是不行。重启VM主机后，变为11G此时光盘挂载正常。后续rpm和cp正常。 查询包里面的文件。 ----------------------- 包里依赖文件叫做能力 上图查询的前提是已经安装完毕 上图是某个能力是由某个包提供的，而下图是哪些包需要这个能力 不安装包仅查询依赖的办法 yum可以查到这些依赖的能力来自于哪些包-不安装也能查询，rpm还做不到这一点。 不对，还有更直接全面的办法。 和依赖相关的能力的四个选项 包查询还有一些选项 ql就是全列出来 qc是列出配置文件 qd是看文档 bash提供了哪些能力，也就是哪些文件。 如果你安装了两个软件，卸载的时候想一起都卸了，可以考虑--allmatches。 rpm命令修复例子 查找rpm命令来自哪个包 所以进入救援模式 这样安装路径是不对的，如下 安装都是在/根下的 所以要以/mnt/sysimage为根安装 至此就修复好了 包校验-就是rpm觉察到包变化了， 对就是觉察到，用词就是这样的恶心。 恢复下上面的tree，怎么把上面的echo 弄进去的一个换行给去掉呢。 ----------------------------------------------- tree刚才被echo了一个换行，所以找到最后一行 删掉就可以恢复rpm -V tree检查 xxd🐖助记词，怎么改二进制 哈哈， 时间没办法，肯定改了，数据内容大小MD5都恢复了 查询所有包有无变化 包校验-来源性 导入这个验证工具--import，其实就是公钥。 导入这个验钞机实际上是放在了这个地方 检查某个rpm包的合法性-也是依赖于公钥的 公钥本地有，光盘也有的 缺少验钞机（公钥），只要安装完系统后，本地光盘上就有 修改一下验证 进rpm包里删除最后一行的回车 如果不转回去，则文件大小会差很多，且都不再是rpm包了 rpm数据库 上图初始化没有啥意义，因为你删掉/var/lib/rpm后系统也会自动给你初始化，但是安装的信息都没了。 yum就是python写的，依赖于rpm的，你把rpm的数据库删了，yum也就挂了。 rpm是单机命令，yum是c/s架构。 yum在client和server都要配置好。c和s通过网络互通，单机就是c、s在一台机器上。 光盘就是一个仓库 EPEL也是一个仓库 仓库的元数据metadata：rpm文件列表和依赖关系就是放到元数据里。还会分组-分门别类。repodata文件夹就是专门放元数据的。 仓库可能会叫做packages文件夹，也可以是别的名称 客户端要配置config文件 yum install httpd就是： 1、查询本地repo文件，看你配置的仓库 2、按照仓库配置的服务器以及对应的路径就是文件夹， 下载元数据到本地，将这些元数据下载到本地的缓冲区中（所谓缓冲区就是磁盘上的某个文件夹）。 去看元数据。是有httpd包的。而且依赖的包也找到。 3、针对httpd和依赖的包，去服务器上下载rpm包。也要放到一个缓冲区里。 4、安装。。。 5、下载完了的rpm包默认自动删除，而元数据不删。下载的东西有两个：元数据和rpm包。元数据下次就不用再下载了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:31 "},"11-软件包管理/3-yum工作原理.html":{"url":"11-软件包管理/3-yum工作原理.html","title":"第3节 yum工作原理","keywords":"","body":"第3节. yum工作原理 在已经安装了autofs后，rpm -ql autofs，看下该包里的关键文件 以后看到 表示/usr/lib/systemd/system下面的都是服务。 基于光盘的yum源 autofs开启后就可以实现自动挂载光盘路径了，号称神奇目录：直接访问/misc下面不存在的文件夹就可以自动挂载。 为什么一定要这个目录呢，因为要当yum源必须有一个repodata文件夹。 当yum源的前提：路径下有repodata 只要看到repodata文件夹，那么其所在的文件夹就是yum源的目录。不要看packages那些。 yum repolist的本质 下载到哪里呢？通过yum.conf可见该目录 [root@centos7 ~]# cat /etc/yum.conf [main] cachedir=/var/cache/yum/$basearch/$releasever keepcache=0 ... 省略... [root@centos7 ~]# cd /var/cache/yum/ [root@centos7 yum]# ll total 0 [root@centos7 yum]# l. . .. [root@centos7 yum]# yum repolist 👈本质上就是把元数据的down到本地 Loaded plugins: fastestmirror Determining fastest mirrors * base: mirrors.cn99.com * extras: ftp.sjtu.edu.cn * updates: ftp.sjtu.edu.cn base | 3.6 kB 00:00:00 extras | 2.9 kB 00:00:00 updates | 2.9 kB 00:00:00 (1/4): base/7/x86_64/group_gz | 153 kB 00:00:00 (2/4): extras/7/x86_64/primary_db | 243 kB 00:00:00 (3/4): updates/7/x86_64/primary_db | 13 MB 00:00:00 (4/4): base/7/x86_64/primary_db | 6.1 MB 00:00:00 repo id repo name status base/7/x86_64 CentOS-7 - Base 10,072 extras/7/x86_64 CentOS-7 - Extras 500 updates/7/x86_64 CentOS-7 - Updates 3,411 repolist: 13,983 [root@centos7 yum]# ll total 0 drwxr-xr-x. 3 root root 15 Feb 14 12:06 x86_64 [root@centos7 yum]# [root@centos7 yum]# ll -R x86_64/ x86_64/: total 0 drwxr-xr-x. 5 root root 87 Feb 14 12:06 7 x86_64/7: total 8 drwxr-xr-x. 4 root root 278 Feb 14 12:06 base drwxr-xr-x. 4 root root 183 Feb 14 12:06 extras -rw-r--r--. 1 root root 84 Feb 14 12:06 timedhosts -rw-r--r--. 1 root root 473 Feb 14 12:06 timedhosts.txt drwxr-xr-x. 4 root root 183 Feb 14 12:06 updates x86_64/7/base: total 6376 -rw-r--r--. 1 root root 6351994 Oct 30 2020 6d0c3a488c282fe537794b5946b01e28c7f44db79097bb06826e1c0c88bad5ef-primary.sqlite.bz2 -rw-r--r--. 1 root root 156763 Oct 30 2020 a4e2b46586aa556c3b6f814dad5b16db5a669984d66b68e873586cd7c7253301-c7-x86_64-comps.xml.gz -rw-r--r--. 1 root root 0 Feb 14 12:06 cachecookie drwxr-xr-x. 2 root root 31 Feb 14 12:06 gen -rw-r--r--. 1 root root 546 Feb 14 12:06 mirrorlist.txt drwxr-xr-x. 2 root root 6 Feb 14 12:06 packages -rw-r--r--. 1 root root 3736 Oct 30 2020 repomd.xml x86_64/7/base/gen: total 30876 -rw-r--r--. 1 root root 31614976 Oct 30 2020 primary_db.sqlite x86_64/7/base/packages: total 0 x86_64/7/extras: total 256 -rw-r--r--. 1 root root 0 Feb 14 12:06 cachecookie -rw-r--r--. 1 root root 248733 Sep 3 23:22 db1c88508275ffebdc6cd8686da08745d2552e5b219b2e6f4cbde7b8afd3b1a3-primary.sqlite.bz2 drwxr-xr-x. 2 root root 31 Feb 14 12:06 gen -rw-r--r--. 1 root root 589 Feb 14 12:06 mirrorlist.txt drwxr-xr-x. 2 root root 6 Feb 14 12:06 packages -rw-r--r--. 1 root root 2998 Sep 3 23:22 repomd.xml x86_64/7/extras/gen: total 1296 -rw-r--r--. 1 root root 1326080 Sep 3 23:22 primary_db.sqlite x86_64/7/extras/packages: total 0 x86_64/7/updates: total 13736 -rw-r--r--. 1 root root 14049533 Feb 9 04:02 c96f20635c7f289398519818a077b294f1855722181378b5105f5ef49f0cf57a-primary.sqlite.bz2 -rw-r--r--. 1 root root 0 Feb 14 12:06 cachecookie drwxr-xr-x. 2 root root 31 Feb 14 12:06 gen -rw-r--r--. 1 root root 589 Feb 14 12:06 mirrorlist.txt drwxr-xr-x. 2 root root 6 Feb 14 12:06 packages -rw-r--r--. 1 root root 3011 Feb 9 04:02 repomd.xml x86_64/7/updates/gen: total 75048 -rw-r--r--. 1 root root 76846080 Feb 9 04:02 primary_db.sqlite x86_64/7/updates/packages: total 0 [root@centos7 yum]# yum clean all 👈这下理解到位了，清的哪里，就是这里 Loaded plugins: fastestmirror Cleaning repos: base extras updates Cleaning up list of fastest mirrors [root@centos7 yum]# ls -R x86_64/ x86_64/: 7 x86_64/7: base extras timedhosts updates x86_64/7/base: gen packages x86_64/7/base/gen: x86_64/7/base/packages: x86_64/7/extras: gen packages x86_64/7/extras/gen: x86_64/7/extras/packages: x86_64/7/updates: gen packages x86_64/7/updates/gen: x86_64/7/updates/packages: 也可以用du -sh /var/cache/yum验证可见大小为0 yum clean all，如果你修改yun源后不清理，就是照着就得yum源那会的yum repolist下载下来的元数据缓存去找rpm包的。所以要清一下的。 yum的问题主要就2个，①路径写错了②缓存没清。你别跟我讲rpm数据库删了。③/etc/yum.repos.d/下面只要有一个repo文件不能用就会影响所有的repo源。 下面是将base源和epel源合在一起讲的： 上图👆是epel的alias 快速启用和禁用。主要是考虑安装rpm包的时候，不让他再去找epel，因为base是再本地的，而epel在网上，所以即使你安装base也会去epel找一遍，浪费时间的。不过yum 本地仓库其实是将base和epel都拉下来的，所以实际工作中base和epel都是内网本地的仓库。也无需禁用epel。 👆上图是将base源和epel合在一个.repo文件里的，也可以分开来，将epel单独做一个epel.repo文件。 这两个应该是从上往下，能走哪里走哪个。 下图是key的写法 key启用后的第一次安装rpm包，会问你是否导入key，👇下图， 然后后面再安装其他rpm包就不会问你了。 YUM出问题就两个：①配置文件以及路径问题，②缓存清一下 比如要安装sl这个小火车，就要删除yum仓库，清除缓存，安装yum源，epel源，比如centos8 https://developer.aliyun.com/mirror/centos?spm=a2c6h.13651102.0.0.23961b11BCsMUT yum cleall all yum makcache yum updata https://developer.aliyun.com/mirror/epel 安装会自动给你安装依赖，那么卸载是否会自动卸载依赖包呢 默认不会自动卸载依赖包的，因为可能别的包也会依赖这些依赖包。 yum list可以看到安装历史 实际上yum安装的时候也有自己的log，👇 Centos8就是dnf.log yum history info 11可见当时做的事情，比如command line install mariadb-server。 把这个事件撤销 undo就是卸载掉 这样就卸掉了，然后看下yum事件，卸掉了10个包Altered 如果你又不想卸了，还可以yum事件回去 redo就是重装一遍。 baseurl还有一个mirroslist(这个后面讲) baseurl可以写多个，如下👇图： 写多个baseurl，都生效， 上图是光盘禁用后，走的网络yum安装的。但是这多个baseurl，优先用谁呢？答案还是在上面的PPT里：failovermethod={roundrobin|priority},cost的值越大优先级越低。 然后是yum的baseurl的4种路径： yum仓库的内置变量 自己写脚本，里面的yum源可以使用多行重定向来写，👇▲脚本惯用手法-cat多行重定向写文件内容。 别忘了别名、vim、history格式、PS1、yum源。 yum list就列出了这1w多个包，一共就是1w多行： yum 看到已安装的包是@打头 anaconda是操作系统安装向导的时候安装的程序，所以yum list不仅看到哪些已安装，还知道从哪装的，①比如anaconda②还有base仓库③epel等其他仓库。 这两个包是base仓库创建后，利用yum安装上去的。 比如，下图👇其实查看已经安装的可以yum list installed更快，必进grep要CPU计算的。 也可以用yum search http 上图，说的是搜索的包名，前提你的知道包名， 包名不知道咋办？以httpd为例，先卸载掉👇 整体卸掉包含之前yum install的所有东西，就需要查案yum history info NUMBER，见到httpd就行。 然后undo就行 上图提示的是依赖的文件名，而不是包名 找到依赖的文件来自哪个包 -qf必须已经安装了的文件名；-pqf后面必须跟包名，package；不对，换方法： 这里注意 yum search 和 yum provides 的差异，provides更胜一筹啊看来~ 查找文件由哪个包提供。这种方法一般也就是用在没有yum源的情况下，自己把依赖准备好，放一起，走哪都yum -y install *rpm就行了。 list查所有的、已经装好的，可用的available-就是未安装的。 yum list 默认就是yum list all yum repolist是将仓库的元数据down下来，同时给你list列出来，不过是针对启用的仓库，加上all就是关掉的也会列出来： yum provides 后面可以是某个文件的，也可以是依赖包。相当于既有了-qf的跟文件能力，也有了-qpi的跟包能力。 👆上图就是展示了怎么rpm查询/bin/tree来自于哪个包。 yum的reinstall和rpm --replaces和--force一样咯。显然不一样，yum的reinstall还带依赖的。 红色框框里的常用，其他不怎么用了，使用yum yum info 类似于rpm -qi rpm包也是yum安装的。需要依赖也会自动给你安装的。 尽量还是不要升级包。 还有centos6不会升级到7，而是重装成7 注意update和upgrade的区别就是没区别 https://cloud.tencent.com/developer/article/1375013?from=article.detail.1604418 update If run without any packages, update will update every currently installed package. If one or more packages or package globs are specified, Yum will only update the listed packages. While updating packages, yum will ensure that all dependencies are satisfied. (See Specifying package names for more information) If the packages or globs specified match to pack‐ ages which are not currently installed then update will not install them. update operates on groups, files, provides and filelists just like the \"install\" command. makecahe一般不需要用 因为，第一次使用过yum，就自动makecahe了 这样一下yum的缓存也就有了。 总结： 这是查这个包依赖哪些文件，然后这些文件又是由哪些包提供的--这就需要再次yum deplist xxx了，比如上面的bash Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:31 "},"11-软件包管理/4-实现yum源仓库和使用yum管理软件以及dnf.html":{"url":"11-软件包管理/4-实现yum源仓库和使用yum管理软件以及dnf.html","title":"第4节 实现yum源仓库和使用yum管理软件以及dnf","keywords":"","body":"第4节. 实现yum源仓库和使用yum管理软件以及dnf 定下来学某个东西的时候，就要摸清楚来龙去脉，这样比较对得起人这种动物它这个脑部、神经、心跳节奏的规律。否则不舒服的，我在试图理解底层心理学？显然这是不对的，感觉对了就好。 配置内网的YUM源 yum -y install httpd 安装httpd服务，通过光盘镜像安装就行（各种rpm包，和yum仓库-repodata元数据） 通过rpm -ql 查看安装的文件 systemctl start httpd 目录结构 所以就是/var/www/html/centos/7/os/x86_64 把centos7的安装光盘整个目录复制过去，也可以挂在光盘的。 再接一个CENTOS6的 把对应的光盘挂过去-工作中复制过去 临时挂用命令mount就行 centos6的也有了 yum源就OK了。 服务端就配置好了，下面开始配置CLIENT 只要不是repo后缀的就不会干扰repo源。 验证可以不写，写的话也很简单 这就是校验文件，赋值一下路径 搞定，当然上面的6也可以用$releasever替代他 测试下 这里注意下，6713个包的解释：我们就挂了一张盘，但是6K个包是两张盘的总量，可能就是第一张盘里有两张盘的包总量信息。然后一些偏门的包在第二张盘里，你是安装不了的。 以上就是基于HTTP协议的yum源 如果没有光驱-显然基本都没有，就用iso文件去挂 光盘可以挂，ISO文件也可以挂 然后web看到就成功了 但是前置条件别忘了：selinux和防火墙 还有客户端 其他补充 mirrorlist是把多个路径写到一个文件里 现在这个机器可以指两个源，准备添加一个网上的阿里源： 复制远端路径 把上面的txt路径一复制，贴到下面去就行了。 搞定了 不信可以看看centos默认的yum源写法，他也是用的mirrorlist的。 包组的管理用group group分为环境组和普通组，环境组就是安装系统的时候让你选择的包--老王使用的事GNOME Destop，老刘使用的是Server with GUI，俺使用的是Minimal Install。 你选择某一种比如Serer with GUI就会对应地安装很多包。 开发包组里的包 \"Development Tools\"注意引号，否则当作两个包组了。 强制组、可选组 不是随着group包组安装的，是随系统安装的。 不同符号不同状态 groupinstall可以连起来写 工作中一般不用包组group，都是需要什么安装什么，group里的东西可能太多了。 yum 的脚本安装可以加上-q --disablerepo和--enablerepo没有试验成功，不知道咋弄的，不过我倒是可以用sed 来开关。恩，也不常用。这么用的，临时的，所以是要配和install xx一起用的 创建自定义的仓库createrepo-其实就是为一堆rpm包生成元数据。 仓库虽然说要rpm包和元数据以及公钥，但实际上站在机器角度，仓库其实就只要一个repodata文件夹和里面的元数据。 自己研发的rpm包，或者网上找的单个rpm包，没有现成的仓库，或是就算你down的是常规的rpm包，但是仓库没down，也可以自己生成。这个题外话参考别人的博客https://www.jianshu.com/p/3b669bcebfb6 👆这样仓库的元数据就有了。 但是有个问题 httpd要开启443 仓库路径得找repodata的路径。 如图是在server下的，而其他的分别是集群、集群存储、server、虚拟化的repodata 多个repodata就是多个yum源。如果你不需要就配一个yum源就行了。 上图👆两个点①cat 重定向写法ctrl + d安全退出才行②baseurl的路径是repodate的pwd路径。 DNF dnf比yum的优势据说就是速度快 推荐yum install *.rpm因为包可能不全，还需要依赖。 这样就可以用dnf了， dnf的使用和yum一样 仓库配置也是一样yum的配置文件。 比较下dnf的性能 = = 效果不行啊，哈哈 time dnf reinstall httpd -y time yum reinstall httpd -y 这个可以看到dnf快些，可能。 yum其实是python写的程序 yum底层就是依赖于rpm的 小软件编译举例 上图gcc hello.c 回车，会自动生成一个a.out。注意上图不是一个命令哦。 指定一个名称-o 将来要编译的软件必然不是一个c文件咯： 看看这个httpd的源码有多少个c 解开的文件里有284个c文件 gcc 能搞定吗？还有编译先后顺序的。所以就不是gcc这种方式了。 那是项目了，项目管理工具了，不同的语言编译工具不一样。JAVA用maven。C语言里make Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:32 "},"11-软件包管理/5-编辑安装httpd2.html":{"url":"11-软件包管理/5-编辑安装httpd2.html","title":"第5节 编辑安装httpd2","keywords":"","body":"第5节. 编辑安装httpd2 理解源码安装 很多较新的版本，光盘里是没有的，这就需要自行到官网下载源码进行安装（官网提供的软件通常就是源码的）。 源码理论上是通过gcc从源代码-->预处理-->编译-->汇编-->链接--最终生成二进制然后执行起来。 但中间过程源码多、不仅仅源代码 还有各种文档 配置文件 说明等，这些文件你放在哪 手动拷贝工作量巨大，所以针对发布的源码 都会有一个官方的 源码编译的项目管理工具 + 配合脚本。 C一般是make这个项目管理器，把软件作为项目统筹管理。利用make里的项目管理功能便捷的部署源码。大概步骤有3： ①创建makefile文件 ​ 这个makefile就定义一些必要信息：各种文件的安装路径（二进制文件放在哪里、配置文件放在哪里、man帮助等分门别类 也可以不分同一个一个总目录放进去 各有各的子目录，这点和yum截然不同了，yum安装时自动给你安排好了，而源码得自己指定安装路径） ​ 为什么要源码编译呢，其中一个原因就是，功能定制，比如源码内置100个功能，如果你是yum或rpm安装，yum的底层也是rpm，官方给你编译完了，官方觉得50个功能是常用的，就给你把这个50个功能编译在了rpm包里，剩下50功能就用不了了。或者生产环境场景单一无需已启用的50个功能这么多，所以就需要定制化。 ----所以就需要把哪些特性功能 放到Makefile里。 ​ 所以makefile尤其2点：安装路径、启用特性。所以configure 后面启用功能的参数就可能很长，cli就会很长了。 ​ Makefile手动写不现实，也是使用工具configure脚本 借助Makefile.in(Makefile模板文件)来生成。 ②利用C语言的项目管理工具make 自动读取Makefile的内容，进行后续工作（编译） ③利用make install 将编译后的文件复制到指定的路径，就是安装动作。 然后configure脚本也是通过autocnf生成的，Makefile.in这个模板也是通过automake生成的。这里不用管，源码下载下来configure脚本和makefile.in模板都是自带的，开发人员做好的。 ./configure要进到当前目录运行的，不然可能人家的代码里写的都是相对路径，会存在问题的。 还有一点./configure的时候会检查依赖，所以会报各种错误，然后工程师就更正错误，补上对应的包，然后再回过来继续./configure。 上述的./configure , make , make install 三大步是通用的，但是每个软件各有特点，所以还需要参考install 和readme里的说明。通常源码解压缩后也会有这两个文件。有时候把install里的东西拿出来复制粘贴就直接执行就行了。 所以源码安装就是上面那回事，easy~，源码安装说的是安装，又不是让你写源码，哈哈，难不成让你跨行理解还是执行吗，no way~。不要有畏难情绪，不要有什么都敢尝试的莽撞。 源码安装tree 上图👆可见tree这个工具-ql就是安装的所有文件了，除了/usr/bin/tree其他都是文档了，不重要。 很多软件都是这个cli格式xx -V yyy --version这种查看自身版本的方式。 右键得到下载链接后使用wget下载 REAME里没啥东西，看看INSTALLL 上图👆说了它这个源码的Makefile是适合各种OS的，但是需要你改一改设置。 CC=gcc，编译器是gcc tree编译后的二进制文件放在/usr/bin下 MAN帮助放在了/usr/man/man1下 默认是linux的。需要什么OS，就取消哪个模块的注释。 源码安装的不要了，只能手工删除 看看install说明，就是常用的make 这和CPU核数有关。多线程 大量的文件才需要-j并行编译。 其实这个时候已经就能用了 当前的tree 1.6和安装后的tree 1.8共存了， 修改PATH变量 你希望tree 1.8执行而不是1.6，所以就要把1.8的path路径放到/usr/bin的前面 这是自己在这些特殊文件下创建的env.sh 上图👆有个sb的操作，就是PATH='/apps/tree/bin:$PATH'，不信你试试，回头vi都用不了，只能/bin/vi编辑。 退出一下shell使之生效 大软件和小软件 大软件是长期站着内存在系统中运行，小软件就是简单的各个选项用的时候就加载一下内存run一下就好了。 而大软件长期运行，就需要把一些设置存到一个配置文件中，下次再用的时候自动就加载配置文件里的一堆配置。 编译安装一个大点的软件httpd 这是光盘自带的2.4.6👆 这是官网下的2.4.25（用这个简单点来感受下编译安装）。2.4.39难度较大。 1、tar 解包 这次发现和tree不一样了，有绿色的configure脚本了 有这个configure脚本，就没有Makefile文件了，因为这玩意就是configure生成的。自然也就有Makefile.in文件了，in就是模板。 参考Makefile.in模板利用configure脚本来生成Makefile文件。 看看readme，也没啥用 看看install ./configure --prefix=PREFIX就是跟了个路径，安装到什么地方。 然后./config需要些指定一些配置：①安装路径指定好，②哪些特性启用哪些不启用。 如上图不写就是默认安装在/usr/local/apache2 然后再看下特性的启用： enable 或 disable 特性 然后开始执行./configure生成Makefile文件 这就制定了安装的总文件夹，和配置文件的存放地点。而且这些文件夹是 不需要事先有的。 需要enable说明默认是未启用的，需要disabled说明默认是启用的。这话对不对哦？⚪疑问 后缀很长建议这么放 我们编译工具之前已经安装了gcc，但是这些特性启用可能还需要依赖 好，下面开始一步步检查和拍错 记住咯，一般提示XXX not found，就是需要XXX.devel这个包，所以需要的就是apr.devel。缺什么就加devel，通常对的。 继续./configure go on 。。。 继续./configure --prefx --xxx 这次依据惯例，使用mode_ssl-devel是没有的 仔细观察上图，提示OpenSSL version is too old 继续 MD，上图我还以为是打了个叉叉，几个月前写的了，现在梳理突然第一眼没看明白，哈哈。是成功两字哦呵呵~。 所以httpd的依赖包列表如下 上面就可以写脚本了~，部署就很快乐~ 字词configure编译完了，就会生成Makefile文件 这个图的步骤在cat install里一样看得到 然后make -j 4 可能是和CPU核数对应的。 我们来复习编译的安装路径 最后一步makeinstall复制文件 主目录/apps/httpd24，配置文件/etc/httpd 这就安装好了，那么就要使用了 每次写路径比较麻烦，所以还是写到PATH变量里去 启动： 这种开机自启动的方法：它又不是sytemctl enable xxx 这种大的软件且是持续运行的就是叫做服务。而不是一次运行的程序。 reboot后测试一下： 以上过程可以写成编译安装的脚本。 下面再装个小软件-黑客帝国的既视感哈哈~ ll 看看 README，看看INSTALL 可以通过./configure --help查看安装路径选项 和 特性启用选项 curses.h，是缺少curses的头 这个头由什么文件提供的呢？ 改为模糊搜索 这个搜到的东西太多了，不仅仅是提供该关键字的包，还有文件，不是一目了然。 或者搜包名这种好一点： 可见是叫ncourses 错误是courses.h安装晚了，应该是configure之前装好的。此时需要make clean一下。也可以把这个目录删了，重新tar xvf解压 重走一遍configure 再make👇： 所以还是删除整个文件夹 脚本写好后，放到网站上，这也是一个常见操作 要想执行往bash里一传就行了 所以将来，你把一键安装脚本放到http网站里面，然后，随便找个机器这么一执行就本地安装成功了。 ★这是个法宝啊 复习下 Ubuntu软件管理 .deb类似.rpm包 dpkg 类似rpm安装 APT类似yum du -sh *查看所有文件和文件夹大小 这就可以看到.deb后缀的包了 dpkg -i xxxxx 安装即可。 -l列出了所有安装的包类似rpm -qa 上图的这个大箭头挺抽象，几个月后也半天(5s)才看明白， 类似于rpm -ql centos如果是minial最小安装，后来又想要GUI，于是可以安装一个包组GNOME。 带空格和不带空格的区别：是grouplist[空格]不是group[空格]list 这个如果你最小安装tab不出来的哦。 16.04以后apt基本整合取代了之前的3个。 但是这个虽然是CN的网站，但是还是不快，所以很多人都替换成阿里的。 类似centos的GNOME的ubuntu里叫ubuntu-destop yum list installed也能看，但是不知道是哪个 友情提示，yum 安装的过程种或者脚本执行的过程中，等等如下图👇 不要敲任何键盘，否则后果比较危险，证明👇 等10s后就发现 复习 补充： tree 编译安装的其他问题处理记录 root@vpn tree-2.0.2]#make gcc -O3 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o tree.o tree.c In file included from tree.c:20:0: tree.h:63:1: warning: C++ style comments are not allowed in ISO C90 [enabled by default] // Start using PATH_MAX instead of the magic number 4096 everywhere. ^ tree.h:63:1: warning: (this will be reported only once per input file) [enabled by default] tree.c:47:1: warning: C++ style comments are not allowed in ISO C90 [enabled by default] //off_t (*listdir)(char *, int *, int *, u_long, dev_t) = unix_listdir; ^ tree.c:47:1: warning: (this will be reported only once per input file) [enabled by default] tree.c: In function ‘main’: tree.c:100:3: warning: ISO C90 forbids mixed declarations and code [-Wpedantic] bool needfulltree; ^ tree.c:124:29: warning: ISO C90 forbids compound literals [-Wpedantic] lc = (struct listingcalls){ ^ tree.c:138:3: warning: ISO C90 forbids mixed declarations and code [-Wpedantic] char *stddata_fd = getenv(ENV_STDDATA_FD); ^ tree.c:145:33: warning: ISO C90 forbids compound literals [-Wpedantic] lc = (struct listingcalls){ ^ tree.c:263:30: warning: ISO C90 forbids compound literals [-Wpedantic] lc = (struct listingcalls){ ^ tree.c:271:30: warning: ISO C90 forbids compound literals [-Wpedantic] lc = (struct listingcalls){ ^ tree.c:279:30: warning: ISO C90 forbids compound literals [-Wpedantic] lc = (struct listingcalls){ ^ tree.c: In function ‘usage’: tree.c:659:2: warning: string length ‘3348’ is greater than the length ‘509’ ISO C90 compilers are required to support [-Woverlength-strings] \" -- Options processing terminator.\\n\"); ^ tree.c: In function ‘patignore’: tree.c:668:3: error: ‘for’ loop initial declarations are only allowed in C99 mode for(int i=0; i 上面error说这个只能用c99编译，结果你的MakeFile 注意，在我的一台centos8里是CC=gcc，没问题的，但是在另一台centos7里，报错c99， 网上搜了下，说gcc默认是使用c89，然后我rpm -ql看了一下 果断修改Makefile里的CC=c99就好了，不明觉厉~ 所以不管怎么说，人家要c99就给他c99好了 改成 再次make就好了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:32 "},"12-磁盘存储和文件系统/12-磁盘存储和文件系统.html":{"url":"12-磁盘存储和文件系统/12-磁盘存储和文件系统.html","title":"第十二章 磁盘存储和文件系统","keywords":"","body":"第十二章 磁盘存储和文件系统 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:33 "},"12-磁盘存储和文件系统/1-磁盘工作原理详解.html":{"url":"12-磁盘存储和文件系统/1-磁盘工作原理详解.html","title":"第1节 磁盘工作原理详解","keywords":"","body":"第1节. 磁盘工作原理详解 设备类型 一般将数据存到文件里，就是说的存到磁盘上了，不要较真说proc也是文件，你存到proc里就是存到内存里。 通常这里理解：1-存文件-存盘，2-生效-就是在内存里运行 块存储是一块一块（比如512为一块单位），当你读到内存里必须是块为单位512 512 地去取，，cpu就可以1bit1bit的处理了。 数据的访问时随机的，块设备的读取，你打开文件就是随机的一个数据，你也不知道存在哪里位置。怎么可能不知道具体在哪呢，inode是吃干饭的啊。 随机的意思是和顺序相对的，块设备和字符设备，一个是看磁盘空间空闲块，然后这些inode快bitmap是空的，然后就可以用来存放数据了没有像磁带那样的一个顺序读取，而是指针地址读取。写还有一定意义上的随机性，读可没有哦。 磁带就不能跳过，只能顺序的快进。 光盘可以跳过，也实现了随机读写 块有缓存，因为是随机的，所以将数据缓存了， 光驱和硬盘里面都有缓存，提供读取效率。 字符设备，比如键盘，就是一个字符一个字符的输入的，且是按照顺序的。字符设备没有缓存~ 块设备要么硬盘、要么光盘 其他都是字符设备，比较多 下图👇是字符设备 这两个数据表示的是设备的类型，8类型，第5个。类型下的编号。 如果两个数字相同，就判定为同一个设备，并不看名称。 比如，构建一个光盘 11 0 创建一个同样编号，名称不一样的 mknod 创建设备 创建了光盘，然后将这个/dev/sr0同类型同编号的设备/dev/cd就是光盘了，挂载到/mnt，可以看到mnt下光盘内容了就： 不同接口命名不同 https://blog.51cto.com/u_12958700/1933385 并行理论OK，但是电磁干扰没办法目前，所以串行才是优选。 redhat 5 添加IDE硬盘 centos 6上加普通的SCSI硬盘 加了好几个 题外：家里搞个NAS？貌似这这样玩的。 上图IDE口在red5红帽5的显示： 除了IDE接口的都判定为sd---这是老的红帽5的做法👆 centos从 6 开始就不一样了，不管是什么接口的IDE SCSI SAS的MVNE的都是sdxx同一个命名 但是工作中不一定是物理介质，还有阿里云等，云服，虚拟机可能是dev/vd /dev/xvd xvd一般是zen虚拟化技术，vd一般是kvm的。 如果是1就是机械，如果是0就是固态--适用于物理机--虚拟机不能这么判断 sda就是第一块硬盘， sdb就是第二块硬盘 sdaa...sdaz sdza...sdzz就这么排下去。 机械盘才是旋转的，所以是1 当然这种方法只能看物理机上的情况，如果是虚拟机就不行了。 MBR方面：单块硬盘最多4个主分区（可以没有主分区），多块硬盘只要有一块有主分区，其他可以全部都是扩展分区， 扩展分区不能直接使用，需要再次创建逻辑分区才能使用。 主分区不能再继续划分成小的分区了，就是独立使用的。 注意sda4是一个扩展分区，下面的所有sda5 sda6（如果继续分的话），都是属于sda4扩展分区的一部分。 这个1K不是真的是1K，只是显示效果。扩展分区不能存数据，所以这里判定为1K，可以用fdisk来看真实大小 上图的红圈是起始位置，还有后面的结束为止，在 centos7里，起始位置和结束位置是以扇区为单位。从start扇区开始到end扇区结束(扩展分区里面分出的小逻辑分区一定是在这个扇区316672000-419430399内的--也就是sda5在sda4扇区里面的。)，Blocks的值是它的真正大小。而1block是1K。所以上图的扩展分区就是50G大小的容量。 sda5就是4G的大小，还剩下40G+的空间未使用。 51379200单位是block，1个block是1kB。这个其实也是扇区512B/1024自然就是kB了。 然后1个block就是2个扇区，读取是整数倍也就是blocks是吧 ------ 这是centos6的，对比上面的7的发现，6比7多了Heads和cylinders。下面开始介绍硬盘结构，从来了解这些现象。 友情提示，硬盘是密闭的，不是真空的~真空早就压扁了。当然你要说是那个真空我也无话可说。 PS：数据是左上图框选的地方放一点，右上图框选的地方放一点，所以要读出来，磁头就要换道了。比如从内圈读完换到外圈，那估计盘片就转走了，就不是同一个扇面了，要等下一圈了就。 磁头换内外圈，就会影响速度了 硬盘的转速就是体现出来速度了。 磁头正反都得有，因为盘面就是正反的 不灵活，整体旋转的。 盘面数=磁头数 不可能是255个磁头，100多个盘片？物理上没这么多，这个head是逻辑上，早期确实就是这样的，后来保留了这种表达方式。 最外圈是0磁道，往里圈编号。 硬盘属于块设备，组织硬盘空间不是一个磁道来整合的。磁道切成一块块扇区。 早期扇区划分(就是在磁道上划分)是同心圆划分的，内外磁道的扇区数一样。这是早期的划法。 一个扇区的大小512字节固定的，整个硬盘要存放更多的数据，就要划更多的扇区。 上面的扇区划分，内圈磁道太短，存放数据有限，随着磁盘密度增大，内圈逐渐可用度越来越差。里圈以达到了工艺极限单位面积存放数据越来越多，但是外圈仍然富裕。同心圆的划分就不行了。 为了解决内圈拥挤外圈富裕的问题，所以产生了下面的扇区划分方法。 拽个名词ZBR zbr 区位记录 法 磁盘扇区划分 这种外圈和里圈放的扇区数量不一样了。 磁头放在外圈数据读取块，放在里圈读取慢。 数据放外圈处理就快些(怎么放外圈啊？⚪)，将来优化系统性能的思想。这是现有的硬盘基本都用ZBR划分的。 如果早期磁盘内外圈速度一样的。 通过磁道来识别外圈还是里圈，0磁道在最外圈，不过实际操作种你看不到磁道号吧，应该都是扇区号。 其实有N个0磁道，和N个n磁道：👇下图有6个0磁道，6个1磁道，6个. . . 所有的0磁道，1磁道，2磁道。。。都是各自组成了柱面 8bit来表示head 255个磁头heads的上限，0不用； 10bit来对应track磁道，1024个磁道上限； 6bit的扇区，63个扇区sectors上限，0不用。 ​ 63个扇区就是一圈 1cylinder（柱面的大小）= 512（1个扇区512个字节）* 63个扇区*255（磁头数也就是盘面数）=8M不到的样子 上面👆这些等到下一章节里就可以对应到MBR里的具体字段就知道怎么算的怎么由来的了。 其实就是这张图 CHS，cylinder柱面，H head，s sector扇区，这就是硬盘的三维 CHS,24bit的局限性 整个硬盘多大？512*63*1024(柱面的上限)*255≈8G，所以传统的磁盘最大8G。这就是CHS的极限。 LBA的容量大 所以又产生了LBA来表达磁盘大小的方式 LBA，只有扇区，全是扇区了就。所以centos7上就只有扇区，没有head和track以及cylinder了。👇 28位寻址就是👉28bit来表示扇区，一个扇区512，所以128GB=2^28*512B 128GB也不够用了 centos6实际上也是支持LBA的，只是为了兼容老版本，所以才是按CHS的方式显示的。 centos7也能够按CHS的方式显示👇 Units = cylinders of 16065 * 512 = 8225280 bytes 的意思是 一个柱面的大小，说明如下： 上文已讲过一个磁盘的容量怎么计算的，当然是最早期的磁盘。CHS，1024个磁道也就是柱面，255个磁头，63个扇区。一个扇区的大小是512B。所以，一个柱面的大小=63*255*512=16065*512 早期red5分区就是按柱面来分的（不是以扇区、磁头、磁道来划分的），就是最小单位是柱面cylinder。整个柱面为单位来划分。就是必须是1个柱面道100个柱面算一个分区，不能是0.5个柱面来划分，然后一个柱面的大小也是8M左右，所以分区的大小也是这个8M的整数倍。 centos6就打破了以柱面为单位，而是以扇区为单位进行划分了。但是centos6为了兼顾centos5，还是以柱面为单位显示的 如上图，第一个1柱面到131个柱面，但是可以打破单个柱面的。说的是partition1分区1跨了柱面了。也就是打破柱面为最小单元了。131到底属于sda1还是sda1,131都属于，说明131柱面跨分区了。也就是说分区的边界线切在了131柱面里面了。 下图就是centos6支持扇区分区，但是兼容柱面，所以显示最小单元是柱面，但是如果存在131这种切到柱面里面的，就会做个提示出来。Partiton 1 does not end on cylinder boundary 上图👆就很明显，分区1不在柱面的边界，那说明就是柱面中间切了一刀。 centos5不能这么干的，7彻底都是扇区了也就不提示这种信息了。 分区的话，扇面或者柱面必须是连续的--也就是单块硬盘的连续空间来作为某个分区。 windows的所谓的跨区卷，但不是分区，也不能跨硬盘。 100G分一个区在一个连续空间中，后来空间不够了，它前后的空间被别人占用了，就算没有被占用，我们统统认为分区不可扩展，不可缩小。后面有些技术LVM、RAID类解决此类问题。 windows某些软件号称可以扩，但是也就是玩一玩，很危险的操作。生产中统统认为分区不能扩！ Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:33 "},"12-磁盘存储和文件系统/2-MBR和GPT分区管理.html":{"url":"12-磁盘存储和文件系统/2-MBR和GPT分区管理.html","title":"第2节 MBR和GPT分区管理","keywords":"","body":"第2节. MBR和GPT分区管理 三步：分区、格式化、挂载 为啥要分区 分区 隔离数据，系统分区，数据分区，系统坏了，数据不受🦅影响， 分区的格式也可以不同。对于修复来说提高了修复的速度，没必要针对总体硬盘修复了。 一个小小重点 日志和DATA要分开来，性能得以优化。不然写日志可能导致东一个扇区西一个扇区，造成了文件的碎片化，将来读取不是太顺畅，性能就不好。这个如果是真的，影响很大的话，确实是个关键点★。 MBR 所以说，分区对读取速度有帮助的，简单来讲就是分门别类提供效率。 △结合上一章，搞清楚几个问题哦，①8G、2T、16B、32位、CHS这些东西分别说的啥，谁的上限，怎么得来的②同样的MBR里有两台表达方式CHS和LBA，这两个自然存在不同的上限的。上面的32位是LBA表达法用了4个B，所以是32个位也就是2^32来表示扇区的，所以MBR的分区上限是这么来的，当然CHS要小咯。然后CHS是3个字节也就是24位来表达扇区的，自然分区就更小了。 主要上次提到2^48*512B是在磁盘空间表示上见到的。而这次是分区空间上限表示的计算方法。计算公式一样，但要注意说的不是同一个东西。本质上是一回事，都是多少位来表示多少个扇区，一个说的是分区，一个说的是硬盘。 你看哦这就是完全错误的理解了，错啦，往下看吧，MBR-如果用LBA它支持的硬盘最大就是2T，如果是CHS，最大硬盘支持的就是8G。不是说的一个分区最大值哦。 [root@vpn ~]#echo 2^28*512/1024/1024/1024 |bc 128 👈128G [root@vpn ~]# [root@vpn ~]#echo 2^32*512/1024/1024/1024 |bc 2048 👈2T [root@vpn ~]# 进一步总结，那么为什么还有大把的人讲MBR单个分区最大支持2T呢，因为如果你将3T的硬盘划分区MBR的哦，划了2T作为第一个主分区，那么剩下的1T空间就用不了了，就灰掉了，说单个分区最大2T的也对，但是不准确，因为单个分区用掉了整个MBR格式的硬盘的2T上限，所以剩下的空间无法划分了。举例 MBR目前依旧是主流的 、 55AA就是标记位。 MBR分区结构 -------------------------- 这0磁道0扇区的512B，不属于任何分区，既然不属于任何分区就没有任何文件系统，没有文件系统自然就没有文件的概念。 最后标记为就是55aa 四个分区，每个16个字节。 活动和非活动，所谓活动分区，就是计算器启动的时候找80标记的分区，从该分区上找操作系统（引导操作系统的相关文件）。如果有两个80，就找不到了。就启动不了了。 这个磁头、柱面(也就是磁道数track)、扇区都有了也就是CHS定位了起始位置就有了，就是该分区从哪里开始。磁头从1开始，扇区也是从1开始的，因为扇区0就是上图拉，MBR和分区起止位啦。所以8bit的磁头不是256而是255，6bit的扇区不是64个而是63个。 在上图👆中指出8bit的磁头head、10bit的track磁道也就是柱面、以及6bit的扇区的具体位置。 第五个字节为0表示空间未使用。都不是0上图，就是分区都使用了，都分区了。 CHS如图才3个字节，也就是3*8=24个bit，也就是2^24*512/1024/1024/1024=8G的空间上限。问这是分区最大值还是所有分区-整块硬盘最大值。就算你不知道直接的答案，使用现有的肯定的知识也能得出来，来，3个字节来表示CHS是吧，假设是表示的单个分区最值，那么3个字节全部设置为1，也就是FF FF FF，好00 00 00-ff ff ff 表示8G一个分区，那么第二分区同样也是00 00 00 - ff ff ff ，第三个 第四个都这样，你觉可能吗，CHS是什么是那张硬盘结构图cylinder磁道也就是柱面，磁头也就是盘面，sector扇区，比如C-H-S值为1111111111-11111111-111111，这个东西是啥就是磁盘上的唯一的个扇形拉扇区对吧，我柱面定了-磁头定了-扇区也定了，东西就一个东西，还能在4个分区里出现4次？！扯淡吧，所以_ ___ __ 三个字节是4个分区整体计算分配的空间，比如分区1 CHS是1-10,分区2 CHS就是11--20，3就是21-30 ，4就是31- 40这样的。就像你写word里的序号是续的，不是重新编号的，当然我用10 20 30 便于理解，其实是下面这些值👇，同理LBA也是一样，所以LBA的2T上限就是整个磁盘的。 8个字节是换LBA玩法了。 所以2T也就是MBR的上限了，“也就是MBR可以表达的一个分区的最大容量”，这句话也是错的，我上面已经解释过了。 EBR表达起始位置和下一个EBR的位置，自然就能知道自己的起止位置了。 EBR不固定，是因为扩展分区不固定，第一个EBR不固定，自然后面的EBR都不固定。 但是MBR是固定的，整个硬盘的第0个扇区就是MBR所在，自然是固定的。 上图👆最左边一列是偏移地址，也不知道是怎么意思？⚪不管他参考基准是什么，反正对不上号和下图：是什么，是你大爷，看东西不会思考，第三次过的我来告诉一年前的你， 以前两行为例： 00000000 00000010 等价于 0000-0010 等价于 0000-001F 也就是2^5=32，也就是32个B字节，你数数前两行是不是32个字节，中间段是不是8*4=32个。 所以下图的红框就是位置，就是上图的位置，只不过下图统一用10 20 30 F0来表示，人家就是行开头了， 比如00000000-00000010，就是0000-0010，就是0000、0001、0002、0003、... 000F、0010就是2^4=16就是一行16给字节咯。 这个在MBR分区结构里也有小字表示的见下下图 上图的Error就是出错信息数据区。 ================ 备份分区表 需求来了，分区表的备份。emm，如果分区没了，上面的数据也没办法读取了。所以分区也有必要备份一下。 如果全公司的硬盘都是一个分区方法，基本也无需备份了。有一个导出分区表就行了。 也可以把这个512字节都拷贝出来 现在只备份这个64B的分区表，因为不是文件 所以cp拷不了，所以用dd把二进制读出来。 看二进制的方法来了：od、xxd、hexdump 举例，删除分区表-不是删除分区 就是把最后的标识位清了，但是skip用的有问题，skip是跳的if的设备，跳of=/dev/sda设备，要使用seek。 fdisk和lsblk的区别来了-一个看的硬盘上的，一个看的是内存中的 一旦重启，内存中的分区表就没了，硬盘里的分区早就没了，重启就起不来了。 当前真正生效的还是内存中的数据（分区表） ps：不管你skip510还是seek510，skipxxxx要得到的是/data/dpt里的55aa，显然dpt只有66B，skip写错了，下面改过来了。 这样就恢复了55aa标记位 举例-破坏分区表-保留了标志位 有些是0，就给你用*省略了，不显示了，详细看的就加参数 因为标志位55aa还在，所以还有个分区标题 虽然fdisk有这些磁盘标题Device Boot Start End Bloks Id System，但是没有东西，因为分区表清零了 系统判定有55aa标识位，觉得有分区，但是又不知道分区的起始结束位置。 此时重启，就起不来了，不重启是可以的，因为当前分区表是用的内存中的，也是说分区表还有的。救援模式来恢复可以吗？刚才的备份数据在/data里，但是分区没了/data个毛啊。 所以要备份一定要scp到远程主机上。当然如果你有可以用两块硬盘，分区破坏了一块，可以从另一块硬盘上手动挂载后获取分区备份（加入你的备份在那块上）。 重启模拟故障处理，重启之前先把分区表备份到远端机器上。 提示没有找到分区了 进入救援模式，硬盘起不来，光盘起。 如果硬盘分区还在，就会看到挂载到mnt/sys里面的，由于我们之前将硬盘的分区删了，所以这里自然也看不到了。 但是好像救援模式没有网络，你的分区备份还在远端主机上呢， 可以拿U盘，其实还可以临时配个IP地址的 这就就完成了救援模式下的scp 同时你要知道，此时是光盘加载的，数据拷贝过来都是放在内存里的，重启就没了。 上图👆注意 ：sdb、sdc、sdd都不管，是刚才加的。 再看下a硬盘 上图是看了全部了，下面只看前512一个扇区的内容 55aa前面的64确实是空的（注意中间*省略了，其实是64B的0）。 写硬盘的话：工作原理，是把内存的数据先放到缓冲区里，过一会再放到硬盘里。 dd 命令你看到提示ok了，但其实还在缓冲区里呢。你立马重启缓冲区内容就清了，此时就造成了数据写失败。所以不能捉急。 手动sync同步一下：多次sync就是担心sync没有立马执行。还是要等等。 此时硬盘已经还原，系统已经能识别了--系统就是装在硬盘里的，能识别自然就说明硬盘分区恢复了。 dpt disk partition table ======================================================== 实际上整个硬盘都不能超过2T，不是说分区不能超过2T。超过MBR就没办法了。为啥，分区表里的的bit位算出来的不是针对单个分区的，为啥说是整个硬盘呢。你再看下原来的图 他那个4字节“分区起始LBA”里填写的是起始和结束位，结束位本身就是2^32*512B=2T的上限，2^32个bit位用来多次表达分区起始位置，所以怎么算整体的表达能力就是2T空间。这个好理解，我们拿两个bit位类比来表示空间，00-01,01-10,10-11,11-00，没了，是不是一共也就会2^4个分段。它是续接的，不是每次都重新编号的。 每块硬盘可以有独立的分区格式，下面一块就是MBR ▲面试题有了：问：windows硬盘分区底色是绿色是啥情况？哈哈哈，气死人不偿命~ 变色，就是自动将第4个分到一个扩展分区里面了，然后自动创建了一个逻辑分区。 删除里面的逻辑卷 、 这就是扩展分区划分逻辑分区 再说会GPT分区 UUID是国际标准，微软发布的GUID属于UUID的具体实现。 UUID后面写的是16进制。128bit，就是32个16进制。 IPv6也是128位，这个UUID也是128位。 题外话： [14:54:45 root@pyConsole ~]#cat /etc/fstab | grep UUID |cut -d \"=\" -f 2 | cut -d \" \" -f1 |cut -d \"$\" -f1 |cat -A 07507cea-e91c-42ff-9cc9-ca3eb61212f0$ [14:54:48 root@pyConsole ~]#cat /etc/fstab | grep UUID |cut -d \"=\" -f 2 | cut -d \" \" -f1 |wc -c 37 wc这个算字符是把最后的$也算上去了哦，我输过了是36个~对输过了，要赢回来的 UUID生成工具uuidgen,▲各种生成工具可以整一波~ Protective MBR完全是保护后续的GPT分区信息的。 👆一组4个分区的定界信息。 Partitioin Header头部和分区表 都 有备份 早期的BIOS启动的蓝色界面操作，现在不是这个颜色了，到后面的支持鼠标操作的启动界面 固件接口， 否则BIOS不支持GPT分区作为引导操作系统的 UEFI启动就得配合GPT分区。 BIOS+MBR与UEFI+GPT 这样的，BIOS启动，UEFI启动，这两个启动，启动的时候要引导操作系统的，①而引导如果安装在GPT，就只能用UEFI启动；②BIOS启动的只能MBR分区方式里的引导。③BIOS可以使用GPT分区，只是用来存放数据而不是OS，这个其实是搞笑的说法，因为BIOS通过MBR分区里的引导操作系统启动后，然后就是通过windows/linux操作系统来识别GPT，从而达到在GPT分区中存放数据的效果。 启动的时候只能靠BIOS和UEFI本身去引导操作系统的。 虽然UEFI也支持MBR，但是没有意义了，UEFI虽然支持MBR启动，但必须要有UEFI引导文件存放在FAT分区下；UEFI是无法使用传统MBR引导来启动系统的。 你要把操作系统装在GPT上，就有要硬件的UEFI支持（就是新的硬件已经不用BIOS咯）才能启动。 常用的BIOS+MBR分区里安装系统+另一块硬盘GPT来支持超过2T的空间。这个可以有，不过直接UEFI+GPT不更香么。这是家庭电脑才需一块磁盘超过2T吧，工作中一般来讲服务器-装软件用不了多大空间，而大数据的话用mysql-mysql本身达到T级别早就分库了--拆成好几份，每个机器上放点。也不需要单硬盘超过2T的场景吧。 开始搞命令：lsbk这些cli lsblk（list block） 在centos 6 和 7上都可以用。 👆就是5的util-linux这个工具集咯应该，里面不带lsblk软件，6\\7 带的。 man fdisk👇 👆man 可见fdisk就是不支持gpt分区的，所以生产中肯定不能用fdisk来分GPT的。 man下gdisk👇 man parted👇 parted工具 分区时即时生效的 操作时要小心，很容易因误操作把你的分区表破坏了就。 partprobe同步的问题，后面再说。 parted使用要小心 fdisk查看分区，了解什么字眼代表什么分区 当前硬盘里啥都没有，所以也不存在什么446B的bootloder，这不废话嘛，它都不是活动分区，哪来的引导。 这条命令就是创建个GPT，就是说这个盘是GPT格式的了。下面可以继续用GPT的命令进行分区了。 这是个假的MBR，呵呵 打印分区表 上图单位M是MB 所以可见分区必须是连续空间。 虽然删干净了，但是还是认为有分区表，因为protection mbr的分区标识位55aa还在 你这个512B看的是protectionMBR字段内容，是不包括GPT分区表信息的 上图也看到了UEFI PART。 下面重点看fdisk和gdisk Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:33 "},"12-磁盘存储和文件系统/3-MBR和GPT分区管理工具详解.html":{"url":"12-磁盘存储和文件系统/3-MBR和GPT分区管理工具详解.html","title":"第3节 MBR和GPT分区管理工具详解","keywords":"","body":"第3节. MBR和GPT分区管理工具详解 fdisk和gdisk基本一样， 上图说明虽然分区删了，但是分区的类型还在（protection MBR里的55aa还在，可以通过dd将整个b硬盘的512B清空，这里B盘就啥信息也没有了）。 此时就是没有任何信息的硬盘了，初始接进来的硬盘就是这样显示的了。 这个时候就可以用一些工具对其重新管理（比如分区）。 保存退出，没有做任何设置 扩展分区是5，swap82，主分区是83 默认都是83，将来可能根据实际情况修改ID 只是一个标签，并不是功能性的强制标识。 就是说当前0个主分区，0个扩展分区，4个都是空闲的。 如上图，可以直接划分e扩展分区，从2开始，也不一定非要从1开始。 划分之后一个p就能看到当前划分的结果 上图是以扇区起始位置标记的，再一个就是ID是5，就是扩展分区的标识。 而且上图观察p和l，以前是p和e，其实就是说扩展分区就一个嘛，然后都是在其上划分逻辑分区了。 注意上体逻辑分区是从扩展分区的4096个扇区开始而不是2048个扇区开始的，所以你要知道每个逻辑分区的头部还有一个EBR字段里面都是本段分区的元数据来着。 逻辑分区也是ID85， 上上图的5也是自动的，固定的 扩展分区上的逻辑分区都是从5开始编号的。 上图就说明一个问题，逻辑分区的设备名是不稳定的。 你将来设备名你写到磁盘配置文件里面，你后来分区调了一下，很可能分区名就变了。 所以逻辑分区不适合写到配置文件里。 这就需要找一个不变的东西来应对。后面会讲到。 同时上图带来的问题就是，如下图： 这才2G的最大空间，所以超出了，注意2G是扇区相减后乘以512B可得出。 # 这句话也是错的，上图是1G的空间，算法是对的，结论是错的。 下面修改一下分区ID L是你记不住了就看下ID有哪些可以选择的。 关注上图的82 83 8e fd是软raid 还有一些淘汰的技术 上图是修改分区id之前的sdb3的id为83 下图是修改为82 此时还未存盘退出(w)，此时是否真的分区成功了没？👇 3种看分区的方法 上面4个方法都是没看到b硬盘上有分区的。 然后w存盘退出后，再观察 硬盘里的分区表和内存里的分区表并不是时时同步的，也就是说fdisk 分区的时候，w保存退出后并不是时时同步的。很多时候硬盘上的分区表分好了，但是内存中的没有。 ========================== centos6，以柱面为分区单位的，一个柱面8M 观察上面WARING，告诉你reboot后就同步了、partprobe或kpartx也行。 此时centos6的磁盘确实分区了，但是内存中同步不了。 不跟选项，但是要跟同步的设备哦 上图就是centos6的bug命令，不好使。记住了！！！partporbe在centos5和7上都可以，就是在centos6上不行。 可以kpartx，或者只直接partx 其实centos8是有-h的 报警error不用管，就看结果对不对-sda6出没出来就行了 上面partx -a是添加分区同步，删除分区同步用-d 但是内存里还在 ==================================== centos7也同样出现了waring ------------------------------------------------ 但实际效果应该是没有的，即使partprobe同步了。 虽然对分区再分区，且同步了，但是最终还是看不到的。 ---------------------------------------------------- 需求：将B硬盘分区分的和A一样 思路：复制A的分区表就行了。 操作：不就是A的那个64字节么，扣出来复制到B那边就行了。 所以克隆只能克隆主分区和扩展分区。但是没有办法克隆逻辑分区，因为512B里的64B是MBR，而逻辑分区是EBR是再扩展分区里的。 1、克隆的目标硬盘空间要大于等于原硬盘； 2、无法克隆逻辑分区，只能克隆主分区和扩展分区。 不过怎么看图上EBR也是可以克隆的啊。估计就是EBR比较多分散，MBR固定好操作。EBR还要计算。 再来，B硬盘和A硬盘一样大，此时克隆MBR就很OK了。 EBR的位置不在MBR那边，你克的是512的MBR里的64B，EBR在后面的扩展分区里面的EBR头 这个就是永久保存了，不存在再write，w写的动作了。 ======================================== gdisk 主要用来分GPT格式的区 都是和fdisk一样的 82\\8e这些都在 分区太多了，清一下，fdisk肯定可以清，就是太慢了 这么记笔记太慢了，体会不到学习的乐趣和快感！，，，i need fly，时隔1年，no u need 跬步 上图hexdump -C -n 512 /dev/sdb -v加个v就能看全部的0，上图默认是省略连续的0了。 不过为啥中间有28 c7 86 4a这几个值，而且我的centos 8 的情况也不符合这里所讲的，但是肯定centos8是使用正常的，那是因为我sb了，因为下图看错了，应该是sda而不是sda1。 如果把数据放在硬盘的磁道的外圈，速度读取和写入就快，如果是内圈就慢。所以再看看内外圈。 如果是centos-6的话就很清楚，本身有柱面，数字越小就越在外圈。越往里数字越大。 所以分区划分编号越小数字越快。6和7就是偏内圆了。 如果你希望数据读取快点，就将数据放到前面的分区。能提高个百分之几就不错了。 sda6里面全是0，你把数据放进去也只能0101往外读，因为还它没有文件概念。 创建完文件系统才有文件的概念，目录、属性、分门别类这些东西才会有。 注意力，持续 不同的文件系统除了必备的功能-文件管理功能，有的可能还有一些额外的功能。比如，NTFS除了文件管理，还有加密、 这就是windows的NTFS加密，只要看到 \"安全\"选项 就是NTFS系统 右键-属性-高级-加密就行了。 这是对别人是打不开的，而本人是无需的还是正常操作，没有影响（打开时自动解密，保存的时候自动解密）。 ext\\xf系统的没有这个额外的加密功能。 但是基本的管理文件功能必然都是有的。 对于linux 来说 xfs 、 ext 文件系统： linux支持的文件系统👇 当然也可以 这是目前linux里支持的文件系统类型。 在centos6上都是ext的 ubuntu用的也是ext centos 7用了个xfs，而ubuntu用的是ext4 ext1没有存在感，ext2存在重大bug，容易奔溃无法恢复，所以后来有了ext3 ext3的日志出现的优点：当系统写数据之前是先写日志的，日志一定是先于数据写，写好日志信息以后再把日志信息同步到磁盘里去，好处就是： ①万一日志写完了，还没来得及存盘，系统突然奔溃没关系有日志可以还原。②如果写日志的时候突然奔溃了，没关系，日志大不了不用了，数据又没破坏。 三刷的时候发现其实本质还是，目录和内容的关系，日志和内容都是数据，只不过日志是内容的简述，日志可以很短的时间内完成写同步的操作，内容量大的时候就比较耗时，所以加了日志容错变好些了。 读取文件，内存中修改，写到日志里，再写到磁盘里。 如果③和④中间断电，系统起来后，磁盘同步日志就好了。 如果③没完成，大不了日志丢了，数据还是完整的。大不了没更新嘛。 但是要注意，如果②没写完，写了30%假如断电了，那么日志里是有30%更新的，而系统后面起来后，磁盘同步日志，那么还是有70%数据没写进去的。 ext3的问题，性能问题在大磁盘中得到凸显，大磁盘T级别的比如，健康检查耗时达数小时之久。 为了解决这个 问题就推出了ext4，相对于ext3性能得到了巨大提升。以前小时级别的，现在分钟级别了就。 但是ext4支持的文件大小有局限性。家庭都是T级别了，生产中高达P级别。 于是xfs文件系统更大了，性能更高，但是linux上单一文件不会超出ext4的局限是，所以unbuntu上还是继续使用的ext4。 插入，在生产中拷贝大量数据时要限速，否则后果兜不住！找个空闲的时候，加上限速复制就差不多了。 上图的btrfs xfs：SGI是老牌的UNIX的厂家。早期的大片泰坦尼克号就是用SGI工作站做的。 btrfs是oracle的号称很优秀的文件系统。centos7上有不过是测试阶段，centos8就不在支持了，该文件系统就没有流行起来。 jfs在AIX小机上用的文件系统。 swap 和 iso9660光盘 FAT12是软盘， 这些是windows的文件系统。 exFAT是U盘的文件系统 U盘右键格式化的时候可以看到 refs是新的了看来 除了上面的单机的文件系统，还有网络文件系统 还有 还有就是分了区但是没有创建文件系统的情况，就是RAW。空文件系统、裸的文件系统。oracle，为了高性能，把数据库的data直接放到RAW上，就是0101010没有文件系统直接存盘，利用oracle自家软件直接利用010101去访问磁盘，由于中间没有文件系统这层，所以读取速度更快，缺点就是没有文件系统，如果oracle软件出问题，运维管理是个麻烦。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:34 "},"12-磁盘存储和文件系统/4-文件系统管理实战.html":{"url":"12-磁盘存储和文件系统/4-文件系统管理实战.html","title":"第4节 文件系统管理实战","keywords":"","body":"第4节. 文件系统管理实战 文件系统的格式化咯 这是centos6和7上的分区，sdb1和sda6\\7，都还没有创建文件系统，而创建文件系统就是所谓的格式化了。 一般就是ext4或者xfs，其实还是看系统默认支持什么（centos6默认ext4，centos7默认是xfs），就用什么。 管理工具就是mkfs这个工具 注意，文件系统本身是属于操作系统的功能，由内核来完成的。内核支持这些个文件系统，就是说内核提供了（内核级的）关于这些个文件系统的驱动模块。这点可以通过locate确认 locate xfs.ko，文件系统都是ko文件，查看有没有xfs文件系统的内核驱动。 查看ext4文件系统的驱动：xz就是压缩格式 这些都是属于内核级的功能。 那么问题来了ls，cat这些命令，对于用户来讲他访问文件，ls、cat去访问这些文件的时候，其实是通过文件系统来存放到硬盘上的。 那么这些命令是怎么和文件系统交互的呢？要知道底层用的文件系统有很多种，这些命令又是如何对应N个文件系统的呢？是不是意味着针对xfs有个ls.xfs子模块，针对ext4有一个ls.ext4子功能呢？显然不是这么干的！不可能让一个软件开发者考虑这么多种文件系统的。 所以为了让用户更专心的访问磁盘文件而不是考虑文件系统的差异，让开发或使用人员更透明的使用各种文件系统，就提出了VFS概念。 什么是VFS虚拟文件系统呢 题外话，其实从上图可以看出来一点点，就是cache缓存偏向于VFS，buffer偏向于硬件了； 还有一个简单得说法--缓存是读，缓冲是写磁盘得时候。其实这种说法里得缓冲也是说得磁盘，这就和上图不谋而合了，冲 更多是针对硬盘或是针对硬盘驱动得，缓存更多是偏向于VFS文件系统得。至于读还是写，这个我暂时人为是不分得。 对于用户cat ls cp mv等命令不会直接跑到磁盘上去操作磁盘文件。用户空间的程序是没有权限没有能力直接访问硬件的（而文件就是放在磁盘硬件上的）。 所以要用系统调用，让操作系统内核帮其完成访问磁盘上的文件 对于fat、ntfs、xfs、ext4 而VFS就是一个集大成者：把各种各样的文件系统通用功能收集起来，对外让用户来访问，用户就无需关心下面各种文件系统的区别，因为统一都是用VFS这个文件系统来访问。 这样用户开发的软件都是和VFS打交道的（比如ls、cat、mv等都是和VFS打交道的），而这个VFS文件系统再和底层的不同文件系统打交道就不用我们关心了，这是有内核来完成的，是操作系统本身来完成的。 这样就开发一个诸如ls for vfs就行了 如图可见当前不支持NTFS这种windows的文件系统，因为用的少，内核就没有内置添加这个功能，你要用自己编译到内核里。 源码编译见上文http的操作。 正常用不到这么大，用到了性能就不行了，虽然支持这么大。 创建文件系统 mkfs.ext4=可能还不等价于=mke2fs好像是默认格式化ext2的！！ 还可以这么写，mkfs -t xfs 、mkfs -t ext4 mkfs 格式化后怎么知道具体是什么文件类型 mke2fs /dev/sbd1 默认是ext2 上图的 因为本质上ext2和ext3是相同的，就是3多了日志，所以这两个是兼容的。 硬盘分区的空间可以理解成两大块内容，一个是metadata元数据-节点表（存放文件的大小、权限、时间、所有者、还有一个指针指向数据存放的位置），一个是数据。 实际上还要更加复杂，分区划分成多个组group，每个组是由多少个块组成。 而每块多大？ 不同地方的blocks意义不同 文件系统的blocks意思和磁盘分区里的blocks的意思不同 存放文件的时候，必须以block的整数倍来计算。哪怕一个字节的文件，你也要给我4K字节的空间。 分区里的block就是1个block就是1k。 如果磁盘里都是大量的1k，2k的小文件。此时4k块大小，空间就浪费了很多。如果文件都小，那么文件系统的block最小单元就别默认4KB了。 而目前文件系统，块大小支持：1K\\2K\\4K这3个单位。 4K是系统根据硬盘分区自动分的。如果分区偏大，自动给你4K的block，如果硬盘小自动给你1K\\2K的block。比如你分区就100M，默认肯定不会4K的、 也可以手动指定，-b 1k ，就是手工指定block大小了。 在磁盘上组织空间的时候，是把若干个连续的块blocks组织成一个group 这样把磁盘划分完。分出来后，每一组group里面在有自己的节点表。 superBlock：①分组的描述 从第几个块到第几块是一个分组，算是一个分组的起始位描述。②自身的元数据，比如块大小比如4K，这是文件系统的属性也要放在超级块里。 还可以手动去查超级快superblock： 这个就是最早提到的元数据 磁盘上每个文件都有元数据。这一条记录就是存放的一个文件的元数据信息。也是节点表的一条记录。 这一条占的空间就叫节点的大小。 256B，就是这一行占256个字节。 当然256B不是固定的，可以再添加属性，比如ACL这种扩展属性，这样一行的空间就变大了。 与此对应的就是节点表inodetable空间占用大了，后面真正存放数据的空间就少了。 DateBlocks数据块，一块4KB；然后inode table里的一行也就是一个节点信息占256B大小的空间。 每个文件分配一个节点号，这里66384个节点号，就代表了该分区一共可以放66384个文件。 所以上图可以反推出磁盘空间大小：所以4096单位是B字节，一般block size就是4KB的大小。 话说回来，虽然块很多，但不代表所有的空间都能真正用起来。 如果你也分100G的空间，那么5G是保留下来的。这个5%是默认的行为。 5G是干嘛用的呢？是给root用的，留给0 ID的人用，防止非root用户把磁盘空间占满导致root没有空间了，改个文件，加个字节都加不进去。保证了运维人员的可维护空间。 5%的预留不一定合理，如果10T的空间，500G的预留就很夸张了。可以调节百分比，支持0.1%。 superBlock超级块还记录了分组情况，只是上面的tune2fs -l /dev/sda1命令还看不出来。 换个命令可以 0 - 8 共9个groups 如果superBlocks坏了，文件系统就完了（因为分组分到哪都不知道了，属性也没了！）。 所以superBlocks需要备份，实际上也有备份 superBlocks备份了好几个地方：32768\\98304\\163840\\229376这四个地方都有备份。 所以利用超级块是可以修复出故障的文件系统的。 比如掉电、软件故障造成文件系统的元数据破坏，实际上是可以修复的，因为有备份的。 貌似是奇数有超级块的备份，偶数没有。但是这不是统一的规律，去看看别的。 dumpe2fs /dev/sda3 |less 分页看下 到group9，后面就没了。就是备份那么多就够了，不需要太多。 ------------------------------- 块位图blockBitmap 就是blocks，一个group里有32768个blocks，总共也有很多个块。 将来使用的时候 存放文件的时候 就需要挑出空闲块出来给到文件存放用。 所谓位图就是1bit1bit的图，0表示对应的block为空，1表示对应的block已使用。将来文件用的时候，就找出位图为0的block给它用就行了 这就是整个ext2为例的文件结构，其他相同的，差不多。 xfs的结构不太一样 meta-data 元数据区 data 数据区 naming 是啥？ log 日志区 realtime 实时区 isize=512是节点inode size，和上面ext4不一样（ext4是256B），这里大小是512B。 sectsz 扇区512B bsize块大小也是4KB log 也有日志功能，日志也有块大小 数据的realtime实时空间，每创建文件的时候，先把数据放到实时空间，等写完了，再放到真正空间中。 xfs查看的方法 xfs_info: /dev/sda is not a mounted XFS filesystem 👈需要挂载后才能查看 [root@centos7 ~]# xfs_info /dev/sda1 meta-data=/dev/sda1 isize=512 agcount=4, agsize=65536 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=262144, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@centos7 ~]# -j 等价于-O has_journal等价于直接-t ext3。-O ^has_journal 是删除日志功能 -i，inode和磁盘空间有对应关系，多大空间对应一个inode。一个文件是要消耗一个inode的，而一个文件有要消耗至少一个块，一个块如果是4K，也就是说以一个文件要消耗至少4K空间，同时消耗一个inode，而inode又被你对应成了1K，也就是说，此时一个文件要消耗可能4个inode才能对应上，而inode本身又是占空间的，inode之前说了一行一个inode信息消耗256B，所以就造成了空间的浪费，积少成多也是可观的浪费了。 -N 指定分区中创建多少个inode，同样也是摆脱不了-i的默认值的，也就是说你指定的一个值，系统并不会精确到你指定的-N的，而是考虑到-i的大小的。 -I 一个inode占用多少磁盘空间，默认256B -O FEATURE ：ext文件系统了除了2，3和4都是由日志功能的， tune2fs只能看ext系列的superBlock信息，xfs的看不了 一般直接创建ext4就好了，不要创建ext2然后-O FEAUTRE没事找事，除非是已经有一个ext2了，然后再追加一下。注意mke2fs是创建的命令，对于已经格式化成为ext2的硬盘分区，需要使用tune2fs命令来追加。 例子：创建ext2 块大小1024B、预留0.1%、inode大小128B、卷标/mnt/sda6 👆上图bolck size是1024B字节，一共是1048576个块，所以这个sda6大小只有1MB。 这个ext2是没有日志的，所以查看下 👇此时sda6的ext2后面就追加了ext3字段。其实ext3就是比ext2多了一个日志功能，此时就是ext3了。 关于acl的功能来源-挂载选项 centos6系统(7除外)后面分的区--不是安装操作系统时分的区，是没有这个Defeault mount options选项的。 某人看到这 就要怼了，“你的沟通有问题，不是没有这个选项，是这个选项的值为none”，所以跟这些人讲话，要小心些，唉~不要碰枪口，除非你比他牛，他就不怼你了，他会反过来将就你，这其实是有问题，客观来讲有一说一，但怼人是不对的(你Y的沟通才有问题，逮着机会怼人你牛逼~操)，这些人的态度两极分化的厉害所以不喜欢他们。不过反过来，你自己要是凶一点(不是让你怼人哦，就是态度明确硬朗、正)，工作上可能会轻松点这倒是事实，一些喜欢借力、甩锅的人就不太愿意和你怼，因为你凶啊，哈哈~。 没有的，可以通过-o acl添加该功能 再查看就有了，此时就可以用FACL了 会加也要会删 补充说明，这里既然是挂载选项，那么就有这个mount -o acl，不过就是centos 6需要这里的两种方式来处理（上面的一种tune2fs，还有这里的mount -o），现在都是7不需要的。 一个分区可以用三个名字来表示它 UUID具有固定唯一性，其他不具备唯一性。 早期都是些卷标，从centos6开始推荐开始写UUID了。 根据UUID查分区名、根据卷标查分区 blkid -U 等价命令 给ext系统加卷标 xfs系统的卷标使用xfs_admin，不过需要先卸载 不带-h就是超级快和group分组全看 dumpe2fs -h /dev/sda7 等价与tune2fs -l /dev/sda7 xfs_info必须要挂载才能看，上文应该有说过了。 # 这个具体看了，centos8和rhel8不同， fsck 可以修复ext和xfs。 [20:41:23 root@localhost ~]#fsck. 👈两下tab补全 fsck.cramfs fsck.ext3 fsck.fat fsck.msdos fsck.xfs fsck.ext2 fsck.ext4 fsck.minix fsck.vfat [20:41:23 root@localhost ~]# 提示可以看到支持修复的文件系统类型，但实际上直接fsck不带后缀更好，因为万一敲错了，反而坏事，不加会自动判断对应的什么系统。 修复xfs文件系统举例 超级块一般多大，在一个分区上都是些描述信息，应该很小，但是具体多少不清楚。 上图的三个选项一个都不用，-f 是修复文件、-d是修复根。 直接xfs_repair /dev/sda3，一定不要挂载的时候修复。 修复后就可以挂载了 但是数据确实丢掉了，看来dd前10M破坏比较严重的，修复估计也就是修复超级块吧，因为有备份，可能也能修复些其他的，具体还需要研究下。 例子2，修复ext文件系统 bad magic number， 超级块没了已经 取消挂载 修复 输入 -y就行了，自动帮你输入yes 修复后挂载，看下空间大小 如果破坏的不是太严重，可以修复一些数据回来的，备份的都是元数据、分区信息、超级块之类的，用户数据是不会备份的。 这是二次破坏！👇 使用一个磁盘空间的3步骤： 以上就讲解了 1、分区；2、创建文件系统、3下面就要讲挂载了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:35 "},"12-磁盘存储和文件系统/5-文件系统挂载.html":{"url":"12-磁盘存储和文件系统/5-文件系统挂载.html","title":"第5节 文件系统挂载","keywords":"","body":"第5节. 文件系统挂载 挂载的理解 一些相关信息梳理 sda是硬盘，123是分区号。 1、硬盘格式化也就是创建了文件系统xfs\\ext4之类的之后，就需要挂载了，这里的挂载应该是带着各自独立的文件系统和VFS打交道了吧，反正硬盘空间还需要挂载到文件系统上， 2、/dev/sda1挂载到了/根上后，根下的所有当前存在的文件会在/dev/sda1里看到么？我们知道安装系统的时候根是挂载到剩余分区 https://www.linuxidc.com/Linux/2019-04/158216.htm 10.4.2.95的前2个盘位各装2块固态硬盘做RAID1，最小化安装CentOS7.9，分区biosboot 2M , /boot 500M,剩余分区/ 。 一旦把sda2设备挂载到data后，后续在data里创建的文件都会出现在sda2中。 1、一开始是sda2挂载到data里，并创建了一个/dir/f2.txt文件 2、然后再将dir挂载到sda3里，这个时候进到dir文件夹实际上就是看到的是sda3分区， 之前的f2.txt文件还在sda2分区里，所以现在挂载到sda3下了，所以就看不到f2.txt文件了。 如果再将sda2挂载到/home目录下后，就又可以通过/home看到f2.txt文件了（看到个屁，看不到的哦，想想就不对，做了实验也一样看不到的），而且还看到dir文件夹，一旦进入该文件夹，就等于进入了sda3分区。所以从/home也是可以一路顺下来到dir里的。同样\"/根下面的放的都是一级目录\" 说的就是这个道理，因为很多一级目录都是自己另立门户的。 一种理论上的循环挂载 sda2挂载到/home文件夹，sda3挂载到home下的wang文件夹，sda1挂载到wang下的dir 这样，进入到/home就看到sda2里的wang，进入wang，就看到sda3上的dir，然后dir再挂载到sda1，于是进入dir就看到了sda1上的/home，如此就完成了理论上的循环，linux不允许，windows时可以实现这个效果的。 就是D盘挂D:\\test文件夹，然后循环点击效果看看 右键更改挂载点 继续点，层级还会增加 数一下多个层 复制到linux里然后grep一下 31个层级就是循环挂载后的 套娃上限。 挂载操作 umount 直接跟设备就行了 这种挂载重启后就丢了 挂载选项-只读挂载 此时就👇 重新挂载成rw读写，没必要unmount 在mount，可以用命令mount -o remount,rw ，它不是真的取消挂载。效果上是重新挂载，但并不是取消挂载的。 来了，一般分区可以取消挂载unmount再mount挂载，而/根不能unmount。 根/ 不支持unmount，但是可以remount。 因为/根要是取消了，/proc这些内存中的东西不也都是在/根下面嘛，如果根取消，那么内存的东西也就没有FS支持，也就看不到了，也许当前操作界面都没了。 此时重点来了，如果/根要修改挂载属性，就得用remount命令，所以remount可不是真正意义上的先unmount在mount。 和挂载密切相关的文件mtab 该文件就是显示的当前挂载情况 这个文件貌似/etc/下面的，而/etc/下面一般是配置文件，一般莱昂配置文件固定不动得，但是该文件确实实时和当前的挂载信息保持一致的，通过ll /etc/mount可见其实是proc内存文件里的软连接👇--这话不对，centos7上是软连接，而centos6上就是普通文件 mtab是实时挂载信息文件，而fstab是实现自动挂载autofs的配置文件？没错吧，错了，fstab何autofs服务并无关系。fstab是独立的系统加载的挂载文件，autofs是个独立的自动挂载的软件需要安装的,autofs和/misc这个神奇文件夹有关系。 挂载推荐使用UUID设备名来挂载 起个卷标 此时可以通过卷标来挂载 上图注意-L /mnt/sdb1是起的个卷标名 故意和文件夹/mnt/sdb1同名的。 使用UUID挂载 写到配置文件需要使用UUID，即时性的cli还是不会写UUID，而是写名字就行了。 伪文件系统 上图其实是共享内存等内存信息。 这些不用管，都是自动挂载。 一个设备能否挂载到不同的文件夹(挂载点) 可以，将已经挂载的sda7，再次同时挂载到dir1和dir2文件夹下，此时dir1下创建的文件，dir2同样可见。 但是一个挂载点也就是文件夹，只能同时挂载一个设备, 存在分区被顶掉和被顶掉的回来的这个一个逻辑。 注意上面几张图是连起来的，中间没有任何其他操作，所以此时/dev/sda7不仅仅挂到了/mnt/dir1上，还同时挂载/mnt/dir2上呢。 被隐藏的文件 ①本来/mnt/dir1/sda7/sda7.txt是存放在/dev/sda7分区的，②现在将/mnt/dir1/sda7文件夹挂载设备/dev/sda1。此时/mnt/dir1/sda7文件夹下面显示的就是/dev/sda1分区的内容，所以原来的sda7.txt不可见。理解思路，相当于只要你进到/mnt/dir1/sda7想看sda7.txt文件时不可能的，因为一进来就到了另一个分区了。 该文件就永远无法访问，除非取消挂载一次，恢复到前一次的挂载。所以当前该文件就变成了无法访问，但是又占着磁盘空间，于是就成了垃圾文件了。呵呵不一定，也许人家就是要这样的私密文件呢，哈哈 \\* 人世间这个空间，就是多层空间的最后一个挂载，前几层的空间被覆盖隐藏了；得道了就umount了，就恢复到上一层空间了。轮回就不断mount覆盖， 方法论：挂载点也就是文件夹，一定要是个干净的空文件夹▲ 这里有个问题，你不图形化，后面不能一下子记住，还是会忘记。 要找一个生活中的场景：有了，房子有多扇门，多个门能进入同一个房子，但是一扇门不能进入2间房子。房子就是设备\\分区-用来存放东西，门就是挂载点文件夹-只是个入口。 助记来了：一个文件夹只能挂一个硬件设备；一个硬件设备可以挂多个文件夹；好比一个房间可以有多扇门，一个门他不能属于多个房间，对吧，别抬杠~ 然后，同一个设备分区，挂载多个文件夹，第一个挂的那个文件夹在df中显示出来的。 如果别的设备也挂到已经被挂载的文件夹，则前一条挂载信息还在，就是被覆盖了，只要后一条mount信息umount掉就能恢复前一条挂载信息了，mount可见的。 其他挂载选项 一般不用写挂载的文件系统类型，mount会自动判断分区设备的文件系统类型，会自动补上-t vsftype选项的 这个时候写不如不写，就和上一节的fsck修复文件系统会自动发现是什么格式的。▲ 上一节竟然将重要索引字段写在了图片里，不利于搜索，以后图片上面尽量别写字。 默认就是可读可写的，所以一般也不用加这些选项。 加上-n选项就不会自动更新了 实际上确实挂上了，但是mnt里没有，所以df 也看不到 这个可以理解成隐藏挂载 mount、df都看不到 有个地方全都看得到，就是内存里，但是centos7下没有这个-n功能，因为7里面的/etc/mnt就是/proc/mounts内容，所以你-n在centos7下没有意义。哈哈 -a 和fstab有关，后面再说 一般是设备往文件夹上挂载，还支持 文件夹 往文件夹上挂， mount -B /boot /mnt/boot 要求文件夹得是个块设备 解决了硬连接不支持文件夹的问题。莫名其妙的，用软连接不行吗。 也支持 文件 往文件夹上挂 要求文件必须符合一定的文件系统要求 前面都是针对分区做格式化 也就是做成ext4\\xfs之类的，现在找一个大文件来弄 1、生成一个文件/data/disk 2、针对该文件创建文件系统 直接查看确实有的 这个disk文件上面就有了文件系统，既然有文件系统，就可以和分区一样挂载到文件夹里 挂上后的显示效果不太符合预期 PS：在系统中，本身挂载时不允许 一个非设备(分区自然算设备的)往文件夹上挂的。现在时文件往文件夹上挂，就分配了一个loop0环回设备，用loop0设备和文件关联，然后再用loop0设备往文件夹上挂载，这样就间接实现了文件往文件夹上挂载的结果。这一点在centos6上需要手动加选项来实现loop设备的分配 然后再看下centos6上面的情况 centos6直接显示的就是/data/disk2文件，而不是loop设备。7是不需要手动指定，显示的是loop设备。 可以通过losetup -a查看分配的到底是loop几？ loop的个数，centos6最多支持8个文件挂载，7没有这个限制 centos7是自动生成的。/dev/loop0就是自动生成的 如果centos6的loop设备消耗光了，也有办法，mknod自己创建就行 7 100是设备编号类型 b是块设备 上面是命令手动添加，可以修改内核实现开机即得 重启就会得到100loop设备，centos7用不着这样 进一步 实现人工指定loop几，而不是自动分配， 题外话，我不知道为啥老师要讲这么久loop，有毛用啊？ 上面重启过了，所以之前mount的应该丢了，就不需要unmount了 上图报错，排查下,loop写错了是66不是6 拷点文件过来 然后unmount后复制到其他机器，去挂载查看文件 然后到192.168.37.7上去 告诉你了，上面有ext4的文件系统 相当于U盘了，不过要这么麻烦么，直接复制不香吗？哦，具有一定的隐藏效果，别人打不开这个/data/disk2，也不知道怎么看，你一挂载就可以看了。 mount 选项复习 通过tun2efs -l /dev/sdb1看下当前分区有没有支持默认acl的挂载功能 挂载后，复制文件过去 可见centos6默认是不支持facl的 两种方法上一节也讲过，①就是修改文件系统的挂载属性，②就是这里的mount -o带上acl 取消acl的也是用mount -o remount,noacl /mnt/sdb1 这个和fstab有关，稍后再说 这是，设备(分区)里面的执行文件能否执行。 这个厉害的，挂U盘的时候，▲如果U盘里放一个带有suid的vim二进制执行文件，然后拷进去，这样就很危险了，相当于继承了管理员权限了。 此时考虑安全，所以挂载U盘都是就要mount -o nosuid 其他： 同步，就是立即写磁盘，内存改了磁盘也改了。异步就是放到缓冲区里等会再写到磁盘里。 异步 速度快，因为对于程序来讲 放到缓冲区里就算存储就结束了。这时候如果掉电就GG了 同步 更可靠些，速度慢点。 mount 后面的选项啥也不跟就等于： 以上都是临时性挂载，稍后介绍持久性挂载。 findmnt看着还可以 fdisk gdisk parted partx partprobe e2fsck e2label blkid dumpe2fs mkfs.xxx mkfs mke2fs mknod tune2fs xfs_info xfs_repair losetup findmnt uuidgen Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:35 "},"12-磁盘存储和文件系统/6-持久挂载实战故障排错和swap空间管理.html":{"url":"12-磁盘存储和文件系统/6-持久挂载实战故障排错和swap空间管理.html","title":"第6节 持久挂载实战故障排错和swap空间管理","keywords":"","body":"第6节. 持久挂载实战故障排错和swap空间管理 如果是别的终端，或者是别的app使用了该分区，就同样无法unmount wall 广播通知功能，也只能通知到登入用户，那些不登入的应用是无法收到wall通知的。 wall没啥用 通过fuser -km /mnt/sda7杀掉所有占用该文件夹的进程 1、通过fuser -v 判断文件夹是否被占用，这虽然是说unmount的前提，但实际上判断文件夹是否被使用 本身就是一个独立的动作，不要和unmount强相关。 2、通过lsof判断文件夹是否被占用 如何判断一个文件夹是一个普通文件夹还是一个挂载了设备的文件夹呢 df 可以看，但是centos6上看的不全--因为挂载的时候可以隐藏的。 通过findmnt /mnt/sda7来查看给文件夹是否为挂载点 而findmnt 的$?的结果也是0表示true--挂载，1表示false--未挂载， 这是脚本方面使用的特点 永久挂载 永久挂载不是自动挂载，autofs才是自动挂载的工具--才是神奇的文件夹/misc 下图可见/etc/fstab是安装操作系统的时候生成的，因为anaconda就是安装系统项目的程序名-应该是应答文件吧。 查看fstab来自于哪个包，再看下该包里有一些其他什么文件 可见setup包都是必装的基础文件 man 5 章可见具体帮助信息 UUID=字段是设备名，还可以用LABEL=\"XXX\"或/dev/xxx来替换，不过一般还是UUID，因为字段稳定不变。 第二列是挂载点，也就是文件夹 第三列是文件系统 要注意必须和设备分区自身的文件系统匹配 第四列 defaults是挂载选项，前面讲过 mount -o ro,acl,nodev,等等，不写也是有很多默认值的，上一章有讲。 最后两列 0 0，在centos7上意义不大，在centos6上有用 倒数第二列是备份频率，需要专门的备份工具配合的 备份工具是专门备份整个分区的，类似于dump，然后这里会记录下来 这种备份工具用的少，更多的还是tar打包文件夹 没必要对整个分区进行备份。 1就是dump了一次，0就是没有备份分区，因为没啥用，所以centos7上都是0了 最后一列 是文件系统的检测顺序，开机的时候会用fsck.工具来检查，因为centos7上都是xfs，所以也不用了。 举例centos6上 先blkid看下设备分区，将/dev/sdb1挂载到/mnt/sdb1文件夹 然后编辑/etc/fstab文件 写/dev/sdb1设备名不推荐这里实验无所谓了 acl,noexec写出来的就是人工设置的，没写的就是按defaults里面默认的一些选项，有冲突的还是按前面写出来的设定。这里算是mount -o和tun2fs -o 一共3个挂载选项修改点了▲ mount -o remount,rw,acl这样就可以修改了应该。 要是fstab文件生效，就需要mount -a重读此文件 然后验证下noexec挂载选项的效果 删掉noexec就vim /etc/fstab里面删掉noexec就行了 上图去掉noexec后，mount -a 想去掉noexec选项，是不好使的👇 mount -a什么时候好使，不得好死~哈哈，人求得好死是吧，mount -a是原来没挂过，然后mount -a就会是选项acl,noexec生效，但是原来挂过，再修改选项就不会生效。 unmount 再mount -a就好了，更好一点得就是使用remount ，也会自动读取/etc/fstab里得配置的 此时就可以执行了 演示一下fstab 设备名不存在的情况 上图👆注意上图的 最后一行0 3，是要做文件系统检查的 上图可见，fsck 做文件系统检查了就出问题了因为没有这个UUID。 ctrol + d还是重启，没用 输入root口令进去 发现只挂载了个根/ vim 进去 搞定开机OK 但是存盘发现写不进去 发现目前就是只读状态，不仅仅是/etc/fstab写不进去，整个根/都写不进去 mount看下 👆确实写的是rw可读可写--但是有warring告警，但是实际结果就是只读的。 重新挂载一下根/ 此时，再去vim /etc/fstab修改最后一列为0，wr就可以保存了 此时就可以正常启动了，不用reboot，直接切换到5模式(图形界面)就行了。 进入系统后，修改正确的UUID就行。这里顺便测试一下LABEL卷标的效果， 然后mount -a就行了 所以/etc/fstab最后两列还是写成0 0 吧，别做检查了就，能不能挂上去再说，先把系统启动起来。▲ 对于centos7不存在这类问题 拿sda6做实验，重新格式化为xfs，为啥要重新格式花，ext2不行吗？不懂 此时就得到了一个xfs格式的sda6 然后取出UUID字段，保持格式上同就行了 修改后保存 确保挂载点存在 如果存在会提示不能创建的，就像这样 [11:20:23 root@localhost ~]#mkdir /root mkdir: cannot create directory ‘/root’: File exists 可见挂载成功， 故意写错fstab里sda6的UUID，重启看能否重启成功？ 上图说要等1min 30s，还是起不来的，又到了和centos6一样的界面 同样通过root口令进去 因为：👇 mount看下就是rw可读可写的，也没有告警提示。所以就可以rw的。 然后直接启动就行了 SWAP df里看不到swap，但是lsblk里可以看到 一般linux服务器上用也不会休眠 如何新建SWAP分区， SWAP是分区，分区时无法扩容的，所以只能新增一个SWAP分区，两个加起来用 SWAP模拟内存，硬盘应该放在磁盘的外圈，所以下图的磁盘，SWAP分区应该放在哪？ 新的SWAP是创建到sda8还是创建到sdb上？答：创建到新硬盘sdb上好，因为新硬盘还没有分区，新分区时从外圈开始划的。 通过fdisk -l 看下swap的编号是82 开始分区 w后没有告警，所以连同步都不用做了。 可见👆多了个新的硬盘分区 分区以前都是mkfs.xxx但是tab补出来可以看到不支持swap类型的 得用mkswap来创建swap分区 创建之前通过blkid /dev/sdb1看下现在是：没有文件系统的--虽然显示了个dos。 mkswap /dev/sdb1，可见👇一些告警：说没有删除一些引导扇区标记，就是保留了硬盘之前的信息，不用管。 此时就可以看到文件系统为swap了 之前分区的时候使用的时默认从sdb硬盘的2048字节开始分的， 这就是swap之前的信息，前面几行到55aa是MBR分区表，后买面是加的SWAP分区的信息。然后SWAP是空的，所以很多都是00。 这块遗忘了就搜一下大概情况如下 然后就是挂载--且是持续挂载： 两个swap，挂载点是swap，文件系统也是swap mount -a 对于swap是不起作用的 此时内存文件中看swap只有sda5没有sdb1呢 使用swapon -a 读取/etc/fstab里的记录是swap生效 等价命令swap -s 和 cat /proc/swap 然后两个设备sda5和sdb1都能放swap的数据，但是sdb1是外圈磁盘，所以速度更快些，所以希望调整一下优先级。 测试一下看下当前哪块分区优先 消耗内存的方法▲ dd往null里写数据，一个bs就是2G，而此时内存空闲只有1G不到，所以肯定会用到swap，此时看谁优先。 此时就很慢，理论上这个命令dd if=/dev/zero of=/dev/null 都是在内存里处理的，都是内存往内存里扔数据，应该很快的，但是现在很慢，是因为内存空间不够，用到了swap也就是硬盘，所以就慢了。 👆而且可见优先级是-2优于-3的，大的优先咯。上下图片都可见用了448M了。 之前修改fstab 然后mount -a是不会对修改某个选项生效的--要么是一行都没有的新挂载的情况才会mount -a生效，此时swapon -a同样不会生效 swap的生效方法如下： swapoff xxx禁用 然后再启用就行 再来测试swap的优先级 可见swap里的sdb1优先得到使用。 一些不规范的操作，举例-由于开始分区没有规划swap，没地方放swap了，只能拿文件来补一个。 看下当前根/的空间利用率比较低，所以将来做swap的文件就放在/下 格式化 blkid看不到没关系，加上文件名/swapfile去查看下，再直接用blkid此时就能看到了 同样写UUID到/etc/fstab实现持续挂载，这里要注意了针对文件作为swap的不能用UUID，这里先用UUID演示看下问题出在哪里。 swapon -a 读fstab进行 挂载，此时swap就多了一个/swapfile 文件实现的swap了。 644就谁都能看这个数据，内存数据建议600 重启后发现： 文件作为swap的丢了，要写设备名也就是文件 重启后再看 swap除了建议放到机械盘的外圈，更加推荐使用固态盘。 有人说现在用不到swap，蓝鲸的平台base和扩展，你看看内存要多大，swap好歹能图个安心对吧，举例别不服--数据检索：针对56台agent查询了1个月的CPU使用率，然后就OOM了。 检索量太大了导致的，正常情况下，没有这个需求，所以如果swap上来，也能解决这种非常规性业务。 下图是蓝鲸base里的pass平台点，有点奇怪没有swap 不管了，暂时不管它了。 之前的 \"文件夹挂载到文件夹\" 如何写到fstab里 注意，文件夹没有文件系统一说，都是硬盘分区是什么文件格式，所以这里写none就是没有文件系统，然后defaults那里就写bind。 然后 mount -a 再，mount查看 可见已经挂上了 fstab如果持久挂光盘 光盘可以直接写设备名也没问题，写UUID也行。 /etc/fstab里写这一行：光盘的文件格式就是iso9660 fstab挂文件，类似用文件做swap (把衣服挂到钩子上，叫做挂衣服，把文件挂到文件夹上，所以叫做挂文件) 将下图稍作变动就可以实现文件往文件夹上挂载了 /dirName/fileName /mnt/dirName ext4 defeaults 0 0 这样就可以了 fstab还可以挂载网络资源：nfs，samb。后面讲 图形界面会自动挂载光盘 如果是开机进入的是字符命令行界面，就不会自动挂载。 普通用户没有权限挂载 神奇文件夹不管你是普通用户还是root，只要一访问呢/misc文件夹，就能实现自动挂载。 这个神奇文件夹，过一段时间不访问，就会给你取消挂载，一访问再次挂载，其实这个是autofs软件实现的，在《linux就该这么学》中有详细讲，其实很简单，哈哈。 删除swap其实上面已经有了，这里再写一下方面看 1、fstab里删掉 2、swapoff /dev/sdb1 如果是文件swapoff /swapfile 名字无所谓，就是意思一下；swapon -s看下确认下 3、删分区，fdisk--->p ----> d ----> 1 意思意思不要照抄；删文件rm -rf /swapfile 4、partprobe同步下 删掉后，再创建/dev/sdb1的时候blkid里会看到之前的swap类型，无所谓，按部就班就是①分区②格式化，格式化就是mks.xfs /dev/sdb1就可以覆盖掉blkid里的原先数据了，然后再③挂载mount或者swap的话就是swapon -a Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:35 "},"12-磁盘存储和文件系统/7-外围设备使用.html":{"url":"12-磁盘存储和文件系统/7-外围设备使用.html","title":"第7节 外围设备使用","keywords":"","body":"第7节. 外围设备使用 案例：home本来是在/下的，现在home越来越大，要迁移到一个新的分区 首先找一个新的硬盘分区 👆这种有提示的，要同步分区表 否则分区sdb2出不来 centos7的同步就一个方法， 然后创建文件系统，并确认 现在就是要将home文件夹挂载/dev/sdb2，要考虑/home下的当前文件，直接挂过去home当前的文件就看不到了--因为现在是挂的/目录关联的设备。这点第5章讲过了。 要把当前的数据迁移过来 数据迁移 要把/home下的文件复制到新的分区，需要中转一下 先用一个临时文件挂一下 确认下文件是否都复制过来了 题外话，复习下： 原/home下的文件可以删除了，否则等你挂载后，这些文件没有入口进去找不着就删不掉了，还得取消挂载从/进入home去删。 使用持久挂载 刚才其实还有一个临时挂载，把/dev/sdb2挂到了/mnt/home下，上图是后挂的覆盖了，mnt可见👇 这里有个问题，原来/home文件夹里的东西被隐藏了，要删掉的，否则一直占着空间。如果前面没有舍得删，这里需要unmount回去删掉的 有个问题，在cp -a /home的时候，文件夹可能有人在用，别人可能在往里写数据。 一般需要把系统处于维护状态--init 1 --单用户模式 进去看下可见是从5 切到 1的 可能有点问题，就是终端那边命令输入有问题， 重启直接进入单用户模式 按任意键 上图输入a， 然后进入下图输入1，进入单用户 单用户模式，就是不联网的，网络都是down的👇 这是建议的维护状态下进行操作；只要别人不能访问就行，比如防火墙deny也可以。 移动设备的使用-光盘-U盘 eject后，光盘就弹出了 弹光驱这件事，有意思在 早期服务器主机都是带光驱，机房里面找机器，可以弹出光驱就能一下子找到了。呵呵~~~现在呢，谈个屁，谈恋爱吧。 eject -t 是弹进去，台式机可以，笔记本不行，VMware Workstation VM虚机更不行了。 把光盘制作成iso文件 这个可以用来挂载iso文件，没光驱，就可以这么玩，挂载iso，一样做yum源。 联系前面的章节： windows里制作ISO是用工具比如这玩意 linxu就一条命令的事： 上图时间较长，你想windows一样要读进度条的，不过你可以ctrl c结束也能看到下面内容，不过数据就不全了。 还一种方法，是把文件夹打包成iso文件 打包成iso 并不会压缩，原来也是这么大差不多 挂载看下 注意这种挂载是read-only，光盘嘛iso就是只读的，不过windows的UltraISO好像可以往里写东西，没用过。 centos6上的一个问题 光盘是3243个包，yum上看到是6713个包，剩下的还有3000+的包在第二张光盘上呢，centos6是两张盘来着。 可以考虑把centos6的两张盘和成一张盘 弄一个大文件夹，然后把两张光盘解压进去攒成一个大文件夹就行，不过这样可能没有引导文件，还需要工具，不过只是做yum源应该没关系吧，安装可能不行。 有个官方脚本\"mkdvdiso.sh\"可以把两个centos6的ISO合成一个iso同时具有启动功能 看下U盘 U盘的系统查到linux上，如果是FAT32可以识别，如果是NTFS就不行了。 wmware workstation 插上U盘后会提示 如果选择主机，那么linux这些虚机识别不聊了，所以选择虚机连接 我们选择主机先看下👇，待会再连到虚机上 把U盘从\"主机连接\" 转到 \"虚机连接\" 👇注意tail -f /var/log/messages监控着： 点击后同时观察后面的messages日志 开始弹出信息：usb xxx 设备名也出来了[sde]。 下面就是和硬盘一样的用法，创建文件夹，挂载 因为是2个分区，所以创建2个文件夹 将来挂载 发现sde1是ntfs的，挂不上去，sde2是vfat的可以 因为linux默认不支持ntfs，所以挂不上去 通过locate xxx查找内核文件可以证明确实不支持，👇没有ntfs文件系统的驱动 👆这是看有无驱动的文件，至于是否加载到内存中了没，还要通过lsmod查看 虽然有vfat.ko文件表示系统支持fat格式，但是并没有加载到内存中👇 因为已经mount在用了，所以系统就给你加载内存里面 fat文件系统虽然在linux里支持挂载，但是有问题的，umask貌似没有起作用 完全改不了 改所有者、所属组也不允许 说明fat文件系统的功能太少，连基本的权限rwx和所有者、所属组都不支持。 还有一个fat文件系统不区分大小写👇 这里就可以说这么一句话:linux区分大小写这种说法不准确，显然上图的linux系统存在不区分大小写的情况，所以讲 区分大小写 它是文件系统的事情，linux一般xfs或者ext这两个区分大小写，如果非要用linux也支持的fat文件系统(fat本身不支持权限umask和所有者\\组)--此时就表现出不区分大小写了。 👇下图人家df定义了别名，所以\\df来使用原来的命令，注意原来的df就是以block块为单位的，1个block就是1KB字节。 -h 是human人类可读性好的选项 -h是以2^xx 也就是1024算的 -H 是10^ 也就是1000算的 -T 显示文件系统 -i inode节点使用情况 -P 是格式化好看些的意思 centos7是优化过了，看看centos6就知道了 这个错位，cut就不能取了啊，要注意。加个-P 就解决了 厉害厉害，上图--skip-alias 细节啊， 所以说7就不需要-P了，因为7上的df的rpm包版本更高，6的版本低还需要-P 查看文件夹大小 du 不带选项，就是/boot目录下，每个子目录的占用空间，单位是KB；最后一行/boot是汇总信息。 这个du看到的和df看到的不太一致 du 算的空间是目录数据本身站的空间，而元数据是不算在内的，还有日志、实时运行区？一些额外的东西。 dd count=0就是数据为0咯，但是会产生元数据的，所以多少会有点空间占用的👇 不过多了点元数据 上图最后一行的命令解释：seek是跳过10个bs，也就是10G开始写数据，结果写0个bs。所以前面0-10G的数据为空。也就是有头有尾，中间没东西。 跳过的也占空间的👆 毛的元数据，说好的一点点呢，啥也不占啊， 应该还是有一点点，文件元数据确实有的啊。但是没看到也是奇了怪了，不管了，反正文件ll都看到肯定元数据占用跑不了的。superblock也在分区上的啊。可能du就看不到元数据的空间占用情况。 回到这种10G大小确一点空间不占的问题上来，图①，下面要引用对比 这种文件称之为 稀疏文件--有头有尾，中间时空的。与之相反的是稠密文件。 这个文件将来学虚拟化有用的，提前接触下这个东西。 显示上都是0，要要注意前面空的虽然也用0表示，但实际上没有数据，然后 后面的1G确实写了数据--0。元数据里加了标记，从哪到哪加了标记没用 是空的 硬盘上没有数据的；后面1G真正的写了0的是有数据的。 ▲所以恶心的事情来了，上图是有1G数据的，和上面的\"图①\"是不同的，图①里的是一个数据都没有的。貌似没法区分咯，，，⚪？ du 可以指定深度 除了/boot本身，再深入2级👇 文件大小空间ls -l f看的不对了就，du可以看的准确点。 du看的是真正的占用空间，而ll看的不是，有点像是真实或者预定规划的空间。 ibs和obs是读和写的块大小各自定义，不像bs统一的。 👆表达了，文件系统的最小存储单位是4K，所以即使你文件只有3个字节Byte，占用空间也是4KB。然后注意以下bs=1说的也是块，不过这个块大小上图设定的是1Byte，加上count=3就是3Bytes。▲块这个词真的是到处乱用。对，我理解不好的，都是词名称起的不好，希望内向的同学多一点这样的观点。 空文件不会分配空间👇 空文件不分配空间，现在不需要空间，只需要元数据就行。而元数据的空间占用 看来并不是通过du查看的。▲ 一个换行也会占用一个文件系统的最小存储单元4K👇 所以，如果都是小文件，就会浪费磁盘空间了，这个小就是相对于文件系统的存储单元来讲的。所以文件系统创建的时候 可以指定块大小的：mks -b。 准备两个文件 请问最终效果是啥，结果是： 默认行为就是，从f1.txt取数据写到f2.txt的时候，默认f2.txt多出来的就截断了。 可以修改这个默认行为，使之不截断，这里使用了我讨厌的之乎者也的之，因为书写简洁。 👆这样就f2.txt多出来的就不截断了。不管截不截断，它都是覆盖的写法，不存在插入哈，插入那是insert键盘的效果哈哈~这里是往固定位写数据肯定会覆盖的。 还有转换大小写可还行，可还行--网络用语，在永生里小郡主说出来就很可爱。 还可以目标存在就覆盖，不存在就不创建 👇这个469MB/s是先放到缓冲区里的速度，它这个是先把2G的数据写到缓冲区，再写到硬盘里。 然后有个参数就是直接写硬盘的，就是命令结束前，先写到磁盘上，然后出统计结果比如时间啊、速度啊。 👆这就是真实写入磁盘的速度了，▲这个也是基本思维咯，写数据往往写的都是内存。能给你一个写到磁盘的统计--这里是统计，或给你一个写到立即写到磁盘的开关nginx还是mysql里有这东西的，也是阔以的，否则我就不知道怎么保证立刻写入磁盘--其实应该不必担心，一般都是非常快同步的，少数比如分区划分需要同步到磁盘。 就是从if=xxx中读取100个字节，但是xxx中只有90个字节，所以剩下的10个字节用NUL补齐。懂了没榆木脑袋SYM。非要顺一遍才能理解。 复习下： dd工具其实就是相当于windows里的ghost工具 有个问题，文件备份一般tar一下对吧，那这里的dd又是否合适呢，显然dd覆盖面会广，占用空间会多。然后网上有一些cp dd tar cpio dump来备份的比较说明。 网上的一些信息：就是说类似dump去做增量级别的备份 https://blog.csdn.net/ether_lai/article/details/12656219 把内存里的数据进行备份，内存修改软件--改游戏角色属性的好像有这个东西。 制作iso镜像，除了这里的dd，通常cp一条命令就行了--上一章有讲。 销毁磁盘用dd，还能找回吗？⚪多写几次unrandom随机数进去，即使硬盘支持几次数据找回应该也没办法了。 练习 RAID 避免单点故障，单块磁盘损坏。 原来随着技术演变或者说应用场景的演变、需求的演变，原来的东西的名字也是会改的。 现在主板上都自带raid卡的--内接式。 游戏笔记本做软RAID提升性能？ Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:36 "},"12-磁盘存储和文件系统/8-raid工作原理.html":{"url":"12-磁盘存储和文件系统/8-raid工作原理.html","title":"第8节 raid工作原理","keywords":"","body":"第8节. raid工作原理 RAID0的空间大小计算： 取成员最小值*磁盘块数； raid是物理上实现的，所以对于OS来讲，分区还是像以前一样，挂在300G上的连续空间的单块硬盘上。 ①以上就说明了总容量大了 ②下面是数据切片后成条带方式存储在成员磁盘上的。而切片的单元叫做chrunk。它这个3块磁盘同时读写就提高了IO速度。 ③RAID0，单单一个RAID0一般工作中不用，家里可能用的。 图中100M的数据切成多个512K，这是举例假设，不一定是512K。 RAID1 1、RAID的硬盘冗余，是针对硬盘损坏的冗余备份，如果上图RAID1你删掉文件，就删掉了，disk1作为备盘，你操作disk0上的数据，删掉其上的f1文件，disk1上作为备份也会删掉的。而raid1，如果坏了一块盘，会导致磁盘IO飙高的吧？还是你拔出坏的，插入新的才会IO堵塞⚪。不查比较合理吧，否则会同步数据IO就高了，它又不像raid5坏一块就开始算故障盘的数据。同样raid5坏一块读写效率下降，新盘顶上来IO肯定要东西向恢复数据，所以IO也是会高的，留给业务的IO就少了。 2、RAID1叫镜像，RAID0叫条带。 RAID4 raid2 3 4 都是消失的技术，不过4具有典型代表，所以了解一下 ①上图是在说raid4，100M数据存放的规律，条带+校验位(异或校验得到的) ②如果条带中某个数据块的磁盘损坏了，是可以算回来的👇 ③raid4的至少3块硬盘(n≥3)；空间利用率是(n-1)/n。牺牲一块硬盘的空间来实现一定的容错性，容错也只能坏1块。 ④raid4淘汰的原因，是校验盘的压力比较大，损坏几率较高。谁当校验盘谁老坏，这个位置不养盘啊，哈哈就想有的工作不养人一样，教师这个行当是养人的，有利于身心健康的保持。 为了解决raid4的缺陷产生了raid5 raid5 ①和raid4类似，但是校验位是分散在每块盘上的，还是条带+校验 一行。分散带来的好处是--原来raid4的时候数据较多就会频繁访问校验盘，校验盘压力大。 ②如果坏了一块硬盘，再插上新的，就会发现此时由于要算新磁盘上的数据，性能下降的非常明显。此时属于降级设备不是正常的raid5了，如果业务反满磁盘IO本来就高的情况下，坏了一块 导致降级，此时就惨了因为👇会计算故障盘数据的，会导致本来脆弱的磁盘即使是好的还能再坏1块~哈哈。 为什么RAID5系统的磁盘组降级情况下，读写效率会下降：因为磁盘每时每刻都在进行数据的写入，当有一块硬盘发生故障RAID会一直在根据剩余每块成员盘的校验码和数据来计算出故障盘内的数据，这样才能使整体数据不会丢失，也导致硬盘在读写的情况下多了一项计算，所以整体上硬盘的读写效率就会下降。 https://forum.huawei.com/enterprise/zh/thread-351133-1-1.html https://cloud.tencent.com/developer/article/1828247 ③spare disk应该说的是热备盘吧 raid6 ①比raid5多了1个校验 ②利用率(n-2)/n;n≥4 ③以前压缩文件bz zip，是用cpu的损耗--假如你是crontab这种持续性的压缩打包任务，来换磁盘的空间节约。现在就是用磁盘空间来换数据的安全。反正都是要换，要么你用身体换钱，要么要钱换身体，唉，唉~我竟然不知觉的情况下开车了。我说的是工作久坐等一些不好的情况。 raid-10 not十而是壹零 解释为什么raid01不好，首先jd上的产品就告诉你raid01不好了👇都不支持，没市场： 原因： raid-10，坏1块盘 后，再坏一个块盘导致整体不可用的几率小于raid-01，下图说明👇 disk0坏了，剩下的3块中disk1坏了就整体不可用了，其他disk2坏或者disk3坏都不会影响整体，整体不可用几率是1/3。 disk0坏了，剩下的3块中disk2或disk3坏了就整体不可用了(因为是raid0坏一块就右边整体不可用了)，整体不可用几率是2/3。 ▲所以raid-01的容错性较raid-10的差。 0这种东西就是提高IO速度的，与冗余无关，5、6就是冗余+速度、1纯冗余 速度一点点。 jbod就是写完一块，写第二块就是整合一下多个块当1块用。比raid0节约些。 raid7 了解下软RAID 想用百度吧，没啥意义应该。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:36 "},"12-磁盘存储和文件系统/9-LVM管理详解.html":{"url":"12-磁盘存储和文件系统/9-LVM管理详解.html","title":"第9节 LVM管理详解","keywords":"","body":"第9节. LVM管理详解 1、创建管理 分区一旦划分好，就没法扩了。而LVM就能扩和缩 操作的时候LVM的底层可能就是以部署raid为主，当然也可以是多块硬盘和分区。 10块组成raid10，OS看到的就是sda一块盘，以前就直接sda上分区，一旦分区就固定了，分区不够用也不能扩(不能扩的原因是，要删除分区后再重新划分和格式化，这样原来的数据就没了，所以才说不能扩，把数据移走，不就行了，不就可以扩了，蛋疼，一般来讲说的是软件应用在使用着这个分区，所以生产环境你没法，而LVM可以支持在线就是热扩，哎~，我发明词汇热扩，反正天下名词各种飘，我也来冒一个，增加读者的难度，对吧 呵呵 悲哀)，所以就再sda上做LVM来管理，如何管，假设一组raid10得到sda，还有一组sdb、再来一个sdc1分区，这些raid也好、硬盘也好、分区也好，都是linux Block Devices。这些块设备大小不限，无所谓。 把这些块设备第一步通过pvcreate变成Physical Volumes物理卷，所以物理卷就是打上标签--意思就是这些块设备不作为独立硬盘用，将来要做逻辑卷的。 有几个块设备(硬盘或分区)就生成几个物理卷，就是打标签吗，原来设备叫啥名，生成后还叫啥名。 然后通过vgcreate将物理卷们 组成一个大的集合 也就是卷组。此时卷组就是一个大的硬盘咯。 然后通过lvcreate将卷组 分成一个个 逻辑卷出来。 逻辑卷不关心上面的数据存放在哪块物理设备上， 逻辑卷的地位就是分区了，该创建文件系统就创建文件系统，该挂载就挂载。 如果LVM1 500G ，LVM2 200G，下面的卷组是1T，将来LVM1满了不够用了，还可以在线扩展到800G。所谓在线就是用户无感--使用不受影响，无需取消挂载，直接在线挂载的状态下，一条命令就挂上去了。 如果lvm1 800G也满了，怎么办，可以再底层块设备上再加入新的硬盘。这一套还是在线扩展吗？⚪ 所以linux默认安装的时候默认就是逻辑卷。 逻辑卷的概念了解后，还有一个PE，叫物理盘区physical Extent。在创建卷组的时候要指定PE，假设PE是16M，卷组就会显示有多少个PE，创建LVM逻辑卷的时候可以从这么多个PE中取出多少个PE来作为逻辑卷。扩展的时候也可以增加多少个PE。PE就是分配的最小单元了。 到现在已经是LVM2代了，LVM2。 在IBM AIX系统unix小型机，就只用LVM，不用分区。 LVM要建立在raid之上，硬盘坏了有冗余。①担心LVM多了一层逻辑层，硬盘坏了导致整个卷组出问题②多一层逻辑层，不是直接面对硬盘，性能是否不好。据说2点担心都是多余的。反正很多企业在用，也有企业不用。 学完再来这里补充发表自己的观点，和网上搜一下。 下面开操作 第一步 准备好块设备 要注意修改分区的ID为8e 存盘退出 同步下 第一步，物理卷的生成-pvcreate x y z\\pvs\\pvdisplay pvs查看下，当前pv是空的 把上面的一个分区、一个硬盘变成物理卷 👆因为sdb1之前做个swap，所以有标记位，所以先dd清空前10M就能覆盖到了。也可以直接敲y就wipe一样也擦掉了。 然后再pvcreate /dev/sdb1 /dev/sdd也可以合起来些。 这样pvs和pvdisplay就能看到了 pvs就是summary概况，其中lvm2就是逻辑卷2代；VG列是空 表示卷组没组建呢还。 pvdisplay同样可见VG是空；PE Size也是0 这个只有创建VG的时候指定才会出现。 第二步 vgcreate创建卷组 vg开头的也是一大堆命令 同样有vgs和vgdisplay，然后pvdisplay肯定也能看VG和PE也就是vgcreate后的信息。 使用帮助看下 -s 指定PE的大小 pe--physical extent size 上图忘记加单位了，不过有默认值的pvdisplay可见。 vg整合好后 ①pvs看 PE的默认值也看到了是4MB，上图PE size是4MB，total PE 1023就是1023个的意思。总容量也就是4G咯。同样/dev/sdd也就是4MB*2559=10G ②然后通过vgs 和vgdisplay看看 👆vgs可见，vg0这个卷组里有2个PV--物理卷，LV还未划分，大小是两个PV的总和14g的样子。 全部PE个数3582个，Alloc PE / Size 0 / 0就是没有分配出去呢--因为逻辑卷lvm还没有分配呢。 由于逻辑卷lvm还没有创建，所以还看不到vg0这个设备 第三步 lvcreate创建lvm lvcreate --help可见用法：从哪个VG--卷组里 通过-L取 多大size ；通过-l 取多少个PE，这是个数 上图有striped和raid1|mirror，这就是raid了，完全可以用lvm再做一层raid，实际上底层物理设备做raid后，上层lvm不会再做raid了。 👆起个名字mysql，-L 直接写大小，-l数个数还要算--不用，从vg0里面划分lvm。 lvs 和 lvdisplay 上图说明： 这个就是完整的设备名，这其实是软连接👇 实际上叫dm-0，还有一个和它一样指向的软连接 所以待会可能看到的是上图这个名字/dev/mapper/vg0-mysql 看下现在的dm设备有几个：如果再创建一个lvm，这里就会多一个dm-1了 上图的Current LE 2048是说的LE的个数。 上图的8G到底用的哪个物理卷，通过pvdisplay可以看到的 可见/dev/sdb1全部空闲就是没有用，所以这个8G都用的/dev/sdd的空间4MB*2048也就是8G。做raid也是不关心具体到底存在那块盘上的，都是当作整体在用的。 2048个PE，在LVM叫LE，就是PE在LVM逻辑卷里的名称。 一旦LVM划分了，blkid就能看到了 现在由于还没有在LVM上创建文件系统呢，sdb1和sdd的UUID肯定是各归各的。[ 所以还看不到一个sdb1和sdd合成一个的情况，其实合成一个情况是blkid是看不清楚那几个合成一个了 ，这个说法不对的，是sdb1和sdd里PV化和VG后再取出PE们来合成的，不是简单的sdb1和sdd直接合成的，之前我想当然的是不好显示的 ] 。只能再下面的格式化看到多出来一个mysql的逻辑卷。而sdb1和sdd是逻辑卷的成员而已，具体是哪个lvm的成员还需要具体去看的。 第四步 mkfs.xfs格式化 这个时候就看到，增加了一个新的记录，mysql这个逻辑卷就有文件系统了。 lsblk是你fdisk后 再 partprobe就可以看到的块设备出来了就 而blkid不一定看到，因为lsblk看到这个块设备后，还需要进一步格式化才能看到的。所以blkid是看带文件系统的块信息的。 第五步 挂载 挂载的时候，要写设备名，可以写这里的/dev/mapper/vg0-mysql可以写当初创建的lvm起的名称/dev/vg0/mysql，这两个都是可以的，因为都是软连接。 上面是临时挂载，永久挂要写fstab 刚才挂过了，所以就行了，reboot也不怕了，正常就是mount -a就行了。 看看性能 上图是不是有点夸张了，逻辑卷能提速这么快的？逻辑卷反正不会比物理分区慢就行了。 上图就是conv=fdatasync就是结果显示的是已经同步到硬盘里的了。上上图的是写到内存里了就显示结果了。 所以lvm + raid，是很好用的。慢也不慢，底层用raid冗余物理设备。 2、管理详解 下面看下逻辑卷的空间扩展 假设这个8G用满了，下面对其进行扩展 这里少了一步看mysql这个lvm到底来自于哪个卷组，其实这里就可以发现人家用vg0-mysql作为自动生成的软连接 来作为设备名 本身就是要告诉你mysql来自于vg0。 逻辑卷扩展 要先看 卷组里有没有空闲空间 可见卷组里还有6G空闲 lvextend -l 小写的l在扩展全部剩下空间就特别好用，可以写-l 1534就是剩下全部的Free PE个数，也可以用上图的+100%free来直接搞定剩下空间的100%。 不需要指定vg卷组，因为mysql这个lvm系统知道从哪个卷组来的，所以无需指定，直接命令就写/dev/vg0/mysql就行。 回车后可见已经扩展到13.99GB共3582个PE。 此时再看vgdisplay可见已经没有剩余了 vg用光了，vg整合的pv自然也就用光了 但是此时确发现df -h 里看到的逻辑卷的空间还没有刷新 注意，这不是没有刷新哦，这是因为 你虽然把lvm扩展了6G，但是这个6G的空间还没有文件系统(还未进行格式化)，不是简单的格式化，而是格式化后并进去，其实确实是一个命令就同步了，也算是笼统的刷新了。 df看到的是文件系统的大小。 现在就是要将扩展的6g空间的文件系统同步到既有的8g空间里去，不是简单的mkfs.xfs哦。 👆可见data blocks 变大了。此时df -T再看文件系统的大小就扩上去了👇 总结下扩展其实就两条命令（在vg卷组有空闲空间的时候） 而用户是无感的，扩展的的时候也没有取消挂载，一直都是挂着的。 再来看看我的centos8的默认就是使用的逻辑卷的 vg没有空闲空间的扩容步骤 此时要扩vg0-mysql，但是vg0已经没空间了怎么办。就再加一块新硬盘。 ①pvcreate 只有分区才要加标签8e，硬盘无需改 pvcreate后再blkid就看到了，就和mkfs.xfs 格式化后就看到了一样，blkid是看带文件系统的块信息的 此时pvs就能看到多了个20g的pv ②vgextend扩展卷组 上图可见现在vg0有两个成员Cur PV 2，再加一个。 vgextend vg0 /dev/sdc 此时卷组就又有空闲空间了，扩展就一样了。还是两条命令的事，可见扩展lvm确实无需取消挂载。 同一个卷组里创建新的逻辑卷 再后面学习中可以把mysql的日志放到专门的逻辑卷或分区中，这里就专门创建一个binlog的lvm ①lvs,lvcreate -n binlog -L 10G vg0 lvdisplay就可以看到有两个lv了 ②mkfs.ext4 /dev/vg0/binlog 格式化逻辑卷为ext4 ③挂载 此处省略了持续挂载fstab的编写，上文有的。 针对binglog逻辑卷扩容，-l +1000个pe 一个pe4MB 此时逻辑卷确实扩展了 由于增加的部分没有格式化，也没有加入进当前的binlog文一个整体，所以 刚才xfs文件系统用的是xfs_growfs /mnt/mysql，现在ext4得用resize2fs /dev/vg0/binlog，前者跟的是挂载点(文件夹)；后者跟的是设备名。注意下 此时就成功了 由于文件系统的不同，最后同步的命令不同，后面跟的部分也不同，所以自动化脚本就有点麻烦，所以有更好的扩展方法来了 不管是xfs还是ext4都一条命令扩 lvextend -r -l +500 /dev/vg0/binlog 翻看前文可知刚才mysql这个lvm是14G，现在扩了500个PE，一个PE是4MB，也就是扩了2G，达到了16G。 上mysql是xfs，下面继续binlog是ext4的也用这一条搞定 一样成功了 此时就发现上面的同步命令白学了？不是 xfs_growfs 挂载点 --- 这个是扩展用的 resize2fs 设备 --- 这个扩展和缩减 都行 lvextend -r -l +500 /dev/vg0/binlog 这个就是扩展lvm的时候自动同步了。 所以resize2fs后面缩减还要用得到。算不上白学~ 其实通过man lvextend可见 其实-r 就是resizefs，哈哈~ 缩减有风险的，如果一不小心写错了--写成缩减到50G，而此时数据就是100G，那么就会造成数据丢失50G~。 以防工作中用，还是要学一下 缩减 缩减上图ext4的这个lvm空间 扩展是在线扩展，缩减必须离线缩减，意味着取消挂载，用户访问受影响了。 ①unmount /mnt/binlog 回想扩展的时候不管是2条命令还是1条命令，底层逻辑都是先扩展lvm，再同步扩展文件系统 现在缩减的时候就是先缩减文件系统，再缩减逻辑卷大小。 缩减前的lvm大小如下15.86G： resize2fs /dev/vg0/binlog 10G # 缩减到10G 注意这个命令①同步的命令(同步的命令其实就是将lvm扩展后的空间，再扩展到文件系统里去，这个场景其实就是不写多少个G就是全部扩展进去，其实你的需求就是lvm扩展后，再同步到文件系统里，自然是全部了)②缩减的命令，在后面跟上空间即可表达缩减到XXG。 缩减的时候会提示你先检查一遍系统完整性，再让你缩减。 ②e2fsck检查下完整性之类的、③resize缩减文件系统 此时由于是resizefs 缩减的是文件系统，所以lvm还没有缩，通过lvs可见大小没变；然后开始缩减lvm:lvreduce -L 10G /dev/vg0/binlog ④缩减lvm -L 10G，不带+加号的就是直接缩减到10G。man lvextend可见👇 ⑤mount，缩完后重新挂载 注意缩减只能缩减ext的，不能缩减xfs的。 上图命令补齐也可见一斑，只有grow，没有reduce resize这些补齐命令出现。 所以▲xfs哪里比ext好了。 逻辑卷的迁移 迁移操作举例 拿这个b硬盘做实验，看下sdb1有没有用，就是看下有无挂载咯 当时老师做实验的，sdb1报错，肯定是fstab里写东西了， 删掉 dd 干掉分区清一下 发现上图dd掉512B后，分区sdb1还在，那是因为没有同步 partx -d --nr 1 /dev/sdb # 不能用-a，-a是增加，-d是删除 注意提示sdb忙 再次df 发现之前fstab里已经删掉了持续挂载哪一行，现在还是有类似的报错--废话你删掉的是mout的配置文件，又没有取消挂载，现在的df报错就是之前mount过--通过mount可见--然后挂载点估计是删掉了被，现在df报错了。通过mount看发现了问题所在， 视频中老师的排错思路看下 看到说明没有取消挂载 哈哈，还是在，我猜是不是dd 512导致的，还没有取消挂载就dd导致的？ 通过fuser -v /mnt/sdb1也没有人用啊，这会umount没报错，lsblk也看到没有挂载了。 取消挂在后，再同步一下，此时分区sdb1就没了 那上面过程总结，①挂载没有unmount②unmout需要时间③好像是/mnt/sdb1先挂，/mnt后挂，这一类也有问题。 下面开始针对sdb创建lvm，然后演示lvm的迁移 先创建一个和将来要过去 名字 存在冲突的vg0和mysql逻辑卷 下图👇创建物理卷、创建卷组并指定PE、创建逻辑卷指名和指大小和所属卷组。 挂载 复制点文件过去 好了 LVM就有了 下面进行迁移 先取消挂载 考虑到迁过去的卷组也叫vg0，lvm也叫mysql，只要vg0改了就行了，因为整体名称vg1-mysql过去就和vg0-mysql 不 冲突了 改名 禁用卷组 卷组禁用后，所有的逻辑卷也就禁用了： 导出vg1 拆硬盘、插硬盘、识别硬盘 正常服务器就直接拔，这里实验用的是Vmware工作站，先关机 是sdb硬盘要迁移 这就是硬盘啦，把这个文件复制到centos7的虚拟机上去 移动硬盘嘛，就是剪切过去 再centos7上加硬盘之前看下 当前硬盘排号已经到d了 完成 不会自动识别，echo - - - 一下 出来是出来了，但是这边的系统还不能识别其上的LVM lsblk看不到没关系，使用vgdisplay可以看到 上图👆可见此时vg1卷组是出于exported导出状态 导入卷组 使用vgimport vg1导入 此时状态OK 通过lvdisplay可见2个卷组 注意上图的not available 启用逻辑卷vg1 启用后再看 就正常了，lvm有了，就可以挂载了 挂载 此时数据就都过来了 以上讲解了lvm的创建、扩展、缩减、迁移 下面来个例子 假设sdd这块硬盘 指示灯开始黄灯 表示快坏了，还没红，但是要坏了。 此时需要把这个块硬盘剔除下来，问题是其上有逻辑卷，不能直接拔。 问，怎么拆走这个块要坏的硬盘 表示sdd这个物理卷已经被别的逻辑卷给占用了已经。 要想拆走这个sdd，必须先把其上的2559个PE的数据搬到同一个卷组vg0下的其他逻辑卷上，必须是同一个卷组。 但是 vg0 发现只有2059个PE空闲 1、只能加个PV了，为啥一定要同一个卷组呢？因为所谓搬家使用的是pvmove命令，估计这个命令需要同一个卷组。 2、当然你也可以缩一下，然后再搬家。 这里还没有加PV，vg0的所剩空间并不够，只是演示一下： 这是把sdd上被占用的PE搬家到同一个vg0下的其他空闲PE上去，前提是空闲PE数大于占用PE数。 有个问题啊，搬家是搬到其他空闲PE是吧，其他空闲PE是否分散在不同的几个LVM上呢，这样文件又该怎么索引找到呢，难道原来一目录下的分散到好几个目录下了？显然不可能这么玩啊。 实际上数据没多少，但是搬家搬的是空间，硬盘拆走，硬盘其上得lvm当初承诺出去的空间是16G和9.8G，现在sdd一拆，承诺出去的2559个PE差不多10G的承诺空间就没了。 当然你也可以缩一下，然后再搬家。 缩的问题也来了 不知道这2559个PE到底是哪个LVM占用的。 通过lsblk查看 可知是vg0-mysql逻辑卷占用了sdd的全部PE，而且vg-mysql还占用了sdc的PE 只要把vg0-mysql的容量变小，就可以搬家了 此时vg0-mysql lvm才用了很少： 再进一步发现，最终还是缩不了 只能加硬盘了 把sde剩下的18G空间加到vg0里面 不需要太多，👇只需要500个PE，也就是2G的大小。 然后...👇 发现整个sde都在vg1里了，没办法给都vg0了。 重新弄个分区来做吧 以上分析就是关键思路 注意此时blkid还没有呢，需要pvcreate后才有 pvcreate后blkid就看到了 加PV就是贴标签 加到vg0卷组里 至此vg0的容量就扩上去，剩下3338个PE，sdd的2559个PE就可以搬家了 此时sdd的PE们就搬家到同一个卷组vg0下的其他PV上去了，至于是哪些PV不关心，反正有地方，有文件夹入口访问就行了。 注意搬的是空间，不是数据，空间过去了数据自然也过去。 刚才的sdb3的5个G空间用完了，当然还有其他的PV也会分担一部分sdd的空间 sdd上的空间就搬走了 此时sdd上既然没有数据了，就可以考虑移除了 当前vg0里4个PV，计划删除sdd这个已近搬完空间的PV 变成3个了 此时/sdd就不属于任何卷组了。， 然后再删除sdd的PV--物理卷 标签 sdd就没有了 此时sdd就是一个完全和逻辑卷没有关系的硬盘了 就可以拔了~ 上面的分析较多，查看较多，真正的命令就3条--当然前提是空间够 注意，从头到位都是在线操作，没有取消挂载。 扩展、缩减、迁移、拆除 都讲了 LVM的删除 某个vg的所有lvm都不要了 ①取消挂载 如果是fstab里有写，也要删除 ②删lvm 删之前要确定上面的数据不要了 此时vg1里的空间就没人(lvm)用了 vg1没人用了也可删了 ③删vg 此时sde这个pv就不属于任何vg了，也可以删这个PV了 删PV之前blkid可见标签👇 ④删PV 此时sde再PVS中不可见，就成了一个纯粹硬盘了 也可以拔走了。 3、快照管理 逻辑卷还有个功能是分区做不到的--快照 VMware里的快照也是一样的道理 卷组里创建多个lvm，/dev/vg0/mysql是vg0里的一个逻辑卷。500G的数据备份时间很长了，但是快照就秒做。你需求不是备份数据，只是能够回到某个时间节点的数据，所以只需要针对变动的数据做备份就行了。 假设待会有100G的数据要产生修改，要是直接备份着500G很花时间，所以在同一个卷组VG0下再创建一个逻辑卷--快照逻辑卷--mysql_snapshot，本身也是个逻辑卷，只不过是个特殊的逻辑卷。因为假设是将来要改100G的数据，所以创建快照逻辑卷的时候就指定创建的空间为100G。所以快照逻辑卷的大小不用和源逻辑卷的大小一样，这个腾讯云上好像也有建议值，一半吧好像。 快照的原理，区别于普通 的备份，是针对变动的文件做的备份，文件要修改前先备份到快照逻辑卷里，所以性能会有所下降，但是还原快，空间占用小。 1、快照的创建的瞬间，其实只是分配了XXXG的空间，并没有备份任何数据， 2、只有后面数据发生变化的时候才会备份，所以性能是有所下降的。 同样改F2为F2'，也会将F2复制过去 3、只要没改过的数据都是在原来的逻辑卷里，只要改过的都会在快照里存在一份，假设F3改了好多次，问第一次的改动的版本在哪，中间的N多版本在哪，最后一次的版本在哪；要回答这个问题，只要抓住快照的本质，就是拍照片的那个时间点，所以第一次改动的版本在快照里，中间都没了，最后一次版本还是在原来的lvm里咯。从这个角度来讲，F2一旦改动过一次，后面的就不会再推送到快照逻辑卷了，也就是说性能就又回升了，所以说只要所有可能发送变动的数据都变过一次后，快照的性能就回升了。 从使用实际角度来讲，不可能所有文件都变过一次，总有第一次变动的时候，所以快照使用完就干净删除，防止性能损失 4、快照的前提是 ，快照卷和需要镜像的卷 必须在同一个卷组中。 5、快照的空间一般是原lvm的 多少呢，反正大于原来卷是没有意义的。 如何实现快照 举个例子 mysql这个lvm目前是16GB不到， 创建快照逻辑卷 要考虑卷组里有没有剩余空间 弄三个小文件做实验 f1删掉 快照里找回；f2修改 快照里找回；f3不动 快照里没有。 -n mysql_snapshot 起个名字 -s 指定为快照snaphost而不是普通逻辑卷 -L 1G 做实验不讲究了就给了1G，因为原lvm的大小虽然是16GB，但是测试的文件才3个小文件，够了。 -p r 就是属性是readonly只读的，这里不写也行，就是后面快照逻辑卷挂载的时候加只读属性mount -o ro 也行。这样逻辑卷就不能被别人篡改了，正常的快照备份业务显然OK的。 /dev/vg0/mysql就是给谁做快照。 此时👆就看到了 多了一个LV snapshot status属性：mysql_snapshot是active destiantion for mysql是mysql这个lvm的快照。 而且看到LV Write Access是 只读的。 同样的去看原lvm就是做了快照的那个lvm也多了1个属性 然后删除f1，修改f2；创建f4 然后去挂载一下快照看下 快照逻辑卷发现不能挂载，blkid可见👇 快照卷和原逻辑卷的UUID一样，XFS文件系统的一个特点就是同样的UUID的设备不能同时挂载的。 提示不能挂载只读的快照，刚才创建快照逻辑卷的时候-p r了。 即使这里写个rw也没用 因为快照是只读的 没有办法挂载的时候改成rw 只能重新创建一个rw的快照，去挂载了，也许有办法修改 把原来的删了吧 原来的也没挂载，所以直接lvremove删了 这次的快照2创建后，同样需要修改数据，数据没变，所以此时快照里没有数据。 👆同样的问题，UUID一样，又加上是xfs系统，挂载需要忽略UUID冲突。 👆此时lvdisplay就看到这个快照就是rw可读可写的。 发现数据没改过，快照文件夹里就已经有了，好奇怪 属性也一样👇 1、文件没有改，快照里就是没有的， 2、虽然我们从快照挂载的文件夹里看到原来的数据，但这些数据实际上还不在快照里。也就是类似于链接之类的机制。 3、这样做的就是让用户有一个心理安慰，让你看到数据在的，其实不在。 下面对文件做一些变动 f2删了、f3改了、f4没动、f5新建 此时看看快照文件夹里的东西，就是当时创建快照时候的数据。 如何恢复快照的文件 接着上图，可以把快照挂载的文件夹里的数据复制回去，其实有专门的命令。 先取消挂载，所以挂载快照卷其实是没必要的操作，只是为了实验看看，所以创建快照卷的时候确实可以-p r设置为只读的。 1、取消挂载快照卷---这是因为之前的挂载操作，是实验环节的演示，正常生成中不需要。 2、取消挂载原逻辑卷的的挂载，这是确确实实需要的操作。 因为要合并快照和原lvm，所以需要取消挂载 等会就好了 然后再挂回去，数据就恢复了 还原后，逻辑卷的快照卷就没了，使命完成，就是说lvconvert --merge xxx后这个xxx快照卷就没了。快照是一次性的？vmware至少不是。云上好像也不是的吧。 以上就是针对xfs系统的快照制作和还原 上图遗漏一个点，lvcreate 去掉-p r后，挂载就需要加上ro，保证快照的安全性。 针对ext做快照 针对binglog这个逻辑卷来做快照 创建3个文件作为镜像还原的对比 创建快照卷，因为不是xfs，所以额可以-p r设置为只读快照。 UUID依然是一样的，不过这回是ext4的 👆ext4不需要-o nouuid--也就是说UUID一样也能挂，然后也不需要lvm是rw的read-only的也能挂 快照里的数据会有的，虽然这是假象--或者叫优化用户体验的效果--类似链接。 原来lvm的数据👇： 删除f1，修改f2，创建f4 还原 之前先取消挂载 还原-合并就是用snopshort覆盖原lvm 此时lvdisplay就看不到这个快照了 挂回去 此时就回来了 完整的过程如下 上图有点小问题，①快照不用删，恢复后自动就没了，除非你还没恢复呢直接删②-p r是xfs的时候去掉，再结合mount -ro -o nouuid③ext4 可以用-p r。具体上文都有说过了。 练习 注意/boot分区不能挂到逻辑卷里。/boot是启动的时候就要挂载使用的，这样系统才能启得起来。而启动的时候系统还不知道啥叫lvm呢。lvm也是要内核驱动模块支持的，刚启动的系统还没有加载lvm的驱动呢。 排版格式化 df - P选项、关键字：太长、对齐、一排。 通过lvdisplay可以看到LogVol00是给根，还有个给到了swap 同样centos8的lvm看看 删除所有的lvm 1、unmount 2、先删快照(如果有)，再删逻辑卷 一条命令好像可以两个逻辑卷都删了 lv就没了 3、删卷组 3、删除PV物理卷 到此就删干净了。 分区用不着删、硬盘不用了拆。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:37 "},"13-网络协议和管理/13-网络协议和管理.html":{"url":"13-网络协议和管理/13-网络协议和管理.html","title":"第十三章 网络协议和管理","keywords":"","body":"第十三章 网络协议和管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:37 "},"13-网络协议和管理/1-网络基础.html":{"url":"13-网络协议和管理/1-网络基础.html","title":"第1节 网络基础","keywords":"","body":"第1节. 网络基础 core-sw： en conf t no ip domain-lookup vlan 100 vlan 101 vlan 10 vlan 127 int e0/0 sw mo ac sw ac vlan 100 int e0/1 sw mo ac sw ac vlan 10 int e0/2 sw tr en do sw mo tru sw tr all vlan remove 1 int e0/3 sw mo ac sw ac vlan 101 int vlan 10 no shu ip add 192.168.10.1 255.255.255.0 int vlan 100 no shu ip add 172.31.0.1 255.255.255.252 int vlan 101 no shu ip add 172.31.0.5 255.255.255.252 int vlan 127 no shu ip add 192.168.127.100 255.255.255.0 ip route 0.0.0.0 0.0.0.0 172.31.0.2 sw1: en conf t no ip domain-lookup vlan 127 int e0/0 sw tr en do sw mo tru sw tr all vlan remove 1 int e0/2 sw mo ac sw ac vlan 127 int vlan 127 no shu ip add 192.168.127.101 255.255.255.0 dnsmasq.conf strict-order no-resolv dns-forward-max=2048 server=/poe.com/8.8.8.8 server=/google.com/8.8.8.8 server=223.5.5.5 server=114.114.114.114 address=/autodesk.com/127.0.1.123 address=/js-agent.newrelic.com/127.0.1.123 address=/teamviewer.com/127.0.1.123 #################################### listen-address=0.0.0.0,192.168.10.2,127.0.0.1 #cache-size=150000 #log-queries #log-facility=/tmp/dnsmasq.log #log-async=50 conf-dir=/etc/dnsmasq.d mkdir /run/systemd/resolve/ touch /run/systemd/resolve/stub-resolv.conf echo nameserver 127.0.0.53 >> /run/systemd/resolve/stub-resolv.conf systemctl status display-manager systemctl stop lightdm dhcp vim /etc/dhcp/dhcpd.conf apt install isc-dhcp-server authoritative; ddns-update-style none; ignore client-updates; default-lease-time 129600; max-lease-time 259200; option time-offset -18000; log-facility local4; subnet 192.168.10.0 netmask 255.255.255.0 { } subnet 192.168.20.0 netmask 255.255.255.0 { option routers 192.168.20.1; option subnet-mask 255.255.255.0; option domain-name-servers 192.168.10.2,192.168.10.3; range 192.168.30.2 192.168.30.200; } host xiaoming { hardware ethernet e4:b9:7a:ea:6f:e9; fixed-address 192.168.30.71; } vim /etc/default/isc-dhcp-server INTERFACES=\"eth0\" mkdir /run/dhcp-server chown dhcpd.dhcpd /run/dhcp-server exec dhcpd -user dhcpd -group dhcpd -f -4 -pf /run/dhcp-server/dhcpd.pid -cf /etc/dhcp/dhcpd.conf systemctl restart isc-dhcp-server 去交换机上配置中继 int vlan 20 ip helper-add 192.168.10.2 ip add 192.168.20.1 255.255.255.0 https://blog.51cto.com/mmx123/6413008 制作linux9.4镜像👆 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-11-01 18:25:17 "},"13-网络协议和管理/2-网络架构.html":{"url":"13-网络协议和管理/2-网络架构.html","title":"第2节 网络架构","keywords":"","body":"第2节. 网络架构 网工，略 工作案例-如何监管开发内网的url访问 这是一个过程记录，没有整理，备忘用的 传统防火墙里的url功能，开源如何实现呢 关键词squid，架构搭建opnsence 系统学习https://www.cnblogs.com/weq0805/p/15242170.html#_lab2_0_0 学习记录 squid作为opnsense的一个插件，所以要全面连接这个插件，还是要进行系统学习 1、squid作为正代使用的，反代就是nginx、haproxy、f5、lvs直流；而正代做代理的用户clients 2、squid虽说是正向代理，但是也有反代，它的反代就是指的本地加速缓存，访问过的网页站点就有缓存，下次用户直接读取，无需去互联网索取。这就是squid反代的体现。 3、肯定是透明模式落地拉，不用用传统的client也就是pc上设置代理的方式的，不过手机端也许可以灵活测试把这个传统模式用起来。恩灵活还是传统模式，不然手机还得网关上配置PBR。或者防火墙配置PBR之类的。 按照上面的网站走 1、systemctl服务自己弄 2、透明代理的dns自己处理 dns同样配置squid的内网IP，192.168.127.144 但是请求失败，查找原因，找到了👇 这里将dns请求转发的哦VM的虚拟网关 通过抓包可见，dns请求出去的ip是内网IP127段的，126.2回包进不了来的，VM的host网络。 只需要iptables 做SNAT就行了 这不就OK了嘛，所以不要加班搞，不要加班搞，贪狼入命的人，不管干什么都要注意节制：打游戏、看小说、玩、研究技术、学习，唉学习不会，看来学习养人啊，哈哈哈。学习也会的，昨天要是继续不下班研究状态就不好，又伤身。所以贪狼入命一定要小心。 再把dns得iptables改成本地3128端口去，尝试模拟传统模式也就是client windows配置代理--sock5肯定是了，因为传统模式也是把dns代理出去得---10.2那边抓包看到了。 关于传统模式和透明模式的dns处理 1、传统模式，代理里的写法其实就是sock5，包含udp的，所以dns也是走的3128squid，suqid机器再给你去10.2本地dns请求的 2、透明模式，iptables 转的，所以出去将udp 53转到本地10.2的dns，同时出向也要做SNAT，否则外界没有内网host网络的IP路由的。这里的外界是VMWork Station的虚拟网关 3、网络结构：windows-192.168.127.143-----host网络-----192.168.127.144-squid代理-192.168.126.134-----NAT网络-----192.168.126.2虚拟网关也是虚拟dns-------192.168.10.2DNS 4、哈哈网络结构上的比较晚，没事，基本都是常规结构。 继续报错 可能是MSS没设置，不是 可能是squid的 http_port 192.168.229.60:3128 transparent 配置有配置 搞不定，透明模式还是有问题，回去开启opnsence看人家的squid是怎么玩的的，那里我是折腾好了的 然后看到AV防病毒 https://docs.opnsense.org/manual/how-tos/proxyicapantivirus.html AV也需要透明代理，这里是老的opnsence集成web proxy时候的说明，正好可以拿来对照squid web proxy插件时代的配置项👇 https://docs.opnsense.org/manual/how-tos/cachingproxy.html 这一篇也要看 https://cloud-atlas.readthedocs.io/zh-cn/latest/web/proxy/squid/introduce_squid.html 插播：dns的坑，也许不算坑 但是会干扰你的排查故障，这也是致命的，你会人为是dns相应慢，其实不慢。 1、现象 windows cmd nslookup 2s延迟 windows cmd dig 无延迟 linux nslookup或dig 都无延迟 上截图 windows nslookup 每次都慢 windows dig 每次都快 linux 不管是nslookup还是dig都ok 排查，去dns server抓包可见 windows的nslookup 每次会多出一步PTR反解 而windows的dig，linux的nslookup、dig都没有这个PTR 为什么同一台PC nslooklup 10.100.8.2 有PTR，nslookup 192.168.10.2没有PTR， 因为下图的Unkown就是原因。去给10.100.8.2配置hostname就行了 PS: dig不管什么OS都不会，然后linux的nslookup也不会 补上dnsmasq的/etc/hosts的A记录同时也是PTR记录就行了 搞定，不卡了 >> 继续squid读文 https://cloud-atlas.readthedocs.io/zh-cn/latest/web/proxy/squid/squid_socks_peer.html#sockssquid 这里调一下，写错了👇 然后读文 https://cloud-atlas.readthedocs.io/zh-cn/latest/web/proxy/squid/squid_gfwlist.html 里面提到的gfwlist是国外IP地址，而且txt里也是处理过的，看不到具体ip地址 还不如用国内IP LIST，对吧除此之外的就走节点出去 https://gitee.com/haiyangyu/CN_ISP_RIB/tree/master 继续处理squid的透明模式 发现按照 https://www.cnblogs.com/weq0805/p/15242170.html 配置 透明模式 3128监听端口就没了 问了GPT才知道要看cache的log 然后这个报错 forward-proxy ports 好像在上面的文里也有 这里有个点就是 1、opsense里是没有 两个端口的，就是上图的8080和3128，就是不透明和透明的两个端口 两行配置 2、然后自己的搭建的squid应该是要些两行，而且端口不能一样 还是通过cache.log可见，重启服务，配置一样的端口 所以上图的3128不能一样，这里不知道opnsense怎么做到不写第一行的3128的 此时http://的就可以访问了，不会出现squid的invliad页面了👇这是当时的报错： 这样配置，第二行不生效的，不过3128能够起来，然后就是上上图的报错 然后继续折腾https的问题👇，那就有的折腾了，不过opnsense已经ok，思路就是自签名，然后同样端口要错开和http的3128，人家用额3129 删掉squid，重新编译带上ssl ./configure --prefix=/usr/local/squid --sysconfdir=/etc --enable-arp-acl --enable-linux-netfilter --enable-linux-tproxy --enable-async-io=100 --enable-err-language=\"Simplify_Chinese\" --enable-underscore --enable-poll --enable-gnuregex --enable-ssl 插播，妈的不知咋回事gitbook太慢了，难道是有收费版的原因？ 就👆这个共享冲突上面会卡特喵几个小时，，，之前还没这么久，共享冲突每次都出现啊，怎么可能是这个问题。 ssl就有了👆 然后还是按他的走一遍，当然systemd自己弄。 https://www.cnblogs.com/weq0805/p/15242170.html 服务起来后，按他的走ssl https://cloud-atlas.readthedocs.io/zh-cn/latest/web/proxy/squid/squid_transparent_proxy.html 不过话说回来，这么多次ssl的证书，nginx的httpd的mysql的haproxy的，不管是openssl的还是makefile的，还是opnsense自己的图形界面的，还是openvpn的，都有在用，但是好像cli都略有不同，基本一致。好讨厌啊 要不，这就总结下吧 ssl自签名的各种cli汇总 1、ssh那会不是基于key，而是基于双向非对称加密的。 1、建立CA：genrsa生成ca私钥--cakey；利用cakey自签名证书--自己给自己颁发证书 (umask 077;openssl genrsa -out private/cakey.pem 4096) openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -out /etc/pki/CA/cacert.pem -days 3650 /etc/pki/CA/serial # 存放下一个颁发证书的序列号，0F改成01从第一个号开始分 2、搞一个证书申请文件 mkdir /etc/httpd/conf.d/ssl cd /etc/httpd/conf.d/ssl (umask 066;openssl genrsa -out httpd.key 1024) #1024可能有问题msyql那会的经验告诉我要4096保持一致 openssl req -new -key httpd.key -out httpd.csr 2、apache那会 CA上👇 cd /etc/pki/CA mkdir certs mkdir crl mkdir newcerts mkdir private mkdir ssl (umask 077;openssl genrsa -out private/cakey.pem 4096) openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -out /etc/pki/CA/cacert.pem -days 3650 touch /etc/pki/CA/index.txt # 存放已经颁发的证书信息 echo 0F > /etc/pki/CA/serial # 存放下一个颁发证书的序列号，0F改成01从第一个号开始分 server上👇 mkdir /etc/httpd/conf.d/ssl cd /etc/httpd/conf.d/ssl (umask 066;openssl genrsa -out httpd.key 2048) #1024可能有问题msyql那会的经验告诉我要4096保持一致 openssl req -new -key httpd.key -out httpd.csr scp /etc/httpd/conf.d/ssl/httpd.csr CAServer:/etc/pki/CA # 把csr申请文件传到CA上，在CA上根据csr文件来颁发证书，也就是对其加密。 CA上👇 openssl ca -in /etc/pki/CA/httpd.csr -out /etc/pki/CA/certs/httpd.crt -days 100 -extensions v3_req scp /etc/pki/CA/certs/httpd.crt root@httpdServer:/etc/httpd/conf.d/ssl # 把证书复制到server上 scp /etc/pki/CA/cacert.pem root@httpdServer:/etc/httpd/conf.d/ssl # 把ca自己的证书也复制倒server上，此举相当于windows预加载了受信任的根证书文件。 3、nginx那会makefile文件弄的 不过nginx那里就看到两个文件，不像httpd那会3个文件，本质一样的，因为nginx那会既做CA也做server所以就2个就行了。 这里继续弄squid的ssl吧 这个没问题 不过我没有下面这个命令， 不用多想，重新编译squid吧，你看看opnsense人家的squid就有这个命令 重新编译，按人家网关提示，对，我就是看看这个，看看官网，再看看opnsense插件的。 https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit 我的编译如下，多带了一个--enable-ssl也是参考的opnsense和上面一个连接里的。 然后squid是私钥和自签名在一个文件里的，所以从httpd的3个文件 到 nginx的两个文件 再到 squid的一个文件，再到openvpn的client也是一个文件。 不过openvpn server还是3个文件+ dh.pem 一共4个文件分开来的，client之所以一就是写在一起了而已。 按这个来： https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit 先修改openssl配置文件 这里不是csr的申请，所以v3的cli变了一下 openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -extensions v3_ca -keyout myCA.pem -out myCA.pem 不过csr颁发server证书的时候才有alt_name也就是subject Alternative name ca里是没有的。。。 然后再 继续吧，这次重新编译后就有这个文件了👇 但是官网它写错了 改成https和3129和默认的3128区分开来，也要加上intercept的， 然后注意重新编译后的log 文件权限给到 然后重启就能看到cache.log了，通过它就能发现哪里出问题了 再试 不行，还是手动敲一下吧 然后由于squid服务一直锲而不舍的尝试，所以不需要重启，就起来了，就看到3128 3129端口了 这不就可以了嘛 继续把证书搞一下，不然还是不行 证书就导出xx.der然后导入到windows里就行了 搞定！ 观察cache.log还有报错，就处理 报错解决 自此，opnsense的squid插件，独立出来 落地实现👆 意味着：我只要在网络核心旁挂一台squid，然后将网关指过去，然后用户PC加载自签名SSL，就可以监控所有上网内容，①这套方案落在开发内网高密环境 ②性能需要测试的。 以下配置cli历史记录，看不到vim里的 cd /squid mkdir /squid cd /squid/ wget https://www.squid-cache.org/Versions/v6/squid-6.10.tar.xz yum -y install gcc gcc-c++ make tar -xvf squid-6.10.tar.xz cd squid-6.10 ./configure --prefix=/usr/local/squid --sysconfdir=/etc --enable-arp-acl --enable-linux-netfilter --enable-linux-tproxy --enable-async-io=100 --enable-err-language=\"Simplify_Chinese\" --enable-underscore --enable-poll --enable-gnuregex 👆这里缺失ssl的功能 lscpu make -j 4 && make install ln -s /usr/local/squid/sbin/squid /usr/local/sbin 👈PATH变量 squid -h which squid ll ll `which squid` ll /usr/local/squid/sbin/squid id squid useradd -h useradd -M -s /sbin/nologin squid 👈 服务的启动用户 id squid chown -R squid.squid /usr/local/squid/var/ 👈 日志生成的权限 ll /usr/local/squid/var/ ll /usr/local/squid/var/ -R vim /etc/squid.conf squid -k parse 👈 没啥用的检查配置语法，其实还得靠cache.log看报错 vim /etc/squid.conf squid -k parse vim /etc/squid.conf squid -k parse squid -z squid ps auxf |grep -v grep |grep squid ss -tlnup |grep squid netstat -anpt |grep squid netstat -anpt ss -tlnup squid -k parse ll /usr/local/squid/var/run/squid* ll /usr/local/squid/var/run/squid.pid cat /usr/local/squid/var/run/squid.pid ps auxf |grep -v grep |grep squid cat /etc/squid.conf ll /usr/local/squid/sbin/squid squid -h 👇编写systemd的过程 ss -tlnup /etc/init.d/squid /etc/init.d/squid restart /etc/init.d/squid status cat /etc/init.d/squid ll /usr/local/squid/var/run/squid.pid /etc/init.d/squid stop ss -tlnup |grep squid /etc/init.d/squid kill /etc/init.d/squid stop ss -tlnup |grep squid squid -k kill touch /usr/local/squid/var/run/squid.pid /etc/init.d/squid stop ll /usr/local/squid/var/run/squid.pid ss -tlnup |grep squid /etc/init.d/squid stop ss -tlnup |grep squid ll /usr/local/squid/var/run/squid.pid touch /usr/local/squid/var/run/squid.pid squid -k kill pgrep suqid pgrep squid kill 86542 pgrep squid kill -9 86542 pgrep squid service squid start service squid status service squid stop service squid status service squid start service squid status pgrep squid kill -9 `pgrep squid` pgrep squid squid -k squid -h rpm -c nginx rpm -qc nginx rpm -ql nginx cd /usr/lib/systemd/system/ ll ls rm -rf /etc/init.d/ vim squid.serviec vim squid.service vim nginx.service vim squid.service netstat -natp | grep squid systemctl daemon-reload systemctl start squid systemctl status squid systemctl enable squid systemctl status squid systemctl stop squid ps auxf |grep -v grep |grep squid ss -tlnup |grep squid systemctl status squid systemctl reload squid systemctl retart squid systemctl restart squid systemctl status squid systemctl reload squid ss -tlnup |grep squid ps auxf |grep -v grep |grep squid cat squid.service netstat -natp | grep squid cat /usr/local/squid/var/run/squid.pid cat nginx.service cat /usr/local/squid/var/run/squid.pid cat squid.service squid -h cat /usr/local/squid/var/run/squid.pid squi cat /usr/local/squid/var/run/squid.pid squid -k shutdown cat /usr/local/squid/var/run/squid.pid systemctl status squid systemctl restart squid systemctl status squid systemctl start squid systemctl status squid systemctl stop squid systemctl status squid systemctl start squid systemctl status squid netstat -natp | grep squid echo $? systemctl stop squid cat nginx.service cat squid.service systemctl stop squid netstat -natp | grep squid echo $? netstat -natp | grep squid || exit 0 cd /run/ echo $? cd /usr/lib/systemd/system ll cat nginx.service cat squid.service nginx -h cat nginx.service cat squid.service systemctl start squid ll /usr/local/squid/var/run/squid.pid ss -tlnup vim squid.service ll /usr/local/squid/var/run/squid.pid squid -k shutdown ll /usr/local/squid/var/run/squid.pid vim squid.service systemctl status squid systemctl start squid systemctl daemon-reload systemctl start squid systemctl status squid systemctl stop squid systemctl start squid squid -k parse cd /usr/local/squid/var/cache/squid ll vim /etc/squid.conf systemctl restart squid systemctl status squid iptables -vnL systemctl status docker iptables -vnL docker ps ip a nmcli conn route -n ss -tlnup tail -f /usr/local/squid/var/logs/access.log ps auxf |grep -v grep |grep squid ss -tlnup |grep 3128 ss -tlnup |grep squid ss -tlnup |grep 3218 ss -tlnup |grep squid ps auxf |grep -v grep |grep squid ss -tlnup |grep 95991 ss -tlnup |grep 95 ss -tlnup ps auxf |grep -v grep |grep squid tail -f /usr/local/squid/var/logs/access.log ss -tlnup |grep 3128 ps auxf |grep -v grep |grep squid ss -tlnup |grep 3128 ss -tlnup ss -tlnup |grep 3128 tail -f /usr/local/squid/var/logs/access.log ss -tlnup tail -f /var/log/nginx/access.log systemctl status squid cat /usr/lib/systemd/system/squid.service cat /etc/squid.conf cat /etc/squid.conf |grep log tail -f /usr/local/squid/var/logs/access.log cat /usr/local/squid/var/logs/access.log cat /usr/local/squid/var/logs/access.log |grep name cat /usr/local/squid/var/logs/access.log |grep query cat /usr/local/squid/var/logs/access.log |grep www.baidu.com tail -f /usr/local/squid/var/logs/access.log iptables -vnL iptables -A -s 192.168.127.0/24 -j DROP iptables -A INPUT -s 192.168.127.0/24 -j DROP iptables -vnL ss -tlnup tail -f /usr/local/squid/var/logs/access.log tail -f /usr/local/squid/var/logs/cache.log tail -f /usr/local/squid/var/logs/access.log history |grep config vim squid.service tail -f /usr/local/squid/var/logs/access.log top ps axuf ps axuf |grep make ps axuf |grep make -j 4 ps axuf |grep 'make -j 4' df -h ll /usr/local/squid/var/run/squid.pid watch ll /usr/local/squid/var/run/squid.pid ll /usr/local/squid/var/run/squid.pid vim /etc/sysctl.conf echo 'net.ipv4.ip_forward = 1' >> /etc/sysctl.conf 👈配置路由转发，传统模式不需要。透明模式才需要 cat /etc/sysctl.conf sysctl -p ip a iptables -vnL iptables -t nat -I PREROUTING -i ens33 -s 192.168.0.0/16 -p tcp --dport 80 -j REDIRECT --to 3128 iptables -vnL iptables -vnL -T NAT iptables -vnL -t NAT iptables -vnL -t nat iptables -t nat -I PREROUTING -i ens33 -s 192.168.0.0/16 -p tcp --dport 443 -j REDIRECT --to 3128 iptables -vnL -t nat iptables -vnL iptables -vnL -t nat iptables -vnL iptables -F iptables -t nat iptables -vnL -t nat iptables -vnL iptables -vnL -t nat iptables -P FORWARD ACCE iptables -P FORWARD ACCEPT iptables -vnL -t nat iptables -vnL iptables -vnL -t nat ip a systemctl restart iptables systemctl restart docker iptables -vnL iptables -vnL -t nat iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 443 -j REDIRECT --to 3128 这里其实要改成3129才对，一开始不知道。 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 80 -j REDIRECT --to 3128 iptables -vnL -t nat iptables -vnL iptables -vnL -t nat iptables -vnL iptables -F iptables -vnL iptables -vnL -t nat iptables -vnL iptables -vnL -t nat ip a route -n iptables -vnL iptables -t nat -vnL ss -tlnup sysctl -p iptables -F iptables -F -t nat iptables -vnL iptables -vnL -t nat iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 80 -j REDIRECT --to 3128 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 443 -j REDIRECT --to 3128 iptables -t nat -vnL vim /etc/squid.conf systemctl restart squid systemctl status squid ss -tlnup squid -k parse ps auxf |grep -v grep |grep squid vim /etc/squid.conf systemctl restart squid iptables -vnL -t nat iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j REDIRECT --to 192.168.126.2:53 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j REDIRECT --to 192.168.126.2 --port 53 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j REDIRECT --to 192.168.126.2:533 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j REDIRECT --to 192.168.126.2:53 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j REDIRECT --to iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j DNAT --to-destination 192.168.126.2:53 👈dns配置对的，就是需要加一个出向的SNAT。下面有。 iptables -vnL -t nat ip a iptables -vnL -t nat cat /etc/resolv.conf dig dig www.baidu.com @192.168.126.2 iptables -vnL -t nat dig www.baidu.com @192.168.127.144 iptables -vnL ip a iptables -vnL -t nat tcpdump -nni tcpdump -nn tcpdump -nn host 192.168.10.2 tcpdump -nn host 192.168.127.144 tcpdump -nn udp host 192.168.127.144 tcpdump -nn udp and host 192.168.127.144 iptables -vnL iptables -vnL -t nat ss -tlnp ss -tlnup ip a tcpdump -nni eth1 udp 53 tcpdump -nni eth1 udp port 53 tcpdump -nnvvvvvi eth1 udp port 3128 tcpdump -nnvvvvvi eth1 port 3128 iptables -vnL iptables -vnL -t nat tcpdump -nnvvvvvi eth1 port 3128 iptables -F history |grep iptables iptables -vnL -t nat systemctl restart iptables iptables - iptables -h iptables -Z iptables -vnL -t nat iptables -Z -t nat iptables -vnL -t nat ifconfig tcpdump -nnvvvvvi eth0 port 3128 tcpdump -nnvvvvvi eth0 tcpdump -nnvvvvvi eth0 host 192.168.126.2 iptables -vnL route -n iptables -vnL -t nat iptables -vnL iptables -A POSTROUTING -j MASQUERADE iptables -t nat -A POSTROUTING -j MASQUERADE iptables -vnL -t nat -A FORWARD -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j TCPMSS --set-mss 1356 iptable -A FORWARD -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j TCPMSS --set-mss 1356 iptables -A FORWARD -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j TCPMSS --set-mss 1356 # 最后不需要 iptables -vnL iptables -vnL -t nat iptables -vnL -t nat -z iptables -t nat -z iptables -t nat -Z iptables -vnL -t nat -Z iptables -vnL -t nat ss -tlnup vim /etc/squid.conf systemctl restart squid ss -tlnup vim /etc/squid.conf systemctl restart squid systemctl status squid ss -tlnup vim /etc/squid.conf getenforce systemctl status firewalld systemctl status iptables sudo lsof -i :3128 vim /etc/squid.conf systemctl restart squid vim /etc/squid.conf systemctl restart squid ss -tlnup systemctl status squid vim /etc/squid.conf systemctl restart squid ss -tlnup iptables -vnL iptables -vnL -t nat vim /etc/squid.conf systemctl restart squid ss -tlnup systemctl status squid ss -tlnup vim /etc/squid.conf docker ps docker stop gitlab ss -tlnup vim /etc/squid.conf systemctl restart squid ss -tlnup vim /etc/squid.conf top systemctl restart squid ss -tlnup vim /etc/squid.conf systemctl restart squid ss -tlnup vim /etc/squid.conf systemctl restart squid ss -tlnup cat /etc/squid.conf squid -k parse vim /etc/squid.conf iptables -vnL vim /etc/squid.conf systemctl restart squid ss -tlnup vim /etc/squid.conf ip a vim /etc/squid.conf sockstat -l yum search sockstat ss -tlnup vim /etc/squid.conf systemctl restart squid ss -tlnup history |grep configure vim /etc/squid.conf systemctl restart squid ss -tlnup iptables -vnL iptables -vnL -t nat iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j DNAT --to-destination 192.168.126.2:53 iptables -vnL -t nat iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j DNAT -port 3128 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j DNAT -to-port 3128 history |grep iptables |grep 3128 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j REDIREECT --to 3128 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j REDIRECT --to 3128 iptables -vnL -t nat iptables -h iptables -vnL -t nat iptables -I -t nat iptables -vnL iptables -vnLI iptables -vnLa iptables -vnLA iptables -h iptables -vnL iptables -vnL -t nat iptables -vnL -t nat --line-numbers iptables -t nat -D 2 iptables -D PREROUTING 2 -t nat iptables -D PREROUTING 3,4 -t nat iptables -D PREROUTING 3-4 -t nat iptables -D PREROUTING 3 -t nat iptables -D PREROUTING 34-t nat iptables -D PREROUTING 4 -t nat iptables -vnL -t nat iptables -vnL -t nat --line-numbers iptables -t nat -I 3 PREROUTING -s 192.168.0.0/16 -p udp --dport 80 -j REDIREECT --to 3128 iptables -t nat -I PREROUTING 3 -s 192.168.0.0/16 -p udp --dport 80 -j REDIREECT --to 3128 history |grep iptables |grep 3128 iptables -t nat -I 3 PREROUTING -s 192.168.0.0/16 -p tcp --dport 80 -j REDIREECT --to 3128 iptables -t nat -I PREROUTING 3 -s 192.168.0.0/16 -p tcp --dport 80 -j REDIREECT --to 3128 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 80 -j REDIREECT --to 3128 iptables -t nat -I PREROUTING 3 -s 192.168.0.0/16 -p tcp --dport 80 -j REDIRECT --to 3128 iptables -vnL -t nat iptables -vnL -t nat --line-numbers iptables -t nat -D PREROUTING 2 iptables -vnL -t nat history |grep iptables |grep 126.2 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p udp --dport 53 -j DNAT --to-destination 192.168.126.2:53 iptables -vnL -t nat vim /etc/squid.conf systemctl restart squid yum search squid cat /usr/lib/systemd/system/squid.service dig dig www.bing.com @10.100.8.2 nslookup nslookup www.bing.com 10.100.8.2 while true;do nslookup www.bing.com 10.100.8.2;done nslookup www.bing.com 10.100.8.2 dig www.bing.com @10.100.8.2 ip a docker s docker ps docker ps -a systemctl status squid ss -tlnpu cd /etc/squid/certs/ ll cat openssl ll rm -rf openssl ll openssl req -new -newkey rsa:4096 -sha256 -days 3650 -nodes -x509 -keyout myCA.pem -out myCA.pem ll cat myCA.pem ll openssl x509 -in myCA.pem -text -noout openssl x509 -in myCA.pem -outform DER -out myCA.der ll sz myCA.* **0100000063f694 ▒**0100000063f694 ▒ cd /etc/squid/certs/ ll sz sz myCA.* **0800000000022d ▒**0800000000022d ▒ ll squid -v 👈类型nginx -v /usr/lib64/squid/security_file_certgen -c -s /var/lib/ssl_db -M 4MB find / -name security_file_certgen rm -rf /usr/local/squid/ cd /squid/ ll cd squid-6.10 ll cd .. l ll rm -rf squid-6.10 ll tar xvf squid-6.10.tar.xz ll cd squid-6.10 ll ./configure --prefix=/usr/local/squid --sysconfdir=/etc --enable-arp-acl --enable-linux-netfilter --enable-linux-tproxy --enable-async-io=100 --enable-err-language=\"Simplify_Chinese\" --enable-underscore --enable-poll --enable-gnuregex --with-openssl --enable-ssl-crtd --enalbe-ssl ll ./configure --prefix=/usr/local/squid --sysconfdir=/etc --enable-arp-acl --enable-linux-netfilter --enable-linux-tproxy --enable-async-io=100 --enable-err-language=\"Simplify_Chinese\" --enable-underscore --enable-poll --enable-gnuregex --with-openssl --enable-ssl-crtd --enable-ssl 👈编译的选项，重点后面三个需要开启。 make -j 4 && make install vim /etc/squid.conf cd cd /etc/squid cd /etc/squid ll rm -rf certs/ ll mkdir ssl_cert ll chown squid:squid ssl_cert ll cd .. ll -d ll -d squid cd squid ll cd ssl_cert/ ll cd .. ll chmod 700 ssl_cert # 配置文件需要有权限读取 ll cd ssl_cert/ ll openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -extensions v3_ca -keyout myCA.pem -out myCA.pem ll rpm -ql openssl rpm -qc openssl rpm -ql openssl rpm -ql openssl |grep cnf rpm -ql openssl |grep conf rpm -ql openssl |grep cnf rpm -qf /etc/pki/tls/openssl.cnf vim /etc/pki/tls/openssl.cnf # 没必要，这里是csr申请server证书的时候配置alter_name的。用来浏览器显示安全的，不过squid这里不需要。 ll rm -rf myCA.pem openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -extensions v3_ca -keyout myCA.pem -out myCA.pem # 私钥和证书在一个文件的产生方式。 ll openssl x509 -in myCA.pem -text -noout # 明文查看证书的方式 find / -name security_file_certgen vim /etc/pki/tls/openssl.cnf vim /etc/squid.conf squid -k parse squid -v ll /usr/local/squid/ tree /usr/local/squid/ systemctl restart squid systemctl status squid ss -tlnup ll systemctl status squid ll /usr/local/squid/sbin/squid ll /usr/local/ -d ll /usr/local/squid/ -d history |grep chown ll /usr/local/squid/var/ chown -R squid.squid /usr/local/squid/var/ systemctl restart squid ss -tlnu ll /var/lib/ -d chmod +w /var/lib/ ll /var/lib/ -d chmod o+w /var/lib/ ll /var/lib/ -d systemctl restart squid ss -tlnu security_file_certgen find / -name security_file_certgen vim /etc/squid.conf /usr/local/squid/libexec/security_file_certgen -c -s /var/lib/ssl_db -M 4MB 👈初始化db不然squid配置ssl起不来，不过人家会不断尝试初始化，手动敲了这条就行了。 ss -tlnu iptables -vnL iptables -vnL -t nat systemctl stop docker iptables -vnL -t nat iptables -F iptables -vnL -t nat history |grep iptables iptables -t nat -A POSTROUTING -j MASQUERADE 👈出向SNAT iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 80 -j REDIRECT --to 312 iptables -D PREROUTING iptables -vnL -t nat iptables -vnL -t nat --line-numbers history |grep iptables |grep -D history |grep iptables |grep '-D' history |grep iptables |grep '\\-D' iptables -t nat -D PREROUTING 1 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 80 -j REDIRECT --to 3128 iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 443 -j REDIRECT --to 3129 iptables -vnL -t nat iptables -vnL -t nat --line-numbers iptables -t nat -D PREROUTING 1 iptables -vnL -t nat --line-numbers iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 80 -j REDIRECT --to 3128 👈 http打到3128 iptables -vnL iptables -t nat -I PREROUTING -s 192.168.0.0/16 -p tcp --dport 443 -j REDIRECT --to 3129 👈https的打到3129 iptables -vnL -t nat ll sz myCA.pem **0800000000022d ▒**0800000000022d ▒ ll openssl x509 -in myCA.pem -outform DER -out myCA.der 👈windows认.der后缀 ll sz myCA.der **0800000000022d ▒**0800000000022d ▒ ll history history |awk '{for(i=5;i 这是所有都ok后的cnf👇 [root@mysql-2 ssl_cert]# cat /etc/squid.conf # # Recommended minimum configuration: # # Example rule allowing access from your local networks. # Adapt to list your (internal) IP networks from where browsing # should be allowed acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 \"this\" network (LAN) acl localnet src 10.0.0.0/8 # RFC 1918 local private network (LAN) acl localnet src 100.64.0.0/10 # RFC 6598 shared address space (CGN) acl localnet src 169.254.0.0/16 # RFC 3927 link-local (directly plugged) machines acl localnet src 172.16.0.0/12 # RFC 1918 local private network (LAN) acl localnet src 192.168.0.0/16 # RFC 1918 local private network (LAN) acl localnet src fc00::/7 # RFC 4193 local private network range acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines acl SSL_ports port 443 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025-65535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http # # Recommended minimum Access Permission configuration: # # Deny requests to certain unsafe ports http_access deny !Safe_ports # Deny CONNECT to other than secure SSL ports http_access deny CONNECT !SSL_ports # Only allow cachemgr access from localhost http_access allow localhost manager http_access deny manager # This default configuration only allows localhost requests because a more # permissive Squid installation could introduce new attack vectors into the # network by proxying external TCP connections to unprotected services. http_access allow localhost # The two deny rules below are unnecessary in this default configuration # because they are followed by a \"deny all\" rule. However, they may become # critically important when you start allowing external requests below them. # Protect web applications running on the same server as Squid. They often # assume that only local users can access them at \"localhost\" ports. http_access deny to_localhost # Protect cloud servers that provide local users with sensitive info about # their server via certain well-known link-local (a.k.a. APIPA) addresses. http_access deny to_linklocal # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS # # For example, to allow access from your local networks, you may uncomment the # following rule (and/or add rules that match your definition of \"local\"): # http_access allow localnet # And finally deny all other access to this proxy http_access allow all http_access deny all # Squid normally listens to port 3128 http_port 8080 http_port 3128 intercept https_port 3129 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s /var/lib/ssl_db -M 4MB acl step1 at_step SslBump1 ssl_bump peek step1 ssl_bump bump all # Uncomment and adjust the following to add a disk cache directory. #cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256 cache_effective_user squid cache_effective_group squid # Leave coredumps in the first cache dir coredump_dir /usr/local/squid/var/cache/squid # # Add any of your own refresh_pattern entries above these. # refresh_pattern ^ftp: 1440 20% 10080 refresh_pattern -i (/cgi-bin/|\\?) 0 0% 0 refresh_pattern . 0 20% 4320 [root@mysql-2 ssl_cert]# 这是服务 [root@mysql-2 squid]# cat /usr/lib/systemd/system/squid.service [Unit] Description=Squid Web Proxy After=network.target [Service] Type=forking PIDFile=/usr/local/squid/var/run/squid.pid ExecStart=/usr/local/squid/sbin/squid ExecStop=/usr/local/squid/sbin/squid -k shutdown #ExecReload=kill -HUP $(cat /usr/local/squid/var/run/squid.pid) ExecReload=/bin/sh -c 'kill -HUP $(cat /usr/local/squid/var/run/squid.pid)' #ExecReload=/usr/local/squid/sbin/squid -k reconfigure #ExecStartPre=/bin/sh -c 'netstat -natp | grep squid &> /dev/null || exit 0' #ExecStartPost=/bin/sh -c 'echo \"squid is starting...\"' ExecStopPost=/bin/sh -c 'rm -f /usr/local/squid/var/run/squid.pid' [Install] WantedBy=multi-user.target [root@mysql-2 squid]# 注意上图的kill 要比reconfiure有用，速度快，而且要用/bin/sh -c去运行，否则报错找不到pid文件。 还有一个补充，缓存的操作👇，当然不要用squid去启动哦，只是看squid -z那一行 我的操作 上图初始化也报错了，那就stop 掉squid继续 此时cache缓存就初始化了 启动squid是OK的，同时看日志可见缓存的信息 其实思路是对的，因为你ssl卸载降低了用户的速度，你又缓存将这个速度又补回去了。 下午研究下过滤配置吧 URL-acl 1、首先我要独立的过滤文件 有的还是参考https://www.cnblogs.com/weq0805/p/15242170.html 看着不错，试试 然后考虑文件的频繁修改和生效，需要修改reload底层cli，用kill -HUP而不用 -k reconfigure-太慢了 注意url过滤域名和ip的两个url都要写 否则就是域名的https://xxx.yy.zz/123 干掉了，但是xxx.yy.zz对应的ip http://a.b.c.d/123没干掉。 白名单和黑名单 要针对一个url要写域名的url和ip的url两行的 chrome的无痕模式是不是有什么特殊性， firefox 和 firefox 无痕 都可以打开http://xxx.yy.zz/annoucement chrome 可以，chrome无痕不行 https://blog.csdn.net/u010059669/article/details/135158912 这就是原因👆 也就是无痕的问题👇关键这个httpupgrade是chrome的优化，但是在无痕下需要点击 继续访问，而这个点击的动作 结合 squid过滤就无法 继续点击了，直接就干掉了。所有 要关闭这个httpgrade试试。 第二个是https的，回退成第一个307了 之前是没有这一步的，现在chrome的无痕加了个这个👆。 307是HTTPS回退http的代码，不是是http升到https👇 尝试关闭这个无痕模式的httpupgrades，无痕用不了插件的。 https://blog.csdn.net/u010059669/article/details/135158912 上面的问题，什么问题：就是squid做了ip和域名的 url过滤后 firefox 有痕 和 无痕 都可以打开，而chrome 有痕可以，无痕NG的问题解决了，就是有痕的模式下没有192.168.200.18:443这么一个connect，而无痕存在，chrome的无痕需要加一个IP:443的放行 这是chrome无痕下访问http://xxn.it/announcement的情况，注意下图明显看到是https的，要注意是自己跳过去的，和我无关，也无需关注这件事情。 通过观察access.log发现有一条 于是在过滤url的文件里补一个IP和443就可以弹出这个点击继续的页面了 此时继续就OK了，自此完成无痕下的url放行优化。 然后还梳理一个这个问题 就是页面点击首页可以进去，首页其实是squid白名单里没有放行的，也就是deny的嘛，原因是访问url的时候加载了部分的首页资源。所以可以打开些 squid只放行了 通过先打开http://lxxn.it/announcement 再点击页面的首页就进去了 虽然squid禁止了log有， 但是其实之前访问子页面的时候就加载了好多👇，所以可以站内点击进去，但是 但是重新发起就不行了👇 ①打开squid放行的url ok的 ②然后点击页面上的首页就进入了squid没有放开的页面 ③再次直接访问这个页面可见是不行的 总之这块差强人意吧，我说差强人意，不代表 别人可以这么说，特别是不懂细节的人，额b（￣▽￣）d　好像也没人跟我这么说。。。 工作案例-如何监管开发内网的HTTPS的payload也就是内容过滤 https://www.cnblogs.com/studio313/archive/2011/09/22/2184969.html https://www.theopensourcerer.com/2014/04/how-to-install-a-squid-dansguardian-content-filter-on-ubuntu-server/![image-20241014140817595](2-网络架构.assets/image-20241014140817595.png) 我也加上了这一段，除了200 MB要有空格以外还需要删除log_fqdn off这一句过失的命令，其他都没有影响squid的启动。但愿能够起到优化的作用。 测试了下logfile_roate 10 有效果👇 dansguardian太老了，应该是收费了吧，所以换成c-icap继续研究 https://c-icap.sourceforge.net/ 挺好，天生一家👆squid和icap，opnsense那里好像叫o-c-icap好像，回头折腾opnsense的时候再说，要弄的！ 看看github上维护很蛮新的 https://github.com/c-icap/c-icap-server yum后的service文件里的启动用户和组要注释掉，否则起不来。。。。而且nginx和squid都是fork的形式启动的也不一样。先root吧 https://blog.csdn.net/liangzhao_jay/article/details/12575839 https://cloud.tencent.com/developer/ask/sof/116943622 https://www.egirna.com/blog/news-2/how-to-configure-squid-proxy-with-icap-10 👇这个是重点 https://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP opnsense是 squid+c-icap+clamav+redis的，好复杂 不过clamav既然能扫病毒，应该就能扫ftp、pdf、之类的正常文件啊。 https://www.cnblogs.com/gaoyuechen/p/9019098.html这是clamav软件单独的 试下👆安装试验下 说这个是默认病毒库 升级病毒库的cli在clamav-devel包里， 有了👇 https://wiki.ubuntu.org.cn/ClamAV 但是病毒库升上去也就是下载下来，有可能导致oom的 我这里不加内存了，删掉一个库测试 既然能扫出来文件，网络流量里的日志也许也有这种文件名称吧。 https://blog.csdn.net/lionzl/article/details/7749334 这个不错👆 squid 、 c-icap、clamav之间配合，肯定要编译的时候带上一些模块的 不过可惜，他没有带上ssl，所以他没法对ssl 也就是https杀毒 这是👇opnsense的编译选项 然后yum 安装squid 一样选项多多 就是版本低了些 https://wiki.ubuntu.org.cn/ClamAV clamav要有conf，看来是clamav-daemon了，可惜我们找到这个包 竟然有squidclamav这个，看名字就是集成好的咯，算了，我还是分开来学吧 👆说明安装2个包其实附带安装了很多的。 https://blog.csdn.net/ygm_linux/article/details/60882597 这个也不错👆 https://www.clamav.net/ 这个是官网👆，看着酷倒是挺酷，就是有点像病毒。。。 https://docs.clamav.net/appendix/CvdPrivateMirror.html?highlight=squid#use-an-http-proxy 瞥了这么多攻略，找一天时间开干 实验 走一遍https://blog.csdn.net/lionzl/article/details/7749334 yum 安装squid就行 由于我这有一个之前编译的版本，所以ln -s下就好了 还得弄证书，唉~，不用，直接把之前编译的配置文件复制过来就行了。 Icap client for squid配置： icap_enable on icap_preview_enable on icap_preview_size 128 icap_send_client_ip on icap_service service_avi_req reqmod_precache 0 icap://localhost:1344/srv_clamav icap_service service_avi respmod_precache 1 icap://localhost:1344/srv_clamav icap_class class_antivirus service_avi icap_access class_antivirus allow all icap_class class_antivirus_req service_avi_req icap_access class_antivirus_req allow all 继续https://blog.csdn.net/lionzl/article/details/7749334 yum的clam的配置文件如下，不是箭头指的那个，而是freshclam.conf才是，上图行内容在该文件里都有 既然有配置文件，说明有服务啊，看来我还少安装一个服务 神了~ install会给你自动对应过去，search没有。 privodes有的，靠谱 结论：search的名称要对，provides 一定程度上可以给你转换你在clamav后面补一个-server，人家自动识别了。 看这个https://www.cnblogs.com/hkgan/p/17346628.html 了解下log和病毒库 systemctl status clamav-freshclam里也有，不过看不全咯，因为一些动作pre post都是启动后或stop后的动作记录是由屏显宽度的👇daily的就上滚掉了。 然后弄c-icap 起不来，发现是clamd没起来 没起来就看下https://www.cnblogs.com/hkgan/p/17346628.html 再看看 找到原因了👇，sock没给， 然后恢复service文件吧，上图👆是排错复制了一份service文件去掉了里面的变量。 用传参的方式来启动clamd服务 有进步，但是 kaill掉之前的进程，restart就行了，可能之前有手动的残留？应该是的，复现了就是手动用service里的cli启动后，ctrl c退出，再用service启动，就这样了，正常现象。干净的进程 systemctl start就行了 ok了 然后启动c-icap服务发现 还是要注释掉 Service antivirus_module srv_clamav.so 才能起来 可能是模块缺失 试试 看来不是xxx.so文件，还需要编译的👆 看看yum的，之前我是yum的 装一下c-icap-modules模块 果然有了很多 删掉tmp吧，哈哈不要自己编译了 如果自己安装呢，试试 总之再次创建m4文件夹，然后注释到 版本的初始化方式(大概)，打开0.3.2自定义版本就行了 configure文件就有了，可以编译安装了 我还是指定一个prefix吧 看看yum的就知道编译安装大概要哪些包了 继续手动编译 估计这里就是关键了，肯能是这个 选项一指定，就模块安进去了，大概吧 通过./configure -h可见 prefix不是瞎指定的，而是要知道c-icap安装prefix里去的 移走两个yum安装的模块，待会看编译安装是否补进来 没找打config的选项，但是找到了c-icap的选项，试试 然后make && make install报错 然后make 继续报错 放弃，就用yum吧。。。 然后改一下配置文件里的模块，找不到的就用看着像的替换试试，结果，不行 突然发现c-icap yum安装的 没有启动 --with-clamav。。。。。我日，yum坑我 先继续用yum的c-icap试试看 怎么试，就是找srv_clamav.so这个模块 算了 编译吧，不是编译c-icap-modules，而是使用--with-clamav 编译c-icap 重新编译安装c-icap https://c-icap.sourceforge.net/download.html 得再下一个c-icap-modules-0.5.7 开搞 ./configure --enable-static --prefix=/usr/local/c-icap/ make -j 4 && make install 没报错 文件有了 看看模块文件，因为是--enable-static的，所以有很多xx.so，但是没有clamav的，肯定啊，人家官网都说了要第二个包 大不了参考yum安装得服务自己写一个 然后继续安装第二个modules包 ./configure --with-c-icap=/usr/local/c-icap/ --with-clamav=/usr/sbin/clamd 好像失败了 那么就clamav也编译安装一下啊？ https://blog.csdn.net/lionzl/article/details/7749334 ./configure --prefix=/usr/local/clamav --with-dbdir=/usr/clamav 不行了，要安这个来👇 https://docs.clamav.net/manual/Installing/Installing-from-source-Unix.html 安装依赖 python3 -m pip install cmake pytest 改用 dnf install -y \\ gcc gcc-c++ make python3 python3-pip valgrind \\ bzip2-devel check-devel json-c-devel libcurl-devel libxml2-devel \\ ncurses-devel openssl-devel pcre2-devel sendmail-devel zlib-devel yum install cmake dnf install -y cargo rust 改用yum安装 yum -y install bindgen 还剩一个了 yum -y install bindgen 都装好了，还是不行 json-c搞不定，继续 现在就有了 继续 https://blog.csdn.net/cnm_King/article/details/136546478 yum update吧 不行再试试这个 https://rockylinux.pkgs.org/9/rockylinux-crb-x86_64/sendmail-milter-devel-8.16.1-11.el9.x86_64.rpm.html update 磁盘爆了，挂了。。重新弄 === 算了，教程都太老了，还用用官方的squidclamav这个组件吧，这下就4个软件配合。。。 如何理解salt 盐 不废话，show u the picture 怎么样，openssl和mkpasswd两个cli工具都可以知道哈希算法和salt的情况下，针对源数据cisco12345，生成一样的哈希值。 mkpasswd 的-S是salte，-s是stdin，也就是密码也就是源数据被加密的数据 openssl 没有-s来做stdin，但是可以利用xargs直接一行搞定。大不了明文嘛~~哈哈，就是展示用的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-10-18 18:38:20 "},"13-网络协议和管理/3-TCP&IP.html":{"url":"13-网络协议和管理/3-TCP&IP.html","title":"第3节 TCP&IP","keywords":"","body":"第3节. TCP&IP 网工，略 略不了，这里面东西有点深，不过一般用不到，先放几个图，后面再说 还有滑动窗口、慢启动、重传机制等 accept()是app及时提取全连接队列的函数吧，如果处理不及时，也会造成全连接队列拥塞。 这块应该叫linux的网络内核参数文件以及调优，需要整理的，可参考小林codding这位的。 还有之前的一个tw内核参数，针对NAT后买的时间序列问题的，也会导致丢包的情况。 还有nginx的限制并发等。 服务器禁ping除了云上的安全组、服务器本身的iptables、还有内核参数 0表示不忽略，就是可以ping通，echo_ignore就是忽略icmp的echo 此时ping就卡住不动了，就是不通了嘛 再改回0就通了，可见中间丢了几十个包 ttl判断系统 之前是64，现在是128了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:37 "},"13-网络协议和管理/4-IP地址规划.html":{"url":"13-网络协议和管理/4-IP地址规划.html","title":"第4节 IP地址规划","keywords":"","body":"第4节. IP地址规划 网工，略 皮一下~ ipv4的link-local地址是：_ https://datatracker.ietf.org/doc/html/rfc3927 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:37 "},"13-网络协议和管理/5-linux的网络和路由配置管理.html":{"url":"13-网络协议和管理/5-linux的网络和路由配置管理.html","title":"第5节 linux的网络和路由配置管理","keywords":"","body":"第5节. linux的网络和路由配置管理 修改网卡名-方法1-网卡驱动模块 研究下为什么是eth1，怎么改，这是centos6的： 可见eth0已经被某个mac占用，所以现在看到的就是eth1了。 删除并修改 修改后，重启网络服务是不行的 要修改eth1这个名称，需要卸载网卡驱动，并重新加载驱动。 mii-tool 查看 查看网卡驱动 👇找到了驱动模块 lsmod 是找到加载的所有驱动模块 卸载模块用rmmod或者modprobe -r 网卡模块卸了也就是网卡驱动卸了。 此时网卡就自然看不见了 然后再重新加载模块(驱动)： 此时就改为eth0了 方法2-ip子命令 ip 子命令，在centos6上tab不出来，centos7可以 centos6要额外安装软件包才能支持tab键补全 网卡配置 1、setup 进去选-选-选 等价于system-config-network-tui # 这两种方法就是算了，而且在centos7里setup里也没有网络配置选项了。 2、重点看命令和配置文件 ifconfig过时了，擦 主要是因为net-tools这个工具包过时了，所以包里的很多比如ifconfig、netstat都过时了。 推荐你用iproute这个包 ifconfig直接回车看的是激活状态的网卡 禁用和激活 ifconfig eth1 down 禁用eth1网卡 ifconfig 就看不到eht1，ifconfig -a 可以看到eth1但是IP没了，ip a就看的更清除了 ifconfig eth1 up启用 禁用网卡还可以ifdown eth1 不过这种down和刚才的ifconfig eth1 down又不同了 这个ifdown 后，ifconfig确实可以看到的，但是没有地址。 这个ifdown属于网络层的down，IP没了但是数据链路层是通着的。 对比ifconfig属于数据链路层的down。 所以ip a看就会发现是UP的，所以ip a看的就是L2链路层咯。 1、ifdown\\ifup是L3层的up\\down， 这个ifdown后ip a看还是UP的 2、ifconfig down\\ifconfig up是L2层的up\\down，这个ifconfig down后ip a看就是DOWN的 3、ip a看的是L2的up\\down 4、还一种是物理层的down，就是拔网线了 5、ip link set eth0 down 也是可以的，一样是控制L2的up和down，见下图👇 6、几个关键词：LOWER-UP和UP的区别、NO-CARRIER DOWN和DOWN的区别 https://stackoverflow.com/questions/36715664/using-ip-what-does-lower-up-mean ifup 要起来还需要依赖一些网络配置文件，所以ip a还是看不到地址 没关系，在centos6上用service NetworkManager restart就可以了。 把网线的演示 这就类似拔网线了 1、ifconfig看就没有地址了 2、ip a看就是down 此时上图👆就能判断是网线拔了，而不是其他的，因为有关键词NO-CARRIER。 对比ifconfig eth1 down的描述信息 配地址-临时 清地址-临时 增加地址-临时 就是huawei里的sub地址咯，或者是思科的second ip。还一种是子接口，子接口在linux里是一种别名 ifconfig eth1:123 1.1.1.1/24 ifconfig eth1:321 2.2.2.2/24 ifconfig可见 这里不涉及子接口的vlan id封装解封装，直接就能和外界同样的IP进行通信，所以我感觉更像是second ip而不是子接口。 删除linux的子接口 ifconfig eth1:123 down 这个down掉就是删除子接口了，这里就称之为linux的子接口咯，虽然它没有vlan的概念。 混杂模式 抓包的时候就用的混杂模式 混杂--不管这个数据是否给我的，我都收。 https://cloud.tencent.com/developer/article/1439013 VMwareWorkstation的话有时候需要手动修改centos的接口为混杂，原视频中我没有找到当时老师的操作，不过理解后也能自己设计一个场景，所以这里就仅仅提示下。 https://blog.51cto.com/nizhuan/724081貌似桥接模式就是混杂模式了。 mii-tool 工具 上图说明 capabilities 是支持的能力比如FD就是full duplex；HD就是hafl duplex 第一行就是协商后的当前工作模式是千兆全双工 ethtool 工具 网线8根线，如果断了一根运气好，还能用就是100M，👆这里就可以检查到。但是要注意如果是vmxnet3的网卡，ethtool看到的就是10G速率，虽然实际上是1G的。所以这里的速率显示显然不是实际值。 ehtool可以修改网卡工作模式，一般不改 CMD的grep 例子，查看多少人连到我的VNC上 第三列就是client IP地址 常见服务端口 cat /etc/services 一般客户端电脑不会使用1W6+(65535-49152)个端口，但是如果你这台linux或啥系统的电脑是作为代理上网，比如SNAT，那么1W6+也不是没有可能，因为1台内网的PC大概10连接，1000台PC对吧，再加上手机，还是有可能让你的这台NAT服务器的端口超出1w6+这个默认值的。 如果要当代理，这个端口就要调大👆 实际情况是60999-32768=28231个随机端口。 TCP的序列号问题整理 数据包的序列号，并非0，0是相对编号，应该是初始ISN，好久不碰了有点忘了都，而是下面的97fa6e02 再CISCO的安全方向里ASA的里面有一个SN、ISN的利用。然后linux的tw内核参数 还有一个问题，一般出现在网吧、公司这种NAT环境 https://blog.csdn.net/enweitech/article/details/79261439 文章讲的太细，简单的故障处理就是 “之前的偶尔打不开是因为开启了一个tcp相关的内核参数, 办公网都是nat出去的, 数据包时间戳的抖动会导致服务器把请求给丢弃, 迁移之后的 oa单点登录, 虚宝网等我都把该参数关闭了, oa.sm.xxx 这个不在迁移的机器中, 刚刚修改过了, 后续在观察一下” -- 这个是案例，呵呵。 https://ppabc.cn/1363.html 这个篇针对性强，说的是时间戳。 tcpdump的举例-整理稍后 上图👆是drop后的一个抓包结果，可以看到很多的[S]，这就是SYN包，是重传机制导致的。 其实更多的时候，我一般就是抓个端口然后|grep 哈哈，是不是没想到，哈哈 |前导码+开始符|DA|SA|TYPLE/LENGTH|DATA|FCS| 一共72B的最小值。 ping 默认就是64也即是没有算开头的8B。 附上之前的一个分片解析图 TCP超时重传 TCP的拥塞控制 四个机制 慢启动、拥塞避免、快速重传、快速恢复 https://allen-kevin.github.io/2017/12/21/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E4%B9%8BCUBIC/ 并发ping-首推方法 seq 1 255 |xargs -i -P 0 bash -c 'ping -w 2 192.168.25.{} &> /dev/null && echo 192.168.25.{} icmp allowed' 还有awk的并发ping，想不起来了，没影响了。 这个也快 ls --hide=proc | xargs -i -P 0 find /{} -name \"sshd\" awk的举例 tcpdump -i eth0 -tnn dst port 443 -c 100 |awk -F \".\" '{print $1\".\"$2\".\"$3\".\"$4}'|sort|uniq -c|sort -rn|head -n20 我之前说过awk和xargs一样快，ping的处理上我怎么不记得有做过呢，肯定是有过的，现在想来是写个for循环awk调用系统命令，但有这么复杂吗，当然其实不复杂，我是说之前的awk实现批量ping 是怎么弄的？ 还有好多工具类的使用iperf 、curl、F12等等，这些就算了，不放到这里了。 广播的情况 👆ping 255广播只有一个37.2响应，因为linux默认是不响应广播ping的，需要开启 1就是忽略，就是不回应，改为0 centos7上一样，centos8也一样 从默认的TTL上可以判断37.2的ttl是128，是台windows机器；37.7和37.6是linux机器。 注意上面这个实验，要在同一个网段做哦，原因很简单，就是跨网段，的192.168.1.100去 ping -b 192.168.10.255，这个广播是不会有任何回应的，因为3层转发广播不转发的，这就是CCNA里的基本概念--广播域--二层广播帧所能到达的范围就是广播域，显然二层广播帧的广播地址全FFFF-FFFF-FFFF显然要比192.168.10.255这种L3广播地址映射到二层的广播MAC地址还要厉害，连这种真广播都跨不了三层，你凭什么能ping通呢，至于为什么垮不了，因为数据要从二层拆包，到三层，再重新封包，这个过程就算3层路由器设备处理广播----其实真可以处理的，CISCO SECURITY 里有个攻击就是用广播做的好像，需要路由器转发广播的。 loopback 略，注意下图 人为将linux的lo环回口改成6.6.6.6/24 ，这样这个段都处于loop，并不是说必须是127。 这里也可以写IP地址，不一定写lo 网卡配置文件 name将来就表现为GUI图形界面里的eth0 改的玩下， BOOTPROTO=none也行都是静态手动配置，BOOTPROTO=dhcp就是动态 1、注意一旦写了dhcp，后买手动配置的IP地址，DNS就会被覆盖了； 2、文件其实就是脚本用的，前面其实就是变量，变量是区分大小写的，然后=左右不能带空格； 这块书上讲的更全，所谓全其实也就是下发明细和下发默认的一些优先级问题。 还能精简成如下3行，再补一个GW和DNS1 改完配置文件后，一般不会立即生效，有时候会立即生效，那是因为NetworkManager服务，不过这个服务不是时刻都能立即发现你修改了文件然后使之生效的。而且这个服务一般也不用都是关掉的。最小化安装好像也是没有的。说反了，一般是我们只用network，但是rocky-linux和centos7最小化安装后好像rocky-linux是没有network服务只有NetworkManager，然后centos7的network是fail的NetworkManager是active的。好奇怪~没事，停用禁用NetworkManager后就可以启用network服务了。 看来rocky-linux不喜欢这个。centos8一样， linux开启路由功能 注意临时修改是用的echo，vim是不能编辑内存里的文件的 vim是改磁盘文件的，不能改内存的数据。 mtr的使用 mtr可以选择icmp tcp udp的，这点要知道，然后我的处理，是通过crontab去弄，大概如下 基本上如果curl的效果不行，就自动触发mtr得到报告，不过with os.popen这种阻塞的方式，在网络质量不好的清苦下，会造成cpu负载高的，因为太多的阻塞等着运行了。网络好就不存在了，30s不到就mtr完了，或者都不会触发mtr。 frr可以弄一下 frr替代quagga了好像 vyos、led、openwrt、strongswan、openswan、routeros、 vyos的ipsec vpn--police的，存在支持上限的问题，未尝试解决、未升级测试是否得以改善，其他功能OK，由于没有大并发，性能未得到测试。 openwrt里的strongswan在旁路模式下，也不知道是我的TP-LINK物理网卡问题还是旁路的问题，反正隧道存在丢包，游戏时存在断线重连的情况，效果不好，未尝试继续改进。 softher vpn这个不多说，emm，真心不错，一键搞定IPSEC、openvpn、pptp、l2tp。其实远程办公，还是要考虑授权、限速的问题，如此衍生出的IP静态分配，等就不是简单使用softehter来实现，从这个角度还不如自己使用开源来弄，顶多写一个脚本来做统一的多协议账号开通。其中openvpn就是要使用radius和mysql或者ldap之类去做，这个不用这么麻烦，直接配置文件里指向一个脚本验证 一个存放用户名密码的文件就行，具体见我的印象笔记的相关模块。 其他还有什么clash、向日葵、vⅡray是吧，哈哈。emm有时间都要整理出来。这些东西可不是仅仅那个作用，可以用来做远程办公，而且客户端软件效果好，openvpn嘛还不必clash的节点测试效果，切换方便，如果公司4个出口，统一到clash就很不错，需要考虑的时clash他后面要自己搞成静态IP，然后用户授权啦--最好是账号密码而不是profile配置文件这种不太好管理的。东西用来做远程办公确实可以研究下的。让技术更好的服务正规需求是我们要考虑的。 不过这里建议大家先从提高收入左手，而不是大量研究这块，恩业务需求的满足，完成就好，完美不好。70个完成远远大于40个完美。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:38 "},"13-网络协议和管理/6-网络配置和故障拍错.html":{"url":"13-网络协议和管理/6-网络配置和故障拍错.html","title":"第6节 网络配置和故障拍错","keywords":"","body":"第6节. 网络配置和故障拍错 netstat 那啥ss可能会更好些，netstat也学一下吧，好像mac里用的也是netstat这个命令 套接字socke分2种 1、ip+port，tcp或者udp都是这种； 2、unix domain socket，这种就表现为socket文件。 s开头就是socket文件 这类的socket文件也能实现和tcp/ip 这种Ip+port的类似的通讯效果，可以实现多个进程之间的网络通讯，问题来了：是本地还是远端呢？ 1、两个程序如果处于不同的主机也即是远端通讯，可以使用tcp/udp的socket--ip地址加端口号；这种通讯存在一个封装解封装问题。 2、如果处于本地--同一台机器，就没必要封装解封装的过程了；可以考虑采用这种unix的socket，两个程序想通讯，就把数据扔给socket文件，通过它中转就行了。 netstat说明 -n 不解析地址为域名，端口为服务，这个是常用的 -t和-u就是tcp和udp咯 -l是看监听的，不是看ESTABLISH的。👈 这就是看打开了(监听了)哪些tcp/udp的端口。 1、注意上图，TCP是有状态的，所以显示LISTEN；而UDP是无状态的 所以没有显示什么LISTEN字样。 2、如果发现有些端口不是我们需要的，可以找到该端口对应的应用程序，然后卸载掉。怎么找呢，就是加个-p或者👇 如果知道端口对应的APP lsof 可见6000这个端口是x11这个图形界面软件在用。 netstat -p或者ss -p，当然要辅以其他选项 切到init3就是纯字符界面，就没有x11了， netstat -e 是extend，会显示节点编号inode netstat -ntua ，a就是监听和连接的都会显示 常用组合 这个还挺不错，我是说netstat -nr，它可以看到MSS和window，MSS肯定就是4层传输单元了MTU\\MSS一类的嘛，window就有点不能理解，TCP的滑动窗口也不至于写到路由表里吧，而且窗口还是动态的。网上搜一把没找到，转头进入man netstat 然后\" + -r\"看到👇 然后进入route的man帮助 你看他这个route只有一个8.gz的man，所以不需要man 8 route，哈哈我是不是太无聊了 瞬间找到了，还真是窗口，不过是AX.25的，所以那啥，MSS再看看搞不好也不是动态测试的，而是设置的 默认就不显示咯，MTU1500嘛MSS就是|DA|SA|TYPE/LENGHT|DATA|FSC DATA： |IP|TCP|DATA| MSS=1500-20-20=1460，一般就这么算吧，不过遇到ASA拨号的时候，L2的DATA就是|da|sa|type|pppoeHead|pppHead|ipHead|tcpHead|mss，此时的MTU=1500-6-2=1492这个是对IP层的整个报文限制了，所以MTU是L2对L3的一个限制，你在ASA上要敲的MTU就是1492，然后MSS就是1492-20-20=1452。注意哦MTU是二层对IP层(L3)的一个限制，限制的是整个IP报文长度，限制产生的效果就是IP层会自己去分片，怎么分的上一篇已经讲过了ping -l 1502的分片计算有讲。而MSS的源头也是MTU，但是MSS是TCP里的DATA的上限，这一点不可类比于IP的整体长度限制。恩其他还有PMTU discovery地理发现贫道~恩没有广告~所以在有电视那会还是基本爱调在这个台，哎~俱往矣，属温馨伐木累还看还看还看还看555...哈哈哈我能把人气成这样也是我占着理拿着需，唉得饶人处且饶人啊，别人也是关心你。 我就直接引用别人的博客了https://zhuanlan.zhihu.com/p/139537936 https://blog.csdn.net/lepton126/article/details/70810316 netsta -i 查看接口统计信息，访问量大不大 动态观察的方法 wathc -n1 就是1s刷新一次 netstat -i这个命令的结果 ping嘛就是1sping一个，但是-s 65507后同样也是1s ping一个，但是这一个要切片成N个咯，所以实际上1s发出的包就是N倍，所以此时再去看对端的watch -n1 netstat -i就会发现RX-OK增长的很快。 还可以更快，就flood的ping 这回RX-OK的增长就更快快快了 大包分片+ -f，够了哦~ 这是WIFI，ping -s 打到了32Mbps 去看看有线 哈哈，瞬间打到250Mbps，1G的带宽也就是950Mbps吧，人家要叫了，哈哈。 这种ping -f打出来没有iperf的大，iperf同样的源目1G可打满950M的。 一些个性的命令写法 敲入：watch -n1 netstat -Ieth0 ip命令 ip命令很强，用来替代ifconfig的 ip 后面加 选项，再加 操作对象；操作对象有 link | addr | 等等👆 在centos6或centos7的最小化安装都不能tab补齐，可以通过安装插件bash-completion来实现，centos7没这个问题。 yum完后exit重新登入一下，此时就可以tab两下补齐了 ip命令主要控制的是link数据链路层、addr网络层、route路由 ip link 查看MAC地址、网卡是否启用 MAC地址在网卡的rom芯片里的，真正要改需要借助设备烧入。 所谓修改就是修改配置文件了，关键词MACADDR 发现没变~ 为啥呢？ 注意上上图的提示消息 由于IP地址用着呢，所以MAC地址也不给你变，restart可能不等价down + up哦，down一下IP地址就不是already in use了，试试看，并不是！ 小改动一下MAC，试试 这会就改过来了 真正的原因可能是11打头的,HCIA里有讲，11:22:33:44:55:66正中组播地址。 尝试改一下 验证下 所以没点NA的知识，还真改不了MAC地址。 ip addr查看网络层 这里也能看MTU，支持广播、组播 ip addr 添加地址 这个不是子接口嘛？ 这种方式加的second IP，ifconfig看不到，ifconfig人家加的是子接口形式的 所以子接口+vlan就是我们网工所熟悉的路由器的子接口 然后second IP或者叫sub ip就是第二IP，就好比linux里的ip addr add 添加的地址咯。暂时这里理解先。 使用ifconfig给网卡加子接口，也叫什么别名， man ifconfig可见👇 人家叫别名，叫second address，不叫子接口，哈哈，但是我觉得加上vlan就是子接口了，ip a才是正儿八经的second address。 用ifconfig看下 然后 ip a也能看到， 结论：ifconfig看的没有ip a全，怪不得推荐Ip a呢。ifconfig 创建的网卡别名 ，ip a可看，ip a创建的second address ifconfig看不到。 注意上图1.1.1.1/24 scope global eth1 2.2.2.2/24 brd 2.2.2.255 scope golbal eth1:2这种别名。 然后ip a a也就是ip addr add也能添加ifconfig的别名 上图3.3.3.3加成了ip a 的second 接口没有加成ifconfig 的别名，修正👇 ip a有两种second address，①一种式非子接口②一种是子接口--也就是别名--这种ifconfig可查 注意广播地址ip a配置的，默认是0.0.0.0，哈哈这TM就不是个广播地址啊。 去查一下ip a里的两个second ip的说法 原来这个就是ip a为了兼容以前的ifconfig而设计的。 写到这，发现这一篇其实也可以直接就扔几个命令这就行了，哈哈，算了写完吧，好歹后面看起来舒服些。 清除所有地址 理解下scope 这里其实好理解link-local 对吧，就是Ipv4 和Ipv6 link-local，一个意思。TTL=1嘿嘿。v4和v6的区别就是v6的link-local在下一跳中起作用的，但是v4的link-local实际上没有应用场景--要说有就是网线直连169.x.x.x就通了，哈哈。 hosts是loopback口用的，主机地址，一般/32，也能对外发布路由互通，但是这里更多指的是127.0.0.0/24的回环检测地址，就是真正的本机可用了。所以上图的host更多的指的是127这种地址。 下面看看授课老师大佬的讲解，这就厉害咯 1、global是内核级的 这个就让我想到ASA 和 router的区别(在ping背向网卡地址的时候)，上图的ping在路由器上行，但是在防火墙上就不行，哈哈，可能就是用的link地址特性。 言归正传，上图这种能通，是因为IP1和IP2表面上是配置在网卡上，其实是工作在内核级别的。scope global就是这个意思。 2、link，如果IP2改为scope link，上图左边进来的ping IP2 就不通了。 3、host，网络访问过来的都是不通的，只能自己访问了就。 我们曾今做的loopback为1.1.1.1/32，一般思科的环回口对吧 其实linux一旦你改了地址，他就是global咯，所以主机地址这个称呼 主机地址这个称呼 1、在linux里指的是本地能通，网络不可能通的127.0.0.0/24 2、在华为、华三、思科里面指的是1.1.1.1/32,2.2.2.2/32这种32位主机地址，但在linux看来他们其实都是工作在内核的global地址。 实验看看 诺，scope不是你想改就能改的。所以实验中止~继续整理markdown。 用ip命令修改网卡名称 在centos6上是修改的文件，然后卸载重装了网卡驱动，上文第5节已讲 在centos7上就没有这个文件了，怎么改呢 先禁用网卡 这样就改好了。 ip route 添加ip route add 删除ip route del 加默认路由ip route 0.0.0.0/0 via a.b.c.d或者👇 两个默认，哪个优先 这里还是要注意下metric，不是我们网络工程师常规的默认cost值咯。 手动修改metric---通过man ip route 或者 ip route add help可得 想实现ECMP等价负载均衡，结果发现linux默认不让 SS命令 ss和netstat一样，但是效率高，在服务器访问量大的时候SS显示结果的速度更快，还带一些统计信息，过滤条件比netstat更加丰富。 tcp有11个有限状态机，ss可以显示指定的tcp状态，或者端口号 orhpaned孤儿连接， 还有什么孤儿进程， ss -nta \\ ss =atn 此处顺序无要求 ss -nt 表示正处于连接状态的 查询并发连接数最多的主机 然后sed的做法👇 sed的分组用法，其实就是python里的格式化字符串。 修改主机名 centos6的修改方法 /etc/sysconfig/network + hostname 改成 然后hostname 配置下，再退出下 还有一个地方也要改，就是/etc/hosts文件里👇 本地的DNS解析用的，hostname一样要写到这里。 dns优先级 默认是hosts文件优先级比dns查询 要高，可以修改 方法如👇 /etc/nsswitch.conf files指的就是/etc/hosts文件，它在dns查询之前，调一下个就dns查询优先了 此时ping xxx.com就不是hosts里写的，而是dns请求的了。 domain name 此时ping www是不会自动给你补上后缀的 需要自动补充 .xxxx.com 的 可以在这里添加domain：👇 改为默认路由送出接口的网卡配置文件后，重启网络服务 上图OK的，也可以下图这样 USERCTL是普通用户是否可以启用禁用网卡 这里的HWADDR是真实的MAC地址，真实的MAC地址啥，这里就填啥， 上文改的是MACADDR 两种方法索引 ifcfg-xxx是哪块网卡就看配置文件里的DEVICE或者HWADDR 1、DEVICE=eth1 2、HWADDR=XXXXX 这里写真实的eth1的MAC就行了，两种写一种就行了。上图DEVICE=ethX可以删的 至于反向，这个后面单独讲DNS的时候就知道了，要有反向的mapping的 路由表的静态文件 IFACE是实际的接口名 一般就用10.0.0.0/8 via 172.16.0.1 这种 这里的网卡配置的知识，还是书上整理的比较到位 前面的网卡别名也就是子接口 这些也存不住的，需要单独写一个网卡配置文件 上图提示1.1.1.1已经用在了eth1上了，好奇怪，改成1.1.1.199再次重启，提示归提示，但是已经生效了 问题-一块网卡上的多个地址获取方式不同行不？ 一块网卡的子接口是DHCP的，物理口是手动配置的，或者还有子接口之间是不同的方法，或者反过来。这样行不行？ windows里好像不行； linux里可以做到-物理口是dhcp、接口别名是手动配置，反之不行。 实验 1、修改eth1主接口位DHCP 一但改成DHCP，下面的IPADDR\\PERFIX\\GATEWAY\\DNS1和2都失效 2、发现可以做到物理接口dhcp、子接口手动的效果。 3、反过来，物理网卡使用static，子接口也就是别名使用dhcp是不行的。 👆物理网卡，别名网卡(子接口)👇 重启后发现不行，物理接口静态OK的，子接口依然是下面的手动配置的IP--它根本不认BOOTPROTO关键字，要是DHCP起作用了，就不会拿到1.1.1.199这个IP。 上面虽然拿centos6举例，但是很多都是通用，一些主机名的改法还有NetworkManager的禁用是6独有的。其他无所谓6 7的 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:38 "},"13-网络协议和管理/7-多网卡集成的企业级应用.html":{"url":"13-网络协议和管理/7-多网卡集成的企业级应用.html","title":"第7节 多网卡集成的企业级应用","keywords":"","body":"第7节. 多网卡集成的企业级应用 数据传输一条龙，从网口进来，到内存、到硬盘，中间还有CPU的运算。 所以看看哪里是瓶颈，iperf就是内存到内存的测试方法，SFTP就是到硬盘的测试手段。 多网卡绑定的模式 👆Mode1其实通过上述，就知道SW那头如果发现只有一个外部端口能看到MAC地址，就说明是Mode1了。 0 2 3 4 都需要SW那头配置portchannel，4还需要动态的协议。 Bonding配置 1、miimon，比如mode1下，就是slave监控master的周期间隔100ms； 2、新建ifcfg-bond0； 3、修改两个网卡配置文件。 实验 VM虚机都改到一个网络里 做bond之前禁用NetworkManager服务，并开机禁用。 删除多余的文件，保证bond成员口的配置干净 创建ifcfg-bond0 这个设备当前是没有的ip a看不到，待会配置完毕后，重启服务器后就有了 配置物理接口加入bond0里 编辑eth0网卡 MASTER=bond0 以及SLAVE=yes，这样，这个网卡就属于bond0了，地址就是bond0说了算，下面的地址就无效了。 再修改eth1网卡配置 然后重启网络服务就OK了 注意，bond0起来后，运来的两个物理成员口的IP就失效了，之前ssh就断开了，只能通过本地终端登入上去看看了。 检查下 eth0和eth1已经不对外了，而且MAC地址是成员口和bond口公用的。 因为做的是bond0，所以有一个主的，通过查看proc确认谁是A谁是S 可见当前是active-backup模式也就是mode1，eth0是主 将eth0断开(前面一个就是eth0)，测试下连通性 发现丢了一个包 两根线都down的样子 cat /proc/net/bonding/bond0 ，下面centos7一样的命令 主备切换，不会抢占。 改成mode3 broadcast看下 37.100就是bond0的IP了，这个DUP就说明broadcast是成员都工作的。 在这个模式下，断开一个接口，发现不通了(在虚拟机环境下)，因为要结合SW配置。 centos7的配置方法 centos7最大区别就是网卡名称不一样，7通常都是ensxxx，一般是cents7是根据网卡插槽定义的，为了稳定，不过一般网卡也不会增加删除，所以更多还是认为改成eth0 eth1 这种方式。 cents7改网卡名称 阔以的，虽然下面的和rocky-linux不同，但是rocky-linx测试一样的操作，就是生成的文件里的ifnames=0的位置不同罢了。 修改/etc/default/grub ubuntu里也是ens33， 切换成root身份： 上面改完还不够，需要利用工具覆盖一个文件 ubuntu需要grub-mkconfig -o /boot/grub/grub.cfg;reboot centos7需要grub2-mkconfig -o /boot/grub2/grub.cfg;reboot reboot即可 不推荐修改，需要借助grub-mkconfig来生成这个/boot/grub2/grub.cfg文件。 grub2-mkconfig -o /boot/grub2/grub.cfg 默认就会读取/etc/default/grub文件来生成最终文件。然后reboot生效。 总之centos7上网卡名称是自动生成的 wlxxx就是wifi网卡， 其实centos7的网卡命名的前因后果是这样的 https://developer.aliyun.com/article/609587 rocky-linux修改大同小异 https://blog.51cto.com/feko/2751292 centos7修改主机名 nmcli nmcli 及时NetworkManger cli的意思，配合NetworkManager服务用的 通过tab键可见有哪些子命令，主要用到的是connection 和 device device是数据链路层 如果网线断掉一根，就会看到如下图情况： 看接口详情 以及 使用connection查看网络层 这里的NAME和DEVICE可以不同，如下 然后systemctl restart NetworkManager可得： nmcli的使用场景和习惯，命令行嘛首先是，灵活 nmcli connection add con-name eth1-test ifname eth0 type ethernet ipv4.method manual ipv4.addresses 1.1.1.1/24 目前针对eth1有两套配置，生效了一套 通过上图过程截图可知，nmcli connection add con-name eth1-test ifname eth0 type ethernet ipv4.method manual ipv4.addresses 1.1.1.1/24 会生成ifcfg-eth1-test网卡配置文件；但是如果是人工创建一份新的网卡文件nmcli默认是不会识别生效的，需要nmcli connection reload一下 网卡的UUDI必须唯一，简单处理方法就是删掉改行就行。 此时eth1-test2就加载了，要是生效同样nmcli connection up eth1-test2 nmcli删除连接 删除在用的连接(也就是网卡配置)，会自动切换到另一个。 nmcli connection delete eth1-test2后/etc/sysconfig/network-scrips/下的ifcfg-eth1-test2网卡配置文件就没了。 手动删除网卡配置文件，nmcli不会自动切，还需要reload 所以总结来了 1、nmcli就是命令行来直接配置网卡配置文件的，nmcli删除创建的连接，就是网卡的配置文件。 2、如果不是使用nmcli创建\\删除connection(其实就是配置文件)，而是手动创建\\删除的ifcfg-xxx配置文件，就需要nmcli connection reload加载一下 nmcli强大在显示信息 nmcli修改接口位DHCP 这样配置文件里的DHCP就修改好了 通过nmcli connection show eth1-test可见 这里的auto就是dhcp 然后在启用该配置 这个看DHCPOPTION还是不错的，什么43、150都能见着了。 不过也不是就这一种方法看DHCP的信息，还有 虽然上图是手动韩国的25.44，但是之前的dhcp获取的信息还在。 有个东西比较呵呵①新的旧的命令技术你都会②老工程师新工程师在你面前就没有优势可言③这个时候人际关系就比较和谐了 nmcli 增加IP地址 这个nmcli connection modify eth1-test +ipv4.addresses 3.3.3.3/24其实就是修改了配置文件 再加一个地址 这加的second ip，ip a可以看ifconfig 看不了，这个在上一节重点讲过了。 其他补充 注意哦加载(reload)是指配置文件的重新同步下：比如人工修改创建的，同步到nmcli这里 这是不需要reload的哦。 nmcli实现bonding 实验 1、将两个网卡都接到一个SW上，WMworkstation就是接入一个网络里 清除之前的网卡配置 开始配置 type很多啊，好友adsl、wifi、vxlan nmcli connection add con-name mybond0 type bond ifname bond0 mode active-backup ipv4.method manul ipv4.address 192.168.37.100/24 但是已经发现远端可以ping通这个额bond0的IP地址了 其实原因是scope global属性导致的，只要这个设备上的接口带global属性，那么就可以背向/朝向都能ping通的。 工作在内核级别，只要流量能发到这个设备上，比如ping 192.168.37.100的ICMP到了这个设备上，那么就可以响应的。 这个ping通不代表bonding绑定成功，而是虚拟的一个网卡通了，还需要进一步绑上物理成员接口的。 上图type 后两个tab里没有bond-slave自动补齐，我的可能版本较高有的 此时就出来了，但是没有起作用，也先不捉急启用 脸厚，你没看错脸厚我们就激活一下 由于现在只加了一个eth1，所以 然后再把另一个接口加进来， 这个，如图就断了，因为ssh的这个机器的这个网卡，现在加进了bond0里了，所以自然就断了。这也是实操bond0的时候要知道的，没事，保证bond0起来就能ssh那个地址就行了。 去终端看下，已ok 同样看下 /proc/net/bonding/bond0 然后测试一下断开的效果，eth1是后面一块网卡 发现丢了1个包哦icmp_seq=309这个。 此时eth1就down，eth0就active了 删掉mybond0这个连接，两个成员的连接就下来了。 网络组 centos7比centos6多了一个网络组 新技术 network teaming和bonding一样，性能据说更好，底层技术实现不同。 创建网络组接口 创建port接口 示例 具体命令和bonding一样，效果一样，就是关键字改了，技术更优。 所以centos7建议用team而不是bond，6嘛只能bond了吧。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:39 "},"13-网络协议和管理/8-网桥实现和ubuntu网络配置.html":{"url":"13-网络协议和管理/8-网桥实现和ubuntu网络配置.html","title":"第8节 网桥实现和ubuntu网络配置","keywords":"","body":"第8节. 网桥实现和ubuntu网络配置 linux实现sw 网桥 vmnet8和vmnet0按理说是VMwareWorkstation的两个网段是不通的，但是通过VM虚机的网桥实现二层互通。 实验环境如上，下面开始配置 配置好两头的IP地址就行，请不要配置网关 我们在做二层实验 配置中间的机器为网桥 方法1：传统命令配置，采用工具集，缺点不能存盘 确认已经安装了bridge-utils工具集，最小化安装时没有的 其中有一个/usr/sbin/brctl工具 将原有的IP地址等三层信息干掉，以内要实现的是二层SW的二层口 btctl show查看当前信息为空 brctl addr br0创建桥接设备br0这个sw brctl addif br0 eth0 eth1将接口加入网桥设备 此时就其实实验就搞定了， 还可以启用STP 查看sw上的mac地址 还没有学到两头的MAC地址，自然还不通，找下故障原因，其实故障原因上图👆可以看到一个br0是DOWN的，肯定不行了啊。所以只需要ip link set br0 up就行了，哈哈大佬竟然没看出来~ 两头都ping这，然后中间的sw上tcpdum抓包结果没有，看来问题不是在中间设备，这话说的，中间设备的br0没起来，就好比a----sw-----b的sw两个网口是down的，你说是不是中间设备问题，问题就是br0没有UP，ip link set br0 up即可。重要的事说两遍~ 方法2：centos7的专门工具nmcli，可以存盘 上次方法1 的配置 然后开始使用nmcli方法进行配置 创建br0接口 nmcli connection add con-name mybr0 type bridge ifname br0 加入成员口 nmcli connection add con-name mybr0-eth0 ifname eth0 type bridge-slave master br0 nmcli connection add con-name mybr0-eth1 ifname eth1 type bridge-slave master br0 其实就是新建了配置文件 加入两张网卡并启用网桥br0 nmcli connection up mybr0-eth0 nmcli connection up mybr0-eth1 ip link br0 set up 你看上图的mybr0是黄色对吧，其实在终端里是红色，也就是说是down的，是有问题。不管是什么颜色，都要up起来的。 此时br0拿到了个地址，就是DHCP的了，这个地址就是管理IP咯，类比于二层交换机的SVI口，类比于透明墙的L3地址。 其实我在做实验的，发现一样需要ip link set br0 up 的，即使用nmcli 来做br0，一样默认也是DOWN的。 以上就可以了，均测试OK Ubuntu网络配置 网卡名的修改和centos 7一样： 网卡配置 切到root 查看IP地址 nmcli 还看不到，ip a是有接口的 目前处于down状态 尝试启用接口，并未拿到地址， 查看网卡配置文件 cat /etc/netplan/01-netcfg.yaml yaml文件和python一样，缩进必须严格统一，否则报错。 查看网卡配置文件，发现写的是ens33而不是eth0，之前改过名字了，所以配置文件里的名字也要改 改成eth0，注意缩进两个空格 重启服务，netplan apply类似于systemctl restart netowrk。 此时就能DHCP动态获取地址了 https://ubuntu.com/server/docs/network-configuration 按图配置好后，重启服务netplan apply，后cat /etc/resolv.conf里是没有DNS信息的，但是实际上是在的 使用systemd-resolve --status进行查看具体配置的DNS信息 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:39 "},"14-进程、系统性能和计划任务/14-进程、系统性能和计划任务.html":{"url":"14-进程、系统性能和计划任务/14-进程、系统性能和计划任务.html","title":"第十四章 进程、系统性能和计划任务","keywords":"","body":"第十四章 进程、系统性能和计划任务 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:39 "},"14-进程、系统性能和计划任务/1-进程管理和内存分配.html":{"url":"14-进程、系统性能和计划任务/1-进程管理和内存分配.html","title":"第1节 进程管理和内存分配","keywords":"","body":"第1节. 进程管理和内存分配 进程概念 进程，正在进行的程序，正在内存中运行的程序；若没有运行，只是安装的系统中的一个软件而已。 ls如果不运行，只是一个文件；当输入ls回车的时候，就会把这个文件读入到内存中，通过CPU读取ls程序里的一条条指令进行执行，最终命令执行完毕，结果输出，程序退出，程序在内存中占用的空间就释放，这个进程也就结束了，所以进程的启动、运行、终止，就是进程的生命周期。 此外有些程序是随着计算机开启后就自动运行的，这种进程一般叫做守护进程，表现为随着计算机启动而运行，随着计算机关闭而终止执行。也可以人为通过工具干涉自动的行为。 进程，不管是普通的进程还是守护进程，本质上都是消耗系统资源的一个单元。 此外还有线程、协程。 每个进程都会分配相应的资源： ①分配独立的内存空间 ②操作系统分配唯一的编号PID进程ID ③其他属性，比如哪个用户运行的此进程 task struct ，进程运行的时候，系统会给他分配一个task struct任务结构表，这个表里存放了进程的PID、谁运行的、占用了哪些内存空间等其他信息。 task list，任务列表，就是多个相关联的task struct互相调用吧 第一进程，centos6上市initd，centos7上是systemd了 systemd不在PATH路径中，是在👇这个路径下 centos7上的init其实没了，只是systemd的软连接 而centos6就是真实的init 第一进程开启之后，然后子进程一般都是fork()函数创建，clone()很少用；创建子进程后，一般采用CoW写时复制机制。 所谓CoW写时复制机制就是 1、父进程已经分配了一定的资源，当创建子进程的时候不会立即给这个子进程分配内存空间，也就是说👇两个父子进程都采用的相同的内存空间 。 2、当子进程涉及到数据修改的时候，就会立即复制父进程的内存空间，然后将子进程指向这块新的内存空间 这就是CoW(copy on write)，只有数据发生变化的时候才会产生复制的行为。 这样就比较节约内存空间。 在cp命令的说明中有提到CoW 进程、线程、协程 进程时独立占用资源的单位，进程A占用的资源和进程B占用的资源时不相干的。 进程相当于项目小组； 线程相当于完成项目的人，所以进程里至少得有一个人--线程。项目复杂了就需要多个人来完成工作，进程里就有多个线程了。 一个进程的任务切成多个小任务，每个任务由单个线程来完成。 线程从哪里可以看到，pstree -p可见 花括号就是线程，其实花不花括号无所谓，一个线程也是线程，如下： 这种就没有多个线程，因为进程下面就一个线程，所以没有花括号。 进程、线程的资源分配都是由操作系统来完成的。 协程和开发语言相关，有的就没有协程的概念，python里有。 协程相当于线程里独立执行的一个语句块；协程的之间的调度由程序员来控制的。 ▲进程和线程的控制由操作系统来完成的；协程的控制是由程序员来完成的。 进程运行必然要分配内存空间，而内存空间的分配是以页page为单位进行分配的。 之前学习文件系统的时候，磁盘上保存文件的最小单位也是4K，那里也有一个最小单元。4K是默认值，额可以修改，一般不改。 内存中给进程分配内存大小，也有一个最小单元，也就是page=4K。这个页最小单元不能改。 ▲磁盘给文件分配磁盘空间是4k4k的给；内存给进程分配内存空间也是4k4k的给。 进程在运行的时候以为自己拥有所有计算机的内存空间，它并不知道还有别的程序在运行。 MMU是CPU里的一个固件单元，就是CPU的一部分咯，复制计算虚拟内存和物理内的映射关系，而这个关系要存下来方便快速取用的，TLB就是这个存下来的映射表。 在一个进程看来，使用的是系统所有的内存就是那么多；而实际上只是物理内存里的一部分，这一部分可能是连续的 可能是离散。所以虚拟内存空间自然也是线性的连续的空间了。虚拟内存空间也叫线性内存空间。 1、用户空间3个G、内核空间1个G 2、用户空间就是给应用程序用的，每个进程都使用的是虚拟地址空间 3、每个进程的虚拟空间又可分为： ​ ①代码段：比如ls二进制程序代码，放在这里； ​ ②数据段：比如程序运行需要的变量，放这个空间； ​ ③BSS:Block Started by Symbol segment: 也是存放变量，不过是一些没有初始化过的变量，比如申明了int或者flat的变量，但是没有赋值。 这个变量n就放在BSS里 这个n就放在数据段里 很好记，有值了就是有数据了，就放在数据段里了。 4、堆：存放系统中需要使用到的一部分内存空间，该空间可动态调整，算是公用的空间；需要时分配，不需要是释放。比如某些进程比如排序类的就比较占用内存，就需要分配一些堆的空间，大概这个意思咯。 5、栈：函数里用到的变量，和堆head不同在于--看下面这段吧，讲的看起还不错 程序的运行场所是内存，栈和堆是进程的虚拟内存中的两部分区域。 当程序被执行时，程序代码，你所创建的变量、常量等都会被压入栈空间里，栈是程序代码的执行区域。栈的内存地址是连续的且被一一记录，所以说当你创建了一个变量(比如int var = 1)，我们就可以通过var这个变量来访问变量的内容。在这里，var就存放在栈中，它的地址已经默认被编译器计算好了，调用过程也不需要你涉及到有关地址的操作。更直观的感受是数组，数组里的元素在栈里面是连续排放的，相邻两个元素的地址相差1。 而堆是不同于栈的另一部分区域，系统会给每个程序分配一部分栈空间让他们能够运行起来，问题就是栈空间必然存在不够用的问题，而堆不属于程序，堆是独立的，是公用的。只要你new，就可以得到相应一部分的堆空间。 有栈，为什么用堆？ 栈里面的东西有生命周期，说俗点就是变量作用域，你在函数内部创建一个变量，函数调用结束这个变量就没了。而堆里面的东西独立于你的程序，new之后，除非你delete掉，否则一直存在。 有什么要注意？ 堆里面申请的东西，是随机分配的，不像栈里面的地址都已经计算好了。所以申请了堆空间之后一定要创建一个指针保存你所申请到的堆空间的地址。不然就找不到你申请的空间了。 堆空间的东西申请好，在用完之后一定要delete掉，以防止堆溢出。 ———————————————— 原文链接：https://blog.csdn.net/u012460314/article/details/52355668 1、创建：fork()函数创建一个程序，先进入了就绪态(ready)； 2、就绪态ready：就是进程需要的资源都准备好了比如内存空间、文件、变量、代码等，准备好了CPU就可以来执行这个程序了。 3、执行态：如果进程执行时间较长，多个进程之间就存在 进程调度 ，而每个进程会分配时间片，时间片用完就回到就绪态，等待下一个时间片的分配。因为时间片特别短，所以即使是单核CPU给人的感觉也是多个程序同时跑的。 ​ 时间片用完，没有执行完，进程就得停止(注意这里不是阻塞哦 不要理解错了，然后这里的停止不是普通意义上的停止就是不断地保存现场和继续执行地过程)，就需要保存现场--将执行的状态保存下来，待会下一次时间片分配了继续执行。这存在一个状态切换的过程，就会带来一点资源的损耗，比如CPU的寄存器的值的存储和清空以及再次读入。 4、阻塞态：进程执行中如果涉及磁盘IO，磁盘IO的耗时远远大于CPU的，所以这个进程就停在这--阻塞态，等磁盘IO的结果出来 再继续执行。 ​ 注意阻塞态完了后 也就是得到结果了 I/O完成了，程序是进入就绪态，才能得到时间片继续运行，所以说阻塞态的进程停在那，实际上也就是现场保存在那，等I/O的结果在继续要时间分片。所以阻塞态必然是进程停止的--这个停止不是kill也不是systemctl stop xxx，而是“不再续杯时间片”的保存现场，I/0有了结果才会去进入就绪态去要时间片去继续执行程序。 5、终止：此时程序执行完毕，所有占用的资源得以释放，比如内存、占用的文件等。 LRU介绍 Least Recently Used 近期最少使用算法，释放内存 在系统中很重要的一个内存使用算法 https://zhuanlan.zhihu.com/p/34989978 不管是大佬讲的还是博客写的，其实主要的理解思路就是：读取数据时从硬盘--写入内存---从内存读取到屏幕打印或者其他输出， 然后PPT也好，博客也罢的图片，都是站在从内存读取数据的角度出发去谈这个事情的，比如读取4 内存中有就调到最优，读取1 内存中没有就压栈的方式，垫底的淘汰掉。 上图的物理块 说的就是内存块的意思。 LRU这个近期最少使用就释放掉的短发，经常用于缓存的处理， 数据从硬盘里读入到内存，再从内存中读取到所需之处，为了提高效率 就会在内存中开辟一处空间用来存放数据，LRU算法就应用于这个内存空间，从而就得到了常用数据就放在了这块空间。而这个空间就叫缓存cache。 面试官-问：常说的经常用的数据放到缓存中，是怎么就做到的，怎么就把经常用的放进了？ 你-答：LRU及变种，回答完毕。 面试官-心里想：被他装到了。 其实这会我学到这，我想搜索LRU及其变种还有之前看到的LFU的具体解释的，但停了一下，判断了一下，不是懒的去查，而是继续往下看，后面看需要再 在这个点上继续深入就好，进一步思考，很多时候大脑需要的是停一下也就是冷静的思考能力，不是被某一个情绪左右，这其实就是真正的自由 学到这，思考一个问题，既然内存都是虚拟内存\\线性内存，也就是每个进程认为是独享的内存-是独立的，那么进程之间要互相通信，也就是内存要互相能访问才能互相通信对吧，所以就需要进一步在内存空间上做文章。 在进程内部的多个线程之间通信还是比较容易的，因为它们都是公用一块内存空间的。而进程和进程是互相独立的内存空间。 进程之间通信分两种 1、两个进程在同一台主机上 ①pipe管道： 比如没有名字的--匿名管道，如：|，题外话这个\"|\"念啥你们知道么，哈哈~ 比如有名字的管道--表现为一些文件，如p打头的 具体就是，进程A把数据传给管道，进程B从数据读取管道。这样两个进程之间就可以交互信息了，思考--这里面是否存在半双工问题啊？是的，是半双工的。 如果希望同一时刻，两头都可以发送数据--全双工，就要用到socket了。 ②socket套接字： socket套接字也分两种 a、unix套接字文件s打头的文件，这是一个全双工的管道 b、ip+port socket，这个写到下面的两个进程不在一台主机上片段里 ③signal 信号 通过一些命令可以给进程发信号，而进程收到这个信号以后，会按信号的定义去操作。 比如，sleep 100的时候 这个ctrl c 就是向这个sleep发送了信号导致其退出。 至于信号有哪些种类，有什么作用，第4节再说。 ④shm: share memory 共享内存，进程和进程之间共享内存 堆是不是就是共享内存呢？ ⑤semaphore：信号量，一种计数器 比如有10个资源，100个进程使用，当然一个资源同时只能被1个进程使用； 如果此时10个资源全都被占用，此时这个semaphore计数就为0； 如果有个资源释放出来了，此时semaphore技术就为1； 以此类推，通过这种方式，进程之间也能通过信号量这种计数器知道资源的使用情况，从而合理分配资源，从而实现了资源的使用上的进程之间的沟通。 2、两个进程不在一台主机上 更多情况下，两个进程并不在同一个主机， ①其中就有socket里的ip+port这种方式。 提到ip+port，就会想到\"面向连接\"和\"无连接\"这两个叫法，一个是tcp一个是udp，而连接就是双方的信息在彼此的内存中进行动态的持续的维护；无连接--就是仅仅初始化 但是不维护咯。比如TCP的滑动窗口，重传这些都是连接这个概念里的内容，UDP显然没有维护这个概念。 https://www.ietf.org/rfc/rfc793.txt里搜索connections关键词可到标准定义 国外的文字表达再翻译国外，往往需要咀嚼一下，比如面向对象编程，面向过程编程，面向连接的TCP，这里的面向XXX，可以体会一下 而面向连接，这连接在了、初始化了，面向它，就意味着要表达它维护它；UDP才不会面向连接，自然不会维护它。了解，这就是最底层的思维，丝滑了把，舒服了吧，无用了吧，费时间了吧，有点点意思了吧。 IP是确定主机，port是确定进程服务，这种sockt是比较底层的方式，一般开发不这么用，比较多地是使用RPC和MQ来实现不同主机进程之间的通信。当然PRC和MQ的底层还是socket(ip+port)。 ②RPC: remote procedure call远程过程调用 情形如下，A上的程序执行一段；然后把数据发给B，调用B上的程序继续执行，得到结果 回传给A；A拿这个结果继续执行。这就是远程过程调用。 ③MQ： 消息队列 左边每个进程A B C 想互相通信，可以通过右边的MQ消息队列来中专，你写我读，就可以了。 进程优先级 注意，在实际系统上优先级不是0-139，而非实时进程的优先级100-139转成了nice值-20到19同样也是比小的。 优先级分为 实时优先级和非实时优先级； 是这样表达的，进程分为两种 紧迫性高的和不高的，高的就是实时处理的，就叫实时进程，不高的就是普通进程。针对实时进程需要优先调度也就是优先级高0-99，这种优先级也叫实时优先级。 1、realtime实时进程的调度方式： ①优先级高的会抢占优先级低进程的资源，0-99比小，0最高； ②如果两个进程优先级一样就遵循FIFO或RR。 ​ FIFO：谁先来，就先处理谁；RR：轮询 每个同优先级的进程互相轮着来。 2、非实时进程也就是普通进程的调度方式 应该也是一样的队列机制肯定也有FIFO\\RR的。 优先级也是按时间片的规则来的，也是存在时间用完就要暂停的情况的。 原则优先级的比较也是存在一个排序的，而排序就要用到什么冒泡、插入等排序的方法，这样就导致花费CPU花费时间的 效率较低，所以实际会存在1-139个队列，将对应的进程划分到不同的队列里去，从而直接达到了排序效果。 当P1进程时间片消耗完后就把P1放到过期队列里，P2时间片用完也移入过期队列；当运行队列1的进程都没了之后，该队列空了 从运行编程过期，此时将过期队列2变为运行队列。 显然这里的队列机制，和我们网工学的QoS的队列类似的，但是这里大佬讲的太浅了，有机会所谓机会就是需要用到的时候，可以再搜一下相关资料。 centos中优先级情况，主要是显示和修改方式 system的优先级 和 命令查看的结果 并不一致，下图是大佬总结的 chrt命令用来修改realtime的，全称就是change realtime， 注意一下👇centos chrt写99就代表0优先级，0就对应上面的99优先级，其实就是centos 优化了一下变成越大越优╮(╯▽╰)╭ 问题来了：上图中，值是越大越优，还是越小越优。回答：越左越优，屌不屌~。 system优先级是0-139区间，但是对应到命令后往往不是这种大小，这个要注意的。 centos的命令 1、chrt命令：是修改realtime； 2、nice命令：就是修改nice值得； 3、top命令里的PR列：看法又不一样了； 4、所以有这里有个小市场--就是写个脚本统一下，哈哈。 TOP里的PR，别看错了，不是NI。 上面讲的PPT如下“： Big O是啥哦？ 进程数量不同，最终的比较的效率存在高低的，存在一个时间复杂度，和软件开发的效率有关，大佬如是说。 工作效率是 当数据量达到一定规模，很多以前正常的工作的，后面就开始出现莫名奇妙的问题了，这就是时间复杂发生了变化，计算不过来，自然各种问题就出来了。这确实挺吓人的哦，你想从故障找规律好像是不可能的了，因为没规律可言了就是处理不过来了。 抢占式多任务和协作式多任务 抢占式多任务：是按照时间片分配资源，进程的时间片消耗完，CPU理解被内核拿回来，然后继续分配，不会被某一个进程卡死了就一直占着资源。 协作式多任务：早期的dos，就是没有时间片的概念，就是一个进程执行，就等他执行完，才释放资源。这样会导致某个进程出问题，连整个操作系统都僵在那了。所以以前老版本的windows动不动就蓝屏死机。 前台进程和守护进程 前台进程依赖于tty线程，所以有2中处理方法 注意console就是本地终端登入的不存在这个情况哦，你怎么关，无非是关闭控制台嘛，关了再进去ping还在的 1、nohup xxxxx & 记得exit安全退出终端，否则不生效 2、screen稳当就是显然没有nohup一条命令帅气 守护进程不是用户登入上来运行的，所以不存在tty关闭就终止的情况。 进程状态除了上面已经讲过的，还有 睡眠态： 可中断--睡觉，叫一下就醒来了； 不可终端--冬眠，回暖才会醒过来，就等着你的IO结果，放到内存里了拿到结果了，它才会继续工作否则只能在这等着。 停止态：冻僵了？消耗资源不，既然暂停于内存 还是消耗点点资源的吧？不过不会消耗CPU，因为不会有时间片分配给它。 僵尸态：异常状态，正常就会释放资源，但是僵死，还是占用内存资源的。因为已经死了，kill也杀不掉了，也激活不了，只能重启计算机处理这种异常状态。 举例mingetty就是 登入界面 的提供程序 上图一直不登入，这个程序虽然还在运行，CPU是否消耗-否，内存是否消耗-是。 ready是什么都准备好了，就等CPU的时间片了。上图是还差输入ID的，显然还没有准备好所有条件，还不是ready就绪态，还不给它分配时间片也就是CPU的使用权。 所以大部分的进程都是睡眠态。可以唤醒，键盘输入回车后，就发送一个 让他进入就绪态。这里应该有个点，就是睡眠态如何进入ready态。一旦进入就绪就等待CPU的时间片进入运行态。 进程分类 1、CPU密集型：CPU消耗大，内存硬盘消耗小，比如数据计算、编译安装等 2、IO密集型：磁盘靠大文件，CPU就发个指令就完了，但是大量数据需要从磁盘考入内存，然后内存到网卡发送出去，这就涉及IO。 早期的时候CPU是参与IO的，说明如下： 1、CPU发送cp f1 f2复制指令后 2、这个过程中，所有的数据全部要经过CPU，当然这是在说以前 3、现在内部架构发生了大变化 CPU只需发送一个指令给DMA，然后DMA就去完成磁盘和内存的数据交互。 这里后面重新画图吧。 DMA有了，CPU就不忙了 到底是CPU忙、IO忙还是网络忙，通过命令去排查。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:39 "},"14-进程、系统性能和计划任务/2-进程管理工具.html":{"url":"14-进程、系统性能和计划任务/2-进程管理工具.html","title":"第2节 进程管理工具","keywords":"","body":"第2节. 进程管理工具 进程启动后，相关信息会自动放到/proc里 上图的1，就是systemd这个系统启动后的第一个进程。进去后一堆信息(比如内存、挂载等信息)通常不直接查看分析，而是通过命令去看。 ps 默认只显示当前这个tty线程的进程 再开第二个终端跑一个sleep 再开第三个终端跑一个ping 但是在第一个tty里，ps只能看到自己的进程 ps 特殊在，它是一个老牌Unix命令，支持3种风格的option选项 man 帮助种也是如此写道 - h unix风格 -- help GUN风格 h BSD风格 tar xvf 和 tar -xvf 都行 ps a查看各个终端窗口下的进程 a看不到守护进程，和终端没有关系的进程是看不到的。 ps x 查看并非所有进程，包含守护进程这种和终端无关的，也包含终端下运行的进程 ？就是和终端无关的；终端下的也会给你显示出来的。 ps ax会多一些 ps u显示进程所有者的信息 顺便看下列标题 ping 一直在运行，但是不消耗CPU，time这一列0:00就是说占用CPU的时间片少。 ps aux 看所有进程的信息，包含守护、终端进程、带进程所有者 ▲插播-1 iptables如何实现代理 这种方法有时候不生效，所以后来我还是用的softether去搭建的 这种方法不用动路由表，其实就好比windows的代理，就好比clash的“非tun虚拟网卡模式”，现在看下来这种方案更为灵活： iptables -t nat -I SHADOWSOCKS 1 # 添加用 iptables -t nat -D SHADOWSOCKS 1 # 删除用 iptables -t nat -D SS -p udp -d 0.0.0.0/0 -j REDIRECT --to-ports 1080 # 删除具体 iptables -t nat -A SHADOWSOCKS -d 167.xxx.xx.xxx -j RETURN # vpn的建立隧道地质得走本地网络出去 iptables -t nat -A SHADOWSOCKS -d 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 -j RETURN # 私网 iptables -t nat -A SHADOWSOCKS -d 127.0.0.0/8,169.254.0.0/16 -j RETURN # 本地回环和linklocal iptables -t nat -A SHADOWSOCKS -d a.b.c.d/n -j RETURN # 公司出口IP iptables -t nat -A SHADOWSOCKS -d a.b.c.d/n -j RETURN # IDC公网IP iptables -t nat -A SHADOWSOCKS -d a.b.c.d/n -j RETURN # 云上公网IP iptables -t nat -A SHADOWSOCKS -d a.b.c.d/n -j RETURN # 本地APP访问或者调用的外部IP，这些都是走本地网络，不走代理的。 iptables -t nat -A SHADOWSOCKS -d a.b.c.d/n -j RETURN # 你要是闲的蛋疼，还可以加上所有国内IP地址段，你不怕CPU不忙对吧。 iptables -t nat -A SHADOWSOCKS -p tcp -d 0.0.0.0/0 -j REDIRECT --to-ports 1080 # 剩下统统走代理，注意UDP其实也需要代理，不过要排除UDP 53，同样TCP53也要排除的，这样一些新的QUIC协议就能应用了。 还差一个排除注意点，单个APP的她可能也会本地调用外界的公网IP(这个是不走代理的)，所以也需要排除。 ▲插播-2：业内趋势QUIC体验 H3介绍：https://blog.51cto.com/u_14888059/3790697 https://network.51cto.com/article/625999.html https://new.qq.com/omn/20210504/20210504A01Z1T00.html 谷歌开启H3：https://zhuanlan.zhihu.com/p/108198664 插件下载：https://chrome.google.com/webstore/detail/http2-and-spdy-indicator/mpbpobfflnpcgagjijhmgnchggcjblin 插件显示对比：绿色是H3，蓝色是部分H3没有H1，灰色是有H1。 企业微信有用QUIC也就是H3，CDN里也支持，谷歌youtube都支持--速度快很多 插播结束，继续linux的ps命令 ps aux看的较全，注意用户通常是root，也有非root，就是以系统账号运行的 切换用户看下 再来看下普通用户运行passwd的情况 发现竟然是root运行的，其实这就是SUID的原因，前文已讲 所以ps aux看到的用户严格来讲是有效用户，是wang运行passwd后实际生效的运行的该命令的那个用户。 再看下ps aux的CPU占用百分比，基本都是0，搞一个不是0的。 单核是100%，两核就是200% 内存情况： RSS是常驻内存，进程申请内存的时候，说是这么多，但并不是马上使用，所以RSS就是目前使用到的内存空间，而VSZ就是申请操作系统承诺给到的内存空间。 VSZ和RSS的单位都是以KB为单位的 tty ？就是和tty无关，不是终端上来的 STAT 状态就是进程的状态，运行态、就绪态等，其中 运行态表现为R，基本上看不到 因为你敲这个ps aux的时候肯定是运行态的嘛，同理 敲的就是这个命令，肯定会给你一个grep 的进程的，啊，ps aux里是没有sssss的，但是你既然grep sssss了就是要运行grep程序的，所以当下就有grep的进程了。 sleep 100 跑起来后也不是R运行态 pts/0就不是当前的pst/0窗口 当前是pts/1终端， 好奇这个“TTY是?号非当前终端”的sleep 60哪来的啊？ 这个sleep不是bash命令，估计是系统默认的，之前没关注过，我这个是centos8的，我去其他centos7上去瞧瞧还真有，rocklinux没有 最小化安装的centos7没有sleep这个东西 然后回到这个STAT 的R这些状态 大部分处于S休眠状态且是可中断的， 还有不可中断的休眠 T：stopped是停止态 Z：是僵死态 +：表示前台执行 l:多线程 会话表现形式有qq的一个窗口，浏览器的一个页面，而这些都是涉及到进程的，具体解释如下： https://blog.51cto.com/u_15015138/2555390 这个命令很棒，不过要区分一些ps f 和 ps -f ps auxf 可见看见树形结构的，同级的进程，父子进程等信息👇 ▲插播-3 hostname你真的不会改，具体如下： https://blog.csdn.net/zhaogang1993/article/details/82769439 ps继续，START是什么时候开始的，TIME就是总的CPU时间(是按时间片分配的) 排序功能以及部分显示功能 ps o过滤 加上ax选项进一步显示所有终端的信息 按CPU排序，先搞一个消耗CPU的进程 ps aux k %cpu 可见排序是升序的--根据CPU的占比 降序怎么排，tac咯 cat和tac以及rev对不对~ 上图是关闭图形界面的程序gnome，降低一些内存的消耗。 降序还可以这样 上图的--sort可以换成k的 再到centos6上看下具体的命令 -e等价于ax，在行数上，但是列上👇少一1列： 配合-F显示多一些 PID是进程id，PPID是父进程ID。 C列，表示CPU的百分比，不过是取整的。 STIME是开始时间 TIME是CPU的分配到时间片换算的累计使用CPU的时间 这个占用CPU的时间就比较多了。 老实讲有效和真正，并不能很好的区分两个选项的意思 -u就是程序最终谁来运行的，最终 执行 的 用户 -U就是程序开始时谁来发起运行的，开始 发起 的 用户 ps aux就看所有咯，然后单看wang用户的就用ps -u wang u， -u wang是最终以wang用户来运行的程序 -U wang就是wang敲的命令，通常是SUID这种passwd带SUID所以wang敲命令，但是是以root运行的程序。 所以常见组合有 ps aux ps -ef 其他就看上面的具体需求用哪个了 这是查命令的，敲的命令，如果时脚本呢？ 直接ps -C f1.sh就行，还挺不错的 我的测试 再开一个终端，发现 并没有，赋予x执行权限再看，其实很简单，你是bash f1.sh跑的，自然要看bash进程，而不是f1.sh，你给了执行权限，直接f1.sh跑的就能看到了👇 注意. f1.sh这样也是看不到的 必须是f1.sh作为命令一样敲入的，而不是通过source bash 或者.来运行。具体再看看下面 bash xxx是通过bash执行的f1.sh，然后f1.sh里面又执行了ping ./f1.sh是通过文件本身什么的shell申明的类型直接执行的，ps -C f1.sh所以查得到 将bash改成sh测试把 看到没，./f1.sh就是直接执行f1.sh文件的，只不过是依据文件里定义好的shell类型去执行的，所以ps -C f1.sh就认 而换成source和. 的话又不一样了 也好理解，这两个家伙是直接在当前bash下以当前bash执行f1.sh的，不会再开启子shell进程。所以这两个家伙source和.你用ps -C bash是看不到的，因为这两种方式运行的程序他直接在当前bash跑的，所以层级比上面少一层，ps -C bash不会增加。就是这么个道理，老哥我研究得到位了把。可惜咱环境不care这些东西，呵呵。也不对，基本功也确实要有的。 再来，如果f1.sh里面就是光秃秃的一行ping 127.0.0.1 对比定义文件的shell后，就知道了，👆上图是文件里没有定义shell于是自动给你用当前的bash，下面是文件里定义了shell的👇，所以其实是跟着文件的shell走的， ps -C bash和ps -C f1.sh分别查看上下两种情况，上图会多一个bash，下图是bash不多，多一个f1.sh，因为下图的shell是集成在f1.sh里的，是通过f1.sh开启的shell。 然后ps -C f1.sh上图👆肯定看不到，因为开了一个子bash；下图👇可以看到是因为是直接运行的f1.sh文件自然看得到，虽然文件里申明了shell的。 所以说一万到一千， 1、文件无执行权限 bash就是开启bash子进程 source或.就是直接当前bash跑的 2、文件有执行权限：①申明了shell；②未申明shell ./xxx.sh执行有申明，就是直接跑的是文件当命令执行的，利用里面的shell,跑的是文件本身； ./xxx.sh执行无申明，就是开启当前shell类型的子进程通常就是bash来执行文件的，跑的是bash； 查看nice优先级 -是用的系统优先级 -20这种就是nice，只是对应system priority的后面一部分[上一节里有讲] 这个不管是centos8还是centos7都是这个样子的，nice是-20~19没毛病 但是pri这个它实际上是翻转过来的system优先级，上图-20对应的就是39。 看到这我TM已经不知道优先级比小还是比大了，NND，搞这么乱的，不能统一下的吗！ 通过renice调整ni值，既然是nice就只能是-20是到19之间了 -n是指定新的优先级 所以▲总结一波，ni是-20最优--比小，pri是139最优--比大。 但是官方自己都疏忽了 altime就是实时进程的优先级--实时优先级。 大部分进程都是nice优先级，实时优先级的少。 直接以某个优先级运行程序，有个无聊的点，下图ping的是127.2，系统会自动识别为172.0.0.2 ps axo pid,ni,pri,cmd 可见ping以优先级10开始运行的，如果是负10，就是nice --10 ping 127.0.0.2 上面的-10和--10都不太好，正规写法是 查看进程跑在哪颗CPU上 通过ps axo pid,cmd,psr查看 上图可见敲的dd命令当前是绑定在cpu 3上的，也就是第4颗CPU。 但并不是固定在CPU3上跑的。 再来看一个ping 多用ps axo pid,cmd,psr看几次，就可以看到不是固定在CPU4上跑的 如上图，ping 172.0.0.1，进程的CPU切换了，就会导致 缓存失效 CPU里 也 有缓存，有L1、L2、L3 CPU以两颗举例，各有各的L1、L2，但是L3是共用的 理论上L1缓存最快，L2次之，L3最慢 它可以把内存中的数据放入缓存中，下次取就直接从缓存中取了，速度就快了很多。 问题来了，如果一个程序跑在CPU1上，那么L1和L2也肯定用起来了，如果该程序跑到CPU2上了，此时之前的L1和L2缓存就无法利用了，所以CPU一切换，就导致效率大大下降， 解决方法，将进程就绑在某个CPU上。 同时也会带来CPU的利用率可能不均衡，就是你绑的那个CPU负载就比较高，这个可以将nginx的多个进程分别绑到不同的CPU上，然后这个机器就跑nginx。这样就比较好了 nginx的配置文件中，是可以把nginx的进程和CPU做绑定的。 怎么绑，taskset可以 这是一个外部命令 用这台机器，就两个CPU，来做实验 dd下 通过ps axo cmd,pid,psr可见当前是跑在CPU0上的 由于现在没有进程和它竞争，所以CPU不会飘，再来一个ping -f 去抢CPU 然后可见dd命令的cpu飘走了 然后视频中老师的xshell崩了，就用终端去演示了 现在是两个dd，然后不断地通过ps axo pid,cmd,psr去看CPU切换， 发现两个没切，于是再加一个dd if=/dev/zero of=/dev/null 然后继续用ps axo pid,cmd,psr去观察 结果半天没看到CPU切换的情况，不过上面有两次已经切换了，就是没有出现频繁换的现象。 通过taskset -p xxx可见当前进程ID可以跑在哪个CPU上，注意下图，跑在CPU0上，但是看到的事mask 3 mask 3就是11，就是说当前是2个CPU，11就是打开开关，两个CPU上都可以跑。 如果是下图，就是当前是4个CPU，这个1332进程可以跑在4个CPU上。 下面开始绑定 ping 127.0.0.1这个进程，绑到CPU1上去 绑定的命令为taskset -cp 1 xxx，注意这里的1就是1号cpu，如果你想绑到0号cpu，就写0，如果你想绑两个，就写0,1，这就没意义了，缓存又不能固定了，所以绑就是绑一个cpu号的。 此时进程29654就变成1了，并固定在1了 taskset -cp 0,1 xxx就是0号cpu和1号cpu都可以用，最终taskset -p 查看就表现为11也就是3了。 再看个CPU多的情况 0-7号CPU ff就是对于7969这个进程来说，8个cpu全部可以用。 现在希望该进程就跑在0号CPU和4号CPU上 0,4对应的affinity mask就是0001 0001 上图其实就是一张图拆开来讲，原图如下 绑到0号和4号CPU了，原来的3就跑到0或者4了👇 现在有个问题来了，上面的taskset -cp x xxx都是绑的进程ID，进程ID这个是会变的，所以还需要优化 用pidof去获取进程的ID，前提是这个命令dd就对应一个进程编号。 如果是bash，就会看到好几个进程ID： 所以进程绑CPU的命令为taskset -cp NO. `pid xxx` 这是taskset优化的手段，当然有些软件比如nginx本身就可以绑CPU，无需手动执行taskset命令。 以上就讲了ps的一些常见组合 示例 pgrep=grep for process 感觉pgrep就可以了，上图的那个ps -C httpd,sshd -o pid=没啥用。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:40 "},"14-进程、系统性能和计划任务/3-进程管理和性能相关工具.html":{"url":"14-进程、系统性能和计划任务/3-进程管理和性能相关工具.html","title":"第3节 进程管理和性能相关工具","keywords":"","body":"第3节. 进程管理和性能相关工具 查看某个终端tty下的进程 区分u和U，这个ps命令，上一篇里也有。 pgrep还支持正则 加上-l或者-a显示进程 pidof 虽然是软链接，但是功能不一样，也就是说，可以创建多个软链接指向同一个文件，但是每个软链接的功能不一样。 uptime dd if=/dev/zero of=/dev/null来消耗CPU然后uptime观察看下 load average这个数值也是进程数量的意思。 一个内核，一搬load average不大于3，所以24是可接受范围以内。 超过24就认为每个内核有3个以上的进程在排队，系统就非常慢了。 uptime一个是命令，一个是proc/uptime文件 单位都是秒，一个系统启动时长，一个是空闲进程总时长(按总CPU数计算的)。 平均下来每个CPU的空闲时间就是/8，7717秒空闲，8604s开机时间，所以大部分时候是闲着的。 w 命令和uptime也是重叠的 top的一行也是uptime 所有进程个数是223个， 3个运行的，220睡眠的，0个停止，0个僵尸态。 cpus的分配情况，us用户空间占用了2.1%，sy内核空间占用了10% 其实us用户空间占用高才是好的，因为这代表着应用程序占用内核率高，而应用程序代表着生产力。 各种软件都是工作在用户空间的 从这个CPU的两个值就可以看出来，你的系统忙不忙，合不合理。 ni是进程的优先级调整 id是空闲，100就是cpu100%空闲，没事干。 wa是wait等待时间，有些进程需要等待资源的访问，准备好了才能运行。 hi是硬终端 si是软中断 st是被盗取的时间片 中断，就是打断了CPU的正常工作。中断什么时候发生呢？ 看下用户空间-内核-硬件的模型图， 内核kernel和硬件的沟通就是通过 中断，比如键盘按一个键，内核就会捕获这个操作，用中断方式响应用户的请求。 当你每次在键盘上按下一个按键后，CPU 会被中断以使得 PC 读取用户键盘的输入。这就是硬中断，还有软中断，比如程序的异常 st：steal，被盗取 早期的计算机操作系统，不支持不理解虚拟机里运行的进程， windows任务管理器打开，看到当前进程，这些进程只是windwos里的进程，然后VMwareWorkStation里跑的进程占用的时间片，就被称为ST-被盗取的时间。 后面会讲KVM，类似于windows的VMwareWorkStation。 内存，总的、空闲、用了多少、被buff/cache占用了多少 然后top下面的主体部分 3s默认的刷新间隔，可改按s建后改。 默认是按CPU利用率高到低，显示的，测试，跑个dd ，top里就看到第一个了。 PID、用户名、优先级PR--这个PR是TOP的PR，它在linux优先级里的情况如下 NI就是nice优先级 VIRT是系统承若的内存、RES是实际使用的物理内存、SHR是共享内存 TIME+是总的CPU分配时长 OOM：out of memory内存泄漏 一般某个进程的内存不断在涨，就认为可能发生了OOM了。 -H显示这个进程下面打开的线程 使用pstree -p 找个多线程的进程PID 花括号就是多线程 这就打开了某个进程里的线程。 top -H -p `pidof xxx` # 这个pid会不会pidof xxx取出来多个啊？就会出错 free看内存 centos6和7不一样 6的buffer和cache是分开的，7是合二为一的 buffer是写，改一个文件后，要写数据， 要先放入buffer缓冲区的，然后buffer里按一定的队列次序写入磁盘。可能就是改一个字符不会给你存盘 就是放在buffer里，等你改了很多字符后才会统一从buffer里给你写入磁盘。这样能提供效率。 cache是读，数据的读取，放入缓存里，下次读取直接从缓存里读取就行了。 默认是KB单位 cp一样会增大cached 看下内存使用情况的计算 used - buffers - cached = 真正使用的内存空间 free + buffers + cached = 真正可用的内存空间 但其实你用echo 3 > /proc/sys/vm/drop_caches释放也不可能将buffers和cached全部释放掉的，所以也没有上面说的那么富裕。 下面是centos7的内存计算方式 total = used + free + buff/cache available 是系统自动给你算的，它不是简单的free + 部分buff/cache，你看上图的available就小于free，这看起来就不科学，因为空闲的竟然不是全部可用的。之所以出现free 注意-g的使用 不会四舍五入 1s刷一次，有助于动态观察 图形界面是很占用内存的 上面的gnome-shell,gnome-software,X,gnome-terminal都是属于图形界面的应用。 通过init 3关闭图形界面后，free大大地增加 所以工作中一般不开图形 vmstat 解释上图 procs列：r b,1 0,这些地意思，r是运行或者可运行的进程数，b是可中断睡眠态的进程数存在阻塞了，这是被阻塞的队列的长度。1 0，1个，0个，这些是动态变化的，不是固定的。 -----memory---- swpd：被交换的内存空间 free：空闲的内存空间 buff和cache：上图buff空间小，cache空间多，说明数据上目前没有什么写操作。 ----swap----- si：进，数据进swap，就是说把内存中暂时不用的数据放到swap里，对于swap来讲是进，对于内存来讲是出。 ​ 可惜我们通常字面意思的理解就错了，这里的swap和后面的---io---都是以内存为参照物的in和out so：出 测试下si so值，构建一个大内存的使用情况，超出内存，然后才会使用swap 此时内存不够用了，就会将内存中不用的数据往swap里写，so就会增长 如图，内存不够用，一开始就是so暴涨，到后面就有进有出了就。 -----io----- io理论上也是磁盘的io，其对应的bi和bo理论上都是说的磁盘的in和out，但这里就不是，这就是统一指的内存的in和out。 如图从硬盘上读数据，表现在vmstat的bi暴涨：因为读数据时先读入内存 如果从内存中读数据，/dev/zero是个内存数据，写到硬盘上，此时---io---里就是bo暴涨 这个命令一会就能把硬盘打满。测试的时候要小心。 ----system------ 进程切换过多会影响效率的 ----cpu---- 这里的us sy id wa st和top里的一个意思 iostat iostat 1s刷新一次 开始读磁盘 读操作瞬间暴涨 对于系统来讲CPU、内存、硬盘、网卡，这是比较关注的4个，和性能密切相关 iftop 需要epel源安装 iftop 可以指定监听的网卡的 iftop -n -i eth1 -n就是不做域名解析 q退出 pmap，显示进程占用的内存空间 每个进程使用的资源都是在/proc下看的很清楚的 比方说，这里开启一个dd命令 这个dd命令的pid呢看下是多少 那么在/proc/11425下去看看 其中就有内存的使用情况 也就是maps文件，打开看看 显示的内容不是特别容易看懂，所以不太使用这种直接看maps文件的方式，而是使用pmap命令去看 这样就可以看到 dd就是程序本身的内存占用 stack就是栈，和堆很相似，都是每个进程占用的内存空间，这个内容讲解在本章 第1节中有讲，了解下就够了。 栈：先进后出， 一般函数，变量赋值，都是用栈 而堆heap，一般都是放大的数据的，面向对象开发的，一些创建的对象都是放在堆里的，堆是在内存中分散的数据块， 还有一些anon也就是anonymous匿名的内存空间，就是没有名字的。 每个应用程序会调用二级制的库，这个库也要占用内存空间，不过这个库是共享库，也会被别的程序调用的，所以这部分内存空间应该是共享的， 将来就可以用这个pmap命令来了解某个应用程序占用的具体的内存空间，比如某个JAVA程序运行的时候内存比较大，还存在不断增长的情况，你就看看，发现 唉~里面的某个模块在不断的消耗内存，这就是OOM的可能了，你就告诉他你的程序某个模块存在OOM内存泄漏的情况。 pmap工作中经常用到据说，回头我就问问应用运维 系统调用 strace可以跟踪 某个进程运行的时候 调用的 \"系统调用\", 就是看看进程占用了哪部分 系统调用 比如说cat这个命令，运行的时候（命令也是程序啊，敲回车就开始运行了） 通过程序运行中调用的 系统调用，就可以发现一些异常，这其实很底层了，这需要经验积累，需要对开发了解。俺没有哦，我就是写下来了解个方向。 上面可能还要cat一些具体的文件 可以看到open的这个系统调用，而且还是RDONLY猜也知道就是readonly了。 这个其实就是 with os.popen('cat ', 'r') as p: z = p.read() strace是看的系统调用， 还有一个ltrace， 看函数库的调用，不是strace看的系统调用 函数库一般就是C语言自己的库， 还有一个ptrace 不太清楚了，哈哈视频中老师就提了这个名字而已， https://bbs.pediy.com/thread-265812.htm https://www.cnblogs.com/tangr206/articles/3094358.html 看不懂，不过知道了strace也是基于ptrace来实现的。 glance可以实现跨网络的监控 然后去到远程的机器上，同样要安装glances 然后client端输入 glances -c a.b.c.d 就可以看到server端的性能 信息丰富、支持跨服务器查看 配合iptables安全策略，可以指定固定来源查看本机信息 dstat 可以替代vmstat,iostat usr用户空间 sys内核空间 idl空闲 wai等待 hiq 硬中断 siq软中断 读写 网卡 swap的分页 int csw 进程的内容切换 这里的int和csw应该就是vmstat的in和cs iotop iostat显示是某个硬盘块设备的I/O使用情况，但是不能精确到进程；此时就可以使用iotop 和TOP很相似，显示的是某个进程的磁盘读写情况 将来发现磁盘很繁忙，进一步想知道哪个进程导致的， 结果肯定是看不到的/dev/zero 是内存里的，/dev/null也是在内存里，所以读写都是在内存里，iftop自然看不到，换一个命令 nload查看网络实施吞吐量 输入nload后回车可见： 当前上图所示没有流量，开始制造流量 上图可见流量开始有了，但是图形显示不是太易读，因为curr 71.63MB/S是这么个图，curr是122MB/S也是这么个图 流量不直观啊有点 lsof 查看某个文件夹是否被挂载或使用 查看某个文件被哪些进程打开 查看某个进程打开了哪些文件 工作中存在 不小心删除 正在使用的文件 制造一个打开的文件效果 注意，这里我们之前讲过使用 > /data/m.txt这种重定向的方式来删除，这个就无法恢复了，因为这个瞬间就将空间释放掉了。 发现正在使用的文件也可以删，当然正在使用的删除就属于误删除了，现在要修复 lsof直接回车，就会显示系统中所有的正在被打开的文件 注意PID 11863，同时此时进程没有停哦 去内存中proc里看 可见4这个文件描述符是删除状态，没关系，可以直接看 照样能看，因为是加载到内存里的 恢复一下就行了 这就找回来了使用中被误删除的文件 尤其日志，经常用这种方法处理。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:40 "},"14-进程、系统性能和计划任务/4-进程信号和前后台管理.html":{"url":"14-进程、系统性能和计划任务/4-进程信号和前后台管理.html","title":"第4节 进程信号和前后台管理","keywords":"","body":"第4节. 进程信号和前后台管理 在本章 第1节中提到了信号 可以实现 进程之间的通信 信号有很多种，trap -l或kill -l可见信号的种类 每个信号都带有特定的含义，这种废话以后不要说， 如何向一个进程发送指定的信号呢，可以使用kill命令发送信号 1、信号的名称如上图，你可以写全名，也可以省略掉SIG这种所有信号都有的前缀，比如SIGINT可以写成INT还可以用前面的数字编号 1 SIGUP 无需重启服务，重新读取配置文件 这个一些服务里有命令的，估计十有八九就是用的这个UP信号实现的。 有个什么reload的命令来着，记不得了 准备将首页信息放到这个地方 这就需要修改配置文件： centos6这样就行了，7可能还要加一段代码 配置文件改完，一般就是重启服务，但是现在可以这样 查看httpd的进程ps aux： 其中root那个是父进程，可以通过ps auxf 就能识别 所以PID就是2778 kill -SIGUP或kill -UP或者kill -1 都行 2 SIGINT 相当于ctrl c 一些需要ctrl c的情况 然后那边的ping就终止了 3 SIGQUIT 相当于ctrl \\ 相当于quit退出 所以用信号测试一下，下面bc挂起来 然后就quit退出了： 上面就看到INT和QUIT可以关闭进程，但有些进程不一定能关闭 vim 一个文件 进去后 踢不掉的， 再发3信号，发现可以的 kill -3 后vim里的内容就 没了？不一定，你在他kill 之前w!保存了就可以。 如果是vim下按i进入了编辑状态且有修改内容，那么kill了就会产生.xx.swap文件 15 SIGTERM 终止进程，kill的默认信号 kill `pidof xxx` 等价于 kill -15 `pidof xxx` 同样15信号也可以杀vim进程，也不是所有进程都能被其关闭的。 比如bash的进程，kill -15就杀不掉 不会报错，但是实际上杀不掉的，还在： 可以换一个强力kill就是-9 9 SIGKILL 强制杀死正在运行的进程 进程就是正在运行的程序，正在运行的进程本身就是废话 这个2827是bash，所以xshell的登入的一个窗口 直接就没了。 这个是窗口多开后进行的操作，杀的是别的窗口，自己的窗口查看方式： ps aux 可见 直接就把自己的bash就干掉了，不过上图是有个1s的重连才会自动连接上的。 -9是否能杀掉所有进程 比如说systemd是否可以被-9强杀 kill 1肯定不行，等价于kill -15 1, 这是15的默认值，kill -9 1也一样杀不掉 虽然杀不了，但是存在问题的。 pgrep -l mingetty 杀了，又再生了， 这种进程就叫再生进程--respawn ，杀不死没事，看下父进程 pstree -p看下mingetty的父进程是init 这种重生进程，其实可以杀，通过kill 1 一下父进程--父进程虽然不会被杀(kill -9 1也杀不掉)，但是你继续看 此时就杀掉了，所以init和systemd不是说杀不掉就可以杀的，还是会有影响的。 mingetty其实就是登入的终端，ctrl_alt_f3对应的就是tty3已经被杀掉了，所以下面的界面就卡住了，输入回车都没有反应了。 换一个ctrl_al_f4可以的就是tty4 有后台的进程才有再生功能，mingetty能再生，是因为有init做后台。 批量杀进程 killall httpd # 使用进程名称来杀 所以到这里就学习了 1、按PID杀，kill 2、按进程名称杀，killall 下面学习3、按模式也就是正则杀，pkill。 pkill的模式和选项和pgrep是通用的，它俩的帮助都是在一块的。 然后就杀掉了 t就是看控制终端的tty的 杀掉pts/1行运行的所有进程 其实图中要用-9,pkill -9 -t pst/1就可以删掉所有能杀掉的了。否则bash杀不掉。 然后去到运行ping的窗口上看到，就看到被杀掉了。 这两个只有pgrep有，pkill没有。 进程的前后台 这个就是占用了终端资源的前台命令 放后台的方法 这就已经放后台运行了的，但是输出还是在前台输出的。 前台执行是占用终端资源的，后台不占。 此时跑是在后台跑了，但是输出还是在前台，所以ctrl c结束不掉了就，ctrl c只能结束在前台跑的进程 怎么关呢，①再开一个窗口，kill掉ping就行了，②将后台运行的进程再次调到前台来就好了。 手速要快 fg 命令就是front groud 既然在前台了，ctrl c就可以了结束了 fg可以把后台的进程也可以是没在执行中的后台程序调到前台来。 再次研究这个现象 ping 127.1挂着，当前ping的状态是可中断的休眠 然后按ctrl z，此时就放入后台，但是不在是运行状态了 T就是后台，处于停滞状态，冷冻？ 如何恢复:1、恢复到后台运行bg；2、恢复到前台运行fg 开始操作，当前通过jobs可见是后台stopped 使用它bg命令back groud，此时就恢复到前台运行了 所以此时ctrl c不起作用，ls看看的 下面是通过fg直接恢复到前台的。 再来一个问题 已经是后台如何让他stopped 如何将已经后台的进程--正在跑着的，变成继续后台但是是休眠态 由于此时已经是后台了，ctrl z发不到后台了。 发19 SIGSTOP，后台休眠信号。 killall -19 ping 确实停止了，达到了ctrl z的效果。 还可以让他继续运行，除了bg或者fg，还可以发送18 SIGCONT信号，由于现在是后台stopped，所以18发过去就是后台运行 但是没有办法说 发个信号让他从 后台stopped变成前台运行哦。 kill的0信号 0信号是不属于信号列表的，通过kill -l可见没有0信号的 killall和kill是共用信号数字的 这说明ping当前是工作的 如果ping没有运行，就是这个结果 案例：如果http服务没有启动，就重新启动 这个可以放到crontab里1分钟跑一次，确保一些进程莫名奇妙挂了，这个情况也是存在的。 假如某个APP挂了，等个1分钟也就自动好了，就是这里的crontab，不过前提是systemctl start httpd要能起的来哦，如果配置文件有问题自然就起不来了。 关于screen和nohup ping & 两种后台执行，直接搜全文，都有的。 这个默认行为不好，因为文件会越来越大 需要丢到/dev/null里。 窗口一关，bash就没了，ping也就没了，其实不是 父进程bash8292没了，但是ping这个8401子进程还在，父进程没了，重新找了个父进程systemd。 其实视频里漏掉了exit安全退出，否则有时候不会后台运行的。当然这里的实验没有问题，但是我以前必须①nohup cmd > /dev/null $ ②exit才行的。 作业操作 kill %xx # 注意是是作业编号，不是进程编号 并发 这么些是按顺序，而且由于linux默认就是永久ping的，所以结果只会有127.1 注意ping -c 3这钟不好，我们一般都是ping -w 1，其实就是我啦，因为-c 1 万一不通，就会等好久才会给你一个结果说不可达，-w 1就是1s没有结果就认为不可达了，这个就够了，就是合适的。 并发-方法1 bash all.sh瞬间出来下图 改成-c 3 多ping几次看看 方法2、3就是子进程后台、线程后台 注意上图看着以为是脚本，其实可以是命令 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"14-进程、系统性能和计划任务/5-计划任务实现.html":{"url":"14-进程、系统性能和计划任务/5-计划任务实现.html","title":"第5节 计划任务实现","keywords":"","body":"第5节. 计划任务实现 一次性任务用的少：at这个包提供了at和batch两个命令 at用的多一点，batch基本不用。其实at也不怎么用 atq和atrm是at的扩展用法 atd.service必须是启动状态，这是前提 centos6上的atd服务： at一般用法就是跟一个时间执行cmd at -t的格式1和at 后不带-t的格式2 at回车会等STDIN，at -l查看计划任务，ctrl d 安全退出 1就是第一个任务 at -c 1可以查看具体任务细节，上面都是各种变量，最下面才是任务明细 等到时间了，任务执行完毕，就没了 at了3个命令，第一个创建文件执行了，但是后面两个ls和hostname怎么理解呢，在哪里体现呢 计划任务是在未来的时间执行的，ls和hostname是在当前终端tty上运行的。 而你不能确定你将来就在这个tty上，说不定别人正好撞到这个tty上了，就看到了ls和hostname执行的结果了。就是你执行的命令，别人看到了，这就不合理了。 所以基于以上道理，计划任务的STDOUT标准输出不会在屏幕上打印的。虽然没有输出到屏幕，但是以邮件的形式发给你了。 由于刚才是以root用户执行的，所以邮件就发到了root： 所以方法论来了，计划任务的脚本，一般不推荐有输出，而是所有输出都扔到日志或在/dev/null里，这话我就不认同哈哈，因为我的py脚本很多还是保留了print的，这些屏幕上看得到的，当然crontab去执行的化，自然屏幕看不到了。 不对，视频老师的意思的，crontab里的脚本不要有标准输出，否则会给你发送大量邮件，emmm，这话也不对，有时候正要考这些邮件当作日志来分析是否运行了。 不过，确实很多文章上的crontab都是由 /xxx/xxx/ /xxx/xxx > /dev/null的，看来不要发邮件才是比较好的一个做法。 ctrl c是强行终止 然后reboot重启，这些计划任务还在吗？ 重启后发现at -l还在 说明重启也不会影响at的任务，说明肯定是找一个地方存起来了。肯定不是内存里放了，肯定是放到磁盘上了。 磁盘上哪里呢？👇 a00003xxxx和a00004就是at -l看到的3和4编号。 这个cat /va/spool/at/a00003xxx文件看到的正是at -c 3看到的 这个路径的权限只有root 换个普通账号创建计划任务也是可以的 然后wang账号的计划任务也是放在/var/spool/at路径下的 对于wang账号来讲，/var/spool/at路径是没有权限的，但是确实将任务写进去了，说明at由SUID权限👇 删除atrm或者at -d一个意思 atrm -4说白就是把/var/spool/at的对应的文件删了 同理，可以直接删文件， noon：中午12点 midnigth：午夜12点，0点 teatime：下午茶时间，4点 tomorrow now + xxx：现在往后多久时间 -f 创建一个文件 -f 其实也就是利用重定向的效果：at 18:00 -m 告知计划任务有无执行 把原来邮件删掉 1分钟后执行 就是个空邮件告诉你执行了 白名单优于黑名单，如果wang既在白名单也在黑名单，白名单里有就不看黑名单了。 如果user2黑白里都没有，我分析就可以执行。 周期性任务用的多：crontab 对应的软件包比较多 cronie是主要工具包 上图的 crontd是主程序，运行后会自动周期运行计划任务。 上图的 /usr/bin/crontab # 是创建用户自己的计划任务的工具，区别于全局的/etc/crontab文件 centos6下的情况： crontabs是创建计划任务的工具 用户自定义的计划任务 通过crontab创建，后存放在/var/spool/cron下 这个/var/spool文件夹也是一个常用文件夹 除了刚才的at和马上正在学习的cron还有mail邮件也都在这里。 cronie-anacron补充性的包，用的不多 使用场景举例： 在家用电脑中安装了一个linux，这种PC台式机不像服务器一样24小时开机的，可能定期就会自动重启的。而加入计划任务是半夜执行的，而此时你关机了，就会导致计划任务没有执行，此时就有cronie-anacron来执行。 ​ 当你开机后的一段时间，会自动检查时间已过了没有执行的任务，找个时间给你执行了。 通过查看/etc/crontab可知，里面的存在各种环境变量的 而直接crontab -e去编辑用户自定义的任务，有时候就需要手动补上变量，比如我这种 这都是报错，后来解决的方法👆， 然后/etc/crontab还有个地方说明下 这个文件只有root才能读写，所以普通用户无法编辑，所以上面的user-name是root指定，意思就是这个计划任务是以某个普通用户来执行的。 写个磁盘空间告警 以前不太理解为什么sed 要先找到再查找替换，为什么不直接查找替换，现在案例就来了 先找到/dev/sda开头的(相当于做了一步过滤)，再针对这些开头进行替换。它不是说真的要替换，如果真的要替换直接s#a#b#就好了，它是要过滤显示出结果，所以需要查找到再替换显示。 这就找到最大值了。 将脚本写到crontab里 第一个*号的意思： 1,10,30 表示每小时的第1、10、30分钟 * 表示每分钟 */10 表示每10分钟 待会用wang普通账号去执行crontab，就是user-name写成wang，所以要看下脚本是否有权限 这样脚本wang用户就可以执行了。 然后就是 每分钟，1-5工作日，wang 去执行脚本，0或者7表示周日 然后跟踪下cron的日志 解释下面的任务 就是 30分 2点 1，10，20号 每月 周六或周日 问题来了，这个1号10号20号万一不是周末了，他们之间是并取还是或的关系呢？ 通过man 5 crontab可以找到逻辑关系，搜下note either就是也，plus就是加上，这些就说明了是 或的关系。 所以每个月的1、10、20号会执行，然后每周的周6和周日也会执行。 问题来了，如果我就要并且呢，计划任务没有这功能，就需要在脚本里去判断 然后脚本里去判断是否为周末，如果是就执行。这就是且的关系的落地。 用这个命令获得今日是周几；👇man date 通常计划任务不会放到这个/etc/crontab里，一般就是crontab -e那个用户就是哪个创建的。 crontab -e创建的时候，就系统就自然就知道是哪个用户创建的，所以格式上就有个默认的user-name不用写了，直接 * cmd 该文件已经有执行，所以就是CMD搞定 不通用户创建的crontab -e其实就是/var/spool/cron下的不同文件 crontab -l看自己的，看别人的加上 -u 删除某某用户的所有计划任务 文件下的脚本都执行 每分钟，执行/data/scripts下的所有脚本，然后验证确实执行了： 这个次序就是按ls的次序执行的 验证上面的次序判定 可见/usr/bin/run-parts /data/scripts 确实是按脚本存放路径的ls次序执行的。 系统本身就有的周期性任务 mlocate就是locate的依赖的默认数据库，而这个数据库是每天刷新一次的。就是靠这里。 为什么放到/etc/cron.daily下的这些脚本(这些logrotate、man-db.cron、mlocate都是独立的脚本)，为啥这些就会daily每日执行呢，其实还需要有个cron去执行他们的。 上图就是cron.hourly文件夹下的脚本都会执行的原因，因为有/etc/cron.d/0hourly去执行的。而daily文件夹下就不会执行，因为/etc/cront.d下没有对应的周期命令。 但其实上面的可能是执行的，因为还有 日志可以观察，也可以帮助还原误删除的计划任务 /var/log/cron 下次开机执行的方法，应该是等价于rc.local的。 crontab -e 便捷 重启后可见，确实执行了 因为wall看不到广播效果，因为没法提前进到那个终端去等到广播信息 换一个方式 多任务时间一致，可以用分号隔开，呵呵，要考虑前一条执行花费时间哦，第二条执行有延迟的。 anacontab 1表示1天执行一次，开机5分钟后自动运行 cron.daily 每天就是1 每周就是7 每月就是@monthly， 5 25 45就是开机后的5、25、45分钟后执行后面的脚本。 45分钟随机延迟 服务器上这个anacron用的少 管理临时文件 centos6上的一些文件 makewhatis就是手动做了makewhatis.cron mlocate.cron相关的信息，手动就是updatedb。 tmpwatch是清除垃圾文件 10天清理一次/tmp 30天清理一次/var/tmp windows没有定时清理的功能 到了centos7就是一个服务专门来做这事了 原来centos6这个路径下，在7上就没有哪个tmpwatch文件了： 不希望wang执行计划任务 再写一个/etc/cront.allow 白的黑的都有wang，其实就只看白的了 crontab -e就进去了👇 这个deny只是说不能编辑，原来如果有权限的时候编辑的计划任务还是会正常工作的。 crontab精确到s的方法 sleep可以精确到0.1s好像 但是你用sleep控制周期，就不是crontab里的每分钟--其实是到整点就执行了。sleep是真的等1s执行，那么如果之前的脚本本身执行就要花4s，那么用with os.popen阻塞的方式，其实就是5s钟才能一个周期了。而且存在队列不断加大的风险。 usleep是微妙级别的睡眠 利用crontab定期同步时间 这个是取之我们的extmail里的一个案例： 这是视频里老师的写法： 上图有错误，所以wq退出会报错 y重新edit为： 据说&> /dev/null是一个好习惯，否则一堆垃圾邮件。 习题 说是*/7不行，因为60/7除不尽，需要用sleep 420 来做，但你想想就是我说的，脚本执行如果要化10s，没关系，那也是脚本执行后sleep了7分钟才继续执行啊。所以两次周期就是严格的7分钟过了。 sleep也是有单位的 crontab里不要用%，这个踩过坑，当然%如果在py脚本里是没有问题的， 有人想在crontab里写date +%F，这种就不行，换个思路，将date +%F写到脚本里，然后crontab里调用脚本就可以了。 还有就是上文提到过变量问题，或者叫二进制的执行文件要写绝对路径--但是有的遗漏的就是通过邮件看到日志提示 之前的坑记录如下，emmm原来慢慢吞吞都1年半下来了。太慢啦，不过这事写脚本，不算linux系统学习，也还说的过去。 这个还是python脚本里调用了mtr命令，然后crontab报错找不到PATH变量 而且，我都在py里的mtr也是写了绝对路劲的，但是就是不认，哈哈，最后还是加了一行path变量才好的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"15-shell脚本编程进阶/15-shell脚本编程进阶.html":{"url":"15-shell脚本编程进阶/15-shell脚本编程进阶.html","title":"第十五章 shell脚本编程进阶","keywords":"","body":"第十五章 shell脚本编程进阶 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"15-shell脚本编程进阶/1-循环01.html":{"url":"15-shell脚本编程进阶/1-循环01.html","title":"第1节 循环01","keywords":"","body":"第1节. 循环01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"15-shell脚本编程进阶/2-循环02.html":{"url":"15-shell脚本编程进阶/2-循环02.html","title":"第2节 循环02","keywords":"","body":"第2节. 循环02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"15-shell脚本编程进阶/3-函数详解01.html":{"url":"15-shell脚本编程进阶/3-函数详解01.html","title":"第3节 函数详解01","keywords":"","body":"第3节. 函数详解01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"15-shell脚本编程进阶/4-函数详解02.html":{"url":"15-shell脚本编程进阶/4-函数详解02.html","title":"第4节 函数详解02","keywords":"","body":"第4节. 函数详解02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"15-shell脚本编程进阶/5-信号处理和函数组详解.html":{"url":"15-shell脚本编程进阶/5-信号处理和函数组详解.html","title":"第5节 信号处理和函数组详解","keywords":"","body":"第5节. 信号处理和函数组详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"15-shell脚本编程进阶/6-高级变量和expect.html":{"url":"15-shell脚本编程进阶/6-高级变量和expect.html","title":"第6节 高级变量和expect","keywords":"","body":"第6节. 高级变量和expect Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/16-系统启动和内核管理.html":{"url":"16-系统启动和内核管理/16-系统启动和内核管理.html","title":"第十六章 系统启动和内核管理","keywords":"","body":"第十六章 系统启动和内核管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/1-系统启动流程和故障排错.html":{"url":"16-系统启动和内核管理/1-系统启动流程和故障排错.html","title":"第1节 系统启动流程和故障排错","keywords":"","body":"第1节. 系统启动流程和故障排错 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/2-启动流程和服务管理.html":{"url":"16-系统启动和内核管理/2-启动流程和服务管理.html","title":"第2节 启动流程和服务管理","keywords":"","body":"第2节. 启动流程和服务管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/3-启动流程详解.html":{"url":"16-系统启动和内核管理/3-启动流程详解.html","title":"第3节 启动流程详解","keywords":"","body":"第3节. 启动流程详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/4-linux启动流程grub管理和故障排错.html":{"url":"16-系统启动和内核管理/4-linux启动流程grub管理和故障排错.html","title":"第4节 linux启动流程grub管理和故障排错","keywords":"","body":"第4节. linux启动流程grub管理和故障排错 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/5-自制linux和源码编译内核.html":{"url":"16-系统启动和内核管理/5-自制linux和源码编译内核.html","title":"第5节 自制linux和源码编译内核","keywords":"","body":"第5节. 自制linux和源码编译内核 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/6-systemd特性.html":{"url":"16-系统启动和内核管理/6-systemd特性.html","title":"第6节 systemd特性","keywords":"","body":"第6节. systemd特性 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"16-系统启动和内核管理/7-systemd和grub2管理.html":{"url":"16-系统启动和内核管理/7-systemd和grub2管理.html","title":"第7节 systemd和grub2管理","keywords":"","body":"第7节. systemd和grub2管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"17-Security-Enhanced/17-Security-Enhanced.html":{"url":"17-Security-Enhanced/17-Security-Enhanced.html","title":"第十七章 Security-Enhanced","keywords":"","body":"第十七章 Security-Enhanced Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"17-Security-Enhanced/1-SELinux实现安全加固.html":{"url":"17-Security-Enhanced/1-SELinux实现安全加固.html","title":"第1节 SELinux实现安全加固","keywords":"","body":"第1节. SELinux实现安全加固 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"18-文本处理三剑客3_awk/18-文本处理三剑客3_awk.html":{"url":"18-文本处理三剑客3_awk/18-文本处理三剑客3_awk.html","title":"第十八章 文本处理三剑客3_awk","keywords":"","body":"第十八章 文本处理三剑客3_awk Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"18-文本处理三剑客3_awk/1-文本三剑客3_awk详解01.html":{"url":"18-文本处理三剑客3_awk/1-文本三剑客3_awk详解01.html","title":"第1节 文本三剑客3_awk详解01","keywords":"","body":"第1节. 文本三剑客3_awk详解01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/19-加密和安全.html":{"url":"19-加密和安全/19-加密和安全.html","title":"第十九章 加密和安全","keywords":"","body":"第十九章 加密和安全 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/1-安全和各种攻击方法.html":{"url":"19-加密和安全/1-安全和各种攻击方法.html","title":"第1节 安全和各种攻击方法","keywords":"","body":"第1节. 安全和各种攻击方法 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/2-安全加密算法体系详解.html":{"url":"19-加密和安全/2-安全加密算法体系详解.html","title":"第2节 安全加密算法体系详解","keywords":"","body":"第2节. 安全加密算法体系详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/3-PKI和证书安全体系详解.html":{"url":"19-加密和安全/3-PKI和证书安全体系详解.html","title":"第3节 PKI和证书安全体系详解","keywords":"","body":"第3节. PKI和证书安全体系详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/4-互联网安全通讯协议HTTPS详解.html":{"url":"19-加密和安全/4-互联网安全通讯协议HTTPS详解.html","title":"第4节 互联网安全通讯协议HTTPS详解","keywords":"","body":"第4节. 互联网安全通讯协议HTTPS详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/5-建立私有CA和颁发证书.html":{"url":"19-加密和安全/5-建立私有CA和颁发证书.html","title":"第5节 建立私有CA和颁发证书","keywords":"","body":"第5节. 建立私有CA和颁发证书 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/6-证书管理和SSH服务工作原理详解.html":{"url":"19-加密和安全/6-证书管理和SSH服务工作原理详解.html","title":"第6节 证书管理和SSH服务工作原理详解","keywords":"","body":"第6节. 证书管理和SSH服务工作原理详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/7-SSH服务配置和基于KEY验证.html":{"url":"19-加密和安全/7-SSH服务配置和基于KEY验证.html","title":"第7节 SSH服务配置和基于KEY验证","keywords":"","body":"第7节. SSH服务配置和基于KEY验证 默认的ssh登入方式 关于SSH第一次链接的安全性，即解决方案： 你可以见Server端的公钥用HASH算一下，将值公布在网站上，CLIENT去链接的时候对比一下一样再yes。SSH后续连接就是安全的了。（*其实谷歌AWS等都不是这种方式了，都是用的公钥文件，下文讲) yes的作用就是，就是把对方的公钥放到的.ssh里。 -b 比较牛哈，ping知道怎么带源，ssh也要知道哈。 关于Xclock所代表的X协议 xclock代表的client端 xServer就是服务端 两者是通过x协议沟通， 图形的显示是通过server上的显卡实现。 xClient和xServer不在同一台电脑的场景，比如我A ssh B 登入后，输入xclock这个谁是server谁是client呢 A是server，B是client，因为登入到B上进行xclock，而要显示时钟图形界面是在A这个server上的。 A和B的X协议是通过ssh协议封装传递的。 注意：如果server端不是xshell，而是scrt软件，就不行了，因为scrt没有给你安装X协议server。xshell是xmanager一套软件安装的，是默认安装了x协议的。 windows没有xServer，xshell安装的时候默认安装了xServer，secureCRT没有xServer要额外安装软件的要注意的。 xshell不是单装的 xshell而是安装的xManager，所以带了xServer软件 改MAC地址的一个案例 当你改完网卡配置文件后还是改不过来的时候可以试试下面的方法： modprobe -r e1000，-r是remove掉网卡驱动，通过ip a发现没有卸掉网卡 ethtool -i eth0 ， 年后看下是哪种模块，请认真学习噗冬伐 modprobe -r pcnet32，找到驱动了，进行删除 modprobe pcnet32 重新安装驱动，mac得以刷新 一跳一跳地ssh上去 server会看到是192.168.37.101连上来的。 下面讲基于KEY的ssh 这里的key就是RSA里的密钥对，其实上面默认的ssh其实也是有公钥的，只不过之前的是单向公钥加密码；现在是双向公钥，并且弃用了密码。 安全性方面注意： 私钥拷走就有大问题 1、私钥拷走 2、修改私钥文件的属性 公钥交换，这个说法就很奇怪，听起来加上看上图，都觉得没错，但是实际上，res=ID^xxx这套东西通常是DH算法，而DH算法，就是在交换密钥，SSH加密可能试用非对称密码来加密传输的数据， 1、加密通道的形成，肯定不是公钥的传递，这个图片只是理论上的非对称做法，实际应该略微修改成对称密钥的传递。 2、所以从实际应用出发，上图的client PBULIC-KEY是不存在的。 3、所以下图的SSH加密通信应该改成对称密码加密而不是非对称。 在上面加密隧道形成后，于是开始认证，包括认证成功后的数据传递 同样上图得改，改成用户名+密码 然后通过对称密码加密发送过去，而不是Public key。 图中倒数第二步的 13579传输可不是明文传过去的，是基于对方公钥加密后传递的，其实这图是第二个阶段了，第一阶段是公钥的交互，交互后以后所有的通信都是使用公钥进行传输的了。 上图这个key的认证倒是对的，另附一图佐证 然后参看资料 https://juejin.cn/post/6844903685047189512 http://www.h3c.com/cn/d_200805/606213_30003_0.htm 密钥认证的实验 ssh-keygen -t rsa 默认就是rsa 注意密钥对存放路径，上图中有，然后上图是切到wang用户去生成密钥的。 生成了公钥和私钥文件 接下来要去弄server端的authorized_keys文件 该文件自动生成 ssh-copy-id -i /home/wang/.ssh/id_rsa.pub root@192.168.37.6 注意即使你敲错了id_rsa，也不会传私钥过去的，会自动给你传公钥的。 确实是写的私钥，实际系统给你传的也是公钥。ssh真贴心 现在直接登了就 scp走的就是ssh协议，所以复制也不要输密码了 直接不用输密码了 也挺安全 但是这台电脑的安全一定要保护好，这台机器的账号要是泄露了就危险了。 以上总结 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"19-加密和安全/8-SSH基于key的expect自动化脚本.html":{"url":"19-加密和安全/8-SSH基于key的expect自动化脚本.html","title":"第8节 SSH基于key的expect自动化脚本","keywords":"","body":"第8节. SSH基于key的expect自动化脚本 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"19-加密和安全/9-SSH端口转发功能详解.html":{"url":"19-加密和安全/9-SSH端口转发功能详解.html","title":"第9节 SSH端口转发功能详解","keywords":"","body":"第9节. SSH端口转发功能详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"19-加密和安全/10-SSH安全实践配置.html":{"url":"19-加密和安全/10-SSH安全实践配置.html","title":"第10节 SSH安全实践配置","keywords":"","body":"第10节. SSH安全实践配置 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/11-dropbear编译安装和文件完整性检查.html":{"url":"19-加密和安全/11-dropbear编译安装和文件完整性检查.html","title":"第11节 dropbear编译安装和文件完整性检查","keywords":"","body":"第11节. dropbear编译安装和文件完整性检查 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/12-sudo实现管理授权详解.html":{"url":"19-加密和安全/12-sudo实现管理授权详解.html","title":"第12节 sudo实现管理授权详解","keywords":"","body":"第12节. sudo实现管理授权详解 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/13-TCPWRAPPER和PAM安全模块.html":{"url":"19-加密和安全/13-TCPWRAPPER和PAM安全模块.html","title":"第13节 TCPWRAPPER和PAM安全模块","keywords":"","body":"第13节. TCPWRAPPER和PAM安全模块 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"19-加密和安全/14-PAM模块使用.html":{"url":"19-加密和安全/14-PAM模块使用.html","title":"第14节 PAM模块使用","keywords":"","body":"第14节. PAM模块使用 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:41 "},"20-网络时间服务和Chrony/20-网络时间服务和Chrony.html":{"url":"20-网络时间服务和Chrony/20-网络时间服务和Chrony.html","title":"第二十章 网络时间服务和Chrony","keywords":"","body":"第二十章 加密和安全 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"20-网络时间服务和Chrony/1-时间计时和同步.html":{"url":"20-网络时间服务和Chrony/1-时间计时和同步.html","title":"第1节 时间计时和同步","keywords":"","body":"第1节. 时间计时和同步 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"20-网络时间服务和Chrony/2-时间同步服务.html":{"url":"20-网络时间服务和Chrony/2-时间同步服务.html","title":"第2节 时间同步服务","keywords":"","body":"第2节. 时间同步服务 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"21-运维自动化系统部署/21-运维自动化系统部署.html":{"url":"21-运维自动化系统部署/21-运维自动化系统部署.html","title":"第二十一章 运维自动化系统部署","keywords":"","body":"第二十一章 运维自动化系统部署 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"21-运维自动化系统部署/1-系统自动化安装.html":{"url":"21-运维自动化系统部署/1-系统自动化安装.html","title":"第1节 系统自动化安装","keywords":"","body":"第1节. 系统自动化安装 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"21-运维自动化系统部署/2-DHCPD服务实现.html":{"url":"21-运维自动化系统部署/2-DHCPD服务实现.html","title":"第2节 DHCPD服务实现","keywords":"","body":"第2节. DHCDP服务实现 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"21-运维自动化系统部署/3-实现基于PXE安装centos7系统.html":{"url":"21-运维自动化系统部署/3-实现基于PXE安装centos7系统.html","title":"第3节 实现基于PXE安装centos7系统","keywords":"","body":"第3节. 实现基于PXE安装centos7系统 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"21-运维自动化系统部署/4-实现基于cobbler的自动安装.html":{"url":"21-运维自动化系统部署/4-实现基于cobbler的自动安装.html","title":"第4节 实现基于cobbler的自动安装","keywords":"","body":"第4节. 实现基于cobbler的自动安装 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"21-运维自动化系统部署/5-实现基于cobblerweb的管理.html":{"url":"21-运维自动化系统部署/5-实现基于cobblerweb的管理.html","title":"第5节 实现基于cobblerweb的管理","keywords":"","body":"第5节. 实现基于cobblerweb的管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"22-DNS服务和BIND/22-DNS服务和BIND.html":{"url":"22-DNS服务和BIND/22-DNS服务和BIND.html","title":"第二十二章 DNS服务和BIND","keywords":"","body":"第二十二章 DNS服务和BIND Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:43 "},"22-DNS服务和BIND/1-DNS服务简介.html":{"url":"22-DNS服务和BIND/1-DNS服务简介.html","title":"第1节 DNS服务简介","keywords":"","body":"第1节. DNS服务简介 bind9是这次主学的，dnsmasq也是不错的，其他微软的，路由器上的都行，但是考虑一下这个案例： 公司的员工从飞塔出去，飞塔基于sourceIP做ecmp从ct和cu出去，此时存在一个问题，内部dns从ct出去，此时从cu出去的用户dns拿到的却是电信的，虽然在当前BGP IP较多的情况下，但是随着用户量变多，鸟也变多了，所以娘好儿子才能好，不对，所以良好的方案就很重要了，你不能凑合着用对吧， 你说为什么不用出口设备做DNS分流+ISP路由清单，①没清单②DNS分流要截获的吧，这个iKUAI肯定可以做，没研究。但是作为DNS的学习来讲，显然必学的，所以我的方案来了。 你说为什么不用出口的pbr，或者核心的pbr，pbr是固定一拨人走一个出口，现在利用dns分为2个奇偶，随着用户奇偶，就能自动将dns分流，而且随着飞塔的ECMP随机性，奇偶一定是分别从2个出口走的，但是至于走哪个口是随机且相对固定的，一段时间奇数时走ct，一段时间奇数又是走cu，与之相对的偶数就走另外的线路。ECMP就是随机且相对稳定的负载分担。 1、当前用户在fortigate上是sourceip的ECMP，所以奇数和偶数IP分别随机又相对稳定的分流到两个ISP上的 2、DNS内部有两台10.2和10.3，但是都是从第三条电信EIP出去的。 3、此时加入一段时间偶数IP是从ct出去，奇数IP从cu出去，而dns从EIP出去，此时奇数IP的dns请求就需要调整。 方案如下 1、将10.2的路由保留继续从EIP出去，10.3的路由从第四条线路联通固定IP出去；如果你没有这么多线路，你就用ct和cu啦。具体就是将10.2和10.3都改为从飞塔出去，然后飞塔就会根奇偶ECMP出去。 此时10.2就成为了偶数IP员工的一员，10.3就成为奇数IP的一员了。 2、需要将偶数IP的dns request 转到10.2(因为dns也是偶数，所以它也是这一部分员工走的一样的线路，这就保证DNS也是从员工出去的线路问的，拿到的自然就是当前使用ISP的解析地址咯)，将奇数IP的dns请求转到10.3 3、由于10.2和10.3是用的dnsmasq做的，本来我是想大家都用10.2，然后10.2上做view将奇数IP的请求转到10.3上去，由10.3去代理查询，但是dnsmqs不支持view，所以就需要学习bind9再两个dnsmaq前面套一层，不能说dnsmasq就是轻量级的，套一层bind9只做view转发，想想就美滋滋~ 但是我就不硬上，反正也不急，慢慢撸一遍DNS~emm，之前就看过一遍忘记咯。 开始 ================================================================================ 一、 bind9和dhcp都是isc开发的， https://www.isc.org/download/ 此外还有一个unbound，不过看版本就是知道太新了，不一定有bind9稳定。 https://nlnetlabs.nl/projects/unbound/about/ bind是伯克利大学开发开源的 主流的CPU芯片： x86: intel amd 兆芯(购买了早期的citrix) 移动手机端绝大多都是ARM：苹果A12，高通-晓龙855，华为-麒麟985，联发科-各种山寨？ 华为被制裁由将来可能就不用ARM了， MIPS: 龙芯-生态圈小-用的少，早期比ARM强，ARM在移动设备爆发的时候起来了。 RISC-V罗马数字5代的意思：开源的芯片级的linux架构，华为被ARM制裁 所以加入到这个CPU开源芯片研发团队了。而这个RISC又是伯克利大学人做的。 FQDN Full Qualified Domain Name oneyearice.github.io, oneyearice就是主机名(或者是别名)，github.io就是域名，这是分层的结构，一级域名，二级域名等 DNS是一个分布式的系统，结合hosts的零散和NIS的集中 两家之长~ www.sina.com.cn. 最后其实是有一个.点的。只是在浏览器输入的时候省略了不写了，windows里dns服务里好像之前见到过。 FQDNIP之间互相是可以互相转换的，通常是域名转IP咯。 NIS是把所有的域名解析放到一个地方，而DNS是在每级域处都建立数据。 shanghai.sina.com 就是这么顺下来的 不过还有一个www.sina.com 就是不是递归的dns数据库而是直接二级域的本地数据库里的一个www主机名 同样shanghai.sina.com除了本地的www.shanghai.sina.com,也许还有下级域 然后用户是如何找到www.shanghai.sina.comde 或mail.iwgame.com或者ftp.iwgame.com等IP地址的,两种查询方式，一般都是递归 名词解释： 所谓权威DNS服务，就是比如你要访问的www.shanghai.sina.com这个域名的权威就是在哪里对外发布的那台，哪里就是权威，对应的上图就是最底层的三级域名服务器。 如果223.5.5.5有缓存就直接给你了 FRU算法 缓存长时间不用就慢慢 删掉了就 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:42 "},"22-DNS服务和BIND/2-DNS只缓存服务器实现.html":{"url":"22-DNS服务和BIND/2-DNS只缓存服务器实现.html","title":"第2节 DNS只缓存服务器实现","keywords":"","body":"第2节. DNS只缓存服务器实现 下面开始部署DNS服务器 主从 whois 1、购买域名 2、云上解析即可，此时云上就是你的权威DNS 3、搭建企业内部的dns服务器 使用BIND，其实是一系列的软件。包括bind、bind-utils、bind-chroot bind-utils应该是客户端工具，比如nslookup、dig、hosts等 bind是服务端提供服务的包 bind-chroot是bind的一个功能,使bind可以在一个chroot的模式下运行.也就是说,bind运行时的/(根)目录,并不是系统真正的/(根)目录,只是系统中的一个子目录而已.这样做的目的是为了提高安全性.因为在chroot的模式下,bind可以访问的范围仅限于这个子目录的范围里,无法进一步提升,进入到系统的其他目录中。bind的默认启动方式就是chroot方式 yum -y install bind rpm -qi bind rpm -ql bind 其中/etc/named.conf是主配置文件 /usr/lib/systemd/system/named.service是服务启动的文件 /usr/sbin/named是主程序 /usr/sbin/rndc也是要用到的啥啥啥 梦飒飒 /var/named是数据库文件夹用来存放域名解析的信息 /var/named/named.ca不是证书那套体系里的ca哦，而是传说中的13个根。 /var/log/named.log是日志 怎么一下起来这么多53，v4的8个（tcp4个udp4个），v6的8个， 那么问题来了，为什么会是4个udp53呢？ fd好像是socke进程的句柄，可能涉及并发 fd一般就是/dev/fd 文件描述符，这里可能是一个意思？ 然后只要启动了bind服务，就默认具备了dns的递归查询功能，因为人家有13个根，所以能够往上游查询，只是效率将不准了，哈哈，也就是说 本地/etc/resolv.conf里写127.0.0.1，然后启动named，本地就能ping通www.xx.com域名了。 此时named默认空配置，服务起来后，自己可以解析；但是代理dns，也就是还不能做内网dns 上图已经关闭了默认的firewalld和selinux 除了nslookup还有host dig都有类似提示 都是超时，因为udp 53都不通啊，👇 当然tcp53一样也不通 但是ping ok 防火墙和selinux不止早就关了吗，还有啥？ 仔细看前面的ss -tlnup 的截图，上面显示是监听地址为127.0.0.1也就是只侦听在本地环回口，而不是外接的网卡上，所以自然不能提供服务啦 改一下，就是改配置文件 这样端口就通了，也可以改成localhost--代表当前机器的所有IP，效果是一样一样的，都是监听本地所有ip上 localhost是本地所有IP≠any=0.0.0.0/0 本地两个ip一个192.的一个127的，所以就监听在这两个上面了 当然tcp，client是不需要的 此时nslookup还是不行 直接拒绝你了，看来还有地方要改 继续看配置文件，找到关键词 allow-query，改成0.0.0.0/0或者any就行 注意nslookup 不涉及本地缓存哦，上图就说了，named是默认就有缓存功能的。进一步验证，就是讲dns 服务的机器设置成两个网卡，一个网卡沟通内部，一个网卡沟通外网互联网，①第一次client内部dns请求后②关闭server的外网接口，再次请求就会发现依然OK。 dig 用的不多，看看效果 这是不能解析的截图 这个不要深究了，及时ping提示网络不可达其实dns已经由IP了。 正所谓凡是不可毕其尽，谁知道系统怎么提示的，又不是天地法则，都是人为编程。 补上网关 补一个不存的网关，欺骗系统一下，它认为远程的主机是可达的，是有机会访问的 这个时候IP就显示出来了，所以网络不通立马跳出来这个消息，就是网关配有配置，有配置就是尝试通信的。 当然一般dns不是同网段， 如果是同网段+dns在一个子网，解析没问题，网关没配置，一样秒报错 网络不可达。 本来不想细究，但是人家视频里正好演示了，我就顺便截个图咯，本来就没啥意义，只有在处理特殊故障的时候才可能用到这种所谓的底层基本功。 缓存存在域dns server的验证实验截图，人家的 哈哈 dns server下图的路由和网卡：👇 关于接口 down的linux 怎么判断是接口不带电了，还是协议down了呢？要知道VMwareworkstaion的VM虚机的网卡断开后，接口DOWN没错，但是ip route show还能看见默认路由的。这就没有数通的产品路由生效要有下一跳可达的前置条件。 ifconfig看不出到底是命令down还是没插网线的down，交换机上物理口就有administra down和普通的down-no conn 其实除了 上面的管理员down和没插线的down 还有协议down，普通以太网口这种情况不多，顶多就是交换机的err-disable down，抓哟是ppp 串行线的口的协议down，tunnel口的协议down，这些吧。 ethtool eth1 看看 mii-tool比较适合干这活 ifconfig eth1 down 等价于网卡禁用 VM里断开网络来凝结 等价于把网线 好~，现在dns server的网关就没了，无法进一步向根询问dns解析了 结果发现client解析之前解析过的，理应在server上有缓存的，竟然不成功！ 发现自动补了后缀，这是杂肥四 先不着急删除/etc/resolve.conf里的自动不清的后缀domain 先去还原dns server外网试试看，恢复dns server的外网后，client ping 域名 OK👇 然后再次禁用dns server的外网网卡 client依然可以ping通 刚才不通，可能就是server的缓存时间比较短。其实不是是本机有缓存。还需要清除本机缓存--好像没必要， https://jaminzhang.github.io/dns/flush-dns-cache-in-Linux/ 因为centos上好像本地没有dns请求的缓存记录。 dns server服务端清除dns缓存可以的，这样①重启服务②rndc flush； 后client无法解析 linux 就没有本机的dns 客户端缓存的，除非你是dns server做中继或者做解析的。 上面讲的dns本地是没有解析的，都是问别人得到的，这种就是 dns 只缓存服务器； 纯二道贩子 ------------------------------------题外话----------start------------------------- 之前我给开发搭建的NPM缓存服务器就是一样的，NPM代理那里有时间复习下，关键点就是①verdaccio这个软件好像是②npm的代理 client通过它下载的软件网页上是没有记录的，代理服务器自己install的才能上网页，所以就写了个脚本讲代理下载的json文件等 复制到上网页的路径里去就搞定啦~ 点开都有帮助 使用案例 1年都没坏，机器上还有一个yum源，也是一周一更，香香的。 ------------------------------------题外话----------end------------------------- 下面搭建右下角的本地dns解析条目 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:43 "},"22-DNS服务和BIND/3-DNS主服务器实现.html":{"url":"22-DNS服务和BIND/3-DNS主服务器实现.html","title":"第3节 DNS主服务器实现","keywords":"","body":"第3节. DNS主服务器实现 dnsmasq就是比较简单的hosts复用就行 bind的它分域，针对一个一级域名进行其下域名解析记录的编写。 前2节内容说白了，就是两行的事 下面继续了解其他配置，针对xxx.com某个域进行dns服务， 第一步，打开主配置文件 配置文件类似C预言风格，都是;分号结尾的 其中有options语句块，用{}花括号括起来的。定义了dns的选项 下面还有logging日志的语句块。 要建立的DNS域信息，区域zone就是针对某个域建立dns解析用的。 zone : oneyearice.asia dbfile1 # zone就是一级域名对应的数据库文件 xkyearsice.asiadbfile2 # 这是第二个区域 上图的file就是数据库的文件名 而且是根，根的tye就是hint，主dns就是写master。这里的file只写了名称，路径没有写。路径在option里定义的👇 还有dns是主还是从 图中的.就是根咯，因为这里填的是域名，而根的域名就是个.点，所以file指向的是named.ca文件，该文件里放的及时13个根域。 根据option里的directory 指定的路径，和zone里自己定义的file 名称，创建数据库文件 既然以named用户运行，那么数据库就要有访问权限 格式 name就是域名 TTL是域名的缓存时间，单位是秒 IN就是固定格式，internet记录，这里固定格式，其实dns还可以做别的工作， rr_type常规就是A记录，AAAA ipv6地址记录， value就是IP地址，当然如果是PTR的类型，前面的name就是IP地址，这里的value值就是域名了。 这些就是rrtype类型：除此之外还有PTR--ip解析成域名的记录，也就是A记录的反向动作。 其中rr_type灰常重要的一种是SOA类型 数据库中可以有很多条记录，SOA必须是第一条记录 SOA就是对该域的初始定义，比如当前域由哪个主dns服务器对外提供服务;写清 主从 主从数据库同步的方式，管理员邮箱 故障通知， 可以使用freenom获得免费域名，然后NS到CF，在使用CF的免费CDN就挺好的。 开始编写，参考模板 接着修改下 下面的NS、A、AAAA分别一行行的，是3条解析记录 上图的SOA记录解释 1、name就是@这个，写全就是oneyearice.asia. 注意最后一个点，不写就当作前缀会自动给你补上zone定义的一级域名oneyearice.asia 也就说写成oneyearice.asia等价于oneyearice.asia.oneyearice.aisa。结尾加上.就不会自动再补了。 然后，@就是zone里指过来的一级域名就可以用@代替。 2、TTL，可以用第一行定义的$TTL 1D，就不用再写了；也可以不用它的，采用自定义比如@ 864000 这样就是10天的缓存 3、IN照抄 4、类型rr_type就是SOA啦 5、value，soa的值不是一个IP地址，而是一个定义的集合，包括 ​ ①当前这个域的主DNS服务器 理论上面的master.oneyearice.asia.可以简写为master，不是说自动补齐嘛，为啥写这么长呢。 通过这里再次明确根就是.咯， 下面的 0 ：serial 表示当前区域数据文件的版本号，数字越大，表示当前内容越新。当我们修改了记录后，就需要手动的递增该版本号。 这个版本的作用就是：主从同步的判断依据，同步考的就是主上的版本号变大了（自己和自己比较，MMP，这同步的思路牛逼了）。只看版本号，不看内容 不看哈希，就看数字。数字还是人写的。牛逼~，看来还得辅以脚本自动判断哈希值 递增主的版本号。 1D：refresh 是从服务器 1 天 去主服务器上拉一次数据库信息。这么理解的，关于主从的同步 分为 推 和 拉 推就是主发现自己的版本号变了，而且是变大了，就会推； 拉就是从没法知道主什么时候变，于是周期性的去拉，周期就是这里的1D设置的。 1H：retry，就是从没拉成功，不用等1D后的周期，只需要1H就会再次尝试去拉。那么问题来，你确定1H不涉及主推的不成功的重试时间？ 1W：如果从服务器长达1W一周没能同主服务器同步，这话不准确啊，是没拉成功还是没被主push成功呢，我初步感觉应该是看日期，不管是主push还是从get，数据库文件的mtime如果小于当前时间 超过1W，就算失联了，对吧。这个就合理了，就算它的逻辑不是我想的这样，我TM写个脚本给你改掉。 3H：minimum是针对client请求的缓存时间，比如client请一个域名，我第一次本地没有的就会去上游dns问，然后缓存3小时内，client再次请求，就会直接把缓存里的记录交给client。如果是本地有的记录呢，是不是也会缓存，应该也不会查本地数据库。错了！ ​ 这3H和上面的TTL是两码事，minimum和TTL都是针对client的请求的记录缓存，minimun是针对本地不存的记录需要去上游问的，不管问到还是没问到的缓存；而TTL是针对本地有记录的用户请求的缓存。 ​ 都不对，两个缓存的理解如下： 所以dnsmasq估计也有这两种缓存，也就是说 修改hosts，说不定不用重启dnasmq，只要等他缓存到期了就行了。 更正 https://www.zytrax.com/books/dns/apd/rfc2308.txt上讲了 Negative caching was an optional part of the DNS specification and deals with the caching of the non-existence of an RRset [RFC2181] or domain name. https://www.zytrax.com/books/dns/ch8/soa.html中讲了 \"the negative caching time - the time a NAME ERROR = NXDOMAIN result may be cached by any resolver.\" 然后人家也说了，older documentation也就是bind4and8不是这么玩的，那会儿表示TTL的默认值。那会还没有explicit TTL呢 nx = nxdomain ttl Signed 32 bit value in seconds. RFC 2308 (implemented by BIND 9) redefined this value to be the negative caching time - the time a NAME ERROR = NXDOMAIN result may be cached by any resolver. The maximum value allowed by RFC 2308 for this parameter is 3 hours (10800 seconds). Note: This value was historically (in BIND 4 and 8) used to hold the default TTL value for any RR from the zone that did not specify an explicit TTL. RFC 2308 (and BIND 9) uses the $TTL directive as the zone default TTL. You may find older documentation or zone file configurations which reflect the old usage. BIND Time format. https://www.zytrax.com/books/dns/apd/rfc2308.txt上讲了 Negative caching was an optional part of the DNS specification and deals with the caching of the non-existence of an RRset [RFC2181] or domain name. 所以关于两个缓存需要重新认识一下 吃饭， 回来了，去的路上我在想，雨果我来开发，我怎么弄，我就会将 请求到的做一个缓存，没请求到的另外做一个缓存，就行拉， 找证据，肯定要去官网了，首先进入到这里：https://www.isc.org/bind/ 页面找了一圈没有看到document关键字，点这里看看 继续点 点 搜 结果在1.4里，哈哈 所以我开发的思路也是对了，就是不存的解析一个缓存。并不是 通过dns server代理查询上游的本地没有的记录的缓存时间。没这个说法！ 然后上图的 For details of what all these fileds mean,please see the authoritatvie server document.去看看明细解释。 SOA Records There is only one SOA that is guaranteed to exist on the internet and that is the one for the root zone (called .). As of 2018, it looks like this: . 86400 IN SOA a.root-servers.net. nstld.verisign-grs.com. 2018032802 1800 900 604800 86400 This says: the authoritative server for the root zone is called a.root-servers.net. This name is however only used for diagnostics. Secondly, nstld@verisign-grs.com is the email address of the zone maintainer. Note that the @ is replaced by a dot. Specifically, if the email address had been nstld.maintainer@verisign-grs.com, this would have been stored as nstld\\.maintainer.verisign-grs.com，This name would then still be 3 labels long, but the first one has a dot in it. The following field, 2018032802, is a serial number. Quite often, but by all means not always, this is a date in proper order (YYYYMMDD), followed by two digits indicating updates over the day. This serial number is used for replication purposes, as are the following 3 numbers. Zones are hosted on 'masters`. Meanwhile, 'slave' servers poll the master for updates, and pull down a new zone if they see new contents, as noted by an increase in serial number. The numbers 1800 and 900 describe how often a zone should be checked for updates (twice an hour), and that if an update check fails it should be repeated after 900 seconds. Finally, 604800 says that if a master server was unreachable for over a week, the zone should be deleted from the slave. This is not a popular feature. The final number, 86400, denotes that if a response says a name or RRSET does not exist, it will continue to not exist for the next day, and that this knowledge may be cached. 所以 1、邮箱的格式要注意 识别不了的时候 \\.的写法 2、时间默认单位是s秒 3、Finally, 604800 says that if a master server was unreachable for over a week, the zone should be deleted from the slave. This is not a popular feature.这句话是说①这是在从服务器上起作用的配置，从服务器判断master不可达持续一定时间后，就将zone删除，是从服务器上删除zone，也就是从自行惭愧了，知道自己差了很久，补提供该方面的服务了②这个功能不受欢迎，很简单啊，master虽然从服务器不可达了，其实大概率就是master挂了，从服务器又不提供服务，那网络里dns一个都没了，喜欢才怪。 4、最后的minimum时间早期是没有定义TTL的时候就用来做默认的TTL，有了$TTL就在BIND9版本里作为新的功能了--NAME ERROR = NXDOMAIN 就是上游dns的response 说不存在，则本地缓存 这个判断 的持续时间。在此时间段里用户请求该域名就直接用缓存回给用户。 5、TTL的作用看来就不是仅仅本地RR，理应包括response 的来的RR。我再去确认， TTL in the DNS context defines the duration in seconds that the record may be cached by any resolver. https://powerdns.org/hello-dns/basic.md.html这里点击 所以没找到 针对本地RR这种说法，所我有理由认为就是所有得有结果得解析记录，后面敲实验的时候验证下就好赖，有啥难的，不查了。 还是这篇就不错https://www.zytrax.com/books/dns/ch8/soa.html 点进去就知道缓存RR超时时间的前生今世了https://www.zytrax.com/books/dns/apa/ttl.html 其中Note: RFC 1912 cautions that 0 = no caching may not be widely supported, however most modern DNS software does support the feature. ，我学这个bind本意就是用来做DNS的请求的负载均衡将奇数偶数分担到10.2和10.3，所以肯定是可以设置为0的，因为10.2 和10.3上就有缓存。不过bind有缓存好像也不影响结果。 然后In BIND 8 the SOA record (minimum parameter) was used to define the zone default TTL value. In BIND 9 the SOA 'minimum' parameter is used as the negative (NXDOMAIN) caching time (defined in RFC 2308).这就是前生今世拉 设置的频率也是一个学问RFC 1912 recommends that the $TTL value be set to 1 day or longer and that certain RRs which rarely change, such as the MX records for the domain, use an explicit TTL value to set even longer values such as 2 to 3 weeks. The value of any TTL is a balance between how frequently you think the DNS records will change vs load on the DNS server. 操作细节In most cases this will not be a problem since IP address changes are normally planned in advance, in which case in advance of the change process the TTL could be reduced to 3h to 12h and then restored to a higher value when the change has stabilized. 然后DNS最长缓存时间是68年The TTL field is defined to be an unsigned 32 bit value with a valid range from 0 to 2147483647 (clarified in RFC 2181) - which is a long time! - somewhere on the other side of 68 years. 看来首位置为0了。 想看再看吧https://www.zytrax.com/books/dns/info/ttl.html 关于www，往往不是A记录，而是CNAME(别名)做CDN这是比较常见的，当然你也可以直接A记录，个人网站其实也有CF的免费CDN也是用CNAME咯。所以除非你不懂，否则还真是用别名，不用A记录。 还是dig看着最舒服，它能够标注出来CNAME，不过host和nlsookup其实也是明确的，要注意它的排版格式就能识别谁CNAME到谁了。 配置CNAME多个看看： 上图的3H更正为：代理查询 没查到 这么一个没查到的结果 缓存的时间。 编辑bind配置文件 不过一般不推荐在这个主配置文件里这么直接加，可以指一个目录，然后统一放在那里，这是常规宇宙法则，其实就是上图倒数第二行的文件，就是统一放区域rr记录文件的。 这里才是专门放区域文件的地方，所以修改一下 至此，该有的配置就配置完了， 1、语法检查：named-checkconf named-checkzone 检查配置文件的命令，也只能检测配置文件，它不能检查数据库文件也就是rr记录文件 检测区域数据的命令： OK就是成功拉，loaded serial 0就是版本号0的意思。 这是报错👇 所以不管最大多少，人家推荐的是 年月日nn nn是当天修订的次数。 验证一下，果然4294967295 是最大值，就是32位，4个Byte的空间。其实你要有这个思路，就是 凡是计算机里的最大值都是几个BYTE空间得出来的。比如IP报文里的 首部长度也是4个B。要知道2^32已经很大了。 2、重新加载配置文件：rndc reload 此时解析就OK 注意区别， 我的测试有点小问题，不递归去问上游dns了，也就是默认的13个根了 开启海外网络，本地dig就成功了，毛仔细看下图 ask的是192.168.10.2. 还是不行，dns server的firewalld和selinux都关掉还是不行，明天再看吧 奇怪了 不过firewalld确实也造成了故障 就是不递归了，下班下班，回去睡觉 好了，好像还是selinux没有disabled之前是permissive，反正今天过来一开始还是老样子，防火墙，selinux，重启linux，就没有啥其他动作了。无非时各种dig，顺带把windows的dig和tcping工具给完善和一下 奇怪的一点时，我怎么查dig都看不到AUTHORITY权威DNS服务器了 以上就完成了主dns的简单搭建 dig的用法 此外还有 从服务器、反向解析、智能DNS、CDN、DNS委托 下一节继续 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:43 "},"22-DNS服务和BIND/4-实现反向区域和主从复制服务.html":{"url":"22-DNS服务和BIND/4-实现反向区域和主从复制服务.html","title":"第4节 实现反向区域和主从复制服务","keywords":"","body":"第4节. 实现反向区域和主从复制服务 PRT是独立了存放的数据 发邮件的时候可能需要PTR反向解析 1、当你发送邮件到达对方邮件服务器的时候， 2、对方邮件服务器就会将发送方的IP地址方向解析成域名，如果该域名和发件人的邮箱后缀不一致，则认为是垃圾邮件。 3、就是防止对方冒充一些合法域名发送邮件咯。这个不一定通过IP，通过邮件的源码好像也能判断。 TXT记录的用在： 1、SPF反垃圾邮件记录 2、HTTPS验证记录 示例：_dnsauth TXT 2022122200000051qgs71beoh5h6wht4n1h0lr021x 厂家提供的字符串 搭建反向DNS 主dns里定义了专门用来存放区域信息的路径： include \"/etc/named.rfc1912.zones\"; 解决wwww.oneyearice.aisa的问题 * （泛域名解析）可以CNAME过去，泛域名解析是垫底的策略，类似路由表，也是最长匹配。与zone数据文件里的记录 顺序无关。 泛域名不能覆盖@功能，就是说*.xx.com 不能覆盖xx.com的解析。 @ 阿里云上可以CNAME，bind配置里不能CNAME，奇怪 rndc flush清缓存 rr的批量简化写法： $GENERATE 1-254 HOST$ IN A 1.2.3.$ 代表了: HOST1.@解析到1.2.3.1 HOST2.@解析到1.2.3.2 ... ... HOST254.@解析到1.2.3.254 开始做反向解析 开始创建 如果是192.168.10.0段的PRT就是协亨 zone \"10.168.192.in-addr.arpa\" { ​ type master; ​ file \"192.168.10.zone\" # 这里知识文件名称，无需倒着写。 }; 创建完后，检查、加载 看下面这张图 简写，的自动补全，存在冲突，也就是说SOA里的master主域写的ns1.oneyearice.asia.其实还依赖于正向解析咯。邮件当然得使用正向解析，否则admin.10.in-addr.arpa.也不是邮箱地址。 看来确实无关功能，解析拿不到的时候倒是可以看到SOA记录，这里其实理解错误，下文有讲。 -x的简化 看看老师的写法 他的图里有ns1记录的，可以看到其实也不涉及SOA里的 所谓的 主域名 ，ns记录的补齐也是补的ptr域。 从DNS服务器 直接克隆一台出来，改一下IP地址 1、zone区域配置得类型得改一下 这个zone的数据文件，其实不需要手动创建 重启服务就可以自动同步了。注意，上图的/var/named/下的oneyearice.asia.zone和10.zone都是克隆过来的，不要管他，要看zone配置的指向路径里的文件--也就是/var/named/slaves下面当前为空， 上图有错误一大堆👆 上图的file名字最好也加上.slave后缀，看起来舒服些。 然后这是一些报错，备忘放在这里了： 使用named-check查看，要改成masters才行： 配置都OK了，但是重启服务，没有同步自动生成zone文件，systemctl status 发现是主拒绝53，基本就是服务没启动了，上文都关掉了firewalld和selinux了。 开启主的named服务后，重启从的named，zone数据自动同步创建了。 不过同步过来都变成了data文件，cat也看不了 好像就是centos7开始这样的，看不了了，以前是纯文本。 这样的好处就是从不能编辑修改，从就是得从主上面拿东西。 到这里，从服务器的解析也就正常了 三个PRT好了👆，然后正向的再看看 考察下主从同步机制 版本号是和自己的上一次比较的。 推和拉都要版本号发生递增，所谓递增就是主自己看自己和上一次相比增加了，否则无法同步，不管推还是拉。 保存，rndc reload 重新加载，主也 不会push过去，从get拉也没到3小时呢。 所以当前还是无法立马同步，从那边是配置了masters的，所以等3H，也就能同步，但是想要主推过去，就得让主知道谁是从服务，当前它还不知道谁是从呢。 利用ns记录来告诉master谁是从 所以学到这SOA记录里的主域名就很有意义咯，就不是上文说的没有功能咯。 这样就OK啦，从就同步了 然后nslookup看看 可以了，从拉的时间也没那么及时，我测试在2分钟左右能拉好 安全性问题 上文中可见，从服务器的配置，只要指明master就可以实现数据的同步了，只要主的版本号递增，就能get也好，push也罢，都能拿到数据了。主上面没有做验证措施。 其实也还好，就算有人自己搭一个从服务器，拉取数据下来，他看不清除，因为file是data数据库文件。 此外还有一个安全性问题，就是能够拉取对应域的所有信息： dig -t axfr oneyearice.asia @192.168.126.129 对比公网人家的dns，肯定做了安全措施了 此时axfr就抓不到了，因为129上做了白名单👇 进一步防止到\"从服务器\"上抓取。 好了，主从就OK了， client端就可以配置2个dns服务器了 windows和linux的两个dns服务器的优先级 需要研究下 总之最基础的来讲，就是主dns挂了down机了，从dns就生效了。 两个都down了，windows有缓存，linux无client端缓存。 主从切换OK，这个其实是依赖于client系统的一个自行判断。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:43 "},"22-DNS服务和BIND/5-实现DNS子域委派和转发.html":{"url":"22-DNS服务和BIND/5-实现DNS子域委派和转发.html","title":"第5节 实现DNS子域委派和转发","keywords":"","body":"第5节. 实现DNS子域委派和转发 oneyearice算作父域，bejing、shanghai是子域， 1、如果父域和子域在一个机器上 shanghai.oneyearice.asia和oneyearice.asia放在同一台电脑上 根据前面章节学的内容，也就是一条A记录的事 www.shanghai A 192.168.100.100 这样虽然有www.shanghai.oneyearice.aisa的解析看着有一个shanghai的子域，但是其实只是条A记录。 如果shanghai这个子域里的记录比较多，是可以独立出来成为单独的配置文件的。 比如 ftp.shanghai. db.shanghai. mysql.shanghai. mail.shanghai. 管理上父域和子域属于集团不同部门或者子公司的权限下发，就需要独立开来。 独立出来就很简单，cp -a 一份出来 改改就行 修改一下zone域配置文件。然后这里有个快捷键挺好玩的，👇记一下可以。 虽然说是父子域，但是从配置角度上来讲，就是两个独立的zone。 这样父子域 还在同一个设备，从管理角度上还没有完全分开。除非你用facl去管理权限？ 最好分开来合乎管理，下面就是学习 父子不在一起的场景。 2、如果父域和子域在不同机器上 其实就是子域委派，感觉就是NS写一下就好了，之前freenom里就这么写的。 上图有问题写错文件了，应该写道oneyearice.asia.zone这个父域文件里的NS记录，然后这个shanghai.oneyearice.aisa.zone的子域文件就可以删除了。 修改一下： 1、删除zone 配置文件 2、删掉对应的zone file记录文件 3、修改添加一条父域里，关于子域shanghai的NS记录，指向对应的服务器，这里就简单点，直接利用从了。之前从服务器，已经同步了主之前还有的shanghai.oneyearice.asia.zone子域记录文件了(文中未体现，其实就是在从那边配置了一个zone type slave并指明master，然后重启从的服务就拿到了。)，所以此时就可以解析了 必须重启192.168.126.129这个主域上的named才能看到现象，因为有缓存的。 还原 不行还是， 重启服务试试 还是不行，会卡在这里 过一会就 找到原因了 可以了👆 补充： 每次修改主DNS的zone记录的时候，一定要记得递增版本号。 NS在很多地方都有看到，这是阿里的关于子域的NS 腾讯上看看 还挺多，功能 不过我记得有oneyearice.asia这种一级域名的NS修改的，现在没找到。。。 还是说必须是www.oneyearice.asia这种，这种倒是直接就能写。 如果是新开一台干净的机器来做子域， 直接上图了，就这么着吧： 然后人家的报错 然后他就改了下图两项为no 然后就神奇的好了。反正我没改也成功，不过我的子域委派的那台其实是从啦，和他在这点上不一样。 我要新建一台试试了，NND 去到父域上修改ns记录👇 上图说明缓存没清除。 systemctl restart named就行，或者用rndc flush 这不好了嘛，也没有修改那两项啊，父域和子域dns服务器上都没有修改啊。 注意缓存一直在，比如你现在修改子域的A记录为，加一条 然后子域DNS上reload 此时client dig结果不会变的 因为你问的还是父域，父域回你还是TTL时间内的缓存给你的，所以要flush一下 好啦。 dig -t NS查看NS 腾讯还不显示，直接自己查 qq的ns 根的 上图ns1是master，其他都是slave；同样可见版本号就很大了，也不是以年月日NO来编写的。 阿里还不错，按官方推荐来编写的版本号 以上就完成了子域创建和委派， 然后研究下53端口的tcp和udp的情况 先上结论，client查询一般就是udp 53，子域委派也就是域间也是只有UDP53；主从复制的时候是TCP53和UDP53都有。 当前client 去问 子域的www.shanghai.oneyearice.asia的解析情况，这样涉及父域和子域的情况，用iptables进行查看 1、打开firewalld 2、放行udp 53 3、OK 说明client和dns server只需要UDP 53， 4、进一步研究父域和子域之间的53 去子域开启firewalld 清父域的缓存 报错 5、打开子域的tcp 53 测试 依然不行， 查看firewall的抓包记录看是否命中TCP 53 https://developer.aliyun.com/article/1100649 https://www.onitroad.com/jc/linux/sec/firewall/firewalld-log.html 明确可见udp 53被拒了，但是tcp 53 没看到，所以 6、子域firewall改放行UDP 53 tcp干掉。 测试，竟然就可以了 只需要UDP53，啊哈哈 记得client 上一次解析成功了，要去父域上flush缓存的 这个firewall不像iptables可以看到命中报文，有点不爽，估计是有其他方法我没找到。 然后考察主从复制的53是哪种L4协议 去从服务器上 将日志挂在那，然后去主上编辑一下，记得修改版本号 1、只是修改了master的版本号 2、然后看到从服务器有拒绝日志 说明UDP53存在 3、继续修改master的记录+调大版本号 依然只有UDP 53 未见TCP53 放行防火墙的UDP53，同时用户解析也需要UDP的 client去dig下 发现named没开 开了，也要同步一下 从服务器上还是没有同步 看看日志 这里就看到日志，但是不清楚是 要主的TCP53还是从的TCP53 1、放行从的TCP 53看看，虽然我觉得是from 129嘛应该是主的TCP，但是，就像小学老师说的，我这个人明知的情况下就爱绕路 然后我就去主上放行了tcp 53 ，啊哈哈哈哈~ 等等，看看之前没同步的，能否再retry 坐等10分钟，不管是从服务来啦，还是10M重试，都差不多了，15分钟吧 要是不成功，这个版本号就太讨厌了。 可以了，看来它还是知道这个同步没成功，会拉也好，会重试也好，这个点需要继续验证的，否则业务处理故障会模糊的。 结论：主从：①从服务器需要访问主的tcp53和udp53；②主服务器需要访问从的udp53。 进一步测试 看来从服务器需要访问主的tcp和udp 53，都要访问的 从上的日志倒没有具体的UDP 和TCP的 字眼 放开主的tcp 上图没有提示了--在从服务器上，说明从服务器的消息是TCP的，正所谓TCP才会有一些完善的报错回显，可靠啊，而UDP正应了那句话，丢了就丢了。 坐等同步，因为此时从服务器上的版本号肯定是低的，因为之前没同步过去啊。 1M？还是10M，哈哈，有懵逼了，应该是1M没成功啊，刚才firewall调整的1M的拉肯定失败，不过1M不是周期的嘛，1M也应该重新拉了啊， 反正之前失败1M周期到了没有拉--从没有拉 然后等10M再看吧， 我就不加版本号。 👇有了大概过了也不到10分钟 最新的两个A记录都过来了 所以验证完毕 主要放开的 从要放开的 UDP要放开的原因，是同步之前需要有查询的动作，而查询就是和client去查询一样了，就是依靠UDP53。 确定了不一样之后，再利用TCP53去同步传递文件。--据说是这样，应该是正确的。 验证 动作1、在master上只修改版本号后rndc reload，也不用加rr，然后看提前挂载那边的tcpdump就能看见 ①先是udp，②后是tcp，前四行就是主获取从的全部记录，从获取主的全部记录，类似dig -t axfr 其实在message里有同步日志的：AXFR-style IXFR started AXFR就是ALL完全RR，IXFR就是增量 卧槽，还是明文的，我的乖乖，不小心被我发现一个漏洞。 总之 主从两头开抓包，效果杠杠的，记得带上v，vv就算了 tcpdump -v -nn -i eth0 host masterIP and slaveIP |grep -E '.53' 一下是一次完成记录 第二次的交互如下，明显比第一次要多，但是不要怀疑，第一次的解析确实同步了--虽然没有看到rr明细的传递，但是我再次测试dig 是确实过去了，而且，两次为一组，第一次抓包条目少，第二次多很多，不细究了。 然后是从上面 关于同步的进一步研究 动作1、仅修改zone 数据文件的RR --->修改zone 数据文件的RR， 但是不改版本号， 也不rndc reload ， 也不重启服务 坐等5Mins 时间还是在变，但是解析就是没过来 当然master上由于没有rncd reload所以也无法正确解析 动作2、master上rndc reload 完整的条件就变成了 ============= --->修改zone 数据文件的RR， 但是不改版本号， ---> rndc reload ， 也不重启服务 ============== master 自然更新了 10分钟过去了，还是没有更新，虽然文件的时间在变 动作3、master上修改版本号 完整的条件就变成了 ============= --->修改zone 数据文件， --->修改改版本号， 不做rndc reload ， 也不重启服务 ============== 从依然没有更新 重启从服务的named也没用 动作4、master上rndc reload 完整的条件就变成了 ============= --->修改zone 数据文件的RR， --->修改版本号， --->rndc reload ， 也不重启服务 ============== 秒同步 结论：不管是推还是拉，都需要2个动作： ①版本号增加↑②reload(重启服务自然也行咯)，至于你改不改RR，哈哈，无所谓反正2个必须要有。不改你也看不出来哈哈。 rndc工具介绍 上图可见query loggin is OFF 意味着查询日志没有开启 rndc querylog即可开启 这是临时开启，测试用 下面开始讲转发，也就是我要学bind的直接目的，不知道能否满足我啊，我可是要奇偶分开转发的哦，不行只能去研究nginx了，都一样，巴不得不行，正好跳到ningx，否则就慢慢腾腾的学了又，哈哈哈 转发听起来有点像NS哦，有点像子域委派，都是请求的转发出去，接着往下看，去看大腿 1、首先不要像也知道，子域委派--指的是 域相关的 请求 转移 2、而转发，必然是针对的请求，去做转移，而不再受限于 本域下的子域去NS，而是针对请求全面的细化的 转移。 1、本地的直接dns服务器，不能访问互联网，所以需要转发 类似案例如下 如果上海、成都的分公司机器的dns请求都从专线 去 问北京的DNS server，就很费专线带宽啦。 此时分公司的dns server可以搭建起来，只要请求过后就有缓存了，可以大大节省带宽 1、此处我用了大大，因为我在推销这种架构 2、不要听信我说的，以为缓存过期后，一样会请求出去，这个时候就有一个矛盾，缓存多久比价好， 3、1D还是1H，假设1H的$TTL，那么1小时候就会产生一波DNS 请求高潮，怎么地~语文老师教的好，所以1D的TTL是比较合适的， 以下👇4 、5 两点理解错误，主从，什么是主从，是针对本地的rr进行同步的，现在讨论的架构哪来的主从模式！ 都是针对互联网上的域名请求，北京本地没有记录，三地都是缓存，都是只缓存服务器啊，同步个啥哦！ 4、其实针对这个架构，我倒是希望北京是master，分公司部署从，这样分公司的请求压根不会到master上去，直接去本地的从就行了，DNS的流量也就是区域间的同步流量啦， 5、此时考虑的问题就变成了同步流量是否很大的问题，而同步 push势必频繁的，改一个就会推一个，pull也是希望能及时处理的，所以此时我想的这个架构就不合适哦，还是希望分公司为转发+缓存的模式 建立主从的概念，前提是你个有本地记录的服务器--区域数据库-区域数据都在互联网上呢，而现在讨论的必然是互联网上的请求，你只是个只缓存服务器。加上北京的说我确实有本地解析，那么4、5的分析就可以i上啦。结论也是 主从可能也不能这么玩。 6、抽象出来一个解决思路--宇宙法则之--专线+缓存才是王道，就是说如何节省专线带宽，主说：缓存。其实SDWAN的2个核心①选路②去重③缓存。 7、其实回过头来，看分公司的dns 缓存，我是这么想的，缓存为1万年，如果rr变了，就重新问一下这个记录，去覆盖老的缓存。这就是老话讲的好--爱你一万年。那怎么才能爱你就1年呢，就是分公司的dns请求AXFR的时候啊，不要请求具体内容，就是请求①整体rr的哈希②每条rr的哈希③哈希要短一点④cisco安全里讲过要给技巧就是将一些值也就是哈希放到报文结构里好像是tcp的seqnumber？ 8、分公司的用户访问xx.yy.zz，此时三地的DNS上就都有缓存了 然后我又继续往下看，发现这个转发不是我之前搜到的bind的view，view看这里就挺香的 https://blog.51cto.com/360admin/677254 然后man named.conf可见view有通配符字眼，一开始我搜regex没有心都凉了 可能是支持通配符的，下午研究研究 貌似他不支持通配符啊，下午敲一敲。 思路如上，正则我要改成这样，这样比对起来也会快很多。 算了，咱继续把转发学玩吧~ 由于都是中间间隔，的所以难免回不到之前的状态，也记不住，总之遇到问题就去查好了~ 用这台130进行转发，所以删除GW后，重启named，等价于缓存没了应该把，不放行再rndc flush 此时解析不成功 看见他是问根去了，所以看看zone的配置，是否有本地的区域数据 他是slave，那也不影响啊 哦，我知道了slave有一个自行惭愧的机制， 由于看不到data格式的内容，我要看soa信息，看下自行惭愧的时间是多少 dig 去看就好了 只能去主上面看了 查下这个 主的namde开机就没enable 还是报错是根的可达性 确实不通，不通你就不给client dns响应？不应该啊，本地有区域数据库的啊 这两个选项开了，dig 发现要等几秒钟才会有结果，然后就是SERVERFAIL。 这两个选项关掉，dig就发现秒出结果，但是是REFUSE，如下图， 感觉还是根的问题，奇怪了 哦，原来是我打错字了，asia不是aisa，唉，要注意睡眠了 其实都是好的 继续做转发实验 关闭192.168.126.130的网关， 哈哈 重启恢复继续 此时130变成 无法访问互联网了，当然可能涉及海外的根需要通的哦，呵呵 删默认之前OK的 此时130 需要转发请求给129去处理，发现最好还是关掉sec选项 让130转发到129 就好啦，与其好，sec也没关 然后 这样两台针对www.bing.com的只缓存dns就都没有缓存了，也无法访问互联网了 此时 然后将only改成first 敲了两次OK了 所以sec应该最好关掉。 配置就这一点点 ZONE特定区域转发 1、此情况 也就是说，130 only 到 129，129没有GW，缓存清空，自然解析不出来了 2、129打开GW 好了 3、将bing.com在zone里去转发 所以zone 指定了域名的转发，抢先了 比option里的先，127.0.0.1不变，本地GW打开 127.0.0.1不行？改成 好了 但是 说来也奇怪，nslookup 127.0.0.1也不行--当 zone里转发写成127.0.0.1的时候，转发不写127或者自己IP就没问题 问为什么写成 非自己就可以了？写自己就不行 1、自己问自己，不让这种写法，我初步判断，自己问自己，你就用first 不要用only 2、写非自己的IP，可以的原因，是比如192.168.126.131不可达，于是用了option里的转发配置到192.168.126.129了，此时你把129的named关掉就没啦。然后再把129的named开启，又可以了。 继续关掉129的named，让他ng，该130的first 不行，必须去改option全局里的转发方式为first才行。 所以我不清楚zone和option里的forward first 是怎么一个逻辑。 我如果设计的话 1、zone里的优先：forwarders的IP比全局里的优先 2、如果zone里的转发ip 失败，是不是就直接用本地出去问呢，不是还有全局的转发，要去问全局的。 3、然后全局里的转发IP失败，再回来看zone里的是不是又first 所以此时改一下测试 然后129 关闭129的named，然后130的flush 解析NG 然后开启130的GW 就好了 看来zone里的only没生效，我估计zone里的forward first|only 不生效！ BIND9的VIEW也是转发 直接上需求，将奇数IP的dns请求转发到10.3，偶数的转发给10.2 开搞，搞不了，BIND view里的acl不支持正则，思路换到智能DNS里吧。看看那篇有什么新的东西，至少常见的基于ISP的归属地的，各种智能DNS不都在商用吗，我不信一个奇偶数还不能做出来了， 当然这里的VIEW也能做，就是把acl add_ips和acl eve_ips里写满所有的私网网段的奇数，和偶数。当然eve_ips可以! add_ips 直接取反。 acl odd_ips { # 匹配奇数IP地址 !acl even_ips; }; 1、配置主备好 vim /etc/named.conf ------------------------------ acl odd_ips { stdlib.regex(\"/(\\d{1,3}\\.){3}\\d{0,2}[13579]/\"); # 可惜这种chatGPT的一厢情愿，然而BIND并不支持 }; acl even_ips { stdlib.regex(\"/(\\d{1,3}\\.){3}\\d{0,2}[02468]/\"); }; view \"odd_ips\" { match-clients { odd_ips; }; match-destinations { any; }; recursion yes; forwarders { 192.168.126.129; } }; view \"even_ips\" { match-clients { even_ips; }; forwarders { 192.168.126.130; } }; 毛啊~更不不支持regex，唯一找到的信息是bind的一条漏洞 https://www.tenable.com/plugins/nessus/65736 这里支持这种，这是zone数据库里支持的写法 就是不支持regex https://serverfault.com/questions/133707/is-it-possible-to-have-regular-expression-cname-record-in-dns Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:44 "},"22-DNS服务和BIND/6-CDN和GSLB工作原理及智能DNS实现.html":{"url":"22-DNS服务和BIND/6-CDN和GSLB工作原理及智能DNS实现.html","title":"第6节 CDN和GSLB工作原理及智能DNS实现","keywords":"","body":"第6节. CDN和GSLB工作原理及智能DNS实现 1、CDN 开局一张图，后面接着编..........................................................................................................................................写 请把CMD里的dig CNAME的信息画到上面的逻辑交互图上去，你就会发现，早期的图里有一个错误 而现在的文档，语文老师估计水平也不行 1、站在用户角度来讲(其实我不喜欢这里我的用词)，应该改成用户请求域名的IP地址，从浏览器打开www.bing.com，F12可见，或者ping www.bing.com 可见就是一个IP啊，只不过dig nlsookup给你看到了交互的信息。 那么为什么cmd里可以看到 LOCAL DNS处理的CNAME呢，用户只有1 6 7 8 理论上用户不知道中间的CNAME啊， 那么肯定是7的内容里带上了2 3 4 5 记录，但是不全，因为你看不到DNS调度系统的IP地址， 难不成是这个NS？ 这个ns应该不是dns调度系统的IP。而作为LOCALDNS一定是知道调度系统的IP的，但是cmd里看不到，说明CMD的信息是报文里携带得到的。 正儿八经CMD里的信息，终端使用的其实就一个第一个问的域名得到的IP地址。 这话也不对，LOCAL DNS他应该不是指的企业内部的，而是ISP的或者是223，114这些公共的LOCAL DNS也就是网卡上能配上去的。这点不纠结，算他没毛病。然后LOCAL DNS请求的看来不是13个根，而是直接抵达权威DNS，这点就和我之前理解的少了很多，我理解是迭代那套，想想也对，LOCAL DNS不可能每次都去国外13个根绕一圈，所以LOCAL DNS肯定直接问的权威--也就是你自己发布的解析--在哪里配置的就是哪里。 所以： 1、CNAME一条记录里就有了两个信息，①LOCAL DNS 询问对象--阿里的DNS调度系统；②LOCAL DNS询问内容--CNAME的域名。 2、而常规的DNS迭代查询，LOCAL DNS询问对象都是固定的，比如先问根，根说你去找.com，.com说你去找bing.com，这一套的①询问对象迭代出来的②询问内容就是域名啦，这两个信息是分开来的。 ​ 就是CNAME里询问，你看不到权威DNS说LOCAL DNS你去DNS调度系统，没有这个独立的动作，它是直接给你一个CNAME，比如cn-bing-com.cn.a-0001.a-msedge.net. 这个就明确的说明了是谁谁谁的调度系统，已经要问这个调度系统的域名(CNAME本身就是一个域名) 废话--就是常规的套路--拿到cn-bing-com.cn.a-0001.a-msedge.net.后，就会找.a-0001.a-msedge.net. 而这个域里自然有智能调度出来的cn-bing-com.cn.a-0001.a-msedge.net. 的IP地址。 3、实际操作LOCALDNS猜测可能直接询问权威。 GLSB 图片还是两家都更新了，^_^；阿里的风格变了，立项了？可能，腾讯基本没变。不过隐藏了GLSB关键信息。 关于\"DNS的调度系统\"，这里弱化为BIND的智能DNS，其中就涉及VIEW和ACL的使用 题外话：acl：路由器交换机里的acl、linux文件的fileacl、还有这里的dns的acl；其实acl叫access list，所以都是过滤而已。 实验-智能DNS 然后创建ACL 推荐写到named.conf的options之前，便于后面调用，匹配是顺序匹配的，所以小网段写到前面。 编写view 写了view后zone文件就都得放入view视图里， 1、将文尾的include \"/etc/named.rfc1912.zones.bj文件放入view里，并分别命令区分，稍后要创建这些文件。 2、将原先的根域的配置段👇删除后，合并到上面view里的各个zone文件里。 zone \".\" IN { type hint; file \"named.ca\"; }; +易犯错误1>> acl名称这里调用别写错，复制过来 创建编写好各个view指向的zones配置文件 [root@django001 ~]# cp -a /etc/named.rfc1912.zones /etc/named.rfc1912.zones.bj [root@django001 ~]# cp -a /etc/named.rfc1912.zones /etc/named.rfc1912.zones.sh +易犯错误2>> view里的include的文件路径和名称对应的文件要创建好， +易犯错误3>> 对应的zone定义文件的权限也要注意 将根的zone配置段，从/etc/named.conf里复制进这些区域文件 y 思考，zone里的顺序问题，恩，思考，先把上面的other后面的;分号补上。 +易犯错误4>> 格式要注意，分号会频繁出现，类似 python里的并发args传参的写法，最后一个参数也要后跟一个分号。 创建编写好各个zone数据库文件 +易犯错误5>>注意zone数据文件通常是vim直接创建，所以权限必然不对，需要改。 奇怪了，不奇怪！因为配置文件里没有语法错误，知识named.conf的VIEW里的路径---zone定义文件的路径----zone数据库文件名称，没有对应上。 哪里没有终结，代码块的东西 奇怪 发现named.conf里写的名字不对 反正各种名称路径写错，真TM操蛋 又把asia写成了aisa。 测试下 这个是外面宿主机直接cmd 去dig的，源IP为192.168.126.1，命中的acl是othernet： 192.168.126.131命中acl shanghai，通过zone配置文件指向到了 zone数据库结果正确 再来看192.168.126.130，发现有点问题 排错1： 排错2： 写错了，老是这样，操 OK了 这就实现了源IP地址的分流，所谓的归属地的查询--智能DNS多少就有这个影子在里面，至于是具体是不是写了很多IP段在里面就不得而知了，我尝试过acl里写正则，都失败了，也没有找到相关资料。 测试匹配次序 这样三个都是beijing acl索引过去的zone数据库文件了 汇总： 1、主配置文件 vim /etc/named.conf acl beijingnet { 192.168.126.0/24; }; acl shanghainet { 192.168.126.131; }; acl othernet { any; }; # 删除zone . IN 根的那段，将include \"/etc/named.rfc1912.zones同样删除， view view_beijing { match-clients {beijingnet;}; include \"/etc/named.rfc1912.zones.bj\"; }; view view_shanghai { match-clients {shanghainet;}; include \"/etc/named.rfc1912.zones.sh\"; }; view view_other { match-clients {othernet;}; include \"/etc/named.rfc1912.zones\"; }; 2、区域配置文件 vim /etc/named.rfc1912.zones zone \".\" IN { type hint; file \"named.ca\"; }; zone \"oneyearice.asia\" { type master; file \"oneyearice.asia.zone.other\"; }; vim /etc/named.rfc1912.zones.sh zone \".\" IN { type hint; file \"named.ca\"; }; zone \"oneyearice.asia\" { type master; file \"oneyearice.asia.zone.sh\"; }; vim /etc/named.rfc1912.zones.bj zone \".\" IN { type hint; file \"named.ca\"; }; zone \"oneyearice.asia\" { type master; file \"oneyearice.asia.zone.bj\"; }; 3、编写zone数据库文件 vim /var/named/oneyearice.asia.zone.other $TTL 86400 @ IN SOA ns1 admin ( 2023022150 1M 10M 12H 1 ) NS ns1 ns1 A 192.168.126.129 websrv A 10.2.1.100 websrv A 10.2.1.101 www CNAME websrv vim /var/named/oneyearice.asia.zone.sh $TTL 1D @ IN SOA vip1 oneyearice.126.com. ( 20220227 1H 10M 12H 1H ) NS vip1 vip1 A 192.168.126.129 www A 192.168.100.150 vim /var/named.oneyearice.asia.zone.bj $TTL 1D @ IN SOA vip1 oneyearice.126.com. ( 20220227 1H 10M 12H 1H ) NS vip1 vip1 A 192.168.126.129 www A 192.168.100.100 4、 amed-checkconf named-checkzone oneyearice.asia /var/named/oneyearice.asia.zone.other named-checkzone oneyearice.asia /var/named/oneyearice.asia.zone.bj named-checkzone oneyearice.asia /var/named/oneyearice.asia.zone.sh 同样别人的配置来一份： 学习了这个VIEW也就是智能dns，还需要再次知道一下，这东西是落在哪里的，是落在CNAME过来的DNS调度系统上的，如下图 尝试做一个完整版的， 原来的智能DNS修改一下 cnd的域名后缀修改为tbcdn.com也就是www.oneyearice.tcdn.com 而www.oneyearice就是tcdn.com的子域，可以建一个下级域 前面顶一个权威DNS NXDOMAIN就是$minum缓存时间里的查不到记录的结果，NX就是Not eXist DNS本身的负载均衡效果 这是ping 两个解析后的IP都不通，所以看不出来DNS是否有默认 健康性检查的效果，改成一个通一个不通的情况测试 发现全是129通的那个ip DNS虽然解析是由100不通的那个IP的，但是这里的ping以及包括CURL的测试，进一步包括浏览器打开都是不会负载到不通的那个IP的。 做成负载的两个IP都通的情况，测试 发现统统变成了131 改回只有一个129通 rndc reload后发现，又只有129了 再次改成都不通，又轮询负载均衡了 要查资料了 这次看到了不一样的效果，.1其实是我的宿主，通的，但是icmp被防火墙干掉了，ping是NG的。 再131上ping ，130上firewall firewall-cmd --permanent --add-icmp-block-inversion 禁掉了ping，结果还是给我一直解析的是130. ping测试负载没有健康检测，而且负载轮询不可靠 再看看http的情况 并没有负载 然后129好的130好了就抢先了， 停掉130，129就回来了，是有健康检测机制的 关于DNS的rr轮询查资料： https://blog.csdn.net/zhu_tianwei/article/details/45071085 还得看官网 https://bind9.readthedocs.io/en/v9_19_9/reference.html 1、默认行为 轮询，通过nslookup可见每次的顺序是轮询的， 通过顺序不一样，来引导客户端自己选择不同， 所以决策者还是client端。 2、人工干预 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:45 "},"22-DNS服务和BIND/7-实现internet架构和DNS服务.html":{"url":"22-DNS服务和BIND/7-实现internet架构和DNS服务.html","title":"第7节 实现internet架构和DNS服务","keywords":"","body":"第7节. 实现internet架构和DNS服务 拓扑 搭建权威dns的主从 67作为httpd server，提供一个测试页 测试OK 47上安装bind作为主MASTER 注释掉listen-和allow- 添加只允许从服务IP来抓取信息（-t axfr） 57上配置从SLAVE 同样yum -y install bind 两行注释 一行拒绝所有 zone数据库文件从上面就无需创建了，重启同步过来就行了。 测试主从解析情况 client的dns指一下主从 看下是否生效--也就是/etc/resovle.conf里自动写进入了 测试解析OK 测试主从复制 slave上文件日期已经变了 37配置成.com域 属于父域的知识应用 在zone数据库里配置子域 magedu是com的子域，被委派给了ns2和ns3，分别对应两个ip地址。 测试下，委派成功👇 27作为根 这里是自己实验里自建根，所以原来的根zone改一下 chgrp named /var/named/root.zone chmod 640 /var/named/root.zone 测试 17作为forwardDNS 修改默认的13个根的zone数据库文件 只需要写一行：27就是实验的根 测试下，失败： 关闭安全选项 然后就OK了 7作为LOCALDNS 安全关了 测试OK👇 测试curl之前改一下默认的dns OK Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:45 "},"23-MYSQL数据01/23-MYSQL数据01.html":{"url":"23-MYSQL数据01/23-MYSQL数据01.html","title":"第二十三章 MYSQL数据01","keywords":"","body":"第二十三章 MYSQL数据01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:45 "},"23-MYSQL数据01/1-数据库基础原理1.html":{"url":"23-MYSQL数据01/1-数据库基础原理1.html","title":"第1节 数据库基础原理1","keywords":"","body":"第1节. 数据库基础原理1 开局一张图： DBMS server:比如mysql-server；DBMS client：比如mysql-client tidb：天生分布式，兼容mysql，程序可能无需改动。 mysql 这些库里： 一行row就是一条记录record， 一列columm就是各种属性的数值 null表示该字段未空，没有赋值，pk主键是不能为null的 ... ... Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:45 "},"23-MYSQL数据01/2-数据库基础原理2.html":{"url":"23-MYSQL数据01/2-数据库基础原理2.html","title":"第2节 数据库基础原理2","keywords":"","body":"第2节. 数据库基础原理2 外键：一对多 上图100是外键FK，如果在外部另一张表里没找到100，就不让你添加。 一对多可以使用主外键的方式来表示出来。 多对多：第三张表 构建第三张表来实现多对多，但是第三张表里的stu_id和class_id不能是凭空写出来的，下图的200和100就不对了 所以需要使用FK外键 通过第三张表，通过主外键的关系，把原来两张表的多对多关系表达出来。 范式 一般掌握前三范式，否则范式遵循太多也会导致数据的查询麻烦。 第一范式： 下图就违反了第一范式： 1、列必须是不同类型 因为mage可能教10门课，而其他的某位老师可能就教1门课，所以其他字段在这一行就都是空。 2、列不能有多个值 可以另起一行，不过也不太好，如果mage教了10门课，mage的个人信息就会重复10行。 为了解决上述单列多个值和单人重复多行的问题，于是可以构建第二张表和第三张表。 第三张表就是FK外键使用上面两张表的PK主键就行了。 第二范式2NF， 首先2NF肯定满足1NF了 下图的PK在那个字段上都不合适，name、city都不存在唯一性 此时需要设置复合主键PK 此时code电话区号依赖于city，现在主键不是city而是name+city这个复合PK。2NF范式2就要求每列的字段也就是属性要与整个PK有直接相关性。只是和部分主键有关联，这是不符合2NF的。 怎么解决呢，方法就是把城市和区号拿出来单独列一张表。然后原表里删除code和city，改为city ID来关联。👇 现在在第一张表(左)里整个属性都是和整个复合主键直接相关的。符合了2NF第二范式。 上文是使用了复合主键来做唯一标识，还可以重新做一个主键 此时就一张表，code虽然存在多行重复出现的问题，不过code和id本身区别也不大。可以接收。 真正的问题是这就违反了3NF第三范式，当然你要是无所谓打破范式也不是不行。 3NF就是属性不能和非主键产生关系 非PK字段之间不能有从属关系 解决方法就是将从属关系的字段，独立出来一张表。 满足1NF 2NF 3NF这三个范式后的好处是数据存放紧凑，没有什么重复冗长多余的情况。 带来的负面影响就是：数据规划要花时间、查询只需要一张表就能查到了，无需跨表查询。跨表查询有一定复杂度和时间的消耗。 3个范式就是推荐建议，范式就是用来违反的，具体情况具体解决。 设计DB的一般是软件开发人员。他们来设计结构。DBA应该可以吧 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:45 "},"23-MYSQL数据01/3-mysql安装和基本使用.html":{"url":"23-MYSQL数据01/3-mysql安装和基本使用.html","title":"第3节 mysql安装和基本使用","keywords":"","body":"第3节. mysql安装和基本使用 基本概念 数据观察角度 1、物理层：这点是运维关注的，磁盘上怎么保存这些文件，将来性能更好。磁盘文件、分区、LVM、raid 2、逻辑层：数据库里存放的哪些数据，数据和数据之间的关系；表之间的关系咯，一对一，一对多，多对多。 3、视图层：用户看到的结果，比如电商网页上看到的页面上的商品价格，但是看不全，比如进货价用户必然看不到。哈哈 运维人员可能更多是关注 物理层和逻辑层，开发更多关注视图层。 ORM，对象关系模型 将面向对象的语言比如PY，和关系型数据库联系在一起 说人话就是，SQL语句你可能就是学一下，后面用python调用库去处理DB的时候都是面向对象的语句，就是比如xxx.value()，这种CLI命令行封装成了python的CLASS调用了。 Mysql安装 1、yum源安装 一般测试环境要求不高的安装方法 2、源码编译安装 3、二进制安装 企业更多是使用2、3两种方式安装 yum -y install mariadb-server rpm -ql mariadb-server 其中 /etc/my.cnf.d/mariadb-server.cnf是配置文件 /var/lib/mysql是数据库存放的路径 /usr/lib/systemd/system/mariadb.service这是服务 mariadb-server是一个单进程多线程的程序，mysql一样 花括号的就是线程 1、yum源安装特定版本 https://mariadb.org/download/?t=repo-config&d=CentOS+Stream&v=10.11&r_m=neusoft 新建repo文件，复制后，yum repolist看下 安装的时候大小写注意下 安装的默认行为，server顺带就安装client了，老的一些lib库会自动升级 rpm -ql MariaDB-server 查询注意大小写 client 包也安装好了 之前的版本 我的报错 上图报错是因为之前安装过并启动过，解决方法就是删除/var/lib/mysql文件夹，重新安装mariadb-server就好了 这样就可以了 可以做安全加固，后补 进入mysql的交互模式后的常用cli 查看帮助 查看状态 user: root@localhost mysql的用户账号是带client端主机地址的，所你同一个用户在机器A上root进来，不代表同样一个用户在机器B上root就进得来。 mysql 服务端的连接，走的是socket，一种是tcp/ip，一种是UNIX socket也就是文件形式--这种就是同一台主机上的不同进程之间通信； 现在client和server都是在一台linux上的，所以没必要走tcp/ip网络socket套接字，直接使用文件套接字就行，节省报文开销。 利用该文件实现了两个进程的通信，client和server 其实上文的mysql回车就是没有跨网络就是走的文件socket的 none就是进来的时候不在任何一个DB里的 默认安装mysql后，默认就自动创建多个数据库。 数据库的概念 和 实例instance 的概念 mysql Ver 15.1 Distrib 10.11.2-MariaDB, for Linux (x86_64) using EditLine wrapper 10.11.2就是版本，不同版本安装在同一个linux就是实例了；可以一台机器上安装多个mysql的版本，错开监听端口，这就叫多实例；一般这样用的不太多，测试环境中使用下。mysql属于比较重的应用 数据的访问压力较大，所以不会使用多实例。 这就是默认安装后生成的数据库 然后msyql这个默认的库里有哪些表 同样在cli里看 mysql是系统数据库。 用户后面自建创建的数据库，也会在/var/lib/mysql里生成自己的文件夹。 可以认为msyql里所谓数据库就是文件夹，然后在文件夹里有若干个表。 看下表的文件，每个表里有3个文件 比如视频版本里的 user.MYD就是MyISAM的引擎 user.MYI就是innoDB的引擎 现在的版本好像没有这么多引擎了 cli命令行 分为client端cli和server端cli 刚才myql进入的\\h看到的就是client命令，是没show的，show是server cli show 这个命令，client上是没有的，是client端发送给server，在server上执行的。 注意：客户端命令，只能是用mysql-cllient工具也就是mysql命令进入才会有的，换个客户端工具就没有上图这些命令。 而服务端命令，不管是什么客户端，只要能连到server上来，就能执行的。 数据库的命令和客户端的命令 表现形式上一个很大的不同，就是行尾带不带分号 server cli要求加分号 而客户端命令比如status不用加分号 然后通过show databases;可见有5个默认库，但是在/var/lib/mysql里只有4个，少了一个information_schema库文件 然后比早期版本多了一个sys库 information_schema不是磁盘文件数据库类型，是内存中的。 test 测试的，可删 performance_scheme放了数据库性能相关的数据 进一步学习如何查看当前有哪些用户， 通过上文知道在/var/lib/mysql/mysq/下面有一个user.frm表文件，可以进入数据库里查看该表内容 这个表列特别多，不可能一下都展现出来。需要赛选 不进入的查表: 进入数据库好比cd进入文件夹 其中就有user表 查看表结构和列 除了select * from user;以外还可以看有哪些列： Field就是域，就是user表的列，就是属性 Null里的NO，就是不允许为空 PRI就是主键，而且如果两个都是PRI就是复合组件，低版本的mariadb里还是看得到的 host+user组合起来成为了一个主键。 版本差异看下也 只看3个字段，的专业术语叫做投影，哈哈了解一下 所以python打开文件，读取文件，设计open，close就可能没有db来的专业方便。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:45 "},"23-MYSQL数据01/4-mysql基本使用.html":{"url":"23-MYSQL数据01/4-mysql基本使用.html","title":"第4节 mysql基本使用","keywords":"","body":"第4节. mysql基本使用 空的意思是用户名随便写，上图👆框错了哦，user为空的是最后两行，PUBLIC本身就是用户名 空用户进来的权限👆 进行安全加固 这是我之前用直接yum的mariadb，是有安全加固脚本的 但是新的mariadb版本改动了一下，👇 https://mariadb.com/kb/en/mysql_secure_installation/ 结合文章，看来软连接就是过渡一下用的，现在的新版本呢都没有mysql_secue_installation这个软连接文件了。 https://mariadb.com/kb/en/authentication-from-mariadb-104/ 文中说“Remember, the best way to keep your password safe is not to have one!” 服了U~ 回到这里学习的安全加固脚本： 跑一下 10.11版的情况 视频中的低版本👇，主要区别是swith to unix_socket authentication吧，反正上面走完root密码是不生效的，因为有插件要改一下。emm不过，人家官网说了没有密码就是最安全的密码。 脚本到这里已经起作用了，要密码登入了，当然我说的是老版本 这个centos7.localdomain是禁止root远程登入干掉的，因为它担心有人使用域名解析将cento7.localdomain解析成远端mysql的IP地址，从而使用这个hosts进来。 回到我的新版mariadb 密码已经改好了，如果非要使用root密码生效，则需要 https://blog.csdn.net/tiny_du/article/details/123924376 https://www.orcy.net.cn/1410.html 修改global_priv里的unix_socket ALTER USER root@localhost IDENTIFIED VIA mysql_native_password USING PASSWORD(\"你的密码\"); 文中的这个方法，记不清我之前是不是直接修改了global_priv里的内容。 ALTER USER root@localhost IDENTIFIED VIA mysql_native_password USING PASSWORD(\"xxxxx\"); 然后再看 我再找找文章 https://blog.whsir.com/post-6795.html 这个靠谱，在配置文件中禁用unix_socket 不过是不是可以直接修改gloabl_priv里的参数的，好像没有唉，👇官网也是这个方法： https://mariadb.com/kb/en/authentication-plugin-unix-socket/ 然后\\G的排版看下 -uroot不写也行，默认就是root，你换linux的用户，也一样，Pia!(ｏ ‵-′)ノ”(ノ﹏ 一样个毛👇 Pia!(ｏ ‵-′)ノ”(ノ﹏ mysql -uroot -pxxx -h a.b.c.d 这个是完全命令 注意-h 127.0.0.1和-h localhost不一样，前者是走的tcp/ip socket，后者是走的unix_socket文件socket 下图虽然是本地就是192.168.126.129自己登自己，但是还是判定为root@django001，解析成了hostname所以不对了。 尝试改host为192.168.126.129看看，果然被反向解析成了hostname了 GRANT ALL PRIVILEGES ON *.* TO 'root'@'192.168.126.129' IDENTIFIED BY 'Cisc0@123' WITH GRANT OPTION; 然后将192.168.126.129改成hostname也就是django001看看，不行，就是IP，虽然显示的是root@django001，但是只是反向解析的结果，只认IP。 报错的情况：update不行？ https://stackoverflow.com/questions/64841185/error-1356-hy000-view-mysql-user-references-invalid-tables-or-columns-o ALTER USER root@localhost IDENTIFIED VIA mysql_native_password USING PASSWORD(\"xxxxx\"); 或者使用SETPASSWORD https://mariadb.com/kb/en/set-password/ Example For example, if you had an entry with User and Host column values of 'bob' and '%.loc.gov', you would write the statement like this: SET PASSWORD FOR 'bob'@'%.loc.gov' = PASSWORD('newpass'); If you want to delete a password for a user, you would do: SET PASSWORD FOR 'bob'@localhost = PASSWORD(\"\"); 查看的细节 新版本就是-server，-client这种 老版本就是 这个就是client mysql-client里的mysqldump很关键 mysqladmin的用法 -？等价于--help 检测mysql服务是否正常 远程测试要密码和端口： 超时时间要设置下的 实测是2m的超时时间 调整探测时间上限方法就是--connect-timeout https://dev.mysql.com/doc/refman/8.0/en/mysqladmin.html 但是密码要想办法隐藏好，否则监控存在密码泄露风险。 关闭服务： 数据的进程，一定不要用kill来杀，一定不要！！！，杀完就可能起不来了。 改口令 以什么身份user@host 连进去的，就改的谁的密码。 还可以直接创建数据库 不进入交互式的cli方法 mysql -e \"show databases\" mysql进入是交互模式，也就是有标准输入的，所以重定向文件内容进去就行了 表格式没了 mysqld_safe 是mysqld的父进程，上图是通过ps auxf看的 pstree -p可见 新版本没有这个safe父进程了 进程是以mysql这个用户运行的。 而该用户是安装程序的时候脚本创建的。 后面源码编译，二进制安装的时候，需要手动创建用户 多实例，也可以同一个版本来做 rpm -ql MariaDB-server可见 端口错开，用户透明，测试环境用 用户账号 _就是等价于regex里的?，可能吧 疑问：172.16.%.%，%匹配任意长度字符，那为啥不写成172.16.% 修改mysql交互模式的提示符 man手册里看 进到/etc/my.conf.d下面，可以看到client.cnf和mysql-clients.conf等文件 打开mysql-clients.conf进行编辑 生效👇 数据库的路径，可以改成LVM里，优点就是可以伸缩空间。 上图还有socket文件的路径，就是mysql -uroot -pxxx 回车走的文件socket 这是日志，进程ID文件 自动补齐 看到没有带任何默认值👇 修改一个选项，然后看下 说明还是可以补的，好吧， 如果源码安装，将来上文提到的路径，就需要自己定义了，包括 1、socket路径 2、数据库路径 3、log日志路径 等 直接连进数据库 -D 可省略 -C 压缩可节省带宽 -e 上文讲过了 -V 版本 -v 明细 上文也做个实验了，127.0.0.1不是走的unix sock而是走的TCP/IP 查看当前用户👇 查看当前版本 注意哦，我这是自己的10.11和视频的5.5混着截图了，意思到了就行 大小写-库名、表名区分大小写，命令和字段不区分大小写 命令不区分大小写 数据名称区分大小写 表名大小写敏感 表里字段也就是列属性不区分大小写 推荐SELECT这种命令也是大写 删库的方式又增加了 配置文件也可以合在一起 通过名称区分 这些配置内容的格式就是上图说的 parameter = value 而value的启用和禁用，又可以有1 ON TRUE等写法如上图所示。 如图配置文件里是不区分_和-的 配置文件路径 优先级上图的从上往下，但是不同的安装方式，优先级的结论不同 不要可以记这些，就是实际使用，配置文件就放一个地方，避免混淆。 维护模式 上图=1可以省略不写，就是=1了 skip-networking=1就关闭了3306，相当于维护模式，外界就连不进来了。 vim /etc/my.conf 再看下端口号就不在了 但是自己可以连 因为走的是unix_socket 配一个配置文件路径 /etc/mysql这个路劲是默认没有的，没有就创建一个 和刚才的配置相反了 重启服务看下 说明/etc/my.cnf是优于/etc/mysql/my.cnf的 二进制安装 二进制安装，配置文件/etc下的XX、用户账号、DB文件 都没有 指定了家目录，就是用来放DB文件的，而/data将来推荐做成LVM 同样的所有者和所有组参考原来的DB文件设置下 文件名有linux字样的的就是二进制的包，大小也不一样，二进制的编译后的肯定大很多。 注意，有个安装路径是固定的，人家编译成二进制的时候就写死了 /usr/local要手动补上的，当初人家configure 编译的时候指定的类路径。 /usr/local/mysql就是当初人家编译的时候指定的路径 这里就是一些mysql的程序 配置文件 二进制安装后有一个模板my-large.cnf复制过来用，然后为了不和/etc/my.cnf冲突，就另起炉灶放到新建的/etc/mysql文件夹下 这就是计划数据的路径，下两行再说是优化的 二进制安装后DB文件时空的，需要生成出来，mysql、test、performance_scheme这些 yum安装其实也是依赖这个脚本生成的 centos7上也能用这个service xxx start，可以参考现在的做成systemctl start xxx的service文件。yum安装的mysql里service文件写法看看。 PATH变量 初始化 源码包编译安装 和二进制差别不大，就是多一步编译 tar解压 cmake编译安装，以前我们都是用configure来弄，这里推荐用cmake，原来的configure make makeinstall也能用，不推荐 支持很多存储引擎，不代表就用得到。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:46 "},"23-MYSQL数据01/5-mysql二进制和源码编译安装及多实例.html":{"url":"23-MYSQL数据01/5-mysql二进制和源码编译安装及多实例.html","title":"第5节 mysql二进制和源码编译安装及多实例","keywords":"","body":"第5节. mysql二进制和源码编译安装及多实例 二进制安装 MSI格式点击安装就好 一、创建账号以及数据库的目录 确定没有使用该账号，再创建 (1) 准备用户 groupadd -r -g 306 mysql # 没必要，删掉这句，本身useradd -r 就会创建同名组，id都是 -r系统用户了，-d就不会创建家目录了，所以没必要-d带上目录， 所以创建账号优化为 useradd -r mysql mkdir /data/mysql chown mysql.mysql /data/mysql 或者 useradd -r -s /sbin/nologin -m -d /data/mysql mysql # -m就是-r的情况下不会-d所以就-m cd /data/mysql rm -rf /data/mysql/.* # 删除家目录里的隐藏文件，否则会有问题 然后这里强调下 useradd -s /bin/false 和/sbin/nologin一个意思，不过false的本质还是 你给了用户/bin/false 的shell后，用户登入使用这个/bin/false后发现返回的是false是错误，于是就不会继续登入了。 二、解压二进制和创建软连接以及文件权限 注意/usr/local路径是固定的，当初官方编译的时候指定的 解压后就得到带版本号的文件夹，大小从600M+变成了1.5G 但是人家编译的时候的路径是/usr/local/mysql，所以创建一个软连接 权限的问题 解释：加不加斜线 1、chown -R root.root mysql # msql是软连接文件，-R也是不认为是个文件夹，不存在递归、 2、chown -R root.root mysql/ # 这样就会判定为文件夹，-R就会做递归了 这里就完成了二进制文件的解压，得到了一堆mysql程序。然后考虑使用这些二进制文件生成DB文件。 三、生成数据库文件 现在/data/mysql里是空的 关注这两个文件夹 /usr/local/msyql/scripts里的mysql_install_db是用来创建数据库文件 创建的时候要指明，在哪个目录下生成DB文件，并这些文件属于哪个用户 执行的时候报错了 解决方法是到/usr/local/mysql下使用相对路径执行 这就是安全加固的指令👆运行完后，就生成了DB文件👇 配置文件一般系统自带就有，也要看什么系统呢，如果是最小化rocky-linux就没有 可见该/etc/my.cnf文件不是二进制安装得来的。 四、从二进制安装包里找到配置文件推荐模板复制到常用配置文件路径下 这些参数就是老文件保留下来的，现在不会给这么小的内存。 和原来的/etc/my.cnf区别开来，放到/etc/mysql/my.cnf👆 修改/etc/mysql/my.cnf里的datadir为之前指定的/data/mysql，其他不用动 五、启动服务 由于是二进制安装的，启动靠脚本 脚本里有start status stop这些动作的 设置为开机启动了已经👆 然后启动服务 六、客户端连接 修改一下PATH路径，便于使用mysql命令 PATH变量写到独立的/etc/profile.d/mysql.sh里 此时就可以连接mysqld了 七、开头提到的家目录里的隐藏文件的问题 如果当初创建用户使用-m -d生成了家目录来作为DB文件路径，然后又没有删除里面的隐藏文件夹，就会遇到问题 删掉就好了 system是在mysql交互界面里调用linux系统命令👆，当然这里仅仅是演示知识点，工作中千万不要这么操作，太危险了，数据库文件都在那呢。 rm -rf .[^.]* 这是删除隐藏文件的准确方法。 八、安全加固 以上就是二进制安装方法的详情👆 源码编译安装 考虑到数据库后面可能需要扩容，所以这次使用逻辑卷。 分区 注意上图👆error 需要同步一下 创建物理卷并加入卷组 加组的时候，指定单个PE大小为16M，将来空间就是16M，16M地分出去的。 创建逻辑卷 磁盘空间用光100%free 挂载文件夹 创建文件夹，记得格式化LVM 持续挂载👆 查看确认下 修改文件夹属性 创建账号先 -------------开始源码编译------------ 安装依赖包👆 yum install bison bison-devel zlib-devel libcurl-devel libarchive-devel boost- devel gcc gcc-c++ cmake ncurses-devel gnutls-devel libxml2-devel openssl- devel libevent-devel libaio-devel 解压缩源码压缩包并进行编译 编译采用cmake而不是configure预编译，记得CPU给够，否则待会make编译的时候耗时更长； cd mariadb-10.2.18/ cmake . \\ -DCMAKE_INSTALL_PREFIX=/app/mysql \\ # 这个是二进制程序安装路径 -DMYSQL_DATADIR=/data/mysql/ \\ # 这是数据库文件存放路径 -DSYSCONFDIR=/etc/ \\ -DMYSQL_USER=mysql \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_ARCHIVE_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITH_PARTITION_STORAGE_ENGINE=1 \\ -DWITHOUT_MROONGA_STORAGE_ENGINE=1 \\ -DWITH_DEBUG=0 \\ -DWITH_READLINE=1 \\ -DWITH_SSL=system \\ -DWITH_ZLIB=system \\ -DWITH_LIBWRAP=0 \\ -DENABLED_LOCAL_INFILE=1 \\ -DMYSQL_UNIX_ADDR=/data/mysql/mysql.sock \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci make && make install 提示：如果出错，执行rm -f CMakeCache.txt make -j 16 && make install && echo -e '\\a' && date 后续安装和二进制安装一样👇 准备环境变量 echo 'PATH=/app/mysql/bin:$PATH' > /etc/profile.d/mysql.sh . /etc/profile.d/mysql.sh 生成数据库文件 cd /app/mysql/ scripts/mysql_install_db --datadir=/data/mysql/ --user=mysql 准备配置文件 cp /app/mysql/support-files/my-huge.cnf /etc/my.cnf [mysqld]中添加三个选项： datadir = /data/mysql innodb_file_per_table = on skip_name_resolve = on 禁止主机名解析，建议使用 准备启动脚本 cp /app/mysql/support-files/mysql.server /etc/init.d/mysqld 启动服务 chkconfig --add mysqld ;service mysqld start 记录编译安装遇到的问题 多实例mysql 其实也可以直接用docker咯，这里先不管容器的事 不管是哪种，端口号必须分开来 开始操作，使用的是yum安装的mariadb进行实验 当前单个实例的情况 1、端口一个3306，要分开 2、数据文件路径一个，要分开 数据库的路径yum安装的是这里👆，待会要做多实例，路径重新规划成三个独立的。 ①创建独立的目录用来存放DB文件、bin文件、etc配置文件、log日志文件 socket就是mysql通过本地socket连接的文件。 ②跑数据库生成的脚本，生成多个DB文件 就是把yum安装的/var/lib/mysql/下的文件，生成到自己创建的多个分开的路径里 yum安装就会自动生成的一个文件mysql_install_db，用它来生成DB文件 如果是二进制安装的就是在scripts路径下，注意不要进去运行，就是在外面运行。 生成动作👇 yum安装的时候自带脚本就会创建mysql用户，所以直接用就行了👆 --datadir 指定DB文件目录 tree可见 已生成了 同样另外两份生成一下： ③修改目录的权限 已经通过mysql_install_db生成指定的权限如下 整个目录都设置一下 ④创建多实例的配置文件 就放到刚才定义的多实例文件夹下各个/etc下 先改一份，再复制到其他实例路径下 因为是多实例，PORT默认是3306也顺便统一都写上 上面的配置优化成变量的方式，可以吗，哈哈 修改一下各自的端口 至此 二进制文件本来共用的不用动、账号共用不用动、DB文件搞定、配置文件搞定、就剩下启动脚本了。那些socket路径、pid、log都是指定后随着服务启动会自动生成的。 ⑤启动的方式，采用脚本后台启动 服务脚本是网上找，或者自己写 拖进去即可 这个脚本的路径要修改的，因为它原本是针对源码编译安装的 比如mysqld_safe和mysqld的路径当前所在👇 找不到是因为which是只在PATH路径里搜索的，招不到没事，服务起一下，ps aux可见 启动后就能看到mysqld的路径 修改的截图 至此就搞完了 3306这个文件夹下的bin下的启动脚本就有了 data下的DB文件也就是数据库也有了 etc下的配置文件也有了 log 、pid 、socket 这些自动生成的 启动程序 ⑥怎么连接 指定各自的socket文件 status看下路径 3307的👇 3308👇 ⑦关闭数据库 报错了👆 应为服务启动关闭的那个脚本里写了固定的口令，口令是错的 就可以啦👆 通过ss -tlnp看看是不是关掉了3308这个实例 ⑧补上root的口令 记得改的时候也带上socket参数-S 改完口令，由于原来是空口令所以现在必须带密码才能登入了 把启动关闭脚本里的密码补回去👇，否则关闭服务还得输入密码。 同样修改另外两个实例的密码 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:46 "},"23-MYSQL数据01/6-sql各种语句1.html":{"url":"23-MYSQL数据01/6-sql各种语句1.html","title":"第6节 sql各种语句1","keywords":"","body":"第6节. sql各种语句1 基础概念 IBM开发出来的SQL语言 SQL语言规范 命名规范 常见SQL语句 DDL：CREATE,DROP,ALTER DML：INSERT,DELETE,UPDATE DCL：GRANT,REVOKE,COMMIT,ROOLBACK DQL：SELECT 创建数据库： CREATE DATABASE|SCHEMA [IF NOT EXISTS] 'DB_NAME'; CHARACTER SET 'character set name’COLLATE 'collate name' 修改数据库： ALTER DATABASE DB_NAME character set utf8; 删除数据库 DROP DATABASE|SCHEMA [IF EXISTS] 'DB_NAME'; 查看支持所有字符集：SHOW CHARACTER SET; 查看支持所有排序规则：SHOW COLLATION; 获取命令使用帮助： mysql> HELP KEYWORD; 查看数据库列表： mysql> SHOW DATABASES; 创建数据库 Name: 'CREATE DATABASE' Description: Syntax ------ CREATE [OR REPLACE] {DATABASE | SCHEMA} [IF NOT EXISTS] db_name [create_specification] ... # create_specification就是指定的属性，包括 # CHARACTER SET 默认字符集：明确当前默认字符集是什么 show character set # DEFULAT COLLATE 默认排序规则 create_specification: [DEFAULT] CHARACTER SET [=] charset_name | [DEFAULT] COLLATE [=] collation_name | COMMENT [=] 'comment' Description ----------- CREATE DATABASE creates a database with the given name. To use this statement, you need the CREATE privilege for the database. CREATE SCHEMA is a synonym for CREATE DATABASE. # SCHEMA和DATABASE是同义词 gb2312是简体中文 big5是繁体字 utf8 全球语言编码 图中Default collcation是排序规则，就是最左边一列的字符集，它默认使用的排序规则是什么，通过show collation查看一下排序规则的具体内容 这👆就是以排序方式作为线索的表单，最左边是排序方式，第二列是可以使用该排序方式的字符集，同时第三列就是字符集是否默认使用的这行的顶头那种排序。 现在不主用UTF8了，数据库里要存放表情包之类的符号，就需要👇utf8mb4才是推荐。 占用字节也是比较大的4个字节，UTF8时3个字节。空间也就是硬盘也便宜。 创建db，实际上的默认语句，其中默认字符集时latin1_swedish_ci，默认数据库就是不支持中文的，需要修改。 创建的时候制定字符集 此时去DB文件里看看 db.opt里两行内容，默认的字符集，默认的排序规则 一个数据库就用统一的字符集，虽然可以针对某个字段设置字符集，但是非常不推荐。 修改数据库的字符集 ALTER DATABASE DB_NAME character set utf8; 如果DB里已经放了数据了，所以调的时候就要小心了。一般不会改。 删除数据库 DROP DATABASE|SCHEMA [IF EXISTS] 'DB_NAME'; 三种创建方法，主用第一种 data_type数据类型 tinyint:8bit，正好是IPv4地址一段的空间。可用在此处规范格式 数据库存角、分，也是单独建立字段角、分，都是整数型而不是小数型。 查看官方文档里的定长和变长 https://dev.mysql.com/doc/refman/8.0/en/char.html CAHR(4) : 代表，不管多少，多砍少补，视频里老师讲是4个字符而不是4个字节，就是说最多可以存4个汉字的。 修饰符 比如员工编号自动增长可以用AUTO_INCREMENT 确保全是正数没有负数，可以使用UNSIGNED tinyint 8bit 最高位如果为0就是正数，为1就是负数。所以真正的数据也就是7位。使用UNSIGNED后符号位就去掉了，8bit全是整数，范围从0-255了 DEFAULT 没赋值的默认值，比如IT行业默认值设定为男... create table student(id int unsigned auto_increment primary key,name varchar(20) not null,gender ENUM('m','f') default 'm',mobile char(11) ); 定义了id为int类型，有点大好处嘛不烦神，坏处就是4B占空间，用不了这么多可能，unsigned就是都是正数，auto_increment自增长的id序号，主键默认就是not null name就是varchar类型20个字符，注意是字符。not null不为空， gender类型是ENUM枚举表示几个中选择一个，default默认值为m， mobile手机号就是，char(11)个字符串，注意手机用数字int这种不合适，因为还要进一步做138开头的匹配过滤操作。数字可能不行，regex好像针对的是字符串来匹配的。 查看默认字符集拉丁，不支持中文。 表格式不要改，能改的不多，比如varchar(20)就不能改小了会造成数据截断，改大倒是可以。 DROP TABLE ALTER TABLE Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:46 "},"23-MYSQL数据01/7-sql各种语句2.html":{"url":"23-MYSQL数据01/7-sql各种语句2.html","title":"第7节 sql各种语句2","keywords":"","body":"第7节. sql各种语句2 DML语句 insert 前后一一对应 id默认自增长、gender性别有默认值 添加多条记录 只要前后的域名和值一一对应就行，顺序不固定 字符集，表是集成的库的 现在的版本都不让你插入不支持的字符。 换个数据库测试一下字符集的支持情况 创建表方法1👆，顺便也换种方式创建表， 创建表方法2：以一个查询语句的结果来创建表， 创建表方法3：复制表 (2) 通过查询现存表创建；新表会被直接插入查询而来的数据 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name [(create_definition,...)] [table_options] [partition_options] select_statement (3) 通过复制现存的表的表结构创建，但不复制数据 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name { LIKE old_tbl_name | (LIKE old_tbl_name) } 但是复制创建表的字符集还是latin是原表的字符集 而新建的表就是集成了数据库的字符集utf8mb4 创建方法3， 此时依据查询结果来创建表，不涉及原表的默认格式比如字符集，所以表本身还是新建的，字符集集成了数据库的utf8mb4。但是！要注意的是，此时依旧无法在此表中创建中文，因为仔细看上图，虽然ENGINE引擎里是修改了utf8mb4，但是name字段还是latin1_swedish_ci，所以姓名依然不能写中文。 三种方法的创建，只有select 结果创建，是带上了原表内容了 insert 插入可以不写域名，就是values里必须写全，如下： 还有一个客户端的字符集的情况 客户端的字符集时auto 自动的 制定一下字符集 对比一下制定字符集和不指定字符集的区别 db并没有区别，然后client characterset客户端字符集同样没啥变化，都是utf8mb4，这不是OK得啊，还指啥指呢。 进入一个DB后，status发现变化了 server charactoerset是整个mysql实例用的字符集，所谓实例，就是mysql这个服务，一般多实例就是多监听端口。 db charactoerset是实例下面的某个db使用的字符集 client characterset是客户端使用的字符集 conn characterset望文生义就是链接的字符集，不明觉厉 但是table里的name还是拉丁字符集啊，对不对，我不用继续听你讲， 都知道你的思路是不对的。 退出一下，改为不制定字符集的进入方式 show variables like \"%character%\" 总结：字符集定义的地方 1、实例 2、DB 3、table 4、字段 除此之外，还看到了client、conn 改字段的字符集 ALTER TABLE student3 CHANGE name name VARCHAR(20) CHARACTER SET utf8mb4; 终于支持中文了，其实一开始创建实例的时候就制定好支持utf8mb4就行了 数据库相关的变量 修改配置文件 客户端的字符集，上面是命令选项，下面是写入配置文件里 虽然这个修改不用重启就能看到效果，但是还是要重一下，因为，如果你改的是/etc/my.cnf里的 这样是语法错误，重启服务报错的，但是不重启，mysql 进入竟然client字符集是修改成功的。所以奇葩吧。 查看default效果 导入一个现成的脚本，生产数据库文件供练习使用 mysql -uroot -pCisc0@123 除了insert语句还可以用的别的 StuID有自动增长属性，所以insert的时候不用手动指定了 set这种格式用的不多👆这是insert的第二种语法 还有insert第三种语法，也不多用的，了解一下， 适合把一个表中的查询结果批量导入到另一个表种 更改的指令 要制定更改的哪条记录里的哪个字段 如果update的时候不加where限制条件，那么所有的记录都被改了，这就是灾难性的操作了，和删库也差不多了 为了避免这种情况的发生 dummy笨蛋的意思 -U是命令里的，也可以写到配置文件里，提到配置文件要知道是 server的配置文件 还是 client 配置文件 这里的mysql -U应该是客户端的命令，所以是client配置文件里修改 效果OK DELET删除命令 由于开启了安全模式，所以不让清表，👆这是清表，不是删表，删表是DROP 当你的表中记录非常多的时候，清表速度就要快，此时就不能用DELETE，而是用TRUNCATE 因为DELETE删表记录的时候，还要记录日志的，所以速度没有TRUNCATE(不生成事务日志)快 事务日志后面讲 DML insert update delete DDL create drop alter 下面继续学习DCL和DQL 之前的一个编译报错，版本较新的包，解决思路如下 但是rm -f CMakeCache.txt删除后要重新cmake可能。然后yum install libdb-cxx-devel要补一个这个包。要么就是和AMD的CPU有关导致的编译到后面失败了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:47 "},"24-MYSQL数据02/24-MYSQL数据02.html":{"url":"24-MYSQL数据02/24-MYSQL数据02.html","title":"第二十四章 MYSQL数据02","keywords":"","body":"第二十四章 MYSQL数据02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:47 "},"24-MYSQL数据02/1-sql语句DQL单表查询.html":{"url":"24-MYSQL数据02/1-sql语句DQL单表查询.html","title":"第1节 sql语句DQL单表查询","keywords":"","body":"第1节. sql语句DQL单表查询 select的其他作用 打印字符串： 数字运算： select * from table_name *表示这个表中所有的字段 挑列 1、挑选的字段，可以自定义顺序，所以设计表的时候，字段的前后顺序并不重要，只要查的时候把它改一下顺序就行了。 2、字段名大小写问题不大，注意规范就好。 别名-优化显示，配合页面的显示 as可写可不写 多表查询的时候的优化表名 挑行 非标准SQL比如!= 不等于的这种写法，别的数据库里就有可能不支持，mysql里是支持的。 不等于的写法 注意where里的表达式里，除了数字其他一般都需要加引号。 多条件选择 是真不区分大小写啊 登入的逻辑， 就是在网页里，输入用户名、密码，最终就表现为一个查询语句， select * from students where username='admin' and password='centos'; 查到了就是存在用户和密码匹配的，如果没查到，就是用户名或者密码不对 构建非法输入字符绕过常规select 利用下面的这个查询or的逻辑，加上面的手法，就可以绕过检查。 构建奇怪的密码 这就是sql注入，很多年前针对DB的安全攻击。大部分软件JAVA PHP都针对这种有相应的措施。 上图admin\\'-- 是用户名，密码是一个单引号 \\' LIKE通配符 % _ % 前后都有的，这种是不推荐写的，因为会严重的影响数据库的性能，因为它不能利用索引。利用索引，才能提升性能。 当数据百万级别的时候，这个前后都有%%的写法就非常不好了！ RLIKE、REGEXP正则 也是不推荐使用的，影响性能 去重-distinct 查空置NULL 不为空 表中查询统计 分组统计，聚合函数，group和聚合函数通常成对出现 GROUP：根据指定的条件把查询结果进行“分组”以用于做“聚合”运算 avg(), max(), min(), count(), sum() HAVING: 对分组聚合运算后的结果指定过滤条件 函数在DB中都是要加()的，括号里放的就是函数的要求 count()统计非空记录数也就是行数 count()，括号里放字段，\\就是所有字段，也就是所有列，不可能所有列都为空 所以也就是表的行数也就是记录数，所以统计表的记录数可以用count(*)还有count(主键)，因为主键也不能为空 max() 和 min()以及avg() 分类汇总-分组汇总 针对性别分别做平均年龄 但是男女没有标明 做分组分类的时候肯定要带上分组的字段名，比如gender分组的，就要select上gender group by 要注意下图的效果 如果group by不配合聚合函数，那么就是查的是第一个分组内容的信息，比如 group by gender就是性别分类，结果不配合聚合函数，就是第一个女生的信息和第一个男生的信息 所以，使用group by分组的时候，前面select后面跟的只有两个东西，一个是group by本身的字段，还一个就是聚合函数 where和group by的组合 要么where放前面，过滤之后再分组 要么改where 为having，分组之后再过滤， as还是比较不错的👆 上面的where也是可以写成having如下👇 分组 再 分组 对上图进行班级分组后，再对性别进行分组 ORDER BY排序 升序ASC 降序DESC 针对数字型的有效 升序中，null石排在前面的 倒叙NULL虽然排到后面了，但是整个也倒叙了 group by 和 oder by组合使用 按班级分组统计学生年龄，按班级别号正序显示 如何去掉null，注意having和order by的前后次序 大概就是：group by 再 having 然后order by，但是having是group by分组汇总后的过滤，就不太好了， 运算减少的思路，就是，先过滤，改成👇 LIMIT限定记录数(行数) 跳过前2行，取后续的几行👆 in的语法 等价写法 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:47 "},"24-MYSQL数据02/2-sql语句DQL多表查询和视图.html":{"url":"24-MYSQL数据02/2-sql语句DQL多表查询和视图.html","title":"第2节 sql语句DQL多表查询和视图","keywords":"","body":"第2节. sql语句DQL多表查询和视图 两张、三张，四张，都是从两张开始的， 两张表查询，应该就是先讨论两张表的组合起来。 JOIN 两种形式的组合 纵向和横向合并 纵向union合并 就像两个文件内容cat file1 file2 追加在一起一样，但是db.tables纵向合并需要title对齐 两张表的TID和stuid不一样没关系，也可以合，只要内容相匹配就行。 union合并 错位合起来，没有意义了就 union的去重功能的开关 通过两张一样的表来测试 去重方法2：distinct 横向合并的逻辑较为复杂 联想paste file1 file2这个shell命令，虽然关系不大 横向1：cross交叉连接,也就是笛卡尔乘积组合，就是10条*20条这种200条结果的组合 好像A CROSS B 和B CROSS A 其实是一样的 cross一般用的少的，一般是找到相关联的信息 内连接-inner join 限制制定列，需要标明哪个表 别名简化下输入 左外连接-left outer join, outer可不写 就是A的全要，并且根据A关联B里的信息都捞出来。 观察下图，理解left out join的意思👇 写命令的时候 left join的左边的表就是全部要，右边的表就是写关联过去的交集。 右外连接 as真的哪哪都可以加哦，优化下表头便于理解： 对查询出来的inner join 、left join、right join进一步做过滤 表的别名和字段的别名，表的别名定义了，调用就要用别名，字段的别名只是一次性有效，调用还得用原名 left join 去掉交集 上图👆是A left join B，下图👇是A left join B and xxx is null： 👆图写错了，不应该是and而是having或者where right join 去掉交集 全连接 full outer join 理论上是full join，可惜mysql和mariadb不支持所谓的full 换种写法 排版换行下 full join去除交集 如果👇这样写是没有用的 要用到子查询：select的结果嵌入另一个select语句或者DML语句。 比如：查询年龄大于平均年龄的学生， 把老师的平均年龄 覆盖到 20号学生的年龄 然后就做一下FULL JOIN 去除交集 所以这个name，age，gender的冲突是和tearcher表里的冲突了 至此，就得到了FULL JOIN 的去除交集的写法👆 现在回头来，👇这些都是存在冲突隐患的 案例1： 需求：查询每个员工的姓名和 上级负责人的姓名 如果是两张表，就可以这么做 这样就学到了自连接👇 涉及三张表的一个关联操作 表1，得分表 表2，学生表 表3，课程表 挑选一些字段 select语句流程图 select的处理次序，看图可以知道select columns这个语句是在from xxx 之后的，所以也能解释from tables as t的t可以在select t.name1里进行调用的原因了。 VIEW试图 视图，在CCNA-SEC里见过，在BIND里见过，现在又在MYSQL里看到视图这个概念。反正不一样，知道一下就好了。 mysql里的VIEW长的像表，但其实是虚拟出来的表，不是真实存在的。 1、作用有点像linux的alias别名，固化查询语句，下次直接调用，省的再次输入长长的SQL CLI。 2、隐藏了真实的表结构。 create databases create tables create view 注意view和tables长的基本一样，所以命名的时候要认为区分注意规范 这样就创建了一个视图，所以和alias别名不一样，alias是将cli简化，而view是将cli的结果固化，相当于多了一个表，只不过是虚拟的表。 如何区分这是一个真实的表table还是一个视图view呢 like是通配符，rike是正则，但是show table status里不支持rlike 试图的tables status一眼看过去都是NULL，只有comment表达一些这个是VIEW 真正的表里是由引擎、不能把、字符集各种设定清空的。而且Comment是空。 这个view是动态的，修改视图里的源表格看看效果 删除学生，在看看view里的 是动态的，这里为什么还有NULL，因为VIEW生成的原SQL是left join的。 view可以插入如何理解 插入50岁的可以，因为创建视图的时候就是age>30 创建20岁的就不行了，因为你这个操作是对view的insert但实际上是修改了原表，而且view本来就是过滤了age>30的，所以就有一个现象：你insert了view，但是view里却看不到👇 所以实际工作中，不推荐用VIEW，因为存在这样的情况。 对视图的操作会影响原表 如果是三张表的join呢，，此时对VIEW的操作，又是对哪张表进行的呢？测试下，拿之前的那个view_test来测 哈哈，不让~ 我感觉就不应该对VIEW进行操作！ 而且也不建议使用VIEW drop view xxx;删除视图 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:47 "},"24-MYSQL数据02/3-函数存储过程和触发器和用户管理.html":{"url":"24-MYSQL数据02/3-函数存储过程和触发器和用户管理.html","title":"第3节 函数存储过程和触发器和用户管理","keywords":"","body":"第3节. 函数存储过程和触发器和用户管理 函数-例1 创建函数 查看函数列表 函数是保存在mysql.proc表中的 排版换一下 删除函数 函数-例2 上图的注意点：改变以往;分号作为一个命令去断句执行的行为。而//才是断句。 然后最后结尾又改回去了，所以DELIMITER是一个全局命令。类似交换机的user-interface里的screen-length 创建函数deleteByid，并传递参数uid，类型是SMALLINT 正数， RETRUNS返回的是字符串 BEGIN和END是标明函数体，如果简单的一句话搞定就不用写BEGIN和END，但是多句就不行了。 对比，跑函数之前的students里的行数是21 跑一下函数后的行数是 第10行没了 函数里 赋值的方法①：set a =x 👇 注意 END// DELIMITER; 其实是错误的写法，它表示已“END // DELIMITER;”结束的，而不是以//结束。 函数里赋值方法②：select into xxx DELIMITER // CREATE FUNCTION deleteById(uid SMALLINT UNSIGNED) RETURNS VARCHAR(20) BEGIN DELETE FROM students WHERE stuid = uid; RETURN (SELECT COUNT(stuid) FROM students); END// DELIMITER ; 可以改成 DELIMITER // CREATE FUNCTION deleteById2(uid SMALLINT UNSIGNED) RETURNS VARCHAR(20) BEGIN DECLARE x int; DELETE FROM students WHERE stuid = uid; SELECT COUNT(stuid) FROM students INTO x; # 这里是将select的结果放到了x变量里，这也是一种赋值。 RETURN x; END// DELIMITER ; 👆DECLARE申明看来要放在函数体的最前面，紧跟着BEGIN才行。 存储过程，更似shell里的函数 调用的时候比上面的讲的函数更像shell里的函数。 IN是给存储过程找个函数功能传递参数，是传进去 OUT是传出来 INOUT是双向的。 存储过程的案例 create创建、call执行 存储过程 show procedure status;查看👇 注意对比函数的type mysql库里也是存放了很多重要信息的：比如用户账号、还这里看到myql.proc里的函数、存储过程。 所以备份一定是要备份的。 存储过程参数的传递 注意如果换一个数据库就会报错，需要指定找个存储过程在哪个库里执行 自定义变量在FUNCTION和PRODECURE里的类型 1、局部变量：var 这种 2、全局变量：@var 这种 理解一下上面的mysql里的脚本，哈哈 变量是会话级别的变量，就是说 退出mysql的当前交互，变量就没了 看下out参数的效果，从函数里传出来给到了全局变量。 上图中的row_count()函数是上一次命令更改了多少行的意思。 本例中就是 select row_count() into num;上一条cli，也就是DELETE xxx 更改了18行。 同样测一下 流程控制--存储过程和函数中可以使用IF CASE这些语句，这些专业叫法叫做“流程控制” LEAVE相当于break ITERATE相当于continue 触发器--准确来讲应该叫事件触发器 举例：比如你在jd购买了100个手机，下单了，那么库存里就要去掉100个【手机，所以事件就是下单100个，触发就是库存里自动减去100个。 DEFINER 是以什么身份来执行 TRIGGER就是定义触发器的名称 BEFORE 在 INSERT|UPDATE|DELETE之前进行的动作，换句话说就是在你增、改、删之前触发了某个动作，实际上就是不会去执行增、改、删了。就是说BEFORE就是用自定义的动作来代替trigger_event事件了。 触发器示例 附带主键的定义写法补充 一般是在定义字段的时候 比如 stu_id INT(11) NOT NULL AUTO_INCREMENT PRIMARY KEY，这样后跟一个主键来定义。 也可以单独PRIMARY KEY(stuid)去定义主键，那么这种是一般用来定义复合主键的。 做两个表student_info和student_count两个表供触发器来测试效果， 然后👇做两个触发器： 插入一行记录后再查看 可见触发器生效了，因为触发器就是检测到该表的row数量，insert就是将另一个表student_count +1 插入两行，就是增加2个计数 而删掉一行，就计数减一 触发器trigger是放在information_schema里的triggers表里的。 但是information_schema库 并不是磁盘上的文件，所有在磁盘上以文件形式存在的库👇 information_schema是在内存里，所以理论上在information_schema里的后果就是重启mysql服务后触发器丢失。但是我甚至重启centos后发现触发器还在！说明其实触发器还在其他地方存放 如果将teachers表里的TID=3的那行删掉，那么students表里的TearcherID=3的怎么办？理论上是不让删的， 这个时候就会有一种做法--叫 级联删除，就是上面的teachers表里的删了，下面students里的涉及对应老师的那些行也都删了。 但其实，好多公司规范里是不让用外键和级联的。阿里的JAVA开发手册里有提到👇 用户账号和权限管理 mysql新的版本，password不是放在这里的，可能放在authentication_string里。 删除drop好了 改密码-方法1 看下passwrod函数：加密口令的方法，password()就是加密口令的。 改密码👇 密码必须加密 看个localhost的坑，生产中也是要关闭反向解析的。 先说结论，就是localhost和127.0.0.1不是一回事，举例 现在有两个root，一个是root@localhost，一个是root@127.0.0.1 然后使用root@127.0.0.1这个账号和centos这个密码登入 发现明明是127，结果被反向解析成localhost，然后localhost的密码又不是centos，所以deny了 关闭反向解析，让127.0.0.1回归IP，就可以对上root@127.0.0.1 centos这个账号密码了 然后去配置文件做永久关闭就行了，这里OFF就是做解析了，ON就是不做解析。 skip_name_resolve 是忽略名词解析，ON，就是打开就是忽略的意思。 然后故障就解决了 改密码-老的方法2失灵了 https://stackoverflow.com/questions/64841185/error-1356-hy000-view-mysql-user-references-invalid-tables-or-columns-o 验证很简单啊，之前view也学过， 可以用alert就可以了 忘记root密码-破解 1、最粗暴的方法就是，肯定不能用了 rm -rf /var/lib/mysql/* 这样数据里的所有东西都没了，重启服务后，自动初始化一些必要的库会。 2、正常方法 重启后就可以不用密码了 但是此时远程用户也同样不用输入密码就直接进来了 这就不太好了，这就需要开启维护模式--也就是只能本地连接，远程就无法连接了如下👇 PS：mysql配置里-和_等价的，重启服务后，远程就挂了 其实就是3306端口关闭了 同样本地通过tcp/ip这种sock一样也进不去了 只能本地走文件socket才能进去 然后言归正传，进行口令修改 刷新一下，再修改就行了， 取消授权REVOKE Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:47 "},"24-MYSQL数据02/4-mysql架构和存储引擎详解.html":{"url":"24-MYSQL数据02/4-mysql架构和存储引擎详解.html","title":"第4节 mysql架构和存储引擎详解","keywords":"","body":"第4节. mysql架构和存储引擎详解 权限类别 管理、程序、数据库、表、字段，这些都可以设置权限 管理类：create、shutdown、show databases、 结尾带上：with grant option就是被授权的用户拿到权限可以转赠给别人。 GRANT SELECT (col1), INSERT (col1,col2) ON mydb.mytbl TO 'someuser'@'somehost'; PS：授权，只能select col1只能查询col1这个字段，只能对col1和col2进行插入赋值，在mydb.mytbl这个库的这个表上，针对‘someuser’@'somehost'这个用户。 就OK啦；所以现在的新版本依然可以用grant去授权的时候直接创建用户。 取消授权REVOKE 上图就是ALL里面单单去掉了delete权限。 查看当前用户权限 一般授权可能需要结合flush privileges使之生效，有时候也不用补。当然忘记密码的步骤里需要改密码cli前flush一次。而授权这里通常是授权cli后补一个flush。 单单授权cli 就是 第一步，创建用户 ​ create user user2@'127.0.0.1' identified by 'centos'; PS：identified by 'centos' 不管在create user还是在grant 授权里出现都是设定密码了。 第二步，授权 ​ grant all on hellodb.* to 'user2'@'%'; mysql的架构 1、connector：连接器也就是API： 理解：各种API咯，JDBC就是jAVA调库的API，此外还有python和其他语言调用mysql的API。 2、connection pool：连接过来后要验证身份、分配线程、等等，这些处理模块就是connection pool连接池： authentication，就是验证是不是合法的用户，验证完了以后，系统会分配一个为你服务的线程。 thread ，就是mysql是单进程多线程的服务。用户上来，验证过了后，分配的就是一个线程。 reuse，用户使用线程查询完后，断开了，线程不会销毁。系统会清理该线程的用户信息，然后重新使用，叫reuse重新使用。 thread，其实有一个pool，池子。从池子里取线程分配。如果池子里总共有1000个线程，来了1w个用户连接，此时就超量了，就有一个连接限制connection limit。这个默认连接数是100，我以前批量并发获取400台SW信息并发写道mysql里就遇到了这个问题，最后查出来就是连接数限制要调大些就好了。 生产中这个连接数的值肯定要调大的。 还有check Memory和cache都是在连接池子connection pool里 cache下文介绍 3、SQL接口 用户通过connection pool拿到线程服务了，就可以通过线程向mysql发送指令了。而client发送过去的cli，server就会检查cli的语法，检查语法就是靠SQL Interface接口来完成的。 SQL语法检查通过了以后就进入了下一阶段 4、Parser：解析器 把用户发来的cli解释成SQL自己身能理解的内容。 在做解析的时候，同时要做权限检查，应该是给到具体的权限吧。然后还要做优化 5、Optimizer,优化 DB在资源访问的时候，访问方式不是单一的，比如：用索引和不用索引。 到底用索引好还是不用索引号，就需要一个优化。 根据这个优化，最终判断出来这个最佳的路径，来进行查询。 6、Caches和Buffers 如果查询过了，则可以利用缓存。 如果缓存里也没有，就需要去磁盘找了那些数据了。 7、数据是放在文件系统上的db格式 数据库底层的数据，除了 数据库本身--db文件，还有一些日志也很重要。日志是一个大的话题，后面会讲各种各样的日志。正常软件就一个日志，而数据库里的日志就很多：redo、undo、Binary、error、查询、慢查询。 这些db文件要对其进行访问，就需要存储引擎进行支持 8、存储引擎 存储引擎负责和磁盘打交道 数据库通过存储引擎连接到磁盘文件系统上来访问磁盘的文件里的数据。 历史上有上百种引擎，目前都是使用innoDB了。 9、数据库其他功能 备份、还原、集群等 存储引擎 历史上出现过上百个存储引擎，现在只需要了解MyISAM和InnoDB了，其实MyISAM都不用了已经。 mysql5.5之前的版本默认用的MyISAM。 1、存储数据大小，MyISAM是256TB，要远远大于InnoDB引擎的支持，但是没有用，因为mysql的整体定位就是中小型数据库。达到TB级别，就要考虑分库了，支持不了这么大的访问量了。 2、Transaction事务，MyISAM不支持，事务里有很多重要特性：稍后介绍CIDB特性，还有很重要的特性-原子性。 原子性：一个事务里面由很多小步骤组成，要么这些所有的小步骤都完成，要么都不做，不能只完成一部分，必须作为一个整体，这就是原子性。 举例：转账，从A转1W给B，A扣除1W和B增加1W，这两个事务就是一个原子了吧，这才是一个完成的操作，如果用MyISAM这种不支持事务的引擎，就有可能这边扣除1W，然后突然停电了，那头钱也没收到，钱就丢了。而InnoDB不会出现这种情况，如果同样出现上面的A转出钱突然的停掉，当A扣除1W的时候，就会记录在事务日志里了，然后那边加了1W也会记录到事务日志，由于一头扣除1W和一头增加1W是一个完整的事务。结果你只是扣了做了一半，结果数据库突然停掉或宕机了，等你后面启动的时候，就会检查发现里面只有扣钱的一半动作，这就是一个不完整的事务，此时就会果断采用一个undo撤销操作--扣除扣钱的动作。 所以如果DB不支持事务，就极度不安全。 3、locking granuarity锁的颗粒度 MyISAM的锁表，会影响并发 InnoDB是锁行也就是锁一条记录的，可以多人同时修改不同的行。并发好。 4、MVCC(多版本并发控制机制)，并发用 InnoDB支持MVCC 数据中的表不是看到的那么简单，你表里由2个字段，比如id和value，innodb引擎的数据 就会给你自动添加两列-字段，分别是create和delete 这两个字段分别存入的是，insert 一行-记录的时候，当时创建记录的时间点记录下来，其实不是时间点，而是事务ID，而事务ID只会递增不会减的，就跟时间一样只会不断增加，所以你也可以理解成时间。 如果第二条记录，删掉了，那么delete字段里就会记录删除的时间点，也就是事务ID。 假设一个人1200的时候(事务ID具有时间特性的，可以理解成时间点的)执行了select查询 问1，此人能看到上表的哪些记录？ 1000的可以看到，其他大于1200的事务ID的记录都看不到了 这个人是1200的时候开始操作的，然后持续到了3000才结束。 他看到的是1200以前的记录。 问2，下图用户1200的时候select能否看到第一行记录？ 答：看不到了，因为1100的时候已经删掉了 以上就是MVCC机制👆 所以在有MVCC机制的数据库中，所谓的删除记录并没有真正的删除。 再问：此时t2这个查询事务能否看到id1的记录 答：可以，因为1050这个事务点再id1记录删除的事务点1100之前，并在创建事务1000之后。 总结：MVCC表里，有create和delete事务ID，而查询有查询的事务ID，所谓事务ID就是时间节点去理解好了。不同的事务查看的的结果是不同的。 所以，MVCC的DB是你访问呢你的，我访问我的，大家不打扰的。原来是这种并发性哦，我TM服了... 这应该叫你看到我看不到的，是吧 然后好像现在mysql这个默认地库还是MyISAM引擎的库文件 xxx.frm 存放表定义，也就是表结构、几列、数据类型 MYD数据本身、MYI存放的是索引，索引可以压缩。 压缩，MyISAM压缩有前置条件和使用限制；而InnoDB应该是数据和索引都支持的。 现在高版本的mariadb的mysql库的引擎好像不是MyISAM，但也不是InnoDB。其实是Aria--事务部的MyISAM。 可见MAD的后缀其实对应的引擎是Aria，而Aria就是MyISAM的事务版 其他的库默认就是InnoDB 但是新版本的mariadb是既有ibdata1合并文件，也有各自的库文件的 上图👆frm就是表结构文件，ibd就是数据库文件和索引文件共用的一个文件。 其实老版本也可以分开来，当然是分开来好的，因为所有库文件和索引合在一起，单个文件很大，你也不知道哪个库变大了。 新版都是默认开启的 performance_schema引擎专用于performance_schema数据库的 Memory引擎，内存里的，临时存放。 Archive引擎：不支持删除，存档用的。 csv引擎，是excel那种？ blackhole，主从 复制的时候，可以用来加快复制。就是A--复制到B.C.D，不是直接复制多份过去，这样会加重机器负担，所以A-复制给Z，Z再分发到BCD行，而Z作为中间人，如果使用InnoDB引擎就会存到本地，而使用blackhole就不会存到本地而是内存中放着的，直接分发出去，减少了磁盘I/O。 查看当前DB支持的引擎类型： 注意InnoDB在不同的mysql版本里可能不是一个东西 上图5.1的mysq的innodb和后面较新一点版本的mariadb的innodb不是一回事。 都叫innodb，但是不同阶段的innodb可能是不同组织开发的， 早期的innodb可能是orcale公司开发的，后面innodb可能是Percona-XtraDB开发的。 修改默认的存储引擎 重启mariadb服务 show tables status from db_name\\G;也可以看到表用的是什么引擎 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:47 "},"24-MYSQL数据02/5-mysql服务器选项变量和性能优化.html":{"url":"24-MYSQL数据02/5-mysql服务器选项变量和性能优化.html","title":"第5节 mysql服务器选项变量和性能优化","keywords":"","body":"第5节. mysql服务器选项变量和性能优化 区分mysql选项、服务器系统变量、服务器状态变量三个东西 1、服务器选项，是写道配置文件里需要重启服务的一般是，可能mysql -u -p --option也行。 这就叫选项，就是运行时候的option咯； 选项来自于哪儿：在/etc/my.cnf里添加就行。 当然新版本就不是cli option的方式显示了，但是也要知道还是在my.cnf添加的那些就是服务器选项--myslqd 选项。 然后选项标准据说是用减号-，下划线只是支持而已，其实也就是不区分-和_的。 2、服务器变量是随时更新的，无需重启服务 然后很多情况下，有些东西，它既是服务器选项 又是 服务器变量。 变量通过show variables;查看 又分类 全局变量 和 会话变量 所谓会话变量，就是mysql -uroot -pxxx 进入的一个交互界面就是一个会话。 比如 这个选项同时也变量，可以如下方式查看 或者这么看 修改变量 这样直接该就是当前会话里生效，推出后失效。 然后再看一个my.cnf里的服务器选项，这里面写的都是服务器选项，既然写在里面能生效的都是选项，至于这个选项还是不是变量就要看了。 比如： 正儿八经上图选项要统一用减号-而不是下划线的，只不过人家不区分而已。然后看看这个innodb_file_per_table是不是同时也是变量，而查看变量的时候就要区分\\减号和_下划线了 上图说明这个变量不是一个局部性的会话级变量，要改就是基于全局去修改的。 然后看一个仅仅是服务器选项，不是变量的东西 哈哈，新版本也是变量了 老板本就不是变量哈哈，所以这个东西研究没啥具体价值可能 上图就是老版本的情况。 show variables \\G; 看所有变量 show variables like 'var_name'; 查看特定变量 show variables like 'var_na%'; 查看特定变量 show variables like '%var_nam%'; 查看特定变量 %就是等价于ls xxx*里的*就是通配符 关于服务器选项和服务器变量的官方表格 https://dev.mysql.com/doc/refman/5.7/en/server-option-variable-reference.html 既是命令行cli、又是option选项、还是var变量。 然后看下skip_grant_tables的这个跳过密码直接登入的东西 在mysql里就是cli、option、但不是var 在mariadb里既是option也是var 👇，同样这一点上文也说过了。 PS：--xxx就是option选项，不带--的就是var变量。 然后同样再次提醒一个写法规范性 - 减号就是纯选项的写法 _下划线，就是只要是变量就用下划线了，有没有纯变量的 还真有👇 还有一个服务器状态变量 这个一般不用改的，就是用来看服务器的当前状态的 登录了两个 msql -uroot -pxxx，所以是两个thread线程。 show status;或show status\\G; 查看状态变量 show status like '%thread%'; 使用通配符看命中的变量 修改最大并发连接数 默认是151 这是个全局变量，需要加上global参数；全局变量就是所有会话生效，意味着退出重进全局变量的值一样保留的住的；重启服务后失效。 那么这个是不是选项呢，去写到配置文件里，重启服务，如果能重启OK，就是选项咯 如果做到限制为最多3-5个用户 思路，可以从系统层面做连接数的限制， 如果仅仅从mysql或mariadb本身来做👇 mysql的最大连接数1-100000 mariadb的最大连接数就有点，最小必须是10起步 然后根据视频上说的，版本小于10.3.5是可以做到1-100000的 关于并发： 数据库的并发一般是1-2k，1w就崩了可能 web的httpd也就是apache可以达到1W nginx可以达到2W 考虑监控这件事，你要从这半百一千个变量中找到你需要的就行了，这么多肯定能满足你拉。 mariadb的状态变量有964个 mysql的状态变量有568个 同样考虑一些需求要找数据库是否有自带的功能的时候，就去服务器选项和变量里找 https://mariadb.com/kb/en/full-list-of-mariadb-options-system-and-status-variables/ 然后可以通过cli方式看选项的启用情况 老的版本呢ps aux能看到一些选项，新版本都看不到了，其实看也看不全，要通过cli去看。 sql_mode，综合性变量 NO_AUTO_CREATE_USER：进展用grant命令创建密码为空的用户 就是要规范命令：你用什么分组的，就要select里面写出来👆，ONLY_FULL_GROUP_BY就是这个意思，可能select出来的几个列，必须在group by里都出现才行呢。不过group by跟多个列是什么意思啊？可能就是先按名字后按日期进行排序比如👇，不过此时聚合函数就没啥意义了： 来个聚合函数有意义的 所以说select后面的几个，必须是在group by后面出现是有一定道理的。 报错error可以在warnings里直接看到 上图是error直接报错，就没写进去，而下图👇 同样有的版本出现warning个数提醒，但是没有详情，也可以这么看 上图是warning，然后截断后写进去了的， 两张图 不同的行为，一个是写不进去，一个是截断后写进去了，其实就是sql_mode的配置区别。 这个就是traditional，其实是一个意思 拿掉就可以做 截断插入了 配置文件就是my.cnf里写 CLI就这样👇 就可以实现截断了 两张图对照理解 这里就是根据优化的结果，可以选择采用索引或 不采用索引。 缓存的利用 必须前后两次的sql语言的哈希值一样才能利用上一次的缓存，也就是说大小写、空格、sql cli必须一摸一样才行。 缓存的不利用 有一些即时你启用了缓存，也用不上的情况：比如：动态的时间、日期、特意不使用缓存、还有什么临时表、共享模式的锁？ 数据有几个query打头变量 query_cache_type：是否开启缓存功能，取值为ON, OFF, DEMAND ON就是开启缓存机制，但是要注意开启了以后，还得设置query_cache_size才行，有的版本size默认=0，就是开了也是白开。 query_cache_min_res_unit：查询缓存中内存块的最小分配单位，默认4k，较 小值会减少浪费，但会导致更频繁的内存分配操作，较大值会带来浪费，会导 致碎片过多，内存不足； 缓存4k、4k地分出去地，如果查询地数据量是5K，但是缓存的占用其实就是8K。或者是查1K，浪费3K。 query_cache_limit：单个查询结果能缓存的最大值，默认为1M，对于查询结 果过大而无法缓存的语句，建议使用SQL_NO_CACHE 单个sql cli捞出来的结果超过1M就无法利用缓存了，所以针对这个sql cli就直接使用SQL_NO_CACHE来告诉数据直接就不去尝试使用缓存。 query_cache_size：查询缓存总共可用的内存空间；单位字节，必须是1024 的整数倍，最小值40KB，低于此值有警报 如果这个值=0，就代表当前缓存未启用，而上图是1M和quer_cache_limit单个查询缓存一样了就，肯定 不好啦！常识就是觉的不好。 query_cache_wlock_invalidate：如果某表被其它的会话锁定，是否仍然可以 从查询缓存中返回结果，默认值为OFF，表示可以在表被其它会话锁定的场景 中继续从缓存返回数据；ON则表示不允许 别人正在查看一个数据，该数据被lock，此时其他人去访问就不给访问了，其实也可以让他访问缓存里的数据，实现方式就是将query_cache_wlock_invalidate=OFF就行了。不过此时由于他是访问的缓存，所以数值可能不正确。 退出重进试试 果然 改size 换种改发，写道配置文件里 结果一样，还是 缓存的启用另一种方法-按需启用 就是使用cli里加sql_cache关键字，但是有前置条件的 query_cache_type的值为DEMAND或2时，查询缓存功能按需进行，显式指定SQL_CACHE的SELECT语句才会缓存；其它均不予缓存。 通过状态变量去看下缓存是否启用 状态变量一般就是看的，不能改的。 Qcache_hits | 0 表示没有一次缓存命中 下图可见cache命中啦👇 所以通过监控这个Qcache_hits可以知道缓存利用率高不高，命中次数越多缓存利用率越高。 内存free的和blocks块free的👆 总块数是4块，空闲的就1块？啥意思 系统看情况自动分的？ Qcache_inserts | 1 这个就是未命中缓存的次数， 解释：本意是缓存插入次数，就是记录缓存的次数，既然是记录缓存，就是第一次查询没有缓存利用，只能从磁盘文件查询，自然就是未命中缓存的次数啦，然后cache inserts记录缓存 查询总次数就是 命中次数+未命中次数 然后命中率就是 hits/(hits+inserts) 通过利用率可以判断你的业务适不适合用缓存，如果太低可能你的业务本身就不适合使用缓存。 注意些sql一定要注意大小写，而且是在web前端开放给用户比如网点查看商品的页面，用户其实是通过web去调db的，所以一定要将sql的cli统一大小写（多一个空格都不行），所以规范就是用SELECT * FROM XXX这种大写去做，才能利用好缓存。 | Qcache_lowmem_prunes | 0 | Qcache_lowmem_prunes：记录因为内存不足而被移除出查询缓存的查询数 内存不够了分配的小，缓存空间不够了，只能利用LRU算法淘汰缓存中的记录。 如果这个值比较多，可以考虑加点内存 | Qcache_not_cached | 2 | 输入的sql cli 无法被缓存的次数 多敲几次select发现 total_blocks变大了 查 询 缓 存 中 内 存 块 的 最 小 分 配 单 位 query_cache_min_res_unit ： (query_cache_size - Qcache_free_memory) / Qcache_queries_in_cache 查询缓存命中率 ：Qcache_hits / ( Qcache_hits + Qcache_inserts ) * 100% 查询缓存内存使用率：(query_cache_size – qcache_free_memory) / query_cache_size * 100% 如果缓存利用率很高，可以考虑调大一点。 优化查询缓存 “发生了很多验证工作”： 验证就是 查询的结果这个，数据频繁的发生了变化，一旦变化，就要验证缓存是否有效，肯定啊，缓存里的结果都不对，肯定要重新查拉。 所以缓存生效，①sql cli是否有hash②有hash也不行啊，本身缓存的sql cli捞的结果都频繁变化，db也来不及刷新了吧，我在想。。算了我不想了，按他的来。 就是设置的单位4k比如，太大了，碎片空间多，就减少。 没有碎片化，就看看是不是内存过低导致。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:48 "},"25-MYSQL数据03/25-MYSQL数据03.html":{"url":"25-MYSQL数据03/25-MYSQL数据03.html","title":"第二十五章 MYSQL数据03","keywords":"","body":"第二十五章 MYSQL数据03 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:48 "},"25-MYSQL数据03/1-索引类型和结构原理.html":{"url":"25-MYSQL数据03/1-索引类型和结构原理.html","title":"第1节 索引类型和结构原理","keywords":"","body":"第1节. 索引类型和结构原理 索引可以极大的提升数据库的访问速度，好像缓存命中和也能提高哦 全表扫描，效率太低了，需要索引。 B树的特点 1、数据也是放在指针层的 2、找17号和找28号，耗时是相差很大的，也就是说查找的速率不是很平均 3、根上只有17和35两个索引号，分成了3个指针，太少了，要多一点，然后整体树结构要成矮胖型的才好一些；而数据也是和索引放在一起的，就导致数据如果太大，就占用了索引的空间，一层空间如果被数据占用太多，索引能用的空间就少了，整体就胖不起来了。 ​ 但也不能太胖哦，胖到极端，就是所有数据铺在根，那TM还有个屁的树结构哦，就变成了所有数据挨个查找了。 4、每次从根开始查找，效率低；比如要找10-30，就每次都要从根开始台麻烦了。而实际上上10-30就是连续的空间。 优化方案就是B+树，不仅仅是mysql这么用的，其他数据库也是这么玩的。 B+树 1、只放数据的摘要比如元数据，比如学生的信息不放，只放学生的编号 这样单层里存放的元素个数会更多，显然不仅仅是图中的3个 5 28 65这么少的了。 2、根和分支仅仅放索引，不放数据，叶子节点才放真实的数据信息 3、查询速度平均了：不管找8号学生的信息还是找28号，由于真实信息都放在leaf层，所以都需要从根编列到底层，但是由于每层索引指针多，整体查询就快了。 4、查找连续的数据，无需每次从更遍历，因为leaf层的数据之间也是有指针的 leaf层之间也是连续的指针指好的，连续的数据段查找不必每个都从根从头查找。 这个指针在叶子节点之间，这个数据结构叫做列表。 在mysql中存放的数据，就是用B+树来存放的。 不一定对数据建立索引，也可以对姓名建立索引 刚才就是对数字(学生编号)建立索引 就是对学生编号从小到大进行排序--看leaf层。 如果现在对姓名建立索引的话 而数据建立的时候有默认的字符集，而字符集有自身默认的排序方式，这个前面的章节里讲过的 简单点举例就是按字母排序，简单举例👇，当然真正的结构还得是B+树的结构。 本质就是将数据的无序变有序，将随机的I/O变成了顺序的I/O，这里是指的数据的r/w咯，从而便捷查找。 有序化后，就不需要全盘扫描，自然降低了磁盘I/O。 索引的缺点 1、目录要随着内容更新而更新，也就是说数据变化了，索引也要变化。比如插入一个章节10章查到11章前面，那么原来的11章就变成了12章，往后章节都要+1。好比插入一条记录。 2、所以db如果查询多，修改少，加你索引是合适的。 3、如果是频繁的更新数据，查询的少，索引该不该建立，就要考虑清楚了。 4、索引还比书的目录，目录本身也要占空间，一本厚点的书，光是目录就要占十几页了。索引就有点 利用磁盘空间 换取 查询的时间的意思在里面。 5、那也是值得的，索引利用好，查询效率提升显著。前提就是数据足够多。 6、1w或10w条记录，10w对于计算机来讲也不算多，建不建索引意义不大。 建立复合索引 firstname+lastname建立索引，不仅仅是某个字段，而是2个或者3个字段的复合索引 建立的时候，要指定谁是第一个字段 这就是复合索引👇： 大致如上图创建好复合索引后，如何利用呢? selcet from xx *where firstname like 'a%' 这种可以利用复合索引吗？可以的！ 但是： where lastname like 'b&' 这种可以利用索引吗？不能！ 1、首先lastname就不是按字母顺序排序，更本质的就是lastname不是独自排序的，是在firstname排序(后固定)的基础上再排的。 2、所以复合索引，只拿第二个或者叫第一个以后的索引，是无法利用的，因为本身就不是有序的（独立来看，自然也无法独立利用）。 where firstname = 'a' and lastname like 'b%' 这就可以了，因为firstname固定了，lastname就是有序的了。 where firstname like '%a' 这种可以利用上面的复合索引吗？where firstname like '%a%'这种自然也无法利用索引。 firstname 以a结尾的查，是利用不起来索引的，因为上图可见很多a结尾的firstname是无序的。 sql写法很关键 1、在经常写where的那里的条件上创建索引，是比较推荐的做法。就是说经常过滤的条件就是你创建索引的点。 2、越是不重复的，越是适合建立索引。反面例子性别总归就两个 男/女，还建个啥索引哦；手机号就比较适合创建索引。 这个图和上面的B+树是一个意思，同样看到叶子之间是由指针的，这就比较适合连续查找，也就是范围查找。 适合用B+树索引来实现的场景 比如查姓名，本身索引里就放着呢，所以直接查索引就行了。 不适合利用B+tree的场景 1、三个字段，姓+名+年龄，你查 姓+年龄就只能利用姓的索引，因为年龄的有序性需要固定住姓和名。 2、两个思路：①开发写好SQL，你去看看怎么创建索引，因为开发可能不会考虑索引的事；②写好索引后，开发去根据索引写SQL。 ​ 据说索引是运维来写的。 3、一般就是分析sql语句，找到适合建立索引的字段。 HASH索引默认就有不需要创建 看看note8 默认就是内部使用的。 比如30岁的哈希，就是这个30岁的哈希索引就会指向多个30岁的人，然后进到这些人里面查找。 但是哈希索引是无法支持连续(范围)类的查找的，因为哈希之间是没有关联性的。 这个哈希索引也是DB自身维护的，不用去太关注。 B+tree才是我们需要手动创建的。 R树用的也不太多 这是和定位相关的，比如手机定位。 全文索引 这和关系型数据没有太多关系，这是文本里的关键字查找。 数据里就不会存入什么视频文件吧，不适合放大量的数据。 聚集索引和非聚集索引的区别就是数据和索引是否在一起 创建主键的过程就是创建索引了，因为主键会自动创建索引。 观察上图的leaf层，此处索引和数据是放在一起的，此时就是聚集索引。 当然人家是创建主键的时候自动创建出来的索引。比如学生编号这种主键索引。 讨论下一个表中 主键不能多个的原因：这里从索引的角度就能解释，假设 1、你创建了学生编号作为主键，那么连带就创建了主键索引，同时数据也要按照学号来排序； 2、假设可以创建第二个主键，此时你按照姓名创建了索引，那么连带就创建了姓名这个主键索引，同时数据也要按照姓名来排序； 3、那么问题就来了，此时数据到底怎么排吗？难不成复制一份数据出来再排成别的结构，肯定不行吗！所以DB中primarykey只能是一个。 除了MYSQL的MyISAM，其他的Oracle或者微软的SqlServer，他们的数据库特性都是和InnoDB引擎差不多的。 再来看二级索引 主键就一个，自然主键索引就是一个， 然后要创建非主键的索引，比如用学生的姓名创建索引，这个姓名又不是主键，索引没问题。 此时按学生的姓名按字符(字母)排序，然后看上图，按B+tree的方式，leaf层就是索引此时，数据也会放在姓名索引这里吗，那岂不是又要复制一份数据啦，不可能！数据只是可主键索引放在一起的。 上图的ROW一行就是一条记录，就是主键索引和记录本身数据在一起的，就是上图的主键索引里的Row一行里面既有主键(本身就是索引)和数据(行记录)，这就是聚集索引。 如果我们创建的是非主键索引，也叫做二级索引，此时的索引指向的就不再是数据本身，而是主键PK。 比如二级索引，假设是年龄排序的，那么17+3，就是17岁+3主键。 想想为什么叫二级索引，就是非主键的索引是指向主键，而主键才是和真实的数据在一起的。 所以索引利用上，直接利用主键效率是最高的，二级低一点。 而MyISAM不管是主键索引还是二级索引都是和数据分离的 联系一下前文学习的，MyISAM每个表都有三个独立的文件 .MYI就是专门放索引的，MYD是数据文件的，索引和数据都不是一个文件，肯定是分离的了，不管是主键索引还是非主键索引 肯定都是分离的。 所以MyISAM，全部都是 非聚集索引 InnoDB，是默认所有的表、数据、索引都是在一起的 但是可以分开，虽说分开来，其实分的是不同的表的东西分开来，各个表里的东西也就是数据和索引还是在一起的，还是不分的。 重启后，hellodb drop删掉，重新导入。 从这个角度看 什么 叫“聚集索引”就清晰了，innodb即使用innodb_file_per_table分开，也只是各个表分开来存放，单个表里的 idb就是索引+数据。 所以MyISAM的 主键索引和二级索引，没什么次序，效率也是一样的。 InnoDB里，一建立主键(即主键索引)，数据的排序就和主键一致了，主键的次序就是数据的次序。 所以进一步讲，InnoDB你插入记录，就会影响插入位置后面的记录的索引编号。所以如果踩坑--管理的不好，就会导致大量的磁盘I/O，因为它要重新排序。所以适合于读多写少的场景。 效率待会演示差多少，有索引和没索引。 稠密索引和稀疏索引： 下图👇就是稀疏索引，索引不是全部都指向了数据的。 就是索引对应的是指针， 而leaf层的索引才是对应的数据👇 比如5号本身里面就存放了数据记录的。 下图👇这个就是稠密索引，所有索引都是指向了真实的数据了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:48 "},"25-MYSQL数据03/2-索引管理和并发访问的锁机制.html":{"url":"25-MYSQL数据03/2-索引管理和并发访问的锁机制.html","title":"第2节 索引管理和并发访问的锁机制","keywords":"","body":"第2节. 索引管理和并发访问的锁机制 冗余和重复索引： 冗余索引：（A），（A，B）。比如firstname是创建了索引，然后firstname+lastname又创建了复合索引，此时其实两个索引都有firstname的排序，就是说不一样的两个索引，其实内部又重复的，存在冗余。 重复索引：已经有索引，再次建立索引。就是比如主键索引，你又创建一个唯一键是主键的索引？ 索引优化策略： 独立地使用列： 尽量避免其参与运算，独立的列指索引列不能是表达式的一 部分，也不能是函数的参数，在where条件中，始终将索引列单独放在比较 符号的一侧 ​ where age > 30 可以利用索引 ​ where 30 ​ where age+10 > 40 这样就不能利用索引 左前缀索引： 构建指定索引字段的左侧的字符数，要通过索引选择性来评估 索引选择性：不重复的索引值和数据表的记录总数的比值 ​ where xxx like 'zzz%' 针对name列做索引，发现name的一个字符已经具备区分性了，所以后面的字符就不要参与索引计算了，name(1)就是截取了索引列的第一个字符。 这就节约了建立索引占用的磁盘空间。 进一步比如：name char(20)，在其上建立索引name(10)取前10个字符就行了可能就能区分了。 这个取多少个能区分，的衡量度是啥，就是name char(20) 是20个字符的长度，你建立索引name(x)取钱x个字符，如果能区分出来90%，就可以了。 多列索引： AND操作时更适合使用多列索引，而非为每个列创建单独的索引 多列索引，就是复合索引，比如经常 将 firstname和lastname做个and一个条件来进行查找，可以考虑把fistname+lastname组成一个复合索引。而不是分别在firstname上建立索引，然后又在lastname上建立索引。 选择合适的索引列顺序： 无排序和分组时，将选择性最高放左侧；比如firstname选择性更高，就放在左侧。 只要列中含有NULL值，就最好不要在此例设置索引，复合索引如果有NULL值， 此列在使用时也不会使用索引 尽量使用短索引，如果可以，应该制定一个前缀长度 对于经常在where子句使用的列，最好设置索引，但是比如性别这种即使where上经常出现，也没必要设置索引，因为类别太少了。 对于有多个列where或者order by子句，应该建立复合索引 对于like语句，以%或者‘-’开头的不会使用索引，以%结尾会使用索引 尽量不要在列上进行运算（函数操作和表达式操作） 尽量不要使用not in和<>操作，确定范围的才能利用索引，比如age > 30 这种就可以利用，age ≠ 30 或者 age <> 30就只能进行全表扫描了，这种不确定范围的就不能利用索引。所以可以分开来？ age 30 这样来利用索引？ 查询时，能不要*就不用*，尽量写全字段名，不然捞出来的表可能多出来别的列。这样原来的sql cli不用动，不会受表格变动的影响。 大部分情况连接效率远大于子查询，用到了子查询的时候，想想能否用外连接的方式来做。 多表连接时，尽量小表驱动大表，即小表 join 大表，这个好像即使你用大表join小表，数据库也会自动给你优化成小表join大表的。 在有大量记录的表分页时使用limit；用limit限制一下，分批取数据，每次取个1w条就差不多了。 一般来讲，生产中数据库的性能好坏，很大程度上就是SQL 语句写的不好。如果你发现数据库性能达不到预期，就去查查sql语句写的是不是不好。 据说很多sql语句都是ORM自动生成的，生成的sql执行效率一般都没有人工手动写的高。所以程序员的sql基本上都是有优化空间的。 对于经常使用的查询，可以开启缓存 多使用explain和profile分析查询语句 这两个工具很好用据说 explain 可以看到你sql的细节，比如是否使用了索引，用的是哪个？ 查看慢查询日志，找出执行时间长的sql语句优化 管理索引 创建索引 1、创建表的时候就会创建主键的时候就会自动创建主键索引 2、表建好了已经，创建索引的方法 CREATE INDEX [UNIQUE] index_name ON tbl_name (index_col_name[(length)],...); UNIQUE就是要确保你的这个字段是具有唯一性的，比如手机号，身份证这些。 inde_name：是索引名称 ON tbl_name：是哪张表上去创建的 index_col_name(length)：字段的名称，在哪个字段上的前多少个字符来创建索引的，如果，逗号再跟一个字段就成了复合索引。 ALTER TABLE tbl_name ADD INDEX index_name(index_col_name); help CREATE INDEX; help CREATE INDEX; 删除索引： DROP INDEX index_name ON tbl_name; ALTER TABLE tbl_name DROP INDEX index_name(index_col_name); 查看索引： SHOW INDEXES FROM [db_name.]tbl_name; 优化表空间： OPTIMIZE TABLE tb_name; 查看索引的使用 SET GLOBAL userstat=1; SHOW INDEX_STATISTICS; key_name，key就是键就是索引。这个key_name是PRIMARY是个主键索引。 Colume_name-StuID：这个主键索引是在StuID上创建的索引。 Index_type: BTREE，写的是BTREE其实是B+TREE。 针对students表来创建 如果没有在age上创建索引，是无法where age = 30 这种就无法利用索引的。 type : ALL 全表扫描 possible_keys：可能用到的key NULL就是没有用到 key：NULL就是确实没有用到 rows ： 25行就是扫描了25行。也就是type：ALL全表扫描了。 创建索引，注意命名规范idx_xxx,比如在age上创建索引，就命名未idx_age 再来select 语句是否利用了索引 possible_keys 可能用到的索引idx_age，用了吗？ key: idx_age看到确实用了。 rows，返回最终结果有两条。 https://dev.mysql.com/doc/refman/5.7/en/explain-output.html name字段上没有建立索引，所以👇 再创建name字段的索引 此时再次对name里s开头的进行搜索，就有索引利用了 然后看下这个现象，视频讲解里的是这样 而我的实验是这样 老师说是数据库自己算出来不利用索引反而更快，不是大小写原因。 创建复合索引 先删除之前创建的两个索引 创建复合索引 注意上图复合索引也是一行一个，但是关注seq_in_index里的两行的值name的索引是1；age的索引是2。 看下复合索引的利用情况 1、对name先排序的，所以可以这样利用索引 2、跳过name_age复合索引的name，直接查age，就无法利用索引了 3、好像通配符类的，除了左前缀的都不能利用索引 4、name固定后，age就有序了，就可以利用索引 看下主键的索引利用情况 1、主键的范围查询，是可以利用索引的 2、想用索引，就别带上运算符 这个其实还蛮重要的，代码里上图的+10可能是个变量，可能会这么写的👇 stuid + $var_number > 30 ; 这样就是变量要放到右边才行：stuid > 30 - $var_number 3、不等于也不能利用索引 <> 这个等价于 != 查看索引的使用情况的统计 需要开启才能看到，开启方法👇 不用重进哦！上图写错了，就是要select 利用到索引才能看到统计值，explain是不记录的。 确认是否开启还可以 删掉这个不怎么使用的索引，就比较好，好在：1、节省磁盘空间，呵呵，2、减少磁盘I/O，插入记录还得重新计算后面的排序，这会导致磁盘I/O的增大。 先看一个字符串拼接👇： 其实可以用concat()来做 再看一个sql的存储过程 创建表testlog，id 自增长 delimiter $$ 定义EOF符号， 然后开始创建 存储过程 循环100000次，插入name,age，然后id没有插入就是从1自增长的。然后插入的name的value就是concat('wang',i)，就是wang1、wang2、wang3的插入， i ++ 然后将这个大表创建出来，供后下面索引的效率展示。 导入这个testlog.sql的方式有了两种 1、进入mydql交互界面后 2、不进入mysql交互界面导入 这样就可以了，不过之前我们用source testlog.sql 敲过一次，sql就执行过了一次存储过程的创建，所以这里会有ERROR报错，没关系，删掉重来。 再来 导入OK，这次表格和存储过程都导入了， 然后此次表格是空的，还需要跑一边存储过程来生成一个大表格。 然后call调用一下函数，也就是存储过程，什么名字记不住，除了上面的查法，还可以这么查👇： 执行存储过程 执行完后，看看一共生成了多少行 主键是id，所以索引也是默认创建在id上的，上图👆where 写的是name，所以没有事先创建号name的索引的。 通过explain确认没有利用索引的 create index idx_name on testlog(name)创建索引👇 所以，从0.017s 无索引到 0.001s有索引。 然后看统计， 确实是2次利用 下图就是👇说明，果然是利用的缓存 所以这里其实有优先级咯 1、缓存有先用缓存 2、缓存不中，再用索引。 创建唯一键索引 就是加个关键字unique就行了 观察上图的Non_unique字段，0就表示是唯一键；主键primary肯定是唯一键咯。 冗余索引举例 已经有了一个复合索引idx_name_age，然后你又创建一个name索引，这个name就是冗余的。 上图可见，idx_name是属于possible_keys可以利用的索引，但实际上key只用了idx_name_age，所以idx_name其实是没有利用上的。 上图是视频里的老师创建age索引，系统判定无需使用，呵呵，我自己敲了一边是用的 总之这里要说的，就是冗余索引的存在意义，针对 1、复合索引 name_age 2、再创建name索引就是冗余的 3、而创建age索引就是不是冗余，因为name_age复合索引里的age本身是无序的是依赖于name固定的情况下才有序，不是独立的索引，不会和单独的age索引重复。 所以你不能导出创建索引，比如100个字段全都创建索引，磁盘占用不说，你加条记录，加记录是很正常的事情了吧，然后所有的索引都要更新。一条记录上的所有字段创建了所有，所以加一条记录，100个字段的索引都要更新，这样会造成性能下降的。 一般就是where条件常用的，才会加索引。不经常出现在where后面的条件字段，就不需要创建索引。 下面看看数据库的并发控制这个话题 并发控制 数据库 vs 文件 的一大特点，就是db可以并行r/w吧，文件不行，确实python读取一堆pdf文件里的邮箱，然后并行写道excel里就不好弄，你怎么并行呢，除非你事先定义好pdf文件的序号，然后序号作为excel表格写入的行号，这样倒是可以理论做到数据不会被覆盖，但其实excel文件本身操作系统可能就不会让你并行写入。应该的是，比如ssh 两个人到同一个机器，然后vim打开同一个文件，后打开的就会提示冲突 所以并行访问在文件级别经常出现冲突，不过IM里的在线表格不知道是怎么做的，还有wps的在线共享不知道怎么解决并行的。 锁粒度：表级锁和行级锁，MyISAM是表记锁，InnoDB是表级锁。 读锁：读锁是共享锁，一个人读的时候，其他人都可以读，但是读的时候不能写。 写锁：写锁是独占锁，一个人写的时候，自己可以读；别人不能读也不能写。 备份数据，应该是所有的数据库都是统一的时间节点，所以需要加服务器级别(也就是实例级)的锁。此时如果备份1小时，那么就锁1小时，用户就无法访问了，业务影响比较大，这种情况也是有办法解决的，在innodb里支持事务，事务可以并行访问。 存储引擎就是自行实现的锁，比如innodb的行级锁，是自动的锁，也叫隐式锁。 显式锁：用户手动加的锁。 自己加了个读锁，自己也不能写了，只能读。 别人能读，但是无法写。 以上就是一个经典故障案例：当然是软件层面读锁导致的，软件打开了读锁，怎么处理呢？ kill调哪个starting的进程：使用kill 3; 此时再看之前卡在那的那个update就发现，能够执行下了 上图可见卡了5分钟👆。 写锁，自己可以写可以读 有人加了写锁，别人就不能读不能写 枷锁还可以用FLUSH命令，不加tb_name表名就是全部实例加锁。 FLUSH TABLES [tb_name[,...]] [WITH READ LOCK] 可用在备份实例的时候进行全实例加锁。 自己查是OK的 别人查也是OK的， 自己写不行了 别人写也卡住了 创建用户也不行，因为创建用户本身就是在mysql数据库的表里插入用户， 使用innodb的时候通常因为有事务的机制，往往不会人为的去加锁，因为事务会自动加锁。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:48 "},"25-MYSQL数据03/3-事务特性和四种隔离级别.html":{"url":"25-MYSQL数据03/3-事务特性和四种隔离级别.html","title":"第3节 事务特性和四种隔离级别","keywords":"","body":"第3节. 事务特性和四种隔离级别 事务的概念 事务Transactions：一组原子性的SQL语句，或一个独立工作单元 事务日志：记录事务信息，实现undo,redo等故障恢复功能 ACID特性： A：atomicity原子性；整个事务中的所有操作要么全部成功执行，要么全部 失败后回滚 C：consistency一致性；数据库总是从一个一致性状态转换为另一个一致性 状态 I：Isolation隔离性；一个事务所做出的操作在提交之前，是不能为其它事务 所见；隔离有多种隔离级别，实现并发。 ​ 隔离有隔离级别：一个修改事务过程中，另一个事务能否看到是取决于隔离级别的，比如你你修改1000未1100，别人不一定能看到1100。 ​ 一个事务没有结束，中间过程的数据就叫做“脏数据”。 D：durability持久性；一旦事务提交，其所做的修改会永久保存于数据库中 撤销叫做回滚，commit就确定了就永久保存在数据库中了。 事务日志类似于ext3的文件系统日志 1、100变成200，200变成300，然后还没来得及commit就停电了 此时后面来电mysql起来后，会对这个100->200,200->300的 事务日志 做undo撤销动作。 1、2、3本来只是完成到事务的日志记录，实际上并未提交 此时就是100大不了没有改成300，就是100不变。 事务日志已经commit提交的是一个完整的事务，就redo--在数据库里重新执行一遍写进数据文件里，没有的undo撤销。 所以事务日志里有redo日志和undo日志。 事务的执行过程 1、刚开始数据库是初始化状态 2、开始一个事务，事务开始的标志是：人工手动，或者 隐式的开始 3、事务中：增、删、改；查应该不算在是事务里了，看来不是说算不算的问题，而是你放不放的问题，你手动制动select就是事务里的，就是啦，隐式估计不放吧。 4、commit，如果事务确定要提交了也就是结束了，就是commit。相当于订单提交，不过也可能是加入购入车哈哈。 ​ 一旦提交就进了新的数据状态了。 5、rollback，回到原来状态。 事务的CLI 自动提交：回车就默认commit了 同样我的测试 主键就是理解成主键索引，同理唯一键也是等价于唯一键索引，比如你创建一个唯一键，其实就是默认创建了唯一键索引 drop index uni_age on testlog;删除唯一键，就可以使用存储过程插入了。 10万条记录，所以耗时13.74 看看我自己的测试 这个call执行完就是自动提交了，而且可能还是一行行的提交的，因为10万行嘛 你也可以改成像orcale一样的方式--默认不自动提交。 可以修改为不自动提交。 然后删除一行记录看看 删掉后，自己看确实删掉了 但是别人看--另外开一个终端ssh去看👇还在： 所以 1、修改autocommit自动提交为OFF 2、自己删除一行，自己看得到；但是属于事务中间状态的数据， 3、别人看不到；如果别人看到就是脏数据。 4、能不能看到和隔离级别有关，默认是看不到的。 这就是事务的隔离性，你没commit就不是一个完整的事务。 5、把shell窗口关掉，模拟事务没有提交的异常断开效果，看看undo效果 肯定的啊，这个动作不就等价于别人看嘛，还验证个啥哦。不过这里和别人看不到是两回事，这里涉及一个undo也就是rollback。 6、commit后别人所见不变；这还是 事务的隔离性，事务的隔离性后面讲，一共有4种。 自己删除25行后，提交后，别人还能看到25行 人为的起止事务 启动事务：下面3个cli照抄就行就是事务开始的cli BEGIN BEGIN WORK START TRANSACTION 结束事务： COMMIT：提交 ROLLBACK: 回滚 注意：只有事务型存储引擎中的DML语句方能支持此类操作 再加一条 撤销 一旦提交，就真的把数据库文件改了 但是我这边没做出来，奇了怪了，难道是mariadb版本太高了？ 我靠，什么时候改掉了，默认不应该是InnoDB吗 靠，删掉，重来一遍看看效果 emmm，删表，重建 好了，再试试call pro_testlog；的手动begin，整体事务的方式 1、直接call就是存储过程里的没一行都会默认自动提交，就会很慢 而且可见innodb的这个调用10w行的存储过程要比MyISAM慢的多的多，可能是MyISAM没有事务，也就没有事务日志，所以快？ 感觉下面begin可能时间也是和MyISAM一样，因为整个call xx就是一个事务，感觉相当于MyISAM的没有事务了。 2、其实innodb手动指定事务还是要比没有事务的MyISAM要快一半的时间。 3、结论innodb 调用10万行的存储过程，使用整体一个事务的方式，耗时也是要比MyISAM要快的，我的机器配置是4s的7s。当然老师的就是更快了，看前面的图1.55s。 以上就验证了 同样drop table这种DDL语言，不是DML语句，也不会支持rollback操作 发现DDL(drop create alter)语句是没法rollback的，这些是和select一样会记录到事务里，但是不是DML(INSERT UDDATE DELETE)，不支持rollback，自然也不需要commit。 事务支持保存点：savepoint SAVEPOINT identifier # 定义保存点 ROLLBACK [WORK] TO [SAVEPOINT] identifier # 回到对应的保存点 RELEASE SAVEPOINT identifier 就是在事务执行的过程种，在某个节点打标签，将来rollback到对应的savepoint。 现在表里加了3条记录 1、直接rollback就全部撤销了 2、rollback to aa_tran 3、撤销过了，一些savepoint没了就没了，bb_tran整个保存点也就没了。 事务的隔离级别 事务隔离级别：从上至下更加严格 READ UNCOMMITTED 可读取到未提交数据，产生脏读。 READ COMMITTED 可读取到提交数据，但未提交数据不可读，产生不可重复读，即可读取到多个提交数据，导致每次读取数据不一致 A分别在事务t1修改100为200，又子事务t2修改200为300， B在一个大的事务t3中，两次时间节点看到的值不同，前面是200，后面又变成了300。 B就在一个事务中读取到了多个不同的值，这就是产生了 不可以重复读的结果，因为重复读数据不同了。 REPEATABLE READ 可重复读，多次读取数据都一致，产生幻读，即 读取过程中，即使有其它提交的事务修改数据，仍只能读取到未修改 前的旧数据。此为MySQL默认设置 说明repeatable read可重复读，就是B在整个事务t3的执行期间，每次读取的数据都是一样的。从结果上来讲就是可以重复的去读数据，数据是一致的。 ​ 但是！数据早就改掉了，甚至删掉了，结果B还是一直认为数据还是原来的样子，这就是幻读。 ​ 结论：虽然可能存在幻读，但是恰恰就是保证数据一致性了，所以这个就是mysql的默认机制--mysql默认事务隔离级别为“可重复读”。 ​ 举例：备份期间，如果以事务开始，就是备份的前敲一个begin的意思了，无论备份执行多久，数据就是一开始时候的样子，是不变的，哪怕别的用户提交了修改数据，在备份的这个事务期间都是不变的，带来的好处就是：数据的一致性，就是在以事务方式进行的备份中，数据都是一个时间节点的数据。 SERIALIZABILE 可串行化，未提交的读事务阻塞修改事务，或者未 提交的修改事务阻塞读事务。导致并发性能差 ​ 就是我读的时候，别人不能改；我改的时候，别人也不能读。 ​ 优点：数据很可靠；缺点：无法并行了。 MVCC 多版本并发控制，和事务级别相关 并不是4个隔离级别都能用上这个MVCC 事务隔离级别对比表 说明： 列上的 \"不可重复读可能性\"，就是读出来数据可能不一致，就不能重复读了，就是这么个意思。 read-uncommitted和read-committed都能读出不一致的情况的。 幻读可能性： 所以：前3个read-uncommitted、read-committed、repeatable-read都能出现幻读。 问：事务和锁的关系： 答：就在上表最后一行啦，串行化事务里就会加读锁。总之锁是锁，事务是事务，锁是并发读写保证数据一致性，事务时讲多个操作看成一个原则来保证数据一致性；前者多个用户I/O数据库的一致性；后者是多个一系列操作的原子性或者叫一致性也行，而且后者事务还提供了隔离级别，这个就是会造成和锁理解冲突或者联系的点。 四种事务级别的设置 默认级别： 可见这是一个服务器变量，这个是不是一个服务器选项呢。 1、官网查咯：该参数不是服务器选项👇 2、自己试咯： 详情倒是没有指明是这一行，不过有unkown variable这个未知变量 该关键字可以联系起来 然后注意看 这是mariadb里的 这是mysql里的 这个transcation_isolation(只是一个选项)，用来对应tx_isolation(只是一个变量) 其实眼神好一点，一开始查到的是可以看到了 同样mariadb也有 继续修改配置文件 OK啦 此时变量就改过来了 开始体会下事务 1、第一种事务级别read-uncommitted 两个窗口都开启事务 左边的用户insert一条记录，但是没有commit， 此时由于事务级别是 READ-UNCOMMITTED 这就是脏数据了 然后左边的用户rollback撤销 左边用户自己肯定也就没了 右边用户自然也就同步了 这种情况对于右边的用户，看到了事务中间过程的数据--未提交的就是脏数据，然后换个角度来讲，对于右边的用户来讲，这个\"33 zz\"行 一会出现，一会又消失，就是幻读啦。 2、再看看地中事务级别-read-committed 重启服务后看下当前两个窗口的事务级别： 左边的用户插入 左边自己自然可见 未提交的时候，右边用户select是看不到的，因为当前事务级别是read-committed 然后左边用户commit提交一下 此时右边的用户就看到了👇。 虽然上面演示完了，但是存在一个点，就是右边的用户开不开其事务，其实在这个实验中效果是一样的，已测试。 存在第二个点，这个是疑点，就是用户begin;开启事务后，你别的人重启数据库，然后该用户虽然界面看起来没有变化--还是在myslq交互界面里的，但是他继续commit就会报错了，当然commit之前insert的数据其实就丢了。看下面的过程演示：👇 1、用户begin一个，然后insert一行，未提交 当然自己可见：👇 还未commit哦。 2、别的用户重启服务 3、回到左边的用户，myslq还是登入着的 但是像接着之前的事务，进行commit，就发现报错了，server has gone away 然后实际上，之前的insert 记录就丢了，下图👇39记录o5o就没了。 3、看第三个事务级别repeatable-read 重启服务，哦，对了删掉，这是默认的事务级别，不用特地手动写。 确认事务隔离级别 开始实验测试可重复读的效果 可重复读嘛，就是B一开始读了后就不会变，但是要注意右边窗口要开启事务的，左边随便开不开都一样，因为这个级别就是可以重复读机制。 左边删了好多，并自己可见，且commit了 右边还在 右边用户只要退出自己的事务commit一下，就可以看待最新的数据了。 所以这个默认的repeatable-read重复读机制是说的一个用户开启了事务后，重复读的结果一样不变，适用于备份，也就是说备份的时候要开启这个事务日志来备份。 4、第四个也就是最后一个事务级别serializable 重启服务后确认 这种begin只是开启事务，但是没有读也没有写，所以不叫 \"未提交的读事务 \" SERIALIZABILE 可串行化，\"未提交的读事务\"阻塞修改事务，或者未 提交的修改事务阻塞读事务。导致并发性能差 此时右边有一个 未提交的读事务 ，所以其他人的 修改都不可以了，看下 此时右边窗口随便开不开事务了应该， 开启事务也是一样的效果 插入一样阻塞在那 这个时候，只要左边的窗口提交commit一下，右边卡住的就可以继续进行下去了。 时间太长也不行， 上图commit也可以缓存rollback一样的效果。 commit是提交+退出了事务，rollback直接是撤回+退出事务 19.254sec的耗时👆， 上图的毛病在于，右边的查看要开启事务的，才会被阻塞；不开启不会被阻塞 ①未提交的读事务，可以阻塞别人的修改事务，别人读，开不开都会被阻塞。不影响别人的读； ②未提交的改事务，可以阻塞别的读事务，但是别人需要开启事务，在事务里读才会被阻塞。不影响别人的改。 恢复默认的事务机制后，研究下死锁现象 死锁现象 这里有两张表t1和t2 1、A用户去需改t1表，B用户修改t2表。互相没关系咯应该~ 2、innodb是行级锁， 现在A 修改t1表的100行 ，那么那个100行就lock了 B修改t2表的200行，200行lock 各改各的，没关系吧 此时，新的动作来了A又尝试访问t2表的200行，B呢又尝试访问t1表的100行。 此时就出现了 一跟独木桥，两个人走到中间的情况 这就是死锁了，死锁不用担心，因为数据会自动发现，会自动选择一个事务牺牲掉(rollback撤销掉)。牺牲哪一个呢？哪个下的成本大支持哪一个，放弃另一个，所以基本就是看执行时间哪个久一些，就放弃另一个。实验看下是不是这样的👇 所以接下来看下死锁的实验 1、两边都开启一个事务 2、然后用两张表来模拟一个是students表，一个是teacher表 左边用户相当于用户A，update一下，注意哦之前是两个用户都开启了事务的。 左边修改teachers表：相当于这个 右边的窗口就相当于用户B去修改一下teachers表 各改各的，目前不相干还。 然后左边A同样改右边B已经改的那一行 因为B已经加了行锁，注意实验的时候会超时哦，上图会自动断开的，趁着还没断开，去右边的B用户执行一下A已经加锁的哪条命令， 会立马发现两个现象 ①B命令敲下去的瞬间，就会报错 ②同时A卡在那边的继续进行下去了，就是A人家已经干了那么久，系统就优先保障A了。 一些猜测 左边A 右边B 此时 左边A可以读右边B修改的那个表的 同样右边B也课可以读左边A修改的那个表的 不是说写锁是别人不能读的嘛？ 如何解决死锁的问题 死锁的原因是👇 避免死锁就要规范用户行为 查询的次序是一致的。上图可以避免死锁，不过锁还是在的哦，就是 A改T1表的时候，B去改就阻塞着，等就行了，不过现在又超时机制的，也没事，就是死锁是程序问题，而正常的锁就是阻塞超时就行了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:48 "},"25-MYSQL数据03/4-各种日志管理.html":{"url":"25-MYSQL数据03/4-各种日志管理.html","title":"第4节 各种日志管理","keywords":"","body":"第4节. 各种日志管理 已经删掉了这一行，且提交，由于默认的事务级别是 \"可重复读\"，所以另一个窗口由于实现开启了事务，所以还是可以看到整个被删掉的数据的。 既然这个用户出现了幻读，那么站在他的角度就是可以改的，看看效果 结果发现没有变化 delete也看起来可以敲下去，但是并不会变化，幻读的结果还是比较稳定的，这种\"可重复读\"的机制，不仅仅是当初修改的人怎么修改提交，这边都看到的是进入事务的时候的看到的值；而且不管自己怎么改、删，自己看到的值还是当初进入事务的时候看到的值，所以这里又对REPEATABLE-READ有了新的认识。 而且敲下去显示的结果是0行受到了影响。 事务日志 MyISAM是没有事务日志的，所以改数据是直接修改的，怎么个直接修改呢，就是把数据库里的数据调到内存里，内存里改完后，直接覆盖到磁盘上的数据库的文件里去了。 InnoDB是有事务日志的，是把数据库的数据调到内存中，把内存里的数据修改后，存放到事务日志中，把什么存放到事务日志中，把内存中修改的过程。事务日志操作一段时间后，再把数据库的更改写入磁盘里。 ​ 写入事务日志有磁盘I/O，将事务日志的更改写入数据库也有磁盘I/O。效率就收到影响。但是多了一层I/O也没有太大问题，因为两个I/O没错，但是事务日志的I/O其实就是类似echo xxx >> filexxx 这种顺序磁盘I/O，然后有无事务都涉及的数据库的修改的磁盘I/O这个是随机的，因为你也不清楚数据库上修改的是哪一行，哪一个数据块（因为你可能是改的第一条记录，也可能是改的第二条记录），所及磁盘I/O。 ​ 事务日志就是追加的磁盘I/O，是顺序性的。所以性能消耗没啥。 预写式的，write ahead logging其实就是，有I/O log的一半都是先写日志，再写磁盘，类似ext3文件系统。 数据库的日志文件 事务日志 transaction log 1、规范：和数据文件分开放，好像log和data本来就得分开来，就像程序和data一样--否则项目程序文件移植后，数据混在里面会有问题。 我遇到得问题就是我是 windows本地编辑，远程调试得，结果呢数据也是放在同级别的代码目录的，所以我的windows上的项目程序里式没有数据的，而远端linux上的项目文件里是有数据的。移植起来就不能说windows同步到另一台linux上，这样没有实时数据支撑，代码的结果可能就不是最新的也就不对了。 2、事务日志的相关信息 innodb_log_buffer_size： innodb_log_file_buffering： innodb_log_file_size innodb_log_group_home_dir 事务日志存放路径，默认写的是相对路径./ 其实就是当前数据库的路径也就是/var/lib/mysql/下，具体的文件就是这两个文件ib_logfile0和ib_logfile1。写满0号文件，然后去写1号文件，写满1号文件后再覆盖0号文件，就是这样来回写。 ​ 为什么是2个，因为变量innodb_log_files_in_group，同时它也是服务器选项。但是我修改了重启服务后，还是看不到该变量，不过默认倒是1，因为ll /var/lib/mysql里就看到1个ib_logfile0。 参考一下视频里老师的变量情况吧，聊胜于无看看呗 上图👆这个file_size就是ib_logfile0/1的大小5M，而in_group 2就是有两个ib_logfile0/1的原因。生产中 大小和数量都需要调大一些。为什么呢，举例①如果一个大事务，第一个文件ib_logfile0写满了，然后写ib_logfile1又写满了，然后有翻过头来写ib_logfile0这样一个事务日志都不全了。 此外上上图还有个buffer_size8M，是缓存 还有个bock_size块大小，512字节， 顺带一提ibdata1就是 包含了 数据库的数据+索引。当使用服务器选项innodb_file_per_table后就会拆分出来放到每个数据库目录里，具体见https://oneyearice.github.io/25-MYSQL%E6%95%B0%E6%8D%AE03/1-%E7%B4%A2%E5%BC%95%E7%B1%BB%E5%9E%8B%E5%92%8C%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86.html?h=innodb_file_per_table 看下重要变量： innodb_flush_log_at_trx_commit 看到了1这个值，其实一共可以设置成0 1 2 3 ，共4个值。 注意哦，不是数据库文件的写入哦，是logfile--是日志文件的磁盘写入--是以这个作为数据的最终存放目标来讨论的。 1、理解buffer(缓冲)和cache(缓存)，浏览器的叫什么叫cache 飞机着陆叫什么缓冲，所以一般来讲，读-缓存，写-缓冲。 2、上图没有画出来的是磁盘里面也有自己的缓冲buffer。 3、然后才是上图本身要表达的意思 0 ，该机制，当你insert into tab 并且提交的时候；首先提交到日志的缓冲log buffer；然后每秒写到系统内存并且紧接着写到磁盘里去，到了磁盘自然就是磁盘的缓冲啦，那个这里不讨论。 1，该机制，插入，提交后；首先一样也是提交到logbuffer里，于此同时立刻写到系统缓冲里紧接着写道磁盘里，就是说commit一提交，就直奔磁盘去了，中间该走的log buffer,system memory就按部就班的走。 这个机制磁盘I/O消耗大，磁盘停不下来。 2，该机制，插入，先提交到logbuffer里，紧接着写入os cache，然后1s一次写入磁盘。也就是说你可能1s中有大量事务并发到os cache里后，才会一起写入磁盘，这就降低了磁盘的I/O。 缺点：1s没到呢，还没写磁盘呢，如果此时有100个事务已经提交logbuffer+写入os cache里，此时断电了，还没写入磁盘呢，所以数据就丢了。是确确实实丢失了100次事务，100次的订单提交，100次啊，一百遍,一百遍,啥看不懂，唐伯虎点秋香啦。 其实很好理解和记忆啦 你要问我推荐哪个，我从来不推荐，我强制你用1，哈哈~，买高IOPS的硬盘得了。 你看哦，之前我们提到事务日志的好处，就说了如果没commit提交断电了故障了，于是有这个undo，如果commit了出了问题就会redo。 ​ 那好，问题来了，commit了，0 1 2 甚至没讲的3 都出现问题，怎么不redo了？啊，啊字在这里是语气助词，表强调，恩，恩在这里也是语气助词，同样表强调，对，对在这里...哈哈哈，哈哈哈给个屁，傻逼东西，redo是会发生的，人家讨论的是事务的ACID没了，又没讲redo不做了。这是两码事，事务的commit后数据没落地本身就丢失了一致性，但是还有redo来保证，也许可以用2这个机制？ ​ 研究下这个redo 和0 1 2的问题； 回答的太好了，虽然我没看懂~但是它明确的知道了redo和事务日志优化0 12 的区别。 redo不能保障ACID，同样事务日志commit提交按理说能够了，其实commit，哦我知道 总结 1、事务的ACID是通过一系列操作的整体堪称原子，原子性，就是要么都行，要么都不行。所以commit提交了意味着一个原子诞生了，了结了。但是从实际上来看，commit了，数据其实可能是还没落到磁盘上，甚至除了问题都不会落到磁盘上！所以事务的原子性没了！所以1 commit直达磁盘的机制会更加能够保障ACID。 2、再来说redo，可能就是commit了，但是数据库文件里没有，他就redo了。 不知道瞎几把讲的什么东西~，继续学，后面好像专门有讲redo日志。那里肯定有我要的答案。 mysql的磁盘I/O是比较频繁的，监控一定要做磁盘I/O的。 事务日志要放在一个专门的性能好的磁盘上，SSD固态是必要的。①事务日志是顺序往里写的，不存在随机访问的问题，要放在干净的独立的哪怕不是一个硬盘，至少是也给分区也是好的。为什么分区/磁盘要独立的啊，因为你不把事务日志独立开来，你和其他日志也好，数据也好合在一起，虽然事务日志是顺序的，但是一个分区来看数据块由于还有别的日志/数据往里写，所以数据块(磁盘空间)对于事务日志来讲就不是连续的了。什么磁盘碎片化就不连续了。 ​ 那么放到一个独立的磁盘或者分区就是修改这个值了 1、加一块硬盘2G的 我用的VMwareWorkstation 2、发现硬盘 3、分区 看情况lsblk看不到就同步一下👇 4、格式化 5、挂载 创建文件夹用来挂载 挂载 修改所有者所属组为mysql 6、修改mysql的对应变量 重启后报错了 当然status也能看，但是对比journalctl看，可知 还是缺少文件的原因， 缺少就拷过去，就OK了，老师演示的直接就是重启自动生成日志文件了，无需像我一样复制或移动过去，不过我用的高版本的mariadb。 再检查下变量 这样就是得到了一个干净的分区，且分区时高速磁盘哦你要生产中换成SSD之类的硬盘的。 错误日志 error log 错误日志不仅仅时错误日志，还包含数据库的启动关闭日志也里面。一些重要事件可能也会放在里面。 我的mariadb时空值 还是和mysql不太一样，这个是默认空值，然后默认文件是 而且现在的配置方法也变了 视频里的默认值这是之前的版本 该值来源于 测试错误日志 重启服务后，就看到错误日志👇 报警信息log_warnings 这是视频里的值 现在的版本值是2，估计是level，从2级开始记录的意思咯，0就是不记录咯，我猜的。 告警也是放在error日志里的，只不过是👇用括号Warning括起来了。 一个error 日志里不仅仅是error还有很多其他的 通用日志 general log 记录的是各种数据库的操作，比如sql语句 中间断了一下重连了，就是因为另外开了窗口去添加了服务器选项打开了普通日志 通用日志的文件 此时就可以看到通用日志了，比如show 比如select 这个文件就是刚才①配置文件里加了选项②重启了服务后才生成的。本来generl_log是OFF，文件时不存在的。 然后写错命令也一样也看得到 一般不推荐启用，估计也就是找bug找故障的时候才会开一下，毕竟什么都记录太费磁盘，费IO了。 进一步思考，这些通用日志，本身也是数据，何不把日志放到数据库里呢！找张表放呗 这个值👇 FILE就是将log输出到FILE文件，改一改，改成输出到数据库也就是将FILE改成TABLE表，而这个表就是mysql库里的general_log表👇 好，改一下 此时日志就要去mysql.general_log里看了 这个通用日志可以用来干啥呢，有个debug的好处，就是程序开发写的web页面，页面上点点点比如购物，其实底层就是去数据库上select 的，可以看到开发程序上写的的sql语句是不是合适~是不是有优化的空间~。 慢查询日志 slow query log 1、打开页面慢，通过浏览器比如chrome的F12查看network的响应时间，也可以通过curl分析http的性能 curl -Lo /dev/null -s -w time_namelookup:\"\\t\"%{time_namelookup}\"\\n\"time_connect:\"\\t\\t\"%{time_connect}\"\\n\"time_appconnect:\"\\t\"%{time_appconnect}\"\\n\"time_pretransfer:\"\\t\"%{time_pretransfer}\"\\n\"time_starttransfer:\"\\t\"%{time_starttransfer}\"\\n\"time_total:\"\\t\\t\"%{time_total}\"\\n\"time_redirect:\"\\t\\t\"%{time_redirect}\"\\n\" https://oneyearice.github.io/ 一旦发现是TTFB耗时时间长，就可以判断是服务响应慢，而服务响应慢，基本上对于一个业务系统来讲，大概率就是数据库的慢查询了。 2、默认是10s 系统认为10s以上才是一个慢的查询，所以不行，得改。 而且只是有个10s的定界值，实际上没有启用慢查询日志，通过查看slow_query_log可见 同时3s就认为是慢查询了。 此时就看到了慢查询日志xxxslow.log 打开挂着看效果 然而并没有，原因就是我的配置里还保留这上一次配置的log_output=table这就是所有日志输出都不走文件， 查看就去mysql.slow_log表里看 恢复输出为文件 重启服务后 要注意都是DML语句执行结束后，那边才会有日志，当然慢查询日志可不是仅仅select，通过之前的call pro_testlog可知也包括了一个慢的存储过程--其中就是循环了很多次的insert。 · 另外log_slow_filter就规定了哪些类的查询会记录慢查询日志，就是当这些操作超出long_query_time的时候。一般不会改这个值，还有就是这里面没有列出的既是超出long_query_time也不会记录到慢查询日志里。 问题来了，如果一个查询速度没有超出long_query_time，但是有提速空间，比如没有利用索引，可以利用起来加速以下，这种查询操作能否也记录下来呢？ log_queries_not_using_indexes=ON，开启后就会同样记录在慢查询中。 默认是关闭的 开启后确认 此时，哪怕查询没有超出long_query_time，只要没有使用索引，也会记录在慢查询日志中。 select * from students where stuid=20; 这就会利用主键索引进行查询了 1、全表查询肯定不会涉及索引，此时慢查询里就有日志了 2、使用where查询对比有索引和没索引的日志 没有利用索引的就会log_queries_not_using_indexes=ON记录在慢查询里 你再看这个利用索引的查询 此时慢查询里就没有日志啦，因为这是利用了主键查询，是用到了主键索引的，所以log_queries_not_using_indexes=ON不会记录了。 再来，刚才select * from students where name='o6o';是记录在慢查询里的，现在添加name的索引后，就不会记录在慢查询里了。 mariadb的特有配置 log_slow_rate_limit ，就是慢查询达到一定占比此以上才开始记录到 慢查询日志里，图中将1改成5试试，所以这个5不是5次，而是5%的意思。 空的，然后输出格式是 改一下 重启后确认 但是要注意哦，如果连着两下或多次select同样的命令，就会有重复的日志记录了哦，因为有命中缓存了我觉得，所以就不会记录在不利用索引的日志里了。 复习下缓存吧👇 然后explain xxx 每次都是记录的 如果查到确实存在一条慢查询了，那么像继续深入研究下这条语句到底慢在哪里，可以这么看，一个负复杂的select语句说不定里面还带有子查询什么的，甚至涉及连接、多表查询等。到底这个复杂的语句中哪一块导致它慢了？ ​ 举例：select sleep(1) from students; 这个是每一行休眠1s的意思；该表里一共20几条记录，大概就要20几秒才能执行完。 通过tail /var/lib/mysql/tail -f django001-slow.log可见 我把这个slow_log的格式恢复成默认形式先 注销掉，恢复成默认空值👇 再看看slow_log就恢复成原来的简短格式了 然后继续深入研究慢在哪里，就得看profiling这个变量 简单点，就用cli开启了，不配置服务器选项了 然后再次执行慢查询的语句，当然此时就会担心会不命中缓存，不是哦，\"索引利用不到\"本该记录slow_log但是却命中缓存所以不会记录；而这里确实不会命中缓存，所以会每次都记录slow_log的。 然后show profiles可见 关注这个Query_ID，输入show profile for query 7;查看具体详情 可以看到sleep了这么多， 图中涉及一些时间 checking permssions 检查权限 opening tables打开表 等等，发现其中sleep 好多花了很久，而在生成中这个sleep很有可能就是某些sql语句。 👆这招很有意义的 再看个其他例子 默认是分开来放的👆 跑一下这个存储过程，就会导致数据库文件不断变大 然后以事务的方式跑这个存储过程 在跑到时候，就可以看到testlog.ibd这个数据库文件在不断的变大；就是说事务的写法不是说单纯地只写道事务日志里地，其实也会写道磁盘里的，其实就是那个事务优化地0 1 2级别好像是 不过我的这个以事务运行的怎么还卡这么久啊，都TM5分钟过去了 好了 此时记住这个数据库文件大小 此时rollback，撤销，数据就写入，理论上应该文件大小会缩回去，但是不是！ 大小是不变的 也就是说事务撤销了，数据不写到表里了已经，但是数据库文件大小还是涨上去了，不变地。 然后再看不以事务方式，就是真正地往里面添加记录了；注意此时testlog.ibd的大小还是上图的33554432字节。 等你执行完，真正添加记录进去后，就会发现testlog.ibd文件大小没有变化 1、rollback，后悔了表里的内容请了，文件大小不会缩回去 2、说明增加的空间大小，其实里面内容是空的，占了磁盘空间，但是里面没有内容。就相当于一个空文件。 3、所以如果把这个表整个情况了，还是一样的ibd文件大小还是不变 所以你就会发现命名数据都没了，数据文件大小还是老样子。数据都没了，还占什么空间呢。 4、所以可以做一下优化，optimize table testlog;整理一下， 之前的占用的空文件就释放了👇这样testlog.ibd大小就缩回去： 数据库的操作日志 用户账号家目录里的隐藏文件 二进制日志 binary log 见下篇 中继日志 reley log 见下篇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:48 "},"25-MYSQL数据03/5-二进制日志管理.html":{"url":"25-MYSQL数据03/5-二进制日志管理.html","title":"第5节 二进制日志管理","keywords":"","body":"第5节. 二进制日志管理 介绍 二进制日志就是二进制数据，不是文本文件 上图可见之前学习的各种日志都是文本文件，事务日志也不是文本文件。 事务日志就是利用事务的操作记录，但是几个文件反复覆盖保存的，只是记录最近的操作，就几个文件来回写的。 二进制日志不依赖于存储引擎，不是说一定要支持事务才行。不管是MyISAM还是InnoDB都可以使用二进制日志。 二进制日志记录的就是对数据库的增删改，不清理的，不断记录的，如果能够打开这个二进制日志就可以知道，什么时间做了哪些操作。 默认二进制日志没有启用的，微软的SQL Server和oracle也是一样都默认没有开启二进制日志的，也是只有事务日志，其中oracle不叫这个名字，而是叫\"在线的重做日志\"；其中二进制日志叫\"归档(重做)日志\"。 二进制日志不仅仅是审计，关键知道做了哪些增删改，所以还可以做数据库的还原。 1、完全备份结合二进制日志就可以做到恢复数据 2、比如上图的周五数据崩了，你用周一2:00的完全备份+之后到周五的二进制日志进行\"重放\"就可以恢复数据。 3、启用二进制日志后，默认是和数据库文件放在一个目录下的。 默认sql_log_bin是开启的，但是还不够，磁盘上还看不到二进制日志文件， 还需开启另一个日志开关--log_bin sql_log_bin和log_bin都启用才行，才算启用二进制日志功能。 可能正常会觉的开关设置两个有毛病，其实还真不是，sql_log_bin可以临时在会话里修改，log_bin只能在服务器选项也就是配置文件里修改。 所以一般玩法：①log_bin在配置文件里开启②然后用sql_log_bin去临时关闭二进制（潜台词就是sql_log_bin就是默认开启的）③也就是说一般都是开启二进制，然后要临时禁掉就用sql_log_bin去在会话里禁掉就行了，而且也只是session级别的，不是全局的。这样就比较好些。 这里有个点，就是log_bin=ON和log_bin=1和上图只写log_bin的区别。都是开启，但是有文件名生成的区别。①log_bin=xx，二进制日志的名称就是xx.00001和xx.index②所以log_bin=xx，xx是用来表示的文件名的前缀。 而不管xx写啥，进去看变量都是ON的。 然后就看到二进制文件了，不过时间好像有点奇怪，不奇怪，看清楚啊，是一致的，只不过data看到的是PM，而ll看到的时间是+12的。 如果时间真的不一致，是因为mysql有自己的时区设置不是用的SYSTEM了吧，下图倒是用的系统时。 然后思考一下由于默认binlog是和数据库文件放在同一个路径下的，万一数据库挂了，宿主机挂了，都GG了。 所以：1、数据完全异地备份；2、二进制文件和数据文件分开也要异地备份 将bin-log放到别处 1、创建文件夹，同样最好单独一个磁盘SSD的，binLOG肯定也是和事务日志一样顺序I/O的。这里就利用之前事务日志存放的磁盘了做实验，实际生产中肯定要单独存放的。 文件夹也要注意权限 3、修改配置文件里的服务器选项，并制定路径和前缀 mariadb-bin就是将来生产的binlog的文件名的前缀，重启服务器后👇 刚开始就是330个字节，后面随着增删改操作越来越多，该文件就会越来越大，增长速度非常快；更可怕的是bin-log的大小要远远大于数据本身(testlog.ibd)的大小 二进制文件我们知道可以dump的，具体就是hexdump -C hexdump -C mariadb-bin.000001 不过不是这样看的，这样只是二进制的一个通用性看法，聊胜于无的看看。 一个call pro_testlog就是往testlog表里增加10W行记录， ①数据库本身增长多大 ②bin log日志增长多大：27M，近似于数据文件大小的2倍。 所以binlog要找一个 大硬盘、ssd的独立就给他存放。 此时如果将sql_log_bin置为OFF，此时binlog就不会增加了。这是种临时处理方法 注意一定是同一session，因为sql_log_bin我们通常就是临时在交互模式里手动改一下改成OFF的，所以必须在原来置为OFF的窗口或者会话里DML才不会导致binlog增长，别的会话不受影响--也就是别的会话如果有增删改还是会导致binlog继续增加的。实验就是在OFF的窗口进行DML，然后观察binlog就会停止增长。 说是DML，其实DML不涉及select查操作，也就是select不会导致binlog的增加， 二进制日志binLog存放的数据的格式 STATEMNT、ROW、MIXED 3种格式： 举例，delete from testlog; 这个testlog一共有10W行，问，binlog里记录的是一条delete语句还是删了10W条的动作。 1、STATEMENT：表示记录的是delete from testlog;本身这条语句； 2、ROW：表示根据sql修改的哪些行，就记录那行记录的，100W条删除就会记录100W条。 默认格式新版的mariadb是MIXED 改一下，然后看下STATEMENT的效果 由于STATEMENT只是记录你敲的原本命令行，所以binlog二进制日志大小增长的很少 现在改成ROW模式，看看大小增长的幅度，因为ROW模式是你虽然敲的是一条CLI，但是影响了多少条，就记下来多少条。 可以看到同一条delete语句，binlog选择ROW模式就要比STATEMENT模式，实际记录的内容多很多很多 再来，刚才STATEMENT格式下，delete from testlog;是一条语句咯，如果同样模式下用call pro_slowlog;就不是一条语句咯，这个存储过程里其实是2999条的insert，对吧，我们知道binlog，啊你们不知道，哦没事我知道就行了，我知道binlog呵呵，通过上文介绍知道binlog是记录了增删改的操作而不是存储过程的一条操作，所以一个存储过程里while循环了多少次增删改，就会有多少条需要去记录到binlog，而格式又是STATEMENT，所以此时binglog就记录存储过程里循环的insert条数了。 然后通过服务器选项--也就是配置文件里修改的方式来做binlog的格式修改 1、首先，你会发现重启服务后，binlog会新建一个文件来保存，原来的文件留着的。 不管你该不该配置文件，只要重启，就会另外创建binlog文件。 对比下STATEMENT和ROW 举例，update test set col1=now(); 这句话就是把col1列改成当前时间，如果你用的是 ①STATEMENT，就是这句话原封不动记录下来，这就危险了，十天后DB故障，你恢复就会错误的恢复成现在的时间，而不是十天前的时间。 ②ROW，就不会，就会真正的记录当前的时间，而不是now()这个函数本身，所以就可以保真，所以生产种推荐使用ROW格式。 但是为什么是MIXED默认呢，也许MIXED才是比较合适的。 首先ROW太占空间了对吧，既是数据库自身判断，如果安全起见就用ROW比如它发现insert里有now()可能就会用ROW，而发现没有歧义就可能节省空间使用STATEMENT格式。 二进制日志可以定义自动删除规则 通过expire_logs_days来做，默认0就是为空了，就是不限不删除 反正要监控磁盘的 二进制日志的最大值 max_binlog_size 最大值size文件满了怎么办，没事就新建一个呗都是自动的。 syn_binlog类似事务日志的优化级别012这种 就是是否理解写磁盘，就是binlog文件里先不捉急写，先放在缓存里； 事务日志那会讲过类似的机制，是数据不捉急写磁盘文件里，而是先写内存里再写道磁盘的日志文件里。 syn_binlog=1：只要发生二级制binlog就立即写磁盘logfile，当然这里的logfile就是binglogfile。 这种和之前事务日志1一样会带来磁盘I/O飙高不是合适。 syn_binlog=0：就是操作系统来决定什么时候同步到日志文件，哎，是OS啊，我以为是数据库呢。哦对的，是数据由mysql放到OS的缓冲buffer区，然后就交给OS自己调度了，人家OS看系统进程资源去安排调度什么时候轮到你写道磁盘文件里去。 index文件就是记录了一共由几个binlog binLog的管理方式 是通过mysql自身命令来操作的 1、比如看binlog大小，不用跑到磁盘上去ls看，肯定一样的 show binary 和show master一样的哦。 2、显示当前正在用的binlog文件信息 注意这个344看着是Position位置，其实对比上上图344就是文件大小的意思。当然叫position位置也没错，就是新发生日志数据往这个位置以后去写的意思。 这个Position位置有啥用，首先大小字节是它的单位，将来操作的时候就知道从这个position之前的是旧数据，用于定位恢复数据的。 比如：早上10点不小心执行了一个drop table误删了表，就可以通过binlog把删表的这个位置给找到，找到这个位置用来定位故障点，进而想办法来还原数据库。emm这边比如比如的不太好，后买你看例子吧。 后面两个参数是数据复制用的，后文讲。 3、查看binlog内容 cat基本是乱码看不掉，虽然binlog不是纯粹的二进制文件，cat可以看到一些明文信息，但是肯定不能这么看啦 show binlog events in 'binglog文件名称'就行了 上图看了寂寞，是个空文件，看看实的吧 show binlog events in 'mariadb-bin.000001'; 👇 具体的cli完整版是这样的，还可以指定查看的日志的位置 记住这个位置899722 现在就开始操作这个表，比如insert一条记录 位置从刚才的899722→899980了 所以查看binlog就从这个位置开始去看show binlog events in 'mariadb-bin.000003' from 899722; 可见确实有一条操作记录insert，其实老版本里面是ROW格式的binlog是看不到这个原来的insert语句的，估计还需要打开某个开关才能调出来，老版本的截图入👇。 其实这行就是insert的ROW记录了，write_rows就是写记录了。 下面一行还有一个commit提交。 再插入一行 现在版本里event_type列里有Annote_rows可以明确写出来你敲的命令，还是挺省心的。 以前版本就只有delete_rows,write_rows可以参考，不过可以调出来应该。 这里的show binlog events in 'mariadb-bin.000003' from 899722 limit 2,3; 就是从899722开始，limit2，3就是跳过2行，看3行。 mysqlbinlog专业工具 mysqlbinlog --start-position=899722 --stop-position=899753 /data/logs/mariadb-bin.000003 这个--stop-postion不写就是默认看到最后 也不是加密咯，是base64编码。然后这个base64编码的特点就是用于网络传输比较规范。 加一个-v就能看到了 目前是ROW格式，我们看下delete from students;的记录 还有，别急，这个cli截图还有往下开，要注意上图👆的#Q> 标识，这就是原始sql cli了。而下面的就是ROW格式所记录的影响到的所有行的操作记录。 所以ROW格式，其实就是影响了多少行，就结论多少行的。 然后注释掉 就是变成statement语句型的格式了 得~新版本是MIXED，算了就看看混合型得了 MIXED混合型，系统自行判断得，显然这里不是ROW，否则就是记录4个delete动作了 再看一个删除操作，这个表行数多，效果也明细 就一条~， 数据重做 mysqlbinlog --start-position=508 /data/logs/mariadb-bin.000004 -v > /data/test.sql 具体怎么还原，下一篇整理。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:49 "},"26-MYSQL数据04/26-MYSQL数据04.html":{"url":"26-MYSQL数据04/26-MYSQL数据04.html","title":"第二十六章 MYSQL数据04","keywords":"","body":"第二十六章 MYSQL数据04 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:49 "},"26-MYSQL数据04/1-数据库备份和还原原理详解.html":{"url":"26-MYSQL数据04/1-数据库备份和还原原理详解.html","title":"第1节 数据库备份和还原原理详解","keywords":"","body":"第1节. 数据库备份和还原原理详解 binlog重置 flush logs刷新binlog日志，就是重新新建一个binlog。 清除binlog，可以rm，但是肯定没有purge来的专业 删除binlog日志文件3之前的日志、删除某某时间之前的日志。 不仅仅文件删掉了，Index文件里的内容也更新了。 reset master 初始化binlog文件名称。 reset master to No 初始化的编号人工指定。 相当于把原来的二进制全清空了。 备份概念 差异备份 1、周日是1T的数据做了完全备份， 2、周一做了差异备份，100G的差异做了备份，问题来了，增、删、改都是差异，这写备份的细节是什么，增100G很好备份，改100G呢，我认为就是所以变化的行就都会备份下来的。 3、周二的差异备份，就是从周日开始的差异，而不是和周一比较的， 4、周三周四的差异备份，都是和周日比较的。 5、假设周五的时候数据坏了，还没备份呢，此时如何还原。 6、先还原周日的数据，再把周四的备份还原一下，就行了。 7、周四的差异备份既然是从周日开始的，问，周三的差异备份是否可以删了，周一、周二同问。不能删！因为如果有一个操作时周二和周三之间做的，你想还原到这个操作之前就可以还原到周二的差异备份。 8、周五差异备份还没做，数据崩了，用周日全备+周四的差异将数据恢复到周四备份的那个点，那么从周四差异备份后到周五数据崩溃的时候，这段时间的数据如何恢复，此时就靠binlog了。此时由于binlog日志通常是独立磁盘进行存放的，所以既是数据库坏了可以利用binlog的。 增量备份 1、就是周日全量，周一基于周日的差异，周二基于周一的差异，周三基于周二的差异。 2、故障点如图，如何还原，周日、周一、周二、周三，依次还原，然后再用binlog日志来重做。 3、事实上，msyql的所谓的增量备份，配合一个mysql备份工具，底层用的就是二进制日志。就是每天复制二进制出来作为增量备份来着。 数据的备份策略怎么指定，和业务需求和数据库的大小有关的。 备份方式 冷备：读写均不可操作 1、简单粗暴，就是cp -a /var/lib/mysql 全部复制出来，同时看看log放在哪里的，也一并cp -a ①、比如mysql.sock这个sock文件服务一停了就没了，也无需备份的。 ②、performance_schema这个也不用备份，和性能相关的。 2、配置文件/etc/my.cnf 或者其他地方也有的配置文件也要备份下来。 实验来一个：👇 另外开一台linux，然后将之前的linux上的db备份还原过去。 ①打包数据库 便于网络传输，这点还蛮能提高效率的，一般人get不到这个点，因为如果不压缩，就是散文件，而散文件的传输存在N次TCP的慢启动，所以速度永远上不去；而tar一下后就一个整体文件，此时慢启动慢慢的启动后，达到传输的最大值了就。既然打包了，顺便也压缩一下更加便于网络传输拉。 使用J就是xz的压缩格式 压缩比还是不错的 ②打包binlog ③/etc/my.cnf配置文件也要备份 ④合并一下scp到另外一台机器上 会自动创建/databack/文件夹，结尾的/也可以不写。 ⑤然后去新的一台linux安装maradb-server 要考虑mariadb的版本兼容性，最好是一样的。我这个都是这几天yum安装的，版本问题不大直接yum -y install mariadb-server确认下就行 我靠，版本低了，删掉重做，我才用undo 的方式，一并卸掉之前安装低版本mariadb的时候安装的依赖包 先undo 16，在undo 15 然后找一下原来10.11.2的版本 ①找到原版本的思路有2，第一个就是去原来linxu的yum源文件看 ②就是去mariadb官网找10.11.2啦，一般可能就是10.11也就OK了 https://mariadb.org/download/?t=repo-config&d=CentOS+Stream&v=10.11&r_m=neusoft 发现及时用原来的yum源文件，此时安装也已经是10.11.4了，没关系，继续 注意安装后的检查也要使用大小写的，虽然yum的时候不分大小写，但是rpm检查的时候却区分大小写的。 ⑥此时由于服务没起，所以/var/lib/mysql下还是空的 然而并不是，我发现里面安装后有东西 删之~ 覆盖也行，好习惯cp的时候cp -a 啊，还有 -b 啦 然后binlog复制过去 担心父目录权限？ 默认有x就行了， 错了错了，无需事先创建文件夹，直接解压就会自动递归创建所需目录 权限也是OK的。 关键你是带目录压缩的吧，你试试当初不是tar -Jcvf /var/lib/mysql 估计就不行了 启动服务查看数据是否过来了 都OK了没问题 冷备的缺点是需要停服，好处是备份快，但是一般用不上，除非已经停了或者就是要维护一下，可以顺带cp一把。 温备：读可以，写不行 热备：读写均可以操作 热备是比较实用的，据说是读写都行，不知道性能是否受影响，然后MyISAM不支持热备，只支持温备--备份的时候不能写只能读，InnoDB是读写都行的。 你做备份的时候，热备的时候，本身是支持读写，如果有大量的增删改操作，你备份的数据其实就不是可靠的！因为数据本身都在变，要保障备份数据是一个时间点的所有数据状态。虽然备份这个动作的过程是半小时，但是备份的数据必然是一个时间点的所有数据。 1、如果是锁表不让写，那么就是温备了，MyISAM就是加共享式-读锁，不能写了就，进行温备。 2、其实很自然就想到事务的级别里的 \"可重复读\"的隔离级别，利用这个来做备份，记得将事务的上文里肯定也提到备份例子的。 3、mysql这个系统数据，到现在也还是基于MyISAM的，不是！老版本还是得，新版本都是Aria引擎，而且mysql.user也只是一个VIEW视图了，本身上还是 即使现在Aria引擎，也不支持事务 aria其实可以理解为不支持事务，但是有一点点的修正就是 https://mariadb.com/kb/en/aria-storage-engine/ 涉及到mysql这个库的，一般就是账号、授权、存储过程这些了吧，一般这个库不大，也不怎么变动。这个库就是不管是早期的MyISAM引擎还是现在的Aria引擎都不支持事务级别去做热备，不过由于这个库和业务不想管，别人也不会频繁去读写它，所以完全可以做温备--也就是加上共享锁，不让别人写，这个时候再去备份就是温备就行了。 通常就是热备针对业务数据库，温备就是针对系统库mysql这种。冷备就是停服维护的时候复制便捷。 物理备份 冷备就是物理备份，或者磁盘dd if=/xxx of=/xxx ：注意一下如果是冷备的复制文件，要知道即使看到文件的大小是比如100G，但是可能很多表格里都是空的也是有可能的，前面章节👇就讲过optimize整理一下表格就释放了。 一般来讲，不可能让你运维进到db的库里去optmize的，所以运维人员如果做冷备，其实就是明明知道实际数据可能没有这么大，也还是要复制这一堆文件的。也就是说冷备存在空间占用较大的情况，说白了就是有点浪费空间。 逻辑备份 逻辑备份时备份的实际数据，是把数据抽出来，进行备份。 而且也不是热备或是温备，而是相当于一个大的select ，所以和存储引擎无关。 缺点：相当于一个大的select，内部的一个client和server的sql语句啦，就是交互的cli方式，效率是低的。再一个就是如果数据库里存放的是二进制数据或是其他非文本数据，导出来的时候可能就会失去准确性。 1、数据肯定要备份的， 2、事务日志如果是正常stop服务，事务日志的记录其实都是已经写到文件里去了，当然最好是也备份一些；二进制日志一样最好也备份。 3、程序代码，这些存储过程、函数、触发器、事件调度器，这些其实不推荐用的，但是有的就用了，此时就需要备份了。不过这些比如存储过程，好像都在mysql系统库里的。 4、然后就是服务器的配置文件，也需要备份的！ 上面讲了备份的一些概念，冷、温、热；物理、逻辑，其实也就是一个冷备进行了操作，下面解释备份工具， 1、cp、tar、chown改权限这个都是冷备，或者是/etc/my.cnf配置文件的备份。 2、LVM利用逻辑卷的快照，这个一般来讲还是比较快的，我的泛微OA 500G的EC，快照在15分钟的样子，当然不包含sqlserver哦，那个是物理机windows的。做法就是①加读锁，②做快照，不过我的泛微OA是vsphere快照要15分钟唉，难道LVM的快照会秒做的？ 视频里老师说是几乎是1s就做完了，几乎达到了热备的效果，所以写个脚本先做读锁，在秒快照，然后解锁恢复。 但是生产中用了VM的快照来做数据的备份，这种方式是用的不多的。 (5) 解释一下：快照里的数据其实是老数据，你不是lock了嘛，挂载一下看到老数据，直接cp出来，这里就和冷备一样 (6)解释一下：快照里的东西cp出来后，完成备份后，立马要删除快照，否则一旦有新的数据发生变化，量一大，就会生成原始数据（什么意？就是快照里的东西你看得到其实还是老数据，只有这些老数据发生变化了，才会真正地生成一份老数据落下来，所以有问题的还是，因为数据没变，老数据其实是链接，你解锁后，哎没关系啊，解锁后发生变化大不了，老数据就从链接变成了真实的落地数据不就行了么，也不影响你的复制的时间一致性的） 当然这个LVM的快照本身不涉及conf文件和binlog的。 mysqldump属于常用的备份工具，不过这个是针对全备的，其他差异、增量 都不支持的其实，他的增量备份是借助于手工处理binlog来实现的。 xtrabackup是支持各种复制的备份，全备、增量都OK，是直接cli就支持的。 mariadb backup是高版本的mariadb才支持，不过这个视频是好多年的了，现在用基本上就支持了，然后这个mariadb backup是基于percona xtrabackup2.3.8也就是上面一个备份工具做的二次开发，个人感觉可能更优秀。 mysqlbackup，听名字就是mysql的专用，然后也是收费工具。他其实就是MySQL Enterprise Edition 企业版的功能来着。 mysql是orcale公司的，做了两个版本，一个社区版，一个是企业版。 mysqlhotcopy，名字里有hot，其实是冷备来着！是perl语言写的，这个perl语言本身国内用的就少。 稍后重点学习mysqldump、xtrabackup，然后个人觉的mariadb backup也要学，应该看着是优化了的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:49 "},"26-MYSQL数据04/2-mysql备份还原mysqldump使用.html":{"url":"26-MYSQL数据04/2-mysql备份还原mysqldump使用.html","title":"第2节 mysql备份还原mysqldump使用","keywords":"","body":"第2节. mysql备份还原mysqldump使用 mysqldump mysqldump是mariadb-client安装的时候就自带的 查看mysqldump的帮助信息： -A 也不是真的如上图所说是all-databases，不会备份information_schema，也不会备份performance_schema， 比如可以这么写 mysqldump hellodb # 必须指定一个数据库 mysqldump hellodb students mysqldump hellodb mysqldump -uroot -pxxx mysql 对 mysql这个系统库进行备份👇 1、mysqldump db_name 是把数据库里的东西做了个一个显示，可以看到user的视图，slowlog的insert等等 看提示，关闭binlog 这样就可以了 然后去用mysqldump 看看是否能看到这个函数 先看下一种错误的mysqldump 全备可以使用重定向到一个文件里 mysqldump -uroot -pxxxx hellodb > /data/hellodb.sql 所谓mysqldump db其实就是保存了之前配置的各种命令 下面开始利用这个dump出来的文件进行还原测试， drop 掉先 由于mysqldump出来的备份文件，你们没有创建库的命令，所以还原之前要手动创建库。 同样由于mysql dump出来的内容里没有创建数据库，所以你手动创建的时候其实可以改个名字的， 进去看一下 1、这种方法备份数据就不太好了。万一原来的db名称你忘了呢。随便写一个，用户那边前端底层sql里面人家还是老的库名，业务肯定恢复不了的。 2、还一个，由于这种mysqldump出来的sql内容里没有涉及库，也就是不知道人家当初创建库的时候是否涉及了一些特性，比如字符集，比如引擎，等。这些都是问题。 mysqldump --databases | -B 原来是这样👇 现在改成这样👇 再来看备份的文件，此时-B就会有创建数据库的命令了 删库 然后恢复一下，此时就不需要手动创建一个可能都不知道的具体参数的数据库了 同样也可以不用cli交互模式，不过-e的方式，无需考虑结尾的分号了;就 如果不是drop删库，而是库里的表格丢了，也一样可以用这种方式， 因为mysqldump -rtoot -pXXX -B hellodb里的create语句里是由IF NOT EXISTS条件判断的。 备份多个一样是-B 注意上图一样hello mysql其实不加-B就判定未hello库里的mysql表了。 再看看-A全备，就是不备份information_schema和performance_schema 果然少了那两个：一个元数据，一个性能 进一步思考-A不会备份information_schema和performance_schema，那么会不会丢失数据呢？ 存储过程、触发器是放在mysql这个系统库里的，所以不会丢的。 上图就是说明一下函数、存储过程、触发器这些可能都是跟这个某一个数据库走的，drop database hello;后，里面的函数也就没了。 -A就包含了-E 和-R因为这些东西基本都在mysql库里，包括--triggers触发器也是在-A里就包含了，都是在mysql系统库里。 mysqldump和binlog 1、夜里2点mysqldump做了全备，第二天18点数据库崩了； 2、二进制是独立的日志记录，是独立于全备的另外一条线索 3、此时用mysqldump的全备文件，只能恢复到夜里2点的数据，还需要结合binglog才能补全恢复数据。 4、此时就需要知道2点全备的时间节点(也不一定是用时间来比对，可能就是记录下全备对应的binlog位置就行了)对应的binlog日志里的位置 5、为了记录下来当时做全备的二进制哪个位置点，就需要mysqldump --master-data这个选项。 --master-data=1和=2的有意义，没有涉及主从复制的时候=2就行了。=1就是加了个change master的命令；=2就是只是一个comment symbol注释而已，不会像=1一样生成一个change master to这个命令。=2是加注释，注释里面记录全备动作对应的binlog的位置。 这样就能在注释里看到binllog的位置position了 -- 两个横线，被注释掉了；这就是--master-data=1的效果，=2就是注释取消--删掉。 所以看到这个MASTER_LOG_POS=2998040;就知道mysqldump全备的动作是对应在binlog里的2998040这个位置的，对应这个位置做了全备，也就是之前的binlog是老日志，之后的就是新日志了。全备要+上从这个位置以后的binlog。 比如现在开始有数据更新了，模拟从这个位置开始以后存在数据变动 数据变动了，binlog就增长了 再加一条 此时二进制日志被我们独立的存放了，也不太担心数据库崩坏。 现在就模拟故障场景 有全备，然后又二进制的新增，然后坏了 删库模拟数据库崩了 就只删除mysql/下面的文件，mysql文件夹保留了，这样就无需再手动创建文件夹+修改所有者。 虽然此时数据库没了，但是二进制文件binlog分开放的，还在 还原操作开始 1、重启数据库服务，会生成崭新的数据库文件。 我实验的时候报错了， 我先重启下看看，重启还是不行，修改 没用，还得谷歌 https://stackoverflow.com/questions/60248748/could-not-increase-number-of-max-open-files-to-more-than-4096-request-4214 找到了，AI有时候还是不靠谱 不过，重置mariadb也行的，啊哈哈 改一下 有进展，但还不够 重装算了，操！ yum -y remove mariadb-server yum -y install mariadb-server 好奇怪啊，重装后，rm -rf /var/lib/mysql/* 后重启立马就起不来了 算了故障演示就不能用rm -rf /var/lib/myqls/*了，就算用这中方式，也只能用重装来弄，否则查这个错太麻烦了 反正重装后， 这个值也不用动了。 好了，小插曲，继续恢复数据吧 现在重装mariadb-server后的数据都是光的，开始恢复 1、准备好全备文件，binlog文件 完了，我没有备份配置文件，哈哈哈!反面教材，算了手动意思意思吧 2、要还原之前假设好数据库一部分用户还在用着，此时需要停服维护 ①再一个停服维护的文本里写上这个机器的信息，让相关人员可以看到，其实优化出来就是一个web展示页面 ②修改/etc/my.cnf里的选项，补一个skip-networking=1就关闭3306端口 3、先把全备文件还原掉 注意，mysql ①停止数据库的binlog先 一般生成中binlog已经开了，两个开关，都需要打开，所以关闭binlog就是进入cli交互模式去关一个就行了。之前我是重装了mariadb，所以 binlog没开，我先开了去配置文件里。 OK了，然后，使用cli关闭binlog，导入全备文件 使用 set sql_log_bin=off；就行了，这个变量是会话级的，不会影响别人，且退出失效。 此时binlog就临时禁用了。 此时binlog的位置在 而全备的动作所在binlog的位置在 上图可见是从mariadb-bin.000004这个文件的2990804这个位置之前的binlog有全备。 这个时候的一个规范操作就是，①由于你停服了，所以数据库不会变化了，binlog不新增了②所以此时可以刷新一个binlog，从新的binlog之前到--master-data那个位置就是全备以后需要补充的binlog 所以就是需要从全备补充的binlog涉及这些文件 呵呵，虽然好多就是空的，但是这里是演示嘛，严谨些也是要这么做的。 关键cli： mysqlbinlog mariadb-bin.000004 >> /data/inc.sql 下图写的有点问题，就是第一个文件它只是精确到了文件，没有精确到位置，所以cli要修改 mysqlbinlog --start-postion=2998040 mariadb-bin.000004 > /data/inc.sql mysqlbinlog mariadb-bin.000005 >> /data/inc.sql mysqlbinlog mariadb-bin.000006 > /data/inc.sql mysqlbinlog mariadb-bin.000007 >> /data/inc.sql mysqlbinlog mariadb-bin.000008 >> /data/inc.sql mysqlbinlog mariadb-bin.000009 >> /data/inc.sql mysqlbinlog mariadb-bin.000010 >> /data/inc.sql mysqlbinlog mariadb-bin.000011 >> /data/inc.sql mysqlbinlog mariadb-bin.000012 >> /data/inc.sql 马上就开始还原了， all.sql全备+inc.sql里的all.sql里的--master-data的位置到行尾 source /data/all.sql 先还原全备 你看就已经恢复全备的东西了 在还原inc.sql这个binlog补全的 source /data/inc.sql 两个数据就回来了 退出，sql_log_bin就直接恢复成ON啦。或者set sql_log_bin=on; 然后修改/etc/my.cnf里的选项打开对外服务 以上就是数据库的备份和恢复， 不过前提就是，binlog得安全得保存起来，以及全备一样得妥善保存。 1、二进制日志启用 2、msqldup -A --master-data=2 > /data/all.sql 数据修改 insert students (name,age) values ('a',20) insert students (name,age) values ('b',40) 3、删库 rm -rf /var/lib/mysql/* 模拟故障，实际上可能生成中就是部分业务故障。 4、还原 卸载mariadb-server，重装 4.1、通告维护 4.2、对外停服，修改skip-network=1 4.3、systemc restart mariadb 4.4、mysql > show master logs; 查看当前binlog位置，就是为了取--master-data到这个位置的binlog去还原。记录此位置值 mysqlbinlog --start-postion=xxxx mariadb-bin.0000x >> /data/inc.sql mysqlbinlog mariadb-bin.0000x >> /data/inc.sql mysqlbinlog mariadb-bin.0000x >> /data/inc.sql 5、mysql > set sql_log_bon=off; 5.1、mysql > source /data/all.sql 5.2、mysql > source /data/inc.sql 6、mysql > set sql_log_bin=on; 开个屁，直接\\q就行了，session级退出就没了。 7、恢复配置文件里的skip-networking，或放开防火墙对外。 8、检查数据，确认恢复。 如果是人为故障比如删表了 下午2点完全备份 18点老6删表 18点10你开始处理去恢复数据 好了故事讲完了，开始恢复吧 思路和之前一样，就是 ①全备文件恢复到2点 ②找到binlog，vim进去去掉18点的那个drop table students;这个删表的命令，恩？你问我为什么去掉，你不去掉，待会恢复binlog一样还是删了啊 ③恢复的操作就和上面一样了，这样的好处是能将数据恢复到18点10分。 再来一遍实验 看着像靠谱的，就是mysqldump默认是单表事务，但是有外链就要多表一个事务了。👆后文会讲事务下备份。 1、全备-模拟图上2点的全备 2、接着模拟2点以后数据变化，以及删表操作。 模拟18点删表了 模拟后续新的数据变化，你只是删了students表，其他业务可能还是可以正常，比如teacher表的写入。 这个时候你收到故障告警了，或者有人找你，删库的老6电话你了，这个肯定不比告警慢了~ ①禁止用户访问，就是skip-networking=1或者iptables deny掉；或者加锁flush tables with read lock；这样不是太好，因为还是能读，数据都被删了，读也有问题了；然后skip-networking=1也不是太好，因为还要重启服务，iptables不需要重启服务，比较方便，其实无所谓。一般都停服维护~ ②开始还原 要注意，还原的时候，要去掉18点的drop删表cli，然后就可以还原到18点10分停服的时候了。 去全备的文件里看下--master-data里的binlog位置。 是15编号的344位置做的全备。 也就是从15binlog344位置往后一直到停服的时候 其实停服的时候binlog也要停掉了，不过停服了binlog自然也不会有了，待会还原的时候停掉binlog就行了。 我们可以刷一下日志 就是17以前的，15以后的都是要处理的文件。也就是看起来舒服点的操作。 就是说17这个新binlog，是我们拒绝了别人访问后刷出来的，不应该再有什么新的数据了，不用管了就。 mysqlbinlog --start-position=344 /data/logs/logbin/mariadb-bin.000015 > /data/inc.sql mysqlbinlog /data/logs/logbin/mariadb-bin.000016 >> /data/inc.sql 找到drop那一行，删掉，规范点就是先复制一份出来，然后再改。 关闭binlog 现在有个问题，你全备里面是普遍存在删表后重建操作， 1、你删掉所有库，拿全备去恢复 2、你直接不删所有看，直接用全备覆盖，这个就有一个问题：如果全备动作之后创建了一个表，你拿全备去恢复，它就不会DROP掉，不知道会不会有什么影响。 然后你还有binlog去补差价，到时候这个表就有问题可能，所以可能还是推荐删掉所有的库。 cp -a /etc/my.cnf /root 备份配置文件 systemc restart mariadb 17以后都不用管，包括17，都是rm和yum自动生成的。 关掉binlog source /data/all_2023-07-20.sql source /data/inc.sql 然后就是退出，修改注释配置文件里的skip-networking，对外服务。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:49 "},"26-MYSQL数据04/3-mysqldump实战和xtrabackup介绍.html":{"url":"26-MYSQL数据04/3-mysqldump实战和xtrabackup介绍.html","title":"第3节 mysqldump实战和xtrabackup介绍","keywords":"","body":"第3节. mysqldump实战和xtrabackup介绍 -F --flush-logs：存在滚动多次数据库-A -B涉及多个数据库，估计一个数据库就会滚动一次；所以需要配合--single-transaction来做一次事务滚动一次，就是相当于不用 人工flush logs刷新binlog了。--master-data肯定要的，mysqldump动作对应的binlog位置得记下来的。 这个就直接STD 输出到屏幕上了哦，然后再看看binlog 从34开始到45，多了12个。 只会刷出一个binlog -F 滚动的好处也就是flush logs的好处，就是上图的000046号文件是新的日志，和老日志分开来放了，就是新老日志存放清晰，处理备份还原就有明确的000045--老日志，000046新日志，明确的文件分界线就出来了，无需去文件里扒位置去定界了。 看下这条优化备份CLI的实操效果 mysqldump -F -A --single-transaction --master-data=2 > /data/all_`date +%F`.sql 现在000046号文件389， drop table coc; show master logs; 好，此时000046的size达到了711，从389。日志增长了很多。 -F的效果就出来了，结合--single-transaction以及--master-data=2 就是新的binlog文件000047的389这个位置作为mysqldump的节点，去看看all_date +%F.sql文件里必然也写着000047 389这个位置点 相当于flush了一个新日志文件，从这个文件以前的binlog就包含在了全备文件了，从这个节点开始以后就不包含在全备里。 全备已经包含的binlog就可以清一下了，哦，所以binlog清里是这么清的！ 磁盘上自然也同步了 👇这个reset没必要吧，不然你还原的全备没问题，补差价的时候处理binlog就不能直接复制mysqldump --master-data 产生得binlog编号和位置了 得改成新的binlog编号，和位置，不过-F是刷新得，位置不用写了直接编号就行了。 温备-MyISAM不支持事务，要加锁保证数据一致性。 MyISAM备份要用-x，全部加上读锁，这样才能保证数据一致性。 -l 别用了，会导致数据不一致，因为一个库一个库的加锁，可能导致数据不一致。 所以InnoDB不要用这些选项，这些选项是MyISAM才需要的。 热备-InnoDB，开启事务 就是所有的mysqldump在也给事务里，而事务的默认隔离级别是\"可重复读\"，所以可以做到所有数据库的数据一致性。 ​ 但是事务可以隔离DML，不能隔离DDL。请看下图👇 DDL语言：drop table, rename table, truncate table；这些事务针对这些是不具备隔离性的。 注意truncate table和delete from的区别，前者是DDL，后缀是DML。这些事务具备对其的隔离性。 -q就是--quick看来还不错 处理方式 一般来讲MyISAM，-x 对于InnoDB，--single-transaction 但是对于数据库来讲，又有mysql这种系统库是MyISAM的，又有常规的其他所有库就都是InnoDB。 如果 mysqldump -x --single-transaction ，-x就失效了，上图里有提到的。 -A应该就包含了-E -R --triggers --default-character这些了，因为-A包含了mysql系统库。 但是-F和--hex-blob理论上是需要的，然后--deafult-character-set这个需要特定指定吗？不写会不会更好，写的就要看下是否和原本的数据库的默认字符集一致，要写成一致的字符集。 --master-data=1就是有主从复制了。 InnoDB建议备份策略 mysqldump –uroot –A –F –E –R --single-transaction --master-data=1 -- flush-privileges --triggers --default-character-set=utf8 --hex-blob >$BACKUP/fullbak_$BACKUP_TIME.sql MyISAM建议备份策略 mysqldump –uroot –A –F –E –R –x --master-data=1 --flush-privileges -- triggers --default-character-set=utf8 --hex-blob >$BACKUP/fullbak_$BACKUP_TIME.sql MyISAM就是需要加上-x选项。 分库备份 之前-A可能备的太多了，没必要，于是这里开始学习一下分库备份 来看看错误得案例： 上图得grep 用的倒是挺溜得，不过也就是-w的事，写成了^xxx$； 然后上图用for 循环里的--single-transaction是不是有毒，一个循环一个数据一个对立的事务，TM有10个db就要开启10个事务，数据的一致性还有吗？ 明明-B就可以跟多个db，非要秀for。 优化下，取出所有需要备份的数据库 这样👇就可以啦： OK，库都备好了👇： 或者👇这样： 看着有点问题，但是echo出来的cli直接复制是可以的，看着像没有调用bash 优化下就可以了 OK了，大小和第一个命令写法一样 和压缩打包结合一下 然后补充一个视频里错误的思路（还是-B 只带了一个db），但是有意思的写法（sed写的6） 但是生成的命令没有执行，还需要重定向到bash，或者``反斜杠才行。 xtrabackup-更专业的备份工具 他有很多产品，👆这是xtrabackup，其实人家还有监控，还有管理，还有号称比mysql更牛逼的db https://www.percona.com/downloads 8.0对应的应该是mysql8.X 2.4对应的就是mysql5.x 这个工具对比mysqldump的优势 1、速度更快，而且不是像mysqldump那种其实就是通过mysql命令进入数据库然后通过大的select命令来查的。extrabackup是通过数据块文件的复制。 2、备份的时候打断正在执行的事务 3、有压缩功能节约磁盘，这个不算什么优点吧，mysqldump出来也能自己zip一下。 4、备份的自动检验，这个是否能保障备份出来的文件可以正常还原的意思呢？如果是，还不错。 5、开源、免费，应该是对标的红帽企业级的备份工具mysqlbackup。 xtrabackup工具的内部的一些工作机制 2.2版本之前比较烂：其实主要有两个程序组成 innobackupex：per脚本 xtrabackup：C/C++写的 还两个不怎么用：xbcrypt加解密的；xbstream并发用的 xtrabackup 不能备份非innodb表， innobackupex 可以用来备份非innodb表。和mysqlserver的交互就是指进入mysql进行锁表之类的操作。 一个innobackupex脚本，一个bin文件，一个还会调另一个，讲完了。 redo拷贝线程：就是os_thread_create，系统创建线程来做redo也就是事务日志的复制。 ibd拷贝线程：就是系统创建线程，来做innodb的ibd文件(里面有数据和索引)的备份。 innodb拷贝完就开始弄MyISAM引擎的数据库了，主要是mysql系统库吧。 MyISAM的被，本质就是复制frm、MYD、MYI文件，etc说明还有别的文件，难道是相关的log？当然要加全局读锁。当然这些动作都是在一个大的事务里进行的。 停止一开始开启的事务后，解锁，退出xtrabackup二进制程序。接着结束innobackupex脚本。 以上就是老版本的xtrabackup的被过程👆。 从2.4以后，就不再是分来的文件了，而是合在一起，都是C写的了。 下面就是开始安装xtrabackup 注意启用epel源因为需要libev.so，这个在epel源里。这是视频里讲的，我发现libev好像现在也在base源了。反正一个rock.repo里有很多源：base、appsteam 现在的版本早就没了innobackupexu这个文件了，可能2.4以后还保留了它作为xtrabackup的软连接，这是考虑一部分人的使用习惯，后面干脆就连软连接都都没有了。 xtrabackup的用法 --defaults-file --user --password --hosts --databases 就登入的用户名和密码，登入主机，备份的数据库 --defaults-file是读取配置文件，比如/etc/my.cnf，这个选项要置顶 --incremental --incremental-basedir，增量备份，已经基于前一次全备或增倍的目录。 差异备份、增量备份的区别 增量备份，可以手动指到前面去，不一定能够是紧接着上一次的增量。 --incremental-dir：是指定还原备份的增量备份的目录 --include=name：指定标明，格式：databasename.tablename xtrabackup备份还原大致分为三个阶段： 1、备份 上面一小段文字就是讲的备份； 2、预处理：看到这就会想到mysqldump的时候存在这些情况的吧，这个最常用的mysqldump怎么考虑和处理这些事情的呢？印象中mysqldump也就是skip-network一下然后开启单个事务备份，还原就完了 3、还原 下面讲讲细节 图中没有画出①，①其实就是第一步备份咯；\"③就是还原，就是等第②步预处理完了得到完全的数据库文件后，复制到/var/lib/mysql/下，就是这么个意思。\" ②就是预处理；是将1的全备和2、3的增量整合在一起，这个整合并不简单。 看这个图，就能看出来xtrabackup比myslqdump专业的地方了，就是担心你mysqldump在做停服或者dump的时候存在跨越备份时间节点的事务--图中t1事务。 来，重新讲讲上图，当进行备份的时候，比如2这个增量备份的节点，有一个t1事务进行了一半，此时2节点处备份下来的事务log肯定会回滚，回滚到前面；还有一个t2事务它是已经执行完了在2节点备份动作的时候就已经执行完了。所以2节点备份的东西不是一个时间节点的了，因为2节点本来备份的节点理论上时间节点是一致的，但是t1由于是半个事务，是后来回滚了的，数据自然就不一致了。这个说法对吗，t1在事务里，外界是可重复读，哪怕commit也是可重复读的，所以不会影响外界吧，对于外界来讲t1是不存在的对吧？问题是，前一半事务你其实备下来了只不过是回滚了，后一半事务在3节点处其实也是在后一个增量备份里备份下来了，所以t1的前一半事务不要回滚，而是等后面一个增量备份后再去拼t2的后一半事务，否则整体上t1就丢了，一旦t1丢了，这个整体事务丢了，那么就和实际数据不一致了，及时t1执行的时候是 可重复读，外界是不受影响的，但是t1整体是会结束commit的，所以外界就受到影响了的，所以t1的事务需要合并的。 如果3节点处也出现了执行一半的事务，那就回滚啊，也没啥大问题。 所以最后一个备份点的跨越事务才会回滚，而前面的跨备份点的事务是需要整合的，做了一半的事务凑全了就不会回滚了，所以才有了②预处理步骤。 所以需要在cli也就是命令敲的时候，要体现出是不是最后一次备份节点的还原动作，如果不是就存在拼凑不完整事务的情况的，有这个情况就要拼凑的；如果是最后一次备份节点，如果存在不完整的事务就回滚啦。这个情况简称为 \"封口\"，同样存在于sqlserver和orcale。 封口就是不完整的事务不回滚，等后续另一半事务拼凑，就叫做不封口，而cli里明确是最后一次还原--这个动作所涉及半个事务是直接回滚的无需等待因为没有后续，所以就叫做封口。这是口语化沟通的时候存在这么个说法。emm就是行家一出手(口)就知有没有。装B用的现在是。当然正途就是理解别人用的，自己说出来就是装B用的。 备份时不会回滚的，只有还原的时候才会回滚！比如，1节点做的备份，如果也截断了一个事务，没关系，备份里的事务只有一半没事，就时一半在那的，还原的时候才会说这个一半的事务回不回滚。 如果你就是那2节点的备份进行还原，那就是要回滚啦，封口在哪里，那里就回滚。 还原的时候，最后一次你定在那里，那里就会回滚。 备份CLI的选项见后面实验，然后xtrabackup备的时候，会生成一些辅助文件，比如xtrabackup_info文件用来记录工具的版本、cli用了哪些选项、花费了多长时间、备份了LSN--日志的序列号。 这个LSN号其实类似binglog的位置，是OS组织数据以PAGE为单位类似块的概念，然后这个PAGE的 继续解释下LSN： 磁盘上的DB组织数据是按页page来存放的， 类似block--块是OS以及上次FS都是这么组织的，不过OS的块和FS文件系统的块不是一回事。 所以这里我们来总结下，os-block，fs-block，db-page page的一个单位容量要比block大的多了去了，一半yum下来或者编译的时候不改源码，也不会改它，默认就是16Kbytes一个单位分配的。 1、数据备份的时候，会记录PAGE里的一个数值，这个数值就是LSN 2、LSN就是事务的编号，在数据库做任何修改操作的时候，会把事务日志的编号写道PAGE里去。 比如PAGE里都放了一些数据，如果你修改了某个PAGE里的数据，那么这个PAGE页里就会记录LSN号。这个LSN号只增不减。 现在接上图继续说，现在xtrabackup备份数据的时候，就会把最大的事务编号LSN记录下来，比如上图就是10002。 ​ 现在又有新的改动，比如10002变成当时的事务日志编号比如是：20000。 现在再一次备份-做增量备份，也就是说从10002上次备份的点到现在20000这个点，中间涉及的数据做增量备份，那么对应到PAGE上，也就是涉及到的几个PAGE（15000和20000这两个页）才会去处理。 就是根据LSN来判断哪些PAGE需要备份，哪些不用，从而实现增量备份。 xtrabackup_checkpoints：这个就是涉及备份时候记录的LSN和备份后再记录一次的LSN，涉及是全备还是增量，涉及是否是prepared状态，也即是上面讲了这么多LSN的内容介绍。 xtrabackup_binlog_info：同样也会涉及binlog的位置记录，上面讲的是事务日志的位置记录。 所以备份还原的时候也是基于binlog的位置的，不需要认为修改，然后事务日志估计是用来\"封口\"--也就是回滚和同步用的吧。同步就是commit后还确认写道磁盘里的意思咯。 backup-my.cnf 就是备份配置文件 xtrabackup_logfile：该工具自身的日志记录。 使用xtrabackup来做备份 说要密码，好的， 结果说xtrabackup版本不支持 https://mariadb.com/kb/zh-cn/percona-xtrabackup-percona-xtrabackup/ 看看视频里老的版本的时候，支持的情况吧 图中可见mysql是MyISAM的，所以本质上就是锁表+复制。 一些备份产生的文件 除了个别之前就有👇 binllog位置👇 上图就是做了一次全备，从lsn=0到lsn=1541816 compact=0应该是没有压缩 👆这是备份过程的一些信息。 这是个data文件，看不了。 这些就是备份下来的文件了👇 下面就是恢复也就是还原，2大步 第一步，预处理，就是上文讲的得到一个完整的数据 xtrabackup --prepare --target-dir=/backup/ scp -r /backup/* 目标主机:/backup # 将备份文件scp到需要还原得机器上 第二步，还原，也就是将预处理得到的完整数据复制过去 配置文件里的datadir可以重新定义👆，不一定要和原来一样。 xtrabackup --copy-back --target-dir=/backup/ # 不用指定数据库的存放路径，会自动读取/etc/my.cnf配置文件里的路径 如图，可见/data/mysql路径是自动读取配置文件里的进行复制过去的，所以备份的db路径和还原的db存放路径可以不一致的。 复制的时候，要保证数据库不能启动，目标文件夹得为空。 所以还原得时候，都会影响业务，源数据库得清空。 最后，恢复下文件得权限 chown -R mysql:mysql /var/lib/mysql systemctl start mariadb xtrabackup增量备份和还原 --target-dir=/backup/base 文件夹不需要手动创建，会自动生成的。 --incremental-basedir=/backup/base 指定基于上一次全备的增量。 --incremental-basedir=/backup/inc1 增量就是基于前一次的增量继续的 还原的时候就要涉及回滚了，涉及是否回滚，也就是黑话-封不封口。 其实只有最后一次还原才会封口，才会回滚，前面的都不会回滚--不会封口。 --apply-log-only 就是仅仅应用日志，就是仅仅恢复日志，但是不做日志的回滚。 其实最后一次也可以加上--apply-log-only去让他不做回滚，无所谓，大不了就是做了一半的未提交的事务，你启动数据库服务的时候也会给你自动回滚的。反正没提交嘛有什么关系。 下面是实验 清理一下备份文件夹 ①做一个全备 全部做好了👇 ②增加点数据 这是第一次做增量备份，基于base内容做的第一次增量备份 inc1增量文件就出来了👆而且做增量是能够很好的节约磁盘空间的，且能提供备份速度的因为备份的数据少啊。 ③第二次增量备份前，再改改数据 然后做第二次增量备份 👆注意--target-dir是基于上一次的增量文件，来做--incremental-basedir新的增量文件。 到此，做了①全备②第一次增备③第二次增备。 复制到新的一台机器上 ④然后开始还原，在新的机器上 还原2大部，预处理 和 复制，当然chown和restart收尾也是自然。 现在是23M的文件 因为有3个备份文件，所以需要分别整理也就是预处理3次。 4.1 第一次prepare 然后就会发现 这个/backup/base全备文件变大了，看来，预处理就是将增量备份文件往前面的全备里添加啊。 4.2 第二次prepare👇 然后发现inc1变大了，base没变还是28，没关系，它内部有一些机制，不过最终确实是往base里整的。 4.3 第三次prepare 也是将inc2往base里整和啊 此时👇 ⑤将上面第4步整合好的文件复制过去，不过是用xtrabackup的cli进行复制的，而且要先清空目标库文件夹。 注意保留文件夹，这样就不用重新建再改所有者了。 停服 图中👆由于是服务器开启的状态进行rm -rf /data/mysql/*的，视频中的老师怕服务停掉会产生什么文件比如日志啥的吧也许，所有又ll /data/mysql/确认是空才放心。 👆复制过去，就好了上面命令敲完👇就有了如下内容： 大小看看 原来的数据文件大小要小一点， 因为还原出来的会又一些xtrabackup_*文件 ⑥修改属性和启动服务 ⑦验证数据 以上就是xtrabackup的内容，不过现在也知道这个不推荐了，虽然还不错。不过呢，也可用用percona的DB，那样备份的方式可能就和xtrabackup一样了，不过xtrabackup虽然不用了，但是它的思路肯定是必须要账务的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:49 "},"26-MYSQL数据04/4-mysql主从复制原理和实现.html":{"url":"26-MYSQL数据04/4-mysql主从复制原理和实现.html","title":"第4节 mysql主从复制原理和实现","keywords":"","body":"第4节. mysql主从复制原理和实现 主从可不是备份，主上面DDL删库，从上也没了，DDL也不是事务日志能够记录的，也无法回滚，更别说恶意的commit了；所以备份时备份，主从是主从。 scale up,scale out 纵向和横向，前者指提升硬件资源cpu，内存、硬盘等；后者指提升横向计算机数量，类似主从，集群来提升性能。 主从复制要配合读写分离 proxy就是当sql操作来的时候，进行识别： 如果是DML就送给主，主要时进行数据的修改； 如果是DQL查询语句，就送给从进行查询； 这种proxy读写分离器，或者叫代理器，或者叫路由器，或者叫调度器。 proxy有很多产品，看你喜欢哪一个了。 下面先搭建主从架构 如果主挂了，从可以在秒级顶替上来。 升级可以先拿一个从节点开始升级，然后用用看，没问题就继续升其他的。 主从架构 sync 主从复制，是依赖的binlog，所以binlog是要在master上启动的。 然后是怎么把二进制日志拷贝过去的，下文会细讲。 很多时候，主从架构的搭建不是一蹴而就的。要知道一个事实规律，架构这种东西从来不是生下来就一个样子的，是不断演进的。所以不能为了好看的架构而架构，一定是适合业务发展需求。 复制依赖于二进制日志，master---slave---slave，如果是这种级联，其实中间的slave对于右边的slave来讲也是主了，而主就需要启用binlog。 1、当数据更新，就会自动生成binlog日志里的内容； 2、因为要主从复制，所以启用了一个服务--叫dump的线程，在Master上启用的。这个线程会负责读取新的二进制日志，把它通过网络发送给Slave。 3、有发就有接收啊，所以在slave上有一个线程来接收--小IO thread。这个io线程就是负责从网络中接收来自于master的binlog。 4、slave的io thread接收下来，放到relay log中继日志文件里，放到中继日志文件也不是最终目的，最终目的是要让从数据库的数据也发生更新。 5、此时就需要slave上的sql thread线程来读取 中继日志里的binlog，在本地slave数据库里运行，从而实现slave的数据同步。 6、主从复制涉及 两个线程，master 那边1个，slave那边2个。dump thread、io thread、sql thread。 7、涉及两个日志，主节点的binlog；从节点的relaylog。 主从架构的主是复制写操作的，主就是一台，如果写压力大业务大，主从，从再多也没用。此时，就需要分库、分表来解决。 拆库和主从不搭嘎，原来一个主服务器拆成3个，这3个依然都是主服务器 拆库的原则，是三个部分或者本来是三个表没有关系，不需要join表，所以就可以拆成各自的库。 上图👆是纵向切片，下图👇是水平切片 1、比如用户表、消息表、其他表，表和表之间存在join关系；此时纵向拆就不行了。 2、可以考虑横向拆，原来1000W条拆成500W+500W两张表，然后放到2个数据库机器上去。 3、拆开来后表的结构都一样的，只是放了其中一部分的数据 4、比如按13579奇偶数拆，奇数放一个服务器上，偶数放另一个服务器上。 5、比如按范围拆，1-500W，501W-1000W。 6、按地区拆，北京、上海 7、按用户级别拆，vip、svip 8、前端还得有调度也就是proxy，查和写都得知道去哪个库。 分库也好，主从也罢，都涉及到调度器proxy去将 读写负载分担到主也好从也好，拆分出来的库也罢。 这里就涉及一个调度器这个负载设备点的 单点故障。 调度器的单点故障是通过keepalive来解决的，nginx，f5应该也可以吧。 组合架构：拆库A+B，然后针对A做从节点，B同样也做从节点。再做好备份就行了。 分库的考量点，不是简单的数据量，而是用户访问量大到一定程度才会考虑分库。数据大，硬盘空间大一些就行了，用户量大了就不是简单加硬盘了，要考虑整体资源，所以分库是要考虑的。考虑的是访问量而不是数据库多大。 master.info里涉及给到slave的也给replication复制的一个权限。 relay-log.info是slave讲复制过来的binlog和relaylog的对应关系，复制了哪些binlog进来。 relay-log.info无需维护，就看看里面的处理的binlog和relaylog的对应关系就行。 复制不及时的问题，就会造成主从数据不一致，这是常见的 1、同步机制就是binglog从dump线程-io线程-中继线程-重放，这套机制就是会延迟的。数据不及时是必然的。 而且会出现延迟很大的情况👇 ①并行转串行： 并行写数据，但却是串行写日志。且网络传输也会涉及FIFO的一个先进先出的默认队列。 PS： 1、dml并发写入db表里 2、日志binlog就是串行写入了 3、binlog再复制到各个下面的从节点，每个节点又是串行传输的。 4、类似上高架，多车道并单车道，类似吧。 5、如果持续出现这种现象--就是用户高并发太频繁了，串行跟不上，且网络FIFO，日积月累的延迟就大了，在有些企业里延迟可能会高达1小时。比如做活动用户访问量大，等活动过去用户访问频次下来，FIFO也就慢慢能够处理得过来了，总之要将队列里的数据先处理掉才行。如果此时主服务器宕机了，从有没有同步到数据，长达一个小时的数据就没了。 6、遇到这种情况，就是备份+binlog来恢复数据吧。 主从复制的流程特定 异步 同步， 如果4 5 遇到高并发，大延迟，此时6就会很慢才会得到处理: 1、用户发起dml 2、proxy转发器代理区分dml往master送，如果是读DQL往slave上送。 3、此时是DML，master收到后，就开始写到本地数据库 此时，如果写完后，直接往proxy回应，说完了，不等从同步确认。就是异步也是默认的方式。 如果，不立刻回复用户，而是先同步，等slave回应同步ok后在回复proxy，则是同步。 半同步，介于异步和同步之间 常见架构，或者也叫mysql复制模型 一主一从，一主多从 级联复制，一主一从级联带多从:这个好处就是master只需要复制给一个，剩下的从都重那一个中间节点复制就行了。master压力小。这让我想起了之前《第二十四章 MYSQL数据02》- 第4节 mysql架构和存储引擎详解里的这张图👇，就是说级联的中间点加速复制不做本地落数据，叫做黑洞技术。 主主会造成数据不一致，因为都能写，但是M/M是可以这么玩的，前置调度器，比如F5将DML都往MASTER-A上送；然后MASTER-B要去从MASTER-A上取数据--本质上MASTER-B还是从只不过是已经提升为MASTER的SLAVE；如果MASTER-A宕机了，就直接F5通过健康检查快速切到MASTER-B去了，MASTER-B因为已经是主了无需从SLAVE再繁琐的提升到主了。 但是主主+调度器，主主里的实为从的那个主，如何做到只接收数据，不去覆盖到真正的主的呢？ 主从的情况，从变成主，需要修改配置。 两主，一从，这是省钱的玩法，啥意思，就是 master-A里是db_1 master-B里是db_2 slave一个没错，但是分别复制db_1和db_2，物理上是一个从，其实是两个从库。所以主从式精确的来讲是库的主从。 slave一个没错，但是分别是两个实例--就是两个mysql服务，mysql_1同步master_1里的所有库；mysq_2同步master_2里的所有库，哈哈，一个套路。 本质上还是一主一从。 这个压根不会用了 其他的都有存在性和落地的可能。 复制的时候是复制的binlog 而binglo是格式的： STETEMENT--语句型，不推荐 ROW--行型，推荐 MIXED--混合型，也凑合 mysql复制配置 主节点配置 关键配置就两个 然后还得授权从能够访问主进行复制权限不必给大 复制的时候不确定是哪个数据库，所以就写*.*所有库的所有表。 从节点配置 ①server_id是主从都需要配置的 ②read_only是防止从节点被改了，被改了就无法去同步主节点了。且数据库就不一致了。其实不加也没事，要从架构考虑，因为主从前端还有一个调度器呢，只要保证前端的调度器能够正常的分离读写就行了，写别发送到从节点就行。 然后用户也不会直接连到从节点，也不会改得到数据。 read_only是针对普通用来将的，不包含root，root这个管理员还是该写还是能写。 ③指定主的host也就是ip，指定从节点以什么账号密码进行复制，指定从什么binlog文件里的哪个位置开始进行复制，从那个点开始复制往后所有的binlog。 mysql > CHANGE MASTER TO MASTER HOST='host',MASTER_USER='repluser',MASTER_PASSWORD='replass',MASTER_LOG_FILE='mariadb-bin.xxxxx',MASTER_LOG_POS=#; 这个MASTER_LOG_FILE，mysql5.6以后就不需要指定了binlog文件自然也无需指定位子了。 然后这里的CHANGE MASTER TO 在前面学到过，就是 ④最后启动一下从节点去复制主节点的binlog的动作 因为，要启动从节点的两个线程IO和SQL thread；需要 其实不必分开启动，直接START SLAVE就默认会启动IO_THREAD和SQL_THEAD两个线程了。 一旦启动，后续就自动持续复制了，不用管了。 主从复制实验 修改配置文件 master上配置 slave上配置 创建复制用的账号 看下slave从哪里开始复制 从mariasb-bin.000032的548位置开始复制 slave那边的cli如果忘记了可以help一下 CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='centos', MASTER_PORT=3306, MASTER_LOG_FILE='mariadb-bin.000032', MASTER_LOG_POS=548, MASTER_CONNECT_RETRY=10; START SLAVE; 再输入上面的CLI之前的slave的状态是Empty空的👇。 然后数据库文件目录下当前文件如下，待会等cli敲下去就看看新生成的文件是那些。 敲下去后，此时 master.info的内容 上图发现IP地址写错了，重新敲一遍上面的CLI就行了。 👆中继日志就是复制过来的的master二进制日志和本地中继日志的对应关系。 master那边的mariasb-bin.000032的548相当于本地的bind-2-rely-bin.000001 上面的mariasb-bin也写错了，将上上面的cli重一遍就行 这两个文件是中继过来的binlog文件，类似binlog一样的，index是binlog的文件名。 敲完上面的CLI之后，从节点再看一下👇 上图可见 这个👆是slave上的参数，表示从节点同步主节点的延迟时长多少秒。其实就是差多少秒的数据了。 说明IO thread和SQL thread都未开启，去master上看看进程 同样slave这边也看一下线程情况 在从节点上启动两个线程 主节点也看下线程也已经启动了 然后测试同步情况 在主节点上创建db 再测试一个大的数据生成的同步情况 主那边存储过程创建，从那边自然也有了 从节点的表格也同步了， 此时在主节点上执行存储过程，然后观察从节点的表格同步情况 主👆节点， 👇从节点 此时从节点的进程如下 目前没有延迟同步的时间，然后主节点那边的binlog文件和位置是多少，从节点自己的rely-binlog的文件和位置是多少的一个对应关系。这比较一开始要多了很多数据。 然后要注意一点，主节点那边的call 调用存储过程 所消耗的时间未1min59s，这个里面由于是要做主从同步的，所以时间要比单节点来的要长些。 所以主的压力就明显变大了，此时前端就需要部署一个调度器来实现读写分离，让读操作DQL就别往主节点发送了。 总结 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:49 "},"27-MYSQL数据05/27-MYSQL数据05.html":{"url":"27-MYSQL数据05/27-MYSQL数据05.html","title":"第二十七章 MYSQL数据05","keywords":"","body":"第二十七章 MYSQL数据05 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:50 "},"27-MYSQL数据05/1-主从服务故障恢复和级联复制.html":{"url":"27-MYSQL数据05/1-主从服务故障恢复和级联复制.html","title":"第1节 主从服务故障恢复和级联复制","keywords":"","body":"第1节. 主从服务故障恢复和级联复制 半途做主从 一半业务刚起步也不会做什么主从，就单机，随着业务体量起来才会考虑架构的事情。所以需要处理主从复制的时候主节点上已经有很多数据的情况。 思路就是：先将主节点的数据备份-还原到从节点--其实如果是VM直接克隆出来一台就行了连mysqldump都不需要，再开启主从复制，复制的位置就是show master logs的位置。 实验一下 1、弄一个干净的mariadb服务 然后这里报错，就是文件夹的权限问题 binlog就有了 2、制造一些数据，代表单节点的数据起来了--我认为因该要做这个 然后备份，还原一下，这里就不偷懒用VM克隆，实际工作中反而是克隆直接，不过具体情况具体看咯，两种方法都得会。 主节点的账号要不要复制过去给从节点，要！主从复制本身从节点上不用有账号库，但是从节点万一成为了主节点，此时其他的从节点就会和这个提升上来的从节点上同步数据，此时该节点就需要有同步用的账号库了。这就是考虑问题的时间跨度要有，所以当面临一个不擅长的问题，就需要有这个时间跨度的意识去拉自己一把。 所以做主从的时候，就是先创建账号，再备份还原过去，这样账号就一起还原到从节点了。 3、有了一些数据了，也先于主从复制创建了账号。现在还是备份使用mysqldump 关键语句如下👇 这条语句其实就是主从复制的，从节点上面用的复制配置CLI。 msyqldump里的--master-data=1这个=1就是在从节点上面还原的时候直接顺带执行了change master to xxxx 了，不过还缺一个mater的IP是多少。 所以直接在从节点执行也是不行的。还得补齐master的IP，复制的账号。 4、将dump出来的备份复制到从节点 将从节点的mariadb 删库，卸载，重装，发现，/etc/my.cnf会自动给你备份，新建，还是挺不错的。 重新安装好mariadb后，不要启用服务，否则会自动生成/var/lib/mysql/的一些文件，这样不利于还原主节点那边的数据。 # 当然这视频里说的，现在版本又不这么回事了，yum install后，直接/var/lib/mysql/下就会直接生成文件，不用启动也会又文件初始化生成的。所以不用管这些。而且你恢复dump文件的时候就是要先启动服务的。 然后vim 进去备份文件all.sql补全从节点的change maser to的相关信息 5、修改从节点的配置文件 6、恢复dump文件 启动服务 db也是默认的几个，slave status也是空的 serverid没问题，注意是下划线。/etc/my.cnf里面可以-可以\\，但是mysql交互模式进来就是变量去看，变量就是_。 但是这个read_only ON是防不住root的哦，root依然可以修改的。 由于vim过all.sql补齐了change master to的相关信息，所以可以直接mysql 此时从节点的配置好了，就差一个启动 然后此时一些 relay文件就生成了 目前没有启动，从节点和master节点的网络连接还没有 待会启动了slave start后，从节点就会主动连接主节点的3306端口去了。 然后就报错鸟 不过由于IO thread起来了，所以ss -nt tcp连接时已经建立 然后拍个错 这对这个报错 flush priveleges；就行了，然后start slave；一下。 然后就发现继续报错 我只能说FUCK，然后我继续排错，过程嘛就是remove \\install \\remove\\install， 发现了一个all.sql这个dump文件里的错误，就是分号 好像就是这个原因，我再改回分号试试，破案了，就是这个分号导致了上面的start slave后的show slave stauts\\G看到的报错，而且细究一下其实mysql 但是你要是进入mysql，敲source /data/all.sql，就看不到报错了，所以由此看来，还是建议在外面做mysql 。 👆应该root不受限，所以可以导入了 总之就是 yum -y remove mariadb-server rm -rf /var/lib/mysql/* yum -y install mariadb-server \\cp -a /etc/my.cnf.rpmsave /etc/my.cnf systemctl start mariadb systemctl status mariadb mysql OK了 tcp 建连也没问题 数据也还原的没问题 然后看看主从复制是否OK能否复制了 同步的很丝滑~ 总结，在现有的mysql服务器基础上，实现主从复制 生成中主从复制的错误案例 1、案例1 有人本该去主上创建库，结果跑到从节点上创建了；发现后又跑到主节点上创建了一遍。 错误如下 然后去主上创建同名的库 但是我这个没报错啊，难道是高版本优化？ 然后我在主节点上删掉这个db003试试 从节点此时就自动同步了，貌似高版本确实优化了 老版本是由这个问题的，如下图👇 这个问题的严重性在于：一旦发生这个错误，后续的主从复制就停滞了，继续在主节点上创建数据库，从节点就不会同步了。 这种冲突不仅仅是数据，这里只是举例，冲突可能存在于表，表里的记录都是可能的。 这里处理的方法不是在从上drop掉冲突的库，及时删掉，也不会继续同步的。需要stop salve和start slave，重启一下。但是生成中不能人工去处理的吖~而其是报错明确的，万一报错里的内容没有明确指出来呢，万一不止一条呢。 按视频里说法就是忽略这个报错的意思咯，我先不急记录他的处理方法，我先看看高版本里的这种错是否由优化自动处理掉了。 结果该问题确实有的，没有自动优化一说，继续处理吧。 此时主节点那边开始写入大量数据，从节点就卡在那边不同步了 此时从节点并不会同步 skip忽略错误-sql_slave_skip_counter 其实不仅仅是忽略错误，只是一般用在忽略错误上，sql_slave_skip_counter是忽略几个同步事件，错误也好，正确也罢，都算！如果忽略2个，结果第一个是错误，第二个是正确的，一样也会被忽略。 此时就可以同步复制了，之前主节点跑的存储过程其实就是创建和insert的testlog也同步过来了👇 当然忽略的1个报错，只是临时解决让数据继续同步下去，但是如果存在关联性，就不太好了，还是要回头过来去解决。 如果问题实在太多，还不如从的删掉，重新同步呢。 skip指定的error 再一个，尝试通过忽略错误编号的方式而不是忽略几个，这里要明确这个错误编号我测试下来不是说代表某一个错误，至少我insert 两次，忽略一次，1062的错误编号没变。 不行唉，read-only也碍事了，改一下 还是不对啊 算了该配置文件去 就好了。。。 所以那几个insert都是1062类型的错？我理解成类型不知道对不对啊。 对的吧，官方有的https://mariadb.com/kb/en/mariadb-error-codes/ 如果主服务器宕机了 1、创建出1主2从的架构先 在主节点上执行 mysqldump -A --single-transaction --master-data=1 > /data/all2.sql scp /data/all2.sql 192.168.126.131:/data 在从节点上执行 yum -y install mariadb-server vim /etc/my.cnf server-id=131 read-only wr systemctl restart mariadb vim /data/all2.sql ----补齐----- CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='centos', MASTER_PORT=3306, #MASTER_LOG_FILE='mariadb-bin.000032', 这两行是自动就有的 #MASTER_LOG_POS=548, 默认就有 MASTER_CONNECT_RETRY=10; # 注意最后一个才是分号 wr mysql 上面执行的过程中报错了 ①是版本不一致导致的 ②是 https://stackoverflow.com/questions/1814532/mysql-error-1071-specified-key-was-too-long-max-key-length-is-767-bytes ③所以看懂了没？真实的意思就是，你的所有的字段，就是列的字符总和长度超了，而且还涉及一个字符占多个Bytes的情况。 这里不换版本，处理很麻烦，搞不懂的，除非你vim进去修改各个字段的长度，保证加起来不超过2000，但是没这么玩的；然后innodb_large_prefix这个10.5.x版本也不让改的-配置文件里修改了启动不了服务了。 ④处理方法就是换一样的版本， 好啦，瞎鸡巴搞，主从还不搞一样的版本！找抽！ 模拟主从切换 此时存储过程大量insert正在进行中，然后关闭虚拟机。 此时两个从节点，都瞎了。 此时需要提升一个从节点未主节点，提升谁的依据就是 谁的同步的二进制日志的位置新就提升谁， 一样的，谁都行。不一样，找编号文件大，文件编号一样，再找Pos位置编号大的。 这里两个从节点同步状态一样，就随便选择一个，这里选择192.168.126.130吧 提升为主的从节点的操作： ------------------------------------- mysql > stop slave; mysql > reset slave; 这个只是删掉master.info文件,删掉了relay-log.info，xx-rely-bin.000xxx也没了？老版本是好像清空但是这个rely-binlog还在的。 vim /etc/my.cnf log-bin # 最好独立路径，这里偷个懒。做主了，从就要开启binlog了 #read-only # 要去掉。做住了，从就要支持写了。 账号之前就复制过来了，所以授权复制的账号不需要再创建了，可以确认下 mysql > select user,host,password from mysql.user; mysql > show master logs; # 看下提升为主得从，binlog从哪里开始的，待会还有一个从节点就从这里开始复制。 reset slave;清的不干净，需要加个all 不过新版，加不加all，/var/lib/mysql/的东西都是一样的效果了。 修改配置文件，这回儿提升为主了，需要修改一下 提供从节点复制的账号本来就是复制过来的，有的 然后还有一个从节点也要修改master信息 msysql > stop slave; mysql > reset slave all; mysql > CHANGE MASTER TO MASTER_HOST='192.168.126.130', MASTER_USER='repluser', MASTER_PASSWORD='centos', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, MASTER_LOG_FILE='bind-2-bin.000001', MASTER_LOG_POS=329; mysql > start slave; mysql > show slave status\\G; 测试下同步效果 主节点上drop库，从上看 以上就是主从切换的手动过程，真实生产中，肯定不会这么手动搞吧，太慢了。 如何实现自动切换捏~(￣▽￣)~*有的，有个软件，呵呵，我还以为自己写脚本呢，对吧，自己写就是精度可能是s秒级的了不起了，后面学到这个软件再说。 减少主节点的压力 主从，主节点的工作就是复制写和同步，如果从过多，就会导致复制的IO过大，此时就会考虑：一主---中间节点----多个从节点，这样的架构👇 从节点照旧，只是修改master指向中间节点就行； 中间节点的配置比较特别而已 那么问题来了 中间的级联节点，是没有dump线程的，slave如何去取数据呢。 要知道简单的主-从，是靠dump thread讲binlog dump出来传出去得，从那边就是靠io thread接收数据，存到realy.log里，然后rely.log文件里得东西，又靠sql thread读取后写入数据库里得。 1、所以级联节点，必须启用binlog 2、而且这个binlog里记录得内容，必须是从master传递过来的内容，可不能是自身的binlog哦哦，哈哈，而默认恰恰是自身的binlog。 3、binlog默认是记录本地写入进去的，而不会记录 sql thread 从relay.log读取写入db的内容。而且中继上要只中继relay.log里的binlog才对。 4、还得dump thread来送出去。 总之 ，dump线程起来，然后dump只会从本地binlog取数据，不会从relay-log取，所以这里要做进一步处理。 要让relay.log的数据写到数据库，然后让数据库里的数据再写道binlog，这样再利用dump进程才可以把从master来的binlog送出去。 讲了辣么多，结果就是一条选项的事咩？ 其实我还知道，这个TMD的处理逻辑还得优化成BLACKHONE的方式。 master上 -------- yum -y install mariadb-server vim /etc/my.cnf server-id=1 log-bin wr systemctl restart mariadb mysql > grant replication slave on *.* to repluser@'192.168.126.%' identified by 'centos'; mysqldump -A -single-transaction --master-data=1 > /data/all.sql scp /data/all.sql root@192.168.126.130:/data/ 中间proxy上 -------- scp -r root@192.168.126.129:/etc/yum.repo/ /etc/ yum -y install mariadb-server vim /etc/my.cnf server-id=130 log-bin read-only log_slave_updates wr systemctl restart mariadb-server vim /data/all.sql CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='centos', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, wr msyql show slave status\\G; mysql > start slave; # 不用再proxy上创建复制账号，本意是创建账号给 从节点们用，其实没必要，proxy拿这个账号同步master，本身也会同步到这个账号到本地，然后从节点就用同样的账号来同步proxy。 mysqldump -A -single-transaction --master-data=1 > /data/all_proxy.sql scp /data/all.sql root@192.168.126.131:/data/ scp /data/all.sql root@192.168.126.132:/data/ slave节点上 --------- scp -r root@192.168.126.129:/etc/yum.repo /etc/ yum -y install mariadb-server vim /etc/my.cnf server-id=131 read-only wr systemctl restart mariadb-server vim /data/all_proxy.sql CHANGE MASTER TO MASTER_HOST='192.168.126.130', MASTER_USER='repluser', MASTER_PASSWORD='centos', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, wr msyql show slave status\\G; mysql > start slave; 以上配置OK，测试过了，过程截图略了，都是从yum -y remove mariadb开始的 最后验证： OK， 这种的搞定 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:50 "},"27-MYSQL数据05/2-mysql主主和半同步复制.html":{"url":"27-MYSQL数据05/2-mysql主主和半同步复制.html","title":"第2节 mysql主主和半同步复制","keywords":"","body":"第2节. mysql主主和半同步复制 补充几个选项 5个写磁盘的及时性： sync_binlog=1 binlog的及时写磁盘， innodb_flush_log_at_trx_commit=1 事务的及时写磁盘，当然不是说及时就一定是好的，也要考虑IO的负担。 sync_master_info是 多少次 事件后 master.info的信息 写磁盘 sync_relay_log 多少次 后同步到relaylog磁盘文件，应该也是指relay-log.info文件吧。 sync_relay_log_info 多少次事务后 后同步到relay-log.info磁盘文件。 skip-slave-start=ON # 要知道一个事实--重启mariadb服务后，slave的两个线程IO和SQL都是OK的，也就是第一次手动start slave;后面都不用管的，都是随服务启动就启动的。这个特性就是依赖于skip-slave-start这个选项，如果置为ON，就不会随服务启动了，到时候就只能手动每次start slave了。 主-主 复制，需要调度固定分配成主-从 主-主前面一篇就讲过了，这种的好处，是多活吗，无需从节点提升 这么一个动作。 所谓主主，其实是互为主从，A-B 两个机器，A是B的主，又是B的从，B一样。 不过有人就是要真正意义上的主主，多活，A/A，类似ASA的A/A，人家就是要并发写，我那个去，怎么搞，这么搞。 所谓的并发写，A/A，其实就是通过 auto_increment：也就是奇偶区分，不同的主起始不通，间隔一样，错开序号了就。 然后再配合前置调度器，比如，来了2条写操作，调度器就直接并发的扔给2个主，而2个主处理的时候又是错开PRI主键ID的，于是就实现了多主。 节点A上 ------ yum -y remove mariadb-server rm -rf /var/lib/mysql/* cp -a /etc/my.cnf /etc/my.cnf.bak yum -y install mariadb-server vim /etc/my.cnf [mysqld] server-id=1 log-bin auto_increment_offset=1 auto_increment_increment=2 wr systemctl start mariadb mysql > grant replication slave on *.* to repluser@'192.168.126.%' identified by 'cisco'; 节点B上 ------ yum -y remove mariadb-server rm -rf /var/lib/mysql/* cp -a /etc/my.cnf /etc/my.cnf.bak yum -y install mariadb-server vim /etc/my.cnf [mysqld] server-id=2 log-bin auto_increment_offset=2 auto_increment_increment=2 wr systemctl start mariadb // mysql > grant replication slave on *.* to repluser@'192.168.126.%' identified by 'cisco'; # 这句就不用敲了，直接复制过来从A。 mysql > CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, MASTER_LOG_FILE='mariadb-bin.000001', MASTER_LOG_POS=330; # 去A看下show master logs;将binglog文件和pos位置复制过来。不过这个要配合mysqldump 从A备份还原到B，如果不想这么麻烦，直接将这里的binglog设置成第一个文件的第一个位置一般是330左右。可以刷新一个binlog，看看初始值多少。放屁！这样从头复制直接报错！哈哈因为复制的时候可不会想mysqldump出来的有IF判断存在就DROP的语句，主从复制如果存在就直接报错卡住了。 所以这里的binlog和pos需要先去master上show master logs;复制过来的，然后master前面的数据需要dump过来的。 报错啦 还是去A上dump吧 mysqldump -A --single-transaction --master-data=1 > /data/all.sql scp /data/all.sql root@192.168.126.130:/data/ 去B上还原下 msyql > stop slave; mysql show databaes; 可见hellodb同步过来了 mysql > start slave; mysql > show slave status\\G; 这回就OK了。 这就好了，果然是primary主键可能不让写奇数吧，因为/etc/my.cnf里是做了偶数限制 果然个屁，再仔细看看报错，是dupicate 这就是已经说主从复制的时候，已经有的数据库，里面的表，再次写的时候就报错了， 但是mysqldump 出来的all.sql里其实也是大量的sql语言，但是人家是写了IF 条件判断的，如果存在这个表格就DROP的啊，哈哈，对吧mydqldump是先铲除后还原啊，所以不会报错啊👇 而复制就不是啦，存在的就冲突了。现在知道为什么每次都dump而不是从头做复制了吧。 现在传统的主-从就OK，测试一下 然后把节点B也就是slave也变成master B上也就是slave： ------ mysql > show master logs; A上也就是master： --------- mysql > CHANGE MASTER TO MASTER_HOST='192.168.126.130', MASTER_USER='repluserB', MASTER_PASSWORD='cisco', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, MASTER_LOG_FILE='mariadb-bin.000002', MASTER_LOG_POS=2539205; 上面的binglog文件和pos位置就是B上看到的复制过来的，敲完A就是从了，当然还得start slave一下； mysql > start slave; mysql > show slave status\\G; 然后就报错了 奇了怪了，用户确认同步过去了啊，密码通过哈希值都能判断是一致的👇 不管先在B上新建一个用户试试 在B上 ----- grant replication slave on *.* to repluserB@'192.168.126.%' identified by 'cisco'; 靠！好了，难道用户要单独创建咯，好吧。难道主主，复制用户必须各自单独创建。不是，后面我又把A的复制用户换回去又可以了。感觉是这个用户没有在B上生效。 好像flush privileges;就好了，不过我记得我刷过啊，估计就是没生效当时，然后过一会生效了，或者是创建repluserB的时候相当于又刷了一次，我当时应该多刷几次的，(￣▽￣)\"。 A->B 和A 模拟写表看看是否冲突-主主的时候 A上 ----- mysql > use hellodb mysql > create table test (id int auto_increment primary key, name char(10)); mysql > desc test; 此时B上也有了 通过xshel的批量下发cli，给A和B同事敲入insert insert test (name) values ('wang'); 这样就同时键入了sql insert 不冲突，左边A是1，右边B插入的是2偶数，奇偶错开来的。 还是一样左边1 3 5 ，右边2 4 6 创建表，理应冲突 但是高版本确实不会了，不知道怎么实现的。 我理解一定是有一个被回退了，内部忽略了，去找找相关资料。 中文会翻译掉了一写 重要的 专业词汇，就是人家起的名字吧，通过英文去问得到英文的回答 通过GPT找到了关键信息，然后通过关键信息去官网再找 https://mariadb.com/kb/en/about-galera-replication/ 然后看看默认的值就是1👇 确认了👆 还有这一篇也写的不错 https://galeracluster.com/library/documentation/certification-based-replication.html 虽然主主现在高版本有write set replication API辅助，但是生产中是不可能直接用的，会在主主前面加上调度器的，只往一个主节点上写数据，另一个虽然是主，本质上是做查询操作的，这样的主主好处就是当主节点挂了，另一台可以快速替代，不用像以前从那样还要费劲地操作一下才能提升为主。 如果是三个环形指主，如果是4个环形指主 都是串起来，头尾相连，本质上和主主一样地，无非是从2个串，变成了3个，4个环型罢了。 上图👆是主主地间隔，如果是3个串起来， 将auto_increment_increment=3就行了；就是1 4 7 ，2 5 8，3 6 9。 同步虽好，异步落地，半同步为优 同步和异步前面讲过了，这里补充一下半同步，所谓半同步就是，只要众多从节点中，有一个回复了(下图中第4步)，master就回复proxy，进而回给用户。相当于一次update在用户那边就完成了，不必像同步那样等太久。 默认是不支持半同步的，需要安装插件，好像确实数据库里也有很多插件。 下面开始实验 1、拓扑：master--带 两个 slave，一个slave断开网络模拟未同步，然后master也能完成记一次updata交互。 2、开始 master上 -------- vim /etc/my.cnf serer-id=1 log-bin wr systemctl restart mariadb mysql > show master logs; # 记一下binlog的编号和pos复制到下面CHANGE MASTER cli里 mysql > grant replication slave on *.* to repluser@'192.168.126.%' identified by 'cisco'; 左一 master； 中间 slave1； 右一 slave2； slave1上 --------- vim /etc/my.cnf server-id=2 wr systemctl restrat mariadb msyqll > CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, MASTER_LOG_FILE='mariadb-bin.000003', MASTER_LOG_POS=781; 遇到一个报错 重启服务后，就可以了，我日，肯定有什么残留导致直接 授权的方式无法创建用户了。 好像有时候flush一下可以，有时候flush 也不行，下图是OK的截图 遇到报错，stop slave还不行，还必须reset！ slave2一样要reset一下，因为之前有做了其他实验 然后两台slave就都好啦👇 下面做半同步啦，安装插件：rpl_semi_sync_master 高版本都是wsrep-write set replication这种API了。 不管，硬上，直接看变量SHOW GLOBAL VARIABLES LIKE '%semi%'；有唉~我日 估计这个插件名称变了，被集成到别的插件里了，可能就是wsrep这个 然后看插件的方法 老版本视频截图 master上安装master字眼的插件 👆安装后，👇看看 看到这我猜这个rpl_semi_sync_master就是被高版本里的👇这个取代了 都是REPLICATION 先不管wsrep这个事，先看老版本的操作流程 插件rpl_semi_sync_master有了，但是还需要启用，通过变量看看状态👇。 这个_enabled是启用与否；_timeout 10000是万一主--从同步从迟迟没有回应，主也就等这么久10000是10s，就会往client/proxy那边回应了。 启用一下-在maser上的半同步 然后看看状态变量 可见👆ON了，然后client那边还没有使用半同步的插件，也没有开启，所以client是0。 回到我的实验，我用的高版本的mariadb，我试试蒙一下，我估计插件不用安装 直接set一下长得像的变量on试试👇 果然，新版本无需安装什么半同步插件了，至于这些个变量的功能来源于哪个插件，或者是不是 被取代了，再说回头我去翻一翻一资料 https://jira.mariadb.org/browse/MDEV-17908👆 官网上也说了 https://mariadb.com/kb/en/semisynchronous-replication/ 好像也没将是集成到wsrep里，呵呵，不管了。 启用一下-在slave上半同步 老版本具体就是10.3.3之前 然后show plugins;看下👇 高版本就不用安装了，默认就有半同步功能 不管是老还是新版本，启用都是要启用的，启用的变量和查看状态的变量都是一样的。 slave2一样启用一下 最后还需要重启slave的线程，stop slave,start slave 记住！不是reset salve。我日你奶，想要reset 还得先stop，不过reset也不会真的清空就是了，只是重新设置的时候有时候需要reset一下。 开始测试同步机制 1、当前状态是同步OK 2、然后半同步不是已经配置且master上认到2个client了吗，所以将两个slave都停服，这样就一个同步都不会发生，理论上master就遇到sql写入，就会卡住，就不会回应cliet/proxy了。但是也不会永久卡在那，因为有超时时间10s 测试 slave2 停服 slave1 正常 master创库之类的写入正常 再继续将slave1停服 此时master 创库卡住，时间卡了10s OK！ 然后看看状态，就有变化了 要知道原来是0都是基本上 再一个slave只要启动mysql，也会自动同步过来，因为slave是默认 第一次手动，后面都是随服务启动而启动的。 但是我发现半同步没有启动，client还是0 哦，不是的，是之前set设置的global变量，所以重启服务后失效了， 去配置文件配置下。 slave上 ------- vim /etc/my.cnf rpl_semi_sync_slave_enabled wr systemctl restart mariadb jiu ok la~ 此时master上show得到client了 相反，set global rpl_semi_sync_slave_enabled=ON;是需要stop 和start slave的👇 这是最终的效果 slave上倒是都是0哦，呵呵 这些变量的意思，去官网看咯 https://mariadb.com/kb/en/full-list-of-mariadb-options-system-and-status-variables/ 一堆，所以dba说白了就是要去学习这个几千个变量~ 复制延迟演示下 1、slave上删掉hellodb库 2、master上在hellodb里执行存储过程，创建大量数据 3、此时slave上就会因为没有hellodb库，而出现同步失败， 4、在slave上补齐hellodb库，然后stop start一下slave。及时show slave status\\G;看看延迟。 开始👇 reset 用了就真的清掉额了👇连master的binlog文件编号都没了。 对比slave2的正常情况 确实reset造成的，翻看了上图的历史截图，重新截了上下文 但是我在slave2上反复测试，都没有看到reset的这种效果了，需要手动 stop slave; reset slave; CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, MASTER_LOG_FILE='mariadb-bin.000003', MASTER_LOG_POS=781; start slave; 反正能搞好也是了，呵呵。感觉slave 复制还有点坑。 其实很好理解吧，就是同步的binlog是 如果没有hellodb库，没有testlog表，就不行，所以同步的哪个，你将上图往上翻一翻看看从master那边复制的binlog起点，如果这个pos始终在hellodb.testlog创建之后，那么报错必然始终都在。 你只有将复制的位置POS点提前去覆盖到库表的创建才能消除报错。我不管你reset是怎么清空了master的binglo位置--其实就提前了吗，也不管你有时候reset slave又不清空，我只管复制的post位置是否符合逻辑对吧。 你看哦，这个现象就对了 下面是一个整图哦，受限屏幕才分开来的 reset后，start 的瞬间是两个YES就是OK的 但是又瞬间报错了 很简单啊，就是，我说的复制的主上的post点又读取了缓存记录--类似的吧，没有从头开始读取。 说了这么屁话，其实就一句话，操作上讲★就是要 stop slave; reset slave; CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, MASTER_LOG_FILE='mariadb-bin.000003', MASTER_LOG_POS=781; start slave; MASTER_LOG_FILE='mariadb-bin.000003', MASTER_LOG_POS=781一定要确实是否合适！ ★其实规范操作就是先mysqldump出来master的数据，还原到slave后，再做slave的复制。 再说回来，你要看复制延迟，简单啊，直接POS指到前面去就行啦👇 那么问题来了3395是秒，还是记录还是，啥，简单 官网变量走一波，你麻痹，别学了，secondes_behind，你跟我讲去查资料。(●ˇ∀ˇ●)，而且明确告诉你没有这个变量的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:50 "},"27-MYSQL数据05/3-mysql复制过滤器和基于SSL的复制加密.html":{"url":"27-MYSQL数据05/3-mysql复制过滤器和基于SSL的复制加密.html","title":"第3节 mysql复制过滤器和基于SSL的复制加密","keywords":"","body":"第3节. mysql复制过滤器和基于SSL的复制加密 部分数据复制，采用过滤器 两种实现方式： 1、仅写入需要复制的数据库，master节点配置 ​ 缺点：binlog不仅仅是用来实现主从复制，更多是用来做备份的一个增量还原用的。这样肯定不行，你本地binlog都不全了。 2、slave节点配置，在rely-log中继日志中挑取出，哪些特定数据库的特定表需要同步到本地。 ​ 缺点：rely-log中继日志里东西是全的，是从master复制过来了，然后再sql thread进行挑选的，所以已经从master复制过来，意味着网络和磁盘IO就花费出去了。 ​ 注意slave过滤需要在所有的slave上都要配置一边。 方法1：只生产特定binlog master上： ----------- vim /etc/my.cnf server-id=1 log-bin # binlog-do-db = binlog-ignore-db = hellodb wr systemctl restart mariadb mysql > show master status; 可见binlog黑名单数据库👇 此时hellodb的binlog都不会生产，自然就不会参与主从复制了，测试👇 slave那边test003还在，确实不同步了 其他数据还是同步的 此时就实现了数据的一个过滤过程 方法2：slave服务器上进行过滤 slave上： ------- vim /etc/my.cnf replicate-do-db=hellodb,db1 # 这里定义了一个白名单只从中继日志里取出helldb,db1两个库的binlog进行sql thread写入slave本地。 wr systemctl restart mariadb 此时没有加白的就不会复制了👇 加白的就继续复制 上图的\"hellodb,db1\"呈现出来的貌似是两个db用逗号分开，其实不一定的，要去my.cnf里看如果是replicate-do-db=hellodb,db1这种写法，系统可能就判定为一个叫\"hellodb,db1\"数据库了，鬼知道；如果是下面的写法，系统就虽然slave status里显示还是一样\"hellodb,db1\"，但是确实是逗号分开的两个数据库了。 结果就是加白的也没有复制，但是slave 状态时OK的啊，奇怪了。 就是写的格式有问题，不支持hellodb,db1这种逗号的方式，一行一个， 此时slave的状态倒时用逗号分开的，貌似和以前一样，但事实上时两个db了，以前时一个db 此时test在slave那边就确实同步 也删掉了 再看一个现象--就是目前的过滤器，必须切到use db进到哪个库里才能同步👇 你看master在none下操作的，没有use进到对应的库，所以就没有同步，下面对比测试use一下就同步了👇 实际工作中不要使用这种跨库操作就好，注意下，都使用use dbxxx进到某个库里进行操作。 数据主从复制安全加密下 mysql -uroot -pxxxx -h xxxx这种是明文的，抓包可得 可见cli和查看的结果数据都能看得到。 利用SSL加密，就是要有SSL证书 实验，在192.168.126.129和192.168.126.130之间实现加密 证书怎么申请， 证书是用来证明公钥的安全性的，每个服务器都有自己的证书，将来才能利用证书交换彼此的公钥，才可以进一笔利用公钥来加密对称密钥，进而实现数据传输的安全。 mysql的ssl是做了双向认证的，和平时https上网-只做服务器的认证，不太一样。 1、ca自己给自己版本证书：生产私钥，生产自签名证书； 2、然后在颁发证书 3、这次做都在一台机器上做完，包括CA、各个节点的证书生成，然后分发下去。 mkdir /etc/my.cnf.d/ssl # 专门放证书信息，利用现成的my.cnf.d文件夹，ssl是创建的 cd /etc/my.cnf.d/ssl 我们需要3个证书：CA的证书、主服务器证书、从节点证书。 有证书，就得有私钥， ①生成CA的私钥： openssl genrsa 2048 > cakey.pem # 以前是专门一个目录，现在简单放一起就行 可能最好加个密，或者改个cakey.pem的权限，安全些。 ②利用私钥生成自签名证书 openssl req -new -x509 -key cakey.pem -out cacert.pem -days 3650 ③生成master的私钥和证书申请文件 openssl req -newkey rsa:1024 -days 365 -nodes -keyout master.key > master.csr # 利用一条命令生成私钥文件master.key，并利用该key生成证书申请文件。 注意！1024得改成2048，否则mysql起不来。可能是之前用得2048的CA私钥吧。 ④有了证书申请文件，就可以签名了--也就是颁发证书 openssl x509 -req -in master.csr -CA cacert.pem -CAkey cakey.pem -set_serial 01 > master.crt 利用CA的信息，根据证书申请文件，来实现生成证书。 _set_serial 01 指定证书编号？ ⑤同样再生成slave的私钥和证书申请文件 openssl req -newkey rsa:2048 -days 365 -nodes -keyout slave.key > slave.csr # 利用一条命令生成私钥文件slave.key，并利用该key生成证书申请文件。 注意：-days 365 好像是默认就有的。 ⑥同样再给slave节点颁发证书 openssl x509 -req -in slave.csr -CA cacert.pem -CAkey cakey.pem -set_serial 02 > slave.crt 利用CA的信息，根据证书申请文件，来实现生成证书。 _set_serial 02 指定证书编号 到此就有了master和slave的证书和私钥了，就可以利用证书来加密了 开始给mysql配置ssl证书 默认是未开启ssl加密的 一些和加密相关的变量 题外话：%ssl%不区分大小写。 需要指ssl_ca CA的证书；ssl_cert自己的证书，ssl_key自己的私钥。 vim /etc/my.cnf ssl-ca=/etc/my.cnf.d/ssl/cacert.pem ssl-cert=/etc/my.cnf.d/ssl/matser.crt ssl-key=/etc/my.cnf.d/ssl/master.key 但是服务起不来 查看报错日志 发现是SSL error: Unable to get certificate from '/etc/my.cnf.d/ssl/master.crt' 可能是之前的master.crt证书文件生成的有问题。 一般是文件权限问题，但是这里不是，需要修改一下相关命令。 https://stackoverflow.com/questions/42145925/mariadb-over-ssl-not-working-certificate-verify-failed 👆这篇是OK的，只不过sha1要去掉就行了，我怀疑只要将上面的cli里的后缀改成pem就行了，试试看。不是，破案了👇，还是用上面的原cli改成2048跑一遍再。 下面对比排障测试： 这段NG： ----------------- openssl genrsa 2048 > cakey.pem openssl req -new -x509 -key cakey.pem -out cacert.pem -days 3650 openssl req -newkey rsa:1024 -days 365 -nodes -keyout master-key.pem > master-req.pem # 1024 改成2048就OK了 openssl x509 -req -in master-req.pem -CA cacert.pem -CAkey cakey.pem -set_serial 01 > master-cert.pem 这段OK： ----------------- openssl genrsa 2048 > cakey.pem openssl req -new -x509 -nodes -days 3650 -key cakey.pem > cacert.pem openssl req -newkey rsa:2048 -days 730 -nodes -keyout master-key.pem > master-req.pem openssl x509 -req -in master-req.pem -days 730 -CA cacert.pem -CAkey cakey.pem -set_serial 01 > master-cert.pem chown mysql.mysql * 如果常规就是起个server名字👇 openssl genrsa 2048 > ca-key.pem openssl req -new -x509 -nodes -days 3650 -key ca-key.pem > ca-cert.pem openssl req -newkey rsa:2048 -days 730 -nodes -keyout server-key.pem > server-req.pem openssl rsa -in server-key.pem -out server-key.pem openssl x509 -req -in server-req.pem -days 730 -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 > server-cert.pem chown mysql.mysql * ssl路径配置，重启服务OK后，此时status和show variables like '%ssl%';就可见 通过SSL相关变量可见 两个都YES了，也有了相关路径👇 此时就把ssl的配置都弄了，但是还没有启用加密方式连接。瞎说，上上图status可见 说明就已经启动了，至于有的低版本还需要这样启用，那是低版本太low。 不管是本地登入自己还是别人远程登入自己，还是本地用socket或是tcp套接字登入自己都是自动的启用了SSL的，status 可见SSL Cipher in use is TLS_AES_256_GCM_SHA384的。 低版本才需要链接的时候调用配置里的ssl证书和key mysql --ssl-ca=cacert.pem --ssl-cert=master.crt --ssl-key=master.key 然后强制某个账号必须使用ssl，高版本默认就是会用ssl，应该这也是低版本才需要的操作 grant replication slave on *.* to repluser2@'192.168.%.%' identified by 'cisco' require ssl; 此时使用repluser2 低版本就需要加上ssl选项，高版本不需要 进一步实现主从复制用SSL，将之前在master上一并产生的slave的证书也复制到slave上 其实用这个三个文件就行了 但是slave这样起不来！ 查看报错，说是SSL路径不对， 其实是权限没有，拿不到ssl文件 起来了，但是有错误 这个报错是主从复制的报错， 也就是不同步的报错，按之前的处理方法试试 SLAVE IO都没有Yes啊，解决思路有2 1、重新从master dump 出来一份，记得带--master-date=1选项，导入后，启动slave即可 2、在master上看看当前的binlog 位置，直接在slave上stop; reset slave all; 充型配置同步信息CHANGE TO 。。。。;在start slave试试，差不多就能同步，当然这里推荐用1而不是2.因为这是你从最新的位置复制过来的，前面很多数据都没有同步，而且2还不一定成功，可能报错👇如下图，不过只要按提示改成'mariadb-bin.000003' at 4 也能同步，呵呵，实验就无所谓了，同步就行了。 CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_PORT=3306, MASTER_CONNECT_RETRY=10, MASTER_LOG_FILE='mariadb-bin.000003', MASTER_LOG_POS=4; 此时搞同步了就，但是SSL还没有在主从同步环节启用，默认配置了ssl在/etc/my.cnf里只是登入(远程本地也好，socker/tcp也好)默认使用ssl。 Slave服务器配置 mysql> CHANGE MASTER TO MASTER_HOST='MASTERIP', MASTER_USER='rep', MASTER_PASSWORD='centos', MASTER_LOG_FILE='mariadb-bin.0000xx', MASTER_LOG_POS=xx, #这里写master的 MASTER_SSL=1, MASTER_SSL_CA = '/etc/my.cnf.d/ssl/cacert.pem', MASTER_SSL_CERT = '/etc/my.cnf.d/ssl/slave.crt', MASTER_SSL_KEY = '/etc/my.cnf.d/ssl/slave.key'; CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_LOG_FILE='mariadb-bin.000023', MASTER_LOG_POS=691, MASTER_SSL=1, MASTER_SSL_CA = '/etc/my.cnf.d/ssl/cacert.pem', MASTER_SSL_CERT = '/etc/my.cnf.d/ssl/slave.crt', MASTER_SSL_KEY = '/etc/my.cnf.d/ssl/slave.key'; 我看别人演示里不写三个ssl文件路径唉，我试试 CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_LOG_FILE='mariadb-bin.000023', MASTER_LOG_POS=691, MASTER_SSL=1; 结果一样的报错，而且，人家会自动去找到三个ssl文件路径并给你显示出来的--这是因为第一次配置了，这点和老版本不一样。放屁！stop slave; reset slave all;exit systemctl restart mariadb就好了，不用配置证书路径！ 尝试去掉ssl，发现去不掉👇，reset slave all;后并未配置ssl，也为启用，但是就启用了YES 手动写MASTER_SSL=0来关闭， start slave一下发现报错变了 问问GPT 实测操作，有效 此时在开启SSL看看 Last_IO_Error: error connecting to master 'repluser@192.168.126.129:3306' - retry-time: 10 maximum-retries: 100000 message: SSL connection error: error:00000000:lib(0)::reason(0) 还是报这个错， master上敲mariadb-admin flush-hosts也没有用！ 如何清空ssl文件路径，需要重启服务 stop slave; reset slave all;exit systemctl restart mariadb就好了，不用配置证书路径！ 你要说用户repluser2是只能ssl的， 没事，换，，改成repluser也一样，至少这里repluser2 成功就说明已经使用了ssl加密复制了。 那么问题来了，要不要配置，如果配置了是否应该配置master而不是slave呢，试试 不行，一样报错，只要配置ssl三个文件的路径，就有问题 这样就实现了带ssl的主从复制，上图hellodb002复制OK，001没有，是因为之前有个白名单 去掉就好了👇 再次尝试一下 去掉配置里的ssl路径，改为配置slave的时候指定ssl文件 CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_LOG_FILE='mariadb-bin.000023', MASTER_LOG_POS=691, MASTER_SSL=1, MASTER_SSL_CA = '/etc/my.cnf.d/ssl/cacert.pem', MASTER_SSL_CERT = '/etc/my.cnf.d/ssl/master.crt', MASTER_SSL_KEY = '/etc/my.cnf.d/ssl/master.key'; 不管是master的ssl证书还是slave的证书都不行，报错一样。算了，就在/etc/my.cnf里配置就行了，别在slave里配置ssl文件了。 CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_LOG_FILE='mariadb-bin.000023', MASTER_LOG_POS=691, MASTER_SSL=1, MASTER_SSL_CA = '/etc/my.cnf.d/ssl/cacert.pem', MASTER_SSL_CERT = '/etc/my.cnf.d/ssl/slave.crt', MASTER_SSL_KEY = '/etc/my.cnf.d/ssl/slave.key'; Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:50 "},"27-MYSQL数据05/4-基于MySQL5.7的二进制安装和GTID复制.html":{"url":"27-MYSQL数据05/4-基于MySQL5.7的二进制安装和GTID复制.html","title":"第4节 基于MySQL5.7的二进制安装和GTID复制","keywords":"","body":"第4节. 基于MySQL5.7的二进制安装和GTID复制 GTID复制-slave配置的是省去指定masterbinlog位置的方式 之前主复制都是指定MASTER_LOG_FILE='mariadb-bin.000023', 和 Master_log_pos=xxx CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_LOG_FILE='mariadb-bin.000023', MASTER_LOG_POS=691, MASTER_SSL=1; 这么多配置，还不如老方法手工指定呢，呵呵 GTID 全局事务ID GTID = server_uuid:transaction_id，在一组复制中，全局唯一 server_uuid 来源于 auto.cnf 开搞 1、下载mysql5.7算了我还是直接用新的吧 https://dev.mysql.com/downloads/mysql/ 没找到新版的二进制安装包，，，5.7的到时候有 算了直接用高版本的mariadb不香嘛，操 视频演示的mysql5.7的一个二级制安装备注： 同样注意下mysql5.7的二进制安装，的一个初始化差一点，它使用mysqld --initialize来初始化数据库的，并且直接在结尾给你生成了root的密码了。这个是和前面单独二进制安装的章节是不一样的点。 这里多了auto.cnf这个文件，和sys默认库 这个就是server的uuid来源了 需要添加client的socket，说什么不然本机连不了，client登入的时候也会找不到socket？之前多实例也没有说配置client的socket啊，优点奇怪👇 其他和之前章节的 二进制安装一样的操作。 登入的时候密码有特殊字符的处理方式 mango里python我是这么处理的 mysql5.7的二进制安装全过程👇 GTID配置举例 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:50 "},"27-MYSQL数据05/5-基于proxySQL实现mysql的读写分离.html":{"url":"27-MYSQL数据05/5-基于proxySQL实现mysql的读写分离.html","title":"第5节 基于proxySQL实现mysql的读写分离","keywords":"","body":"第5节. 基于proxySQL实现mysql的读写分离 简单介绍下工作原理 proxySQL通过read-only选项来识别谁是slave的，然后讲DML写道主上，DQL查询往从上发。 官网的指南： https://proxysql.com/documentation/installing-proxysql/ 我用的是rockylinux，通过它的这个URL 网上层看 所以待会可能用centos8来试试 通过上述就知道直接用他的指南肯定报错，因为我的rocky和他的URL对不上的 直接epel里就有这个proxysql 版本也不低了2.4.8，直接用吧 ProxySQL组成 服务脚本：/etc/init.d/proxysql 配置文件：/etc/proxysql.cnf 主程序：/usr/bin/proxysql 基于SQLITE的数据库文件：/var/lib/proxysql/ # 读写规则调度策略配置信息都是放到这个小型数据库里的。不是有配置文件吗，奇怪了，继续看吧，据说是配置文件仅仅可以改一小部分，基本上全部的配置信息都是放到SQLITE里的。所以将来管理proxySQL不是修改配置文件，而是通过像连接mysql一样去配置SQLITE。而且sql修改了SQLITE其实也会自动不配置文件里的内容给改了。 启 动 ProxySQL：service proxysql start 启动后会监听两个默认端口 ​ 6032：ProxySQL的管理端口 就是进入SQLITE的途径。 ​ 6033：ProxySQL对外提供服务的端口 6033默认面向用户的，可以改成3306这样用户那边就不要变动了，直接把proxySQL当做mysql连就行了，透明处理--对于用户来讲他只会以为连接的就是DB。 proxySQL默认数据库 main 是默认的”数据库”名，表里存放后端db实例、用户验证、路由规则等信息。 表名以 runtime开头的表示proxysql当前运行的配置内容，不能通过dml语句修改， 只能修改对应的不以 runtime 开头的（在内存）里的表，然后 LOAD 使其生效， SAVE 使其存到硬盘以供下次重启加载 disk 是持久化到硬盘的配置，sqlite数据文件 stats 是proxysql运行抓取的统计信息，包括到后端各命令的执行次数、流量、 processlist、查询种类汇总/执行时间，等等 monitor 库存储 monitor 模块收集的信息，主要是对后端db（主从）的健康/延迟检查 开始配置 192.168.126.129 - master 192.168.126.130 - slave 192.168.126.131 - proxySQL 搭建主从 master的配置：在129上 vim /etc/my.cnf server-id=129 log-bin wr systemctl restart mariadb mysql -e 'show master logs' # 从这里开始复制，把下面的创建账号也复制过去，其实正儿八经是先dump在开主从 ，就是按前面专门讲主从的章节来配置就行了 mysql -e \"grant replication slave on *.* to repluser@'192.168.%.%' identified by 'centos' \" mysql -e \"select user,host,password from mysql.user\" #检查下上一行的用户是否创建成功 slave的配置：在130上 vim /etc/my.cnf server-id=130 read-only wr systemctl restart mariadb msyql # 进入mysql的交互cli模式 MariaDB[(none)]> CHANGE MASTER TO -> MASTER_HOST='192.168.126.129', -> MASTER_USER='repluser', -> MASTER_PASSWORD='centos', -> MASTER_PORT=3306, -> MASTER_LOG_FILE='mariadb-bin.000001', -> MASTER_LOG_POS=xxx; # xxx就填上面master里的log的位置。 MariaDB[(none)]> start slave; # 如果不是干净的slave之前配置过，就stop slave;reset slave all;再start slave; MariaDB[(none)]> show slave status\\G; # 看到两个yes就基本OK了。 再测试一下同步是否OK，去master上创建也给库看看同步与否 到此主从就搭建好了，下面开始弄proxySQL 安装直接yum，epel里就有版本不低，反正没有rockylinux9的，有aws的新版倒是。这里选择直接yum了。 这里👇有最新版 https://github.com/sysown/proxysql/releases 也许rokcylinux可以用almalinux9的，试试~ [root@bind-child ~]# curl -OL https://github.com/sysown/proxysql/releases/download/v2.5.5/proxy sql-2.5.5-1-almalinux9.x86_64.rpm % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 22.0M 100 22.0M 0 0 1458k 0 0:00:15 0:00:15 --:--:-- 1690k [root@bind-child ~]# ls anaconda-ks.cfg go lib64 pyvenv.cfg bin lib proxysql-2.5.5-1-almalinux9.x86_64.rpm tcping [root@bind-child ~]# yum -y install proxysql-2.5.5-1-almalinux9.x86_64.rpm # 当前目录下的rpm包直接yum它就行了 再安装一个mysql-client用来做msyql db的client连接用，不知道是不是必须的哦，继续梳理 [root@vpn001 ~]# rpm -qf `which mysql` mariadb-5.5.64-1.el7.x86_64 [root@bind-child ~]# yum -y install mariadb # 注意不是mariadb-client，直接mariadb就行，mysql这个命令就是来自mariadb这个包的。 [root@master ~]# rpm -qf `which mysql` MariaDB-client-10.11.5-1.el9.x86_64 [root@master ~]# 虽然显示mysql这个客户端命令就是来自于MariaDB-client,但是也可以 yum -y install MariaDB-client 同时也可以 yum -y install mariadb 一个效果。 有个用户组没得，先不管它 有的吖，操，瞎报什么...安装后脚本看看 哈哈，果然，这标志着我的这种慢慢吞吞就是吞吞怎么滴，学的法子，就不差，就能反映过来，yum的时候是安装报错没有group，就是一定是没有gorup的，但是结果id又看到了group，说明是安装后脚本，对吧，我是查看之前判断的，还不错。 版本还可以这是目前最新的了，毕竟是AlmaLinux9用的proxysql。 https://www.51cto.com/article/705594.html 启动proxysql服务后可见监听端口6032 6033 老版本是通过service启动的 6032是管理端口，进行配置proxysql的 6033是用户去连接的。 proxy的配置文件一般不用去改，一般都是通过sqlite这个小库去dml修改配置，sql语句修改了sqlite，就会自动把这个配置文件/etc/proxysql给改了。可以看一下配置文件 配置文件里就有端口号，可以自定义修改。 备份配置文件 ①注意用-a ②覆盖不想看到交互让你y，就用\\cp \\就是转义转成原始的cp，要知道cp一般是alias 但是没有改过来 改不过来就算，直接进SQLITE去用sql去改吧，反正后面都是用这种方式。 恢复一下 注意 移除的是server，其实client也会被当做依赖一并移除。 可以直接yum mariadb就是client 也可以MariaDB-client,一样的效果。 有了mysql 这个client命令，就可以进入proxysql的sqlite了👇 连接proxysql的sqlite mysql -uadmin -padmin -P6032 -h127.0.0.1 默认db 默认看的就是main库👇 monitor库可以看log了解相关信息，比如复制是否成功 看下这个表 select * from sqlite_master ;这个命令中sqlite_master没有指定哪个库，就是默认的main库 通过select * from db_name.sqlite_manster;可以看到每个db都有一个默认的表，且内容不同，且show tables from db是看不到的！ select * from main.sqlite_master where name='mysql_servers'\\G;可以看到当初是怎么创建定义字段的。 同时mysql里的desc命令在sqilte里是没有的，就是通过👆上图的方式来查看类似信息的。 开始配置proxySQL 添加mysql节点，默认就是操作的main，无需use main，等价于use main。 MySQL > insert into mysql_servers(hostgroup_id,hostname,port) values(10,'192.168.126.129',3306); MySQL > insert into mysql_servers(hostgroup_id,hostname,port) values(20,'192.168.126.130',3306); insert后检查： load和save，一个是加载到内存，一个是保存到磁盘，类似juniper，vyos的commit和save； MySQL > load mysql servers to runtime; MySQL > save mysql servers to disk; 创建登入mysql节点们的账号，用来查看read-only选项，以判断主从。 在mysql节点上 master节点： MySQL> grant replication client on *.* to monitor@'192.168.%.%' identified by 'cisco'; 一般此时主从都是正常同步的，所以slave上也会自动创建同样的账号密码。 在proxySQL上使用该密码用来做监控，其实就包含主从的判断 MySQL [main]> set mysql-monitor_username='monitor'; MySQL [main]> set mysql-monitor_password='cisco'; mysql > load mysql variables to runtime; mysql > save mysql variables to disk; MySQL [main]> select * from runtime_mysql_users; 但是找不到配置的信息在哪里查看，虽然可以肯定已经生效了，因为后买你可以通过log看到连接OK的。 本质上就是proxysql通过这种方式来判断主从的👇 查看相关日志，看连接有无问题 查看监控连接是否正常的 (对connect指标的监控)：(如果connect_error的结果 为NULL则表示正常) MySQL> select * from mysql_server_connect_log; 查看监控心跳信息 (对ping指标的监控)： MySQL> select * from mysql_server_ping_log; 查看read_only和replication_lag的监控日志 MySQL> select * from mysql_server_read_only_log; MySQL> select * from mysql_server_replication_lag_log; 从上往下插入的，上面很多都是一开始没有配置密码的时候的报错，账号是一致的，没有配置前默认就是用monitor作为账号的。 默认密码在proxysql的配置文件里可见👇 但是这两个日志还没有记录👇 通过前面的配置此时proxysql已经能识别出来谁是master，谁是slave了。 读写分离：分组，那个是写组，哪些事读组 MySQL> insert into mysql_replication_hostgroups values(10,20,\"test\"); 修正为： MySQL> insert into mysql_replication_hostgroups values(10,20,\"read_only\",\"test\"); MySQL> load mysql servers to runtime; MySQL> save mysql servers to disk; 通过下图可知，前面写的是写的，后面是读的，不过疑问就来了这里的10，20是否一定要和前面定义节点的时候的10，20保持一致，为了得到验证，这里先把10，20改成100，200 按PPT来，报错了，自行排错 好了加了ready_only的变种关键字，应该是新版本的变动。 别忘了load和save 为了测试10，20是不是要和前面的定义节点的时候保持一致，我觉得肯定要一致就是把定义节点的hostGroupID写到这里来的，才符合逻辑，这里先改掉，改个屁，给老子改回来，明摆着的事情不要瞎弄。 特意将两处的id改为不一致 改回来，不浪费时间！ 这个不知道怎么查看主从判断的结果。 读写分离：proxySQL使用的业务用户做DML和DQL用 配置发送SQL语句的用户，在master节点上创建访问用户，让它同步到slave上。 MySQL> grant all on *.* to sqluser@'192.168.%.%' identified by 'cisco'; 在ProxySQL配置，将用户sqluser添加到mysql_users表中， default_hostgroup默认组设置为写组10，当读写分离的路由规则不符合时，会访问10这个默认组的数据库；关注这里的10，从①定义节点②分组写读③默认路由要串起来理解。 MySQL> insert into mysql_users(username,password,default_hostgroup) values('sqluser','cisco',10); MySQL> load mysql users to runtime; MySQL> save mysql users to disk; 使用sqluser用户测试是否能路由到默认的10写组实现读、写数据 mysql -usqluser -pcisco -P6033 -h127.0.0.1 -e 'select @@server_id' mysql -usqluser -pcisco -P6033 -h127.0.0.1 -e 'create database testdb' mysql -usqluser -pcisco testdb -P6033 -h127.0.0.1 -e 'create table t(id int)' 就是在proxysql上通过127环回口连自己，模拟用户进行DQL查询，发现的到server_id都是129，说明都是走的proxySQL的默认路由其实人家叫default_hostgroup 10，10里放的就是192.168.126.129这个master。 同样用真实的client测试，在windows上要注意引号不能像linux那么随意👇 就是外面用双引号，里面用单引。 linux倒是不问，其实就该这样 继续测试路由功能-也就是proxysql的分发功能，目前都是默认转到10这个hostgroups里的成员，也就是master。 读写分离：添加路由规则 MySQL> insert into mysql_query_rules (rule_id,active,match_digest,destination_hostgroup,apply)VALUES (1,1,'^SELECT.*FOR UPDATE$',10,1),(2,1,'^SELECT',20,1); MySQL> load mysql query rules to runtime; MySQL> save mysql query rules to disk; 注意：因ProxySQL根据rule_id顺序进行规则匹配，select ... for update规则的 rule_id必须要小于普通的select规则的rule_id 看似条，实则，有条默认就是👇从上到下，漏下来的就路由到10组里。精细匹配的放在前面和交换机的ACL一样~其实在哪都一样，全TM从上往下匹配的，宇宙通用法则 测试：select一下 mysql -usqluser -pcisco -P6033 -h127.0.0.1 -e 'select @@server_id' windows cmd 也看看👇 好了，这些路由到slave了 再看下这个图👇 由于规则是select开头， # 注意哦，不区分大小写，通过上图就知道小写select也是命中rule2的。 所以show 走的是默认规则，看到的也就是129的server_id了。 测试：用事务的方式测 一个意思，begin自然也不会命中rule2 👇这些都直接会写到master，并同步到slave的，不会说事务没结束巴拉巴拉 slave就直接有了👇 测试：创建表 题外话-试试不进到库里create 发现是可以同步， 再试试不进到库里insert 竟然可以！看看前面的章节发现是过滤器的bug 去到交互模式试试事务 事务不提交倒是不会同步到slave的 事务级别默认就是 ，不提交，对于本地master来讲，也只是当前session里看到而已，本地都没有commit，如何能同步给slave呢。只不过上面的不进入交互模式是由区别的。 为什么上图的操作都是往slave发送的呢 没有commit的就没有结果，当然这里疑问是问什么往slave上发送。 当然！不是，你看rule就是走的默认发到master的，你为什么只是salve上看到，是因为你的master交互界面还在上一个事务里，没有commit 提交后就看到啦 路由的信息：查询stats库中的stats_mysql_query_digest表 MySQL > SELECT hostgroup hg,sum_time, count_star, digest_text FROM stats_mysql_query_digest ORDER BY sum_time DESC; 别学了前面忘记后面，as的用法：别名 SELECT hostgroup as hg,sum_time, count_star, digest_text FROM stats_mysql_query_digest ORDER BY sum_time DESC; 主从学过了，proxySql读写分离学过了，主从故障手动切换也学了，那么问题就来到了HA了，所以学技术好听点就是永无止境，难听点就是没完没了，本质上还是和自己争斗，希望各位能知止而后能定有所得，艮土乾金山天大畜。 proxy的单点故障、master/slave的切换后、proxy的路由变化 下面是一些高可用技术的介绍 1、MMM：很老的软件了，过时了，基于perl写的 Multi-Master Replication Manager for MySQL，Mysql主主复制管理器是一 套灵活的脚本程序，基于perl实现，用来对mysql replication进行监控和故障迁移，并 能管理mysql Master-Master复制的配置(同一时间只有一个节点是可写的) 官网：http://www.mysql-mmm.org https://code.google.com/archive/p/mysql-master-master/downloads rockylinux9都搜索不到这个软件了 2、MHA 也是perl语言写的，也是老产品了，但是不排除可也用 Master High Availability，对主节点进行监控，可实现自动故障转移至其它从 节点；通过提升某一从节点为新的主节点，基于主从复制实现，还需要客户端配合实现， 目前MHA主要支持一主多从的架构，要搭建MHA,要求一个复制集群中必须最少有三台 数据库服务器，一主二从，即一台充当master，一台充当备用master，另外一台充当从 库，出于机器成本的考虑，淘宝进行了改造后，淘宝TMHA支持一主一从 官网:https://code.google.com/archive/p/mysql-master-ha/ 3、更好的解决方案Galera Cluster Galera Cluster：wsrep(MySQL extended with the Write Set Replication) 通过wsrep协议在全局实现复制；任何一节点都可读写，不需要主从复制，实现多主读写 类似之前主-主结构--那个会出现冲突-通过autoincrement错开自动增长的键，但是建表没办法，但是Galera Cluster应该可以避免这些冲突，而且支持多主不仅仅是2个，3个也行~。 4、GR GR（Group Replication）：MySQL官方提供的组复制技术(MySQL 5.7.17引入的技术)， 基于原生复制技术Paxos算法 5、此外还有天生牛逼产品，分布式db Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:50 "},"27-MYSQL数据05/6-mysql的主从复制高可用性解决方案MHA.html":{"url":"27-MYSQL数据05/6-mysql的主从复制高可用性解决方案MHA.html","title":"第6节 mysql的主从复制高可用性解决方案MHA","keywords":"","body":"第6节. mysql的主从复制高可用性解决方案MHA 我为什么要用这个东西呢，一个perl 语言写的东西，最新版本2012年，难道是因为免费实用够用？好吧 MHA架构 manager节点，管理着多个主从，某一堆主从的master挂了，就会把在那堆主从里的找一个slave提升为master。 manager管理节点，是单点，但是问题不大，因为用户访问的业务不走它走，它挂了，不影响业务；除非运气好，manager挂了的时间段里，某个主从的主也挂了，此时由于没有管理节点，所以就无法自动提升slave，所以也就导致业务出问题了，按传统的思路写是要分流到master上的，此时master都没了，所以业务收到了影响。 MHA的工作原理介绍 先复习一下主从的底层 再来理解下图MHA的思路 1、三根柱子代表什么 2、柱子之间的落差，就是binlog落差 3、slave(i)这个i是变量，就是很多个slave的意思，u known~，这些不太优秀的slave(i)们，不仅仅从master那拿到的binglog比最优秀的lastest slave要少一截，而且写到本地还写的慢。 4、所以矮子当中选将军咯，新的master选出来后， 5、想办法补齐DeadMaster和LatestSlave之间的binlog差，怎么想办法：①dead不是机器dead只是服务dead就去找binlog文件；②binlog是否有异地备份；③没有异地，本地重启看看也行啊对不对，总之找到dead master上的binlog，补齐到LatestSlave上，不过这些理论上都是MHA实现的，所以MHA不太可能说给你异地取备份，也不太可能给你重启deadmaster机器，所以MHA能自动处理的应该就是机器没死mysql服务死的的情况去去binlog文件。 6、slave(i)同步之前可能也要做两个事情：①SQL Thread进程走完，把已经拿到的binlog写到本地文件里；②把和Lastest Slave之间的差距通过I/O Thread补齐；然后就继续同步。 看它意思， 这么麻烦的吗，难道不是slave 所有的save包括最优的那个slave，统统从deadmaster同步X，唉不对，同步各自的差异binlog嘛。 估计是怕deadmaster连机器都挂了，ssh上不去，MHA的manage拿不到binlog，就做不了X，所以才有了i1-->i2-->X这么一个先后同步的过程，就是每次同步的动作不大，能成功的同步先做了。 i1就是本地的sql线程将relylog写到库里，i2就是slave们从最优的slave去取差异binlog；X就是deadMaster上取binlog这个可能就不一定成功。 所以MHA要利用到ssh，scp这些协议 manager工具包需要epel源，node工具包不需要epel源。 manager工具包安装在manager节点上，node工具包安装在监控的主从节点上和manager节点上。 半同步复制，默认是异步会存在slave和master差异越来越大的可能，而半同步，就能保证只要有一台slave回应我master了，master就回答client了。参见 第二十七章 MYSQL数据05-第2节 master作为半同步的一个关键点，它的行为就是只要收到一台slave的响应就会回应client。那么问题来了，这种行为下，slave(i)们是否是保证一定有一台是最优的，会不会出现不同的数据分散到不同的slave上，不会！同步的机制就是binlog的posti位置，按序复制的，所以某台slave回应了master，那么这台slave一定已经取到了最新的数据了！ 呼~~~ 简单了解👇 MHA有个配置文件： ​ 其中global配置：就是为各个主从(它叫做application)提供默认的统一配置模板 ​ 如果想为单个某个主从，配置，就针对他们配置application关键字的配置。 在管理节点上安装两个包 mha4mysql-manager mha4mysql-node 在被管理节点安装 mha4mysql-node 在管理节点建立配置文件 vim /etc/mastermha/app1.cnf # app1就是应用1，就是对应一组主从，另建一个app2.cnf就是对应第二组主从 [server default] user=mhauser # 用来提升slave为master的账号，就是需要登入mysql去修改配置的。 password=cisco manager_workdir=/data/mastermha/app1/ manager_log=/data/mastermha/app1/manager.log remote_workdir=/data/mastermha/app1/ # 远端主机上的存放路径 ssh_user=root # binlog利用ssh scp拷贝过来 repl_user=repluser # 主从复制的账号也要 repl_password=cisco ping_interval=1 # 1s ping探测主服务器是否在线 [server1] hostname=192.168.126.129 candidate_master=1 [server2] hostname=192.168.126.130 candidate_master=1 [server3] hostname=192.168.126.131 在Master上配置 vim /etc/my.cnf [mysqld] log-bin server_id=1 skip_name_resolve=1 # 在MHA里必须要配置，否则默认MHA行为是做ip地址反向解析，导致用户ID识别出问题。 mysql>show master logs mysql>grant replication slave on *.* to repluser@'192.168.8.%' identified by ‘cisco'; # 主从复制账号和授权 mysql>grant all on *.* to mhauser@'192.168.8.%’identified by‘cisco'; # mha的管理账号，用来提升slave为master，需要修改配置的。 在slave上 vim /etc/my.cnf [mysqld] server_id=2 #不同节点此值各不相同 log-bin # 从节点本来无需binlog，但是将来要提升为master，所以需要事先开启binlog read_only relay_log_purge=0 # 默认会清除中继日志，这里不清楚，将来可能恢复使用。 skip_name_resolve=1 mysql>CHANGE MASTER TO MASTER_HOST=‘MASTER_IP’, MASTER_USER='repluser', MASTER_PASSWORD=‘cisco’, MASTER_LOG_FILE='mariadb-bin.000001', MASTER_LOG_POS=245; 在所有节点实现相互之间ssh key验证 MHA验证和启动 masterha_check_ssh --conf=/etc/mastermha/app1.cnf # 检查ssh key验证是否OK masterha_check_repl --conf=/etc/mastermha/app1.cnf # 检查主从复制是否OK masterha_manager --conf=/etc/mastermha/app1.cnf # 启动MHA集群，此时就可以正常使用了。 排错日志 /data/mastermha/app1/manager.log # 比如服务起不来，可以看看日志。 实验 4台就够了 命名 hostnamectl hostname mha-manager hostnamectl hostname master hostnamectl hostname slave1 hostnamectl hostname slave2 配置主从 ​ master上创建了两个账号，slave 配置的时候就从binlog的初始pos开始(通过flush logs刷新binlog可知初始位置在哪我的是389)；就可以把这两个账号带过去，不过最好手动配置吧。 修改一下，我的操作是在master下flush logs; show master log;然后手动在slave那边创建账号，master我们本次实验就当作是干净，或者不用同步之前的数据。 CHANGE MASTER TO MASTER_HOST='192.168.126.129', MASTER_USER='repluser', MASTER_PASSWORD='cisco', MASTER_PORT=3306, MASTER_LOG_FILE='mariadb-bin.000030', MASTER_LOG_POS=389, MASTER_CONNECT_RETRY=10; # 这个就是默认值 START SLAVE; 这样slave2也就同步了，slave1一样的，略 两个slave确认下 read_only变量 这个只读，mha后面就而已直接改了，好像是通过set 设置变量，而read_only本身就是变量，所以mha可以直接修改为OFF的。 我就想问了，不能修改/etc/my.cnf吗，ssh都有了，估计底层不是用走的修改配置文件的逻辑。 安装mha包 直接yum可没有的，都是老软件了，不过可以用的。 https://code.google.com/archive/p/mysql-master-ha/downloads 跳过报错试试 可能少一个perl-Log-Dispatch包 slave节点安装没问题 基于key的ssh配置 这里偷懒，直接做成4个机器 两两 ssh key 互通，所以操作如下 这样就实现了ssh-key 一个主从对应一个app1.cnf配置文件👇 上图👆就表示，这个包安装有问题了，还是之前的报错，不能跳过了就。 算了算了，👆按这个跑一边还是报错 估计就是el5的版本，太老了，算了，放弃这个工具了，也就只能在centos5\\6上跑跑了。 以下是视频里的老板不能继续演示，不过视频，应该是centos7，所以centos7也能用咯，还不错 ...中间省略... 最后就执行mha运行👇，从warning消息可见，是有一个默认的cnf存放路径的，我们这里手工指定的。 这个程序是一个前台执行的程序，就是一直盯着集群，只要master挂了，就提升一个slave。只要master正常，这个mha程序就卡在这里。 做mysql集群的时间也要同步 最好用ntp一级chronyd 模拟测试集群的HA 此时直接down掉master，然后看manager上 发现此时manager已经推出了 然后看看日志 最后可见从192.168.37.17切换到了192.168.37.17这台新的master。 此时192.168.37.37由于是master，show slave status\\G;就是空信息了。然后read_only就通过set的方式修改了变量为OFF了，但是配置文件是不会改的-->这就带来了只要重启服务就会变成read_only=on了，需要手动去改一改的。 此时192.168.37.27就指向了新的master37 挺好的一个软件mha，怎么不更新了，rocky9，直接无法安装manager了。。。回头再搞搞 接上面主从切换，原来的master挂了，然后slave升为master，然后业务OK了，去修复老的master，然后将其配置为slave即可。然后重新在manager上运行管理命令masterha_manager --conf=/etc/mha/app1.cnf 然后这里的mha不能解决proxySQL读写分离的场景，因为主从自动切，但是proxySQL的分组我记得是写死的hostgroup就固定了，比如10组里就是写操作层，10组里加入得就是master了，这个地方就存在自动切得需求，而mha方案里没有提到，所以mha也不是很好得一个方案。 重点学习Galera Cluster这个工具 mha还是单主多从，总归有数据没有同步丢失的可能；多主可能才是更极致的方案，percana、mariadb、mysql都有各自的多主方案。 多主存在的冲突如何解决的？下一篇说明 calera cluster 至少3个master，每个机器都可以读和写 多主架构，对于client来讲，到底访问谁呢？ 这里是不是就涉及负载均衡啦，不过还可以轻量化的用keepalived的VIP虚拟IP来做--只不过就是单节点读写了-应该。而LB就是针对不同session可以做到负载分担的。 https://galeracluster.com/library/documentation/certification-based-replication.html 图中关键字：global trx id就是 全局事务ID，关于GTID前文也讲过👇 上图说明了如何保证多主集群下的数据一致性： 1、client往多主--Galera Cluster集群里写数据，多主至少是三主了，图中简单画成了2个server示意； 2、client update 数据，不管是LB还是keepalive都是发送到一个master上的； 3、这个master，也就是图中server，就开始处理啦，OK，后就提交，因为涉及事务，还需要提交； 4、提交能否真正提交成功，还不一定的，往下看，此时就会触发replicate writeset应该也叫write set replication (wsrep)写集复制这个功能， 然后所有的server就都收到一个GTID全局事务ID，然后就开始处理 5、接收到update的server就检查，检查不通过就rollback_cb回滚，通过就是commit_cb提交到db里去。 同时；别的server也会检查，不通过就discard--由于数据不是本地提交的是别的server的，所以直接discard，如果通过，就应用数据apply_cb就是update一下，然后commit_cb提交事务。 有时间可以看看这个 https://mariadb.com/kb/en/getting-started-with-mariadb-galera-cluster/ 其中提到了 所以我的实验环境是默认就有的应该，无需安装，同时也可以安装吧。 Galera Cluster官方文档 http://galeracluster.com/documentation-webpages/galera-documentation.pdf http://galeracluster.com/documentation-webpages/index.html https://mariadb.com/kb/en/mariadb/getting-started-with-mariadb-galera-cluster/ Galera Cluster包括两个组件 ​ Galera replication library (galera-3) ​ WSREP：MySQL extended with the Write Set Replication WSREP复制实现 ​ PXC：Percona XtraDB Cluster，是Percona对Galera的实现 ​ MariaDB Galera Cluster 参考仓库国外的慢：https://mirrors.tuna.tsinghua.edu.cn/mariadb/mariadb-5.5.X/yum/centos7-amd64/ 注意：都至少需要三个节点，不能安装mariadb-server Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"27-MYSQL数据05/7-实现galeracluster和性能测试.html":{"url":"27-MYSQL数据05/7-实现galeracluster和性能测试.html","title":"第7节 实现galeracluster和性能测试","keywords":"","body":"第7节. 实现galaracluster和性能测试 老版本的安装就是uyum install MariaDB-Galera-server，老版本的方法这里不管了，直接集成到新版里的。 然后就是看配置文件，在/etc/my.cnf里明确说了include galeracluster的配置就在这里 wsrep_provider= 其中wsrep_provider=要填写一个叫做libgalera_smm.so的库文件，而这个文件是安装mariadb自带安装的， 而且就是mariadb的yum源安装的@mariadb可见。 👇这个源就是按官网的yum源复制过来就行啦，详情将前文mysql安装吧。 好了，基本零件都不缺了。 wsrep_cluster_address= 三个节点的各自地址 binlog_format=row，就是二进制格式，一般默认新版都是MIXED，这里取消注释用ROW吧，忘记了，看前文👇 default_storage_engine=InooDB肯定是InnoDB了，MyISAM都不支持，官方就是说仅仅实验环境吧我看到好像。 innodb_autoinc_lock_mode=2这个不管默认就好 其实就是改wsrep_provider和wsrep_cluster_address以及binlog_format=row。 具体配置选项👇 https://mariadb.com/kb/en/configuring-mariadb-galera-cluster/ 实际需要的操作 vim /etc/my.cnf.d/server.cnf [galera] wsrep_on=ON # 10.1.1多了个开关 wsrep_provider=/usr/lib64/galera/libgalera_smm.so wsrep_cluster_address=\"gcomm://192.168.126.129,192.168.126.130,192.168.126.131\" binlog_format=row 这个gcomm是协议，就好比ftp://还比https://一样，gcomm是glaeraCluster内部通讯的协议。 👆就改这么多，其他不动 优化设置 wsrep_cluster_name = 'mycluster' # 默认my_wsrep_cluster wsrep_node_name = 'node1' # 本机器的名称，这里暂时不加 wsrep_node_address = '192.168.126.x’ # 当前机器的IP，不用写，这里只是强调写出，实际不写 通过scp复制到其他2个节点 启动注意点 三台不是一样的方法启动； 集群存在/不存在，启动方式也是不同的。 1、原来没有cluster的启动方法和原来有cluster集群的启动方法是不同！ 2、首次启动，其他节点，systemctl restart mariadb就行 本实验是初始化集群，开始启动 任意一个节点，来初始化集群 galera_new_cluster 其他节点就 systemctl restart mariadb 这里我就用192.168.126.129，这里的master主机名，不要在意，不用纠结，这是以前实验 主从起的名字，这里多主，每个节点既是master也是slave。 其他节点重启报错 而且初始化集群的node，直接重启也会报错 此时处理方法，在没有头绪的情况下，俺是第一次接触这个集群，所以注释掉/etc/my.cnf.d/server里的galer模块配置，重启mariadb。 然后再重新走一遍就行了。注意重新启动就用galera_recovery来启动集群 然后莫名其妙再敲一遍就OK了？ 然后去重启其他节点的服务，看看能不能成功， 不行 排错 之前的配置注释掉 没用，重新做吧，把mariadb remove掉重弄？ 重新rm -rf /var/lib/mysql/* 反复3次后倒是成功了 mb的怎么弄都OK了现在！！ 应该就是/var/lib/mysql/下要干净， 然后有个明确的故障点处理方式就是 👆这个文件要么删掉，要么修改safe_to_bootstra:1就可以启动服务成功。 错误-2 这是cli用错了，重启不能这么干， 好像所有的node 都stop mariadb就起不来了，因为可能没有gcomm的通信方了 我的理解，就是集群里必须有一个活着才能重启成功，测试下 现在3个node都挂了，如何起来了，简单， 这样处理就行了 随便找一台node，一般是最优的吧 果然就好了，所以前面的配置OK就是这里的处理手法要注意，这里你可以理解为是技术细节，也可以理解为产品不傻瓜化。 此时理论上另外2个node直接restart就行了 本来创建集群的那一台tm的竟然起不来 判断失误，没事，老方法，狗屁啊，safe_to_bootstrap=1这个参数是针对the most advanced node最优节点也就是数据最新的节点的操作，就是确定后要敲galera_recovery的，前面已经有一台node初始化了cluster，此时其他只能是加入了，所以操作就是systemctl restart mariadb，通过报错日志可见 说明是SSL证书问题，关掉ssl 重启就好了 总结 1、初始化cluster 2、重启cluster 操作要小心 galera_recover重启cluster，报错没关系，集群不受影响👇 总结2：重启集群 上图👆就告诉你了，如果nodes及群里的节点全部都stop了，此时就需要有一个node去做bootstrapped创建集群，如果只是简单的started normally就是systemctl start mariadb就会找wsrep_cluster_address里的IP地址去做gcomm协议连接，如果没有一个nodes是起来的，那么就会启动服务失败，这很关键。 测试下： 当前集群OK 停掉3台中的2台node，尝试将两台stop的start 没问题，处理方式OK 集群里的所有nodes全部停掉，这里就是3台咯 此时就需要恢复集群， 1、galera_recovery 按上文的说明，就是要修改 还是起不来 再次查看，发现人家让你找最优的node，何为最优，就是seqno 找到了，去84这台敲 galera_recovery 不行啊，操！ 不过通过上面的操作已经知道了 不是恢复而是创建，用galera_new_cluster就可以了 而且集群ID也不会变 剩下两台node就简单了，重启服务就行了 到此不为此，基本的处理方法就有了。重点看总结就行，然后之前不行的原因就是①ssl没有弄好，之前实验遗留的，只配置了2个node的ssl，这里涉及3个呢，不过有一次ssl没关也OK，不过通过上文的报错可见确实有问题的；②就是集群的启动和重启有讲究的，重启有问题还是用的初始化创建的命令来解决问题的。 继续看集群的数据读写处理 说明：hostname无所谓master slave，这仅仅是之前实验的遗留，这里是多主，都是master。 导入一个sql脚本，会创建新库和表，这是在第一个节点上做的。 然后其他所有nodes也就自动同步了👇 表同步自然也OK 变量查看 SHOW VARIABLES LIKE 'wsrep_%\\G'; 有当前自己的IP信息👇 有/etc/my.cnf.d/server.conf里配置的信息👇 SHOW STATUS LIKE 'wsrep_%'; # 状态变量 SHOW STATUS LIKE 'wsrep_cluster_size'; 查看集群nodes数👆 添加新成员node 不过人家官方说了，the first node has x.x.x.x，才能加入x.x.x.x，呵呵~。👇 其实就是每台node都补一个IP，然后一个个重启就行了 只要cluster里有一个活着，就能重启服务OK，除非同时down了 人家说的就是这个意思，所有nodes都down了，才需要重新初始化(这个我走成功了），或者没有走成功的galera_recovery； 然后新加入的node，也会很快同步db的 建表也不会冲突，因为有wsrep机制啊 只会有一个成功👇 然后看看大量数据写入的一个速度，是明显比主从慢很多的，因为👆 下班关机 算了明天继续弄吧，由于机器关机了，之前就是临时做实验，所以mysql服务都停了，这里正好重演一次cluster的启动 可见👆所有服务都没起来 随便选一台初始化集群 查看seqno虚拟号，👇下图注意哦，135的seqno最优秀，所以safe_to_bootstrap:1就是1，其他都是0，这是系统给默认设置好的，然后有个node3的1是我改的！我之前准备用node3初始化新建的，所以看到的是1。 node2最优秀，尝试不初始化，重启的专用cli试试 起不来啊~ 直接new吧哈哈 然后其他node 都systemctl start mariadb就行了 同时创建的cli的没问题👇 测试开始 然后就看看数据的增长 发现这个速度比简单的主从还要慢很多，然后其实可以做成事务，事务就是一起提交，会快一点。 像这种就好比大量用户的写咯，所以具体用的时候还存在问题，需要优化吧应该！好像他们业务实际也不这么玩。 还发现一个现象👇 就是galera cluster默认就给你做了table的插入的自动间隔，以前是这么配置的。 👇12分钟终于结束了： 试试 事务的方式，理论上会快一些 这TM也太夸张了，不对吧 第二十运行事务，一样是6秒种，一共插入了3次，每次99999行，300000-3=299997行，对的👇 再次不跑事务看看 结论，galeraCluster集群，大数据并发写入，使用事务就很快！不使用事务就灰常慢！ 复制的问题和解决方案 其实以下这段文字算不得什么问题和方案，聊胜于无，看看吧👇 (1)数据损坏或丢失 Master： MHA + semi repl Slave：重新复制 (2)混合使用存储引擎 MyISAM：不支持事务 InnoDB： 支持事务 (3)不惟一的server id 重新复制 (2)复制延迟： 需要额外的监控工具的辅助 一从多主：mariadb10版后支持 多线程复制：对多个数据库复制，好像是高版本的特性。 TiDb概述 主从、多主，对于写操作，本质上都是在一台上操作的，所以mysql这种HA，本质上就是有瓶颈的。这话欠妥，没有讲到点子上，我们将LB的行为，在很多地方体现：①portchannel②f5这些其实都是针对多个session多个会话或多个用户去负载分担的，mysql的瓶颈本质的一个点就是如果是一个session里的一个事务，而这个事务里面有大量的操作，那么这个事务肯定是往一台机器持续读写的，而针对这个单个事务的负载分担解决方案就是TiDB它可以做到分布式事务。 牛逼👆 而更优的解决方案就是TiDb分布式数据库。 RDBMS关系型数据的 数据一致性ACID特性；NoSQL性能好但是没有保证数据的一致性；TiDb结合了两者的优点，又叫做NewSQL 所以DB分为了：RDBMS、NoSQL、NewSQL 据说：mysql的业务迁到TiDb，基本上不用改动，直接搬过去就能用，不用改代码。 TiDB的核心特点 1 高度兼容 MySQL 大多数情况下，无需修改代码即可从 MySQL 轻松迁移至 TiDB，分库分 表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移 2 水平弹性扩展 通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻 松应对高并发、海量数据场景。 3 分布式事务 TiDB 100% 支持标准的 ACID 事务 4 真正金融级高可用 相比于传统主从 (M-S) 复制方案，基于 Raft 的多数派选举协议可以提 供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动 恢复 (auto-failover)，无需人工介入。 5 一站式 HTAP 解决方案 TiDB 作为典型的 OLTP 行存数据库，同时兼具强大的 OLAP 性能， 配合 TiSpark，可提供一站式 HTAP解决方案，一份存储同时处理OLTP & OLAP(OLAP、OLTP 的介绍和比较 )无需传统繁琐的 ETL 过程。 6 云原生 SQL 数据库 TiDB 是为云而设计的数据库，同 Kubernetes （十分钟带你理解 Kubernetes核心概念 ）深度耦合，支持公有云、私有云和混合云，使部署、配置和维护变得十 分简单。 TiDB 的设计目标是 100% 的 OLTP 场景和 80% 的 OLAP 场景，更复杂的 OLAP 分析可以通过 TiSpark 项目来完成。 TiDB 对业务没有任何侵入性，能优雅的替换传统的数据 库中间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力. 数据库的整理差不多了就 下面介绍以下压力测试 性能衡量指标 数据库服务衡量指标 QPS: query per second # 查询性能： 每秒处理的查询次数，简单的单表select和多表join对系统资源的消耗是截然不同！所以压力测试的时候是要事先定义select查询规则--涉及哪些查询方法。 TPS: transaction per second # 事务的处理，主要指的就是数据的修改性能了，涉及增删改。 压力测试工具 mysqlslap # 系统自带，无需安装 Sysbench：功能强大 https://github.com/akopytov/sysbench tpcc-mysql MySQL Benchmark Suite MySQL super-smack MyBench mysqlslap使用 该工具来源于MariaDB-client软件👇 注意：虽然这个工具测试完成后不会在DB中留痕，但是binlog肯定会大量被它修改的，所以测试的时候binlog要么关闭，要么单独存放。 binlog除了在/etc/my.cnd里定义，也可以放到/etc/my.cnf.d/server.conf里一样的，👇涉及集群galera里定义了binglog的格式row，所以也可以log-bin开启也放在这个配置文件里。 binlog是否启用，最好还是看变量，而不是ll /var/lib/mysql/去看相关文件有没有对吧，你这样还得去先看看cnf人家配置在哪里了。 生产了大量的binlog 如果要停binlog，set 变量这种挺不掉，因为是基于你当前cli交互进去的session的，而压力测试是多session并发的，完全没有一点效果，因为session完全撞不到一起去，哈哈。而且sql_log_bin是session级别的变量，没有全局的。真的是session的，看看官方 再试试 果然👆，无法实现：关闭全局binlog的效果，只能去cnf文件了。 等等👇这TM什么回事： 问👆binlog基于本会话到底是关了还是没关？ 确实关了 测试引擎之间的差异👇 [root@node1 ~]# mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --engine=myisam,innodb --debug-info -uroot -pmagedu --concurrency=50,100 # 50到100个并发数，是个范围区间。 案例 也不是绝对的👇，网速随便搜搜都有推荐配置优化方案，也可以参考他们的。 max_connections的默认值看看 因为这个默认限制，所以并发测试看下效果👇,并发就是多个sessions同时测试的。 修改并发数👇 重启服务后 那个遇到低版本的并发改不上去(mariadb5.5.60就是上不去151只能通过my.cnf改成214的上限)的处理方法，这里也做一下记录，因为涉及底层逻辑：151的默认，明明mysql的配置文件里改的是2000，但是实际只能到214，是因为并发底层走的是文件socket，这个socket要调上去的，是系统层面的东西，底层的东西。打开一个socket就会开启一个文件描述符fd。 1、mysql的配置文件里 /etc/my.cnf里或者/etc/my.cnf.d/server.cnf的max-connectsions=2000 2、系统级别的socket，文件socket的限制扩容 ulimit -n 66666 # 设置 ulimit -a # 查看 就可以看到open files这项改了，不过这个命令是基于session修改的，也就是当前shell窗口有效。 3、LimitNOFILE 高版本就是改了的，至于第二点ulimit应该不用改！确保下面👇的数据足够以及mysql配置文件里max_connections改了就行了。 一般数据库1000-2000并发就差不多了，不想apache或nginx上万都行。 所数据库的并发数，不是越大越好，设置一个你的服务器资源能够承受的值就好，如果设置过大比如8000，结果真的来了这么多并发，就会导致db处理不过来，结果就是一个用户都访问不了了。 back_log 并发加入是2000，那么如果超出2000，又来了10个人，那么10个就是进入backlog进行排队。这个排队好像和QoS里的队列不是一回事，是保持的tcp连接数，本来2000个tcp连接上线，超出了也不是说就拒绝掉，而是用back_log机制暂存以下这些tcp连接。 max_connect_errors 针对单个用户，如果连接报错的次数达到一定的值，就会禁止该client连接过来。直到mysql服务重启，或者flush hosts命令清空此client主机的相关信息。所以这个参数的计数周期也就是两次服务重启或者flush hosts之间的时间。 比如黑客不断尝试连接，密码出错了10次，此时应该就可以触发此机制。 open_files_limit 这个其实就是 /usr/lib/systemd/system/mariadb.service 文件里的LimitNOFILE参数 两个都是干一件事的，不过使用场景不同 open_files_limit是适用于二进制安装或者编译安装的，这两类的服务启动都是二进制文件启动的，此时调整socket文件的打开数就这么调，这就是一个配置选项直接配置在my.cnf配置文件里。 LimitNOFILEs是用于 systemctl 启动的服务 本身就是一个配置选项，也是变量，所以直接配置到/etc/my.cnf里就行啦，他👇这里的三个配置方式，最后一个是/etc/security/limits.onf估计也是一个意思。 就是知道有这么个东西，具体最合适的还需要自己修订，和找最新的实践分享。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/28-运维自动化之ansible.html":{"url":"28-运维自动化之ansible/28-运维自动化之ansible.html","title":"第二十八章 运维自动化之ansible","keywords":"","body":"第二十八章 运维自动化之ansible Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/1-自动化运维介绍.html":{"url":"28-运维自动化之ansible/1-自动化运维介绍.html","title":"第1节 自动化运维介绍","keywords":"","body":"第1节. 自动化运维介绍 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/2-ansible安装和架构介绍.html":{"url":"28-运维自动化之ansible/2-ansible安装和架构介绍.html","title":"第2节 ansible安装和架构介绍","keywords":"","body":"第2节. ansible安装和架构介绍 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/3-ansible常见模块.html":{"url":"28-运维自动化之ansible/3-ansible常见模块.html","title":"第3节 ansible常见模块","keywords":"","body":"第3节. ansible常见模块 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/4-ansible相关常见工具.html":{"url":"28-运维自动化之ansible/4-ansible相关常见工具.html","title":"第4节 ansible相关常见工具","keywords":"","body":"第4节. ansible相关常见工具 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/5-playbook的实现软件配置和部署.html":{"url":"28-运维自动化之ansible/5-playbook的实现软件配置和部署.html","title":"第5节 playbook的实现软件配置和部署","keywords":"","body":"第5节. playbook的实现软件配置和部署 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/6-playbook的高级用法模板template.html":{"url":"28-运维自动化之ansible/6-playbook的高级用法模板template.html","title":"第6节 playbook的高级用法模板template","keywords":"","body":"第6节. playbook的高级用法模板template Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/7-ansible的高级用法role1.html":{"url":"28-运维自动化之ansible/7-ansible的高级用法role1.html","title":"第7节 ansible的高级用法role1","keywords":"","body":"第7节. ansible的高级用法role1 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"28-运维自动化之ansible/8-ansible的高级用法role2.html":{"url":"28-运维自动化之ansible/8-ansible的高级用法role2.html","title":"第8节 ansible的高级用法role2","keywords":"","body":"第8节. ansible的高级用法role2 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"29-HTTP协议和APACHE/29-HTTP协议和APACHE.html":{"url":"29-HTTP协议和APACHE/29-HTTP协议和APACHE.html","title":"第二十九章 HTTP协议和APACHE","keywords":"","body":"第二十九章 HTTP协议和APACHE Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"29-HTTP协议和APACHE/1-httpd基础知识.html":{"url":"29-HTTP协议和APACHE/1-httpd基础知识.html","title":"第1节 httpd基础知识","keywords":"","body":"第1节. httpd基础知识 通过man socket查看 man 2 socket 开发看的 这里的accept bind connect都是C语言写的，他们都是也给tcp连接的更加底层的功能模块。 man 3 socket 也是开发看的 BIND 模块，是socket套接字绑定用的 1、创建socket描述符 2、绑定socket描述符和协议|IP|PORT 3、listen，打开socket监听 4、client，同样打开socket，通过connect函数发起请求，连接的时候指定目标服务器的IP和PORT。 5、请求发过去后，server端就有一个accept函数负载接收用户的connect请求。 6、到此就形成了一个连接。比如TCP/UDP的连接，什么UDP无连接，谁说无连接的，只是不面向连接，什么叫不面向连接，就是不时刻维护连接信息。但是连接的初始化工作还是要做的，否则怎么通信呢。 同样进一步理解socket的函数模块和工作思路 还是和上文一样，打开socket，bind协议IP端口，监听socket，处于accept状态 此时client1来了，就开启一个New Socket，然后发送/接收数据， client2来了，同样新建一个socket，来处理，多个不同的socke连接来实现不同client的请求。这里面可以联想到一个窗口一个软件一个页面打开可能涉及多个session会话，一个session应该就是对应一个连接(socket)吧。 看下函数调用，py案例 注意看下小端口的一个保留特性，权限不够的报错，这个在一些linux比如centos7上可能是存在的，但是我用rockylinux9并没有发现什么问题。 结果并没有出现小端口不让普通用户用的情况， Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"29-HTTP协议和APACHE/2-http协议各种版本详解.html":{"url":"29-HTTP协议和APACHE/2-http协议各种版本详解.html","title":"第2节 http协议各种版本详解","keywords":"","body":"第2节. http协议各种版本详解 网站要防盗链的，否则，比如人家的图片都引用的你的网站的图片， 1、网站1盗链网站2的图片 2、用户打开网站1，看到的图片，其实都是网站1链接的2的图片 3、网站2的带宽白白被网站1的访问给占用了。 这里视频讲了一些HTML\\CSS\\JS的知识，只是简单的说了说，还不如走一遍《python编程从入门到实践第二版》P371的django案例。这块东西接触过了就知道确实不错有时间可以继续折腾下，做好了可以收纳自己的脚本工具到页面方便管理和用户入口。 MIME是邮件的一个功能，HTTP集成了👇 text是大类，/css这些是小类型。 然后浏览器里对应txt文件或者conf文件，都是不能直接打开看的，都是判断为下载，原因就是MIME里没有这个后缀的支持👇 浏览器的源代码点不比操作系统简单； 都是浏览器给你下载好资源，比如图片。 动态文件和静态文件 动态：3P：php、jsp、asp都是执行程序，用户看到的是这些3p程序执行后的结果--实在server端执行后的结果，而不是程序本身。 静态资源下发到CDN，动态资源需要回源。 访问https://oneyearice.github.io/其实就是打开这个根目录下的index.html文件。 HTTP的串行和并行连接 上图👆并行连接时，近乎同时发送多个tcp连接。 👇这篇的两个图，应该和串行和并行的案例还不同，👇这里提到的是一个tcp连接里传多个http的请求。 https://cloud.tencent.com/developer/article/1893815 3.1 Fully multiplexed 解决了队首阻塞的问题。对于同一个TCP连接，现在可以发送多个请求，接收多个回应了！在HTTP/1.1里面，如果在一个连接里上一个请求发生了丢包，那么后面的所有请求都必须等第一个请求补上包，收到回应以后才能继续执行。而在HTTP/2里面，可以直接并行处理。 回到并发，服务器本身并发支持是OK的，浏览器的并发，发起是有限制的。 对于大的网站资源，表面上打开的是首页，其实背后的很多资源(首页的index.html里会涉及很多url)，比如图片、JS 都是放在不同服务器上的，相当于client同时从不同的服务器上下载资源。其实是大大消耗的是客户端的带宽。 持久连接，这个好像就是上文提到的Fully multiplexed 👆图(a)就是串行，针对多个资源的下载，tcp连接一个个建立，http请求一个个发送； (b)是优化了tcp就一个，在一个tcp连接里，串行的发送http请求； (c)是进一步优化，tcp一个，然后在这个tcp连接里，并行的发送http请求。 不过，这理论感觉看看就就好，正向gpt所说，也不是并行就好的，早在硬盘的IO上就知道串行才是王道，并行干扰解决不好，同样http传输就涉及两个并行①tcp连接②http请求；干扰我不知道有没有，也许这里不同于硬盘不存在干扰，但是并行一定涉及排序重组，本身就消耗CPU了，所以也不一定就很好，考虑到浏览器的复杂性，这没必要深入，大概浏览器本身也会择优应用机制。 不过，串行的最大问题就是 one by one 一个过不去，后面都阻塞。 HTTP工作机制总结 HTTP协议 head是什么，头，就是数据包的头，类似于IP头，TCP头一个意思。 IP|TCP|HTTP|DATA 这个HTTP的头就是涉及http数据包格式了。 head里会涉及的内容，头部本身就是个文本。 1、content-type：告诉上层应用，拆包的机器一开，就是里面是啥，比如是data是也给音频、文本、图片啥的。就是MIME的大类/小类值咯。 比如用户收到这个response的头，看到这个image/png，浏览器就会用对应的图片解释模块来渲染DATA，来显示图片来。 2、表示http服务是什么软件提供的，nginx、github、tencent-cos桶。还真是五花八门，自定义的啊？估计是。 单个TCP的利用效率从Http1.1开始的 注意上图，除了tcp 持久化之外，还有一个浏览器同时6个持久连接，这个我在做URL访问LOG的时候，其实iptables的审计LOG啊， # 👇这是iptables的log配置 -A FORWARD -s 192.168.30.181,10.100.2.93,10.100.2.182,172.16.31.31 -m set --matc h-set xxx.com dst -j LOG --log-prefix \"[ user_name xxx.com ]\" --log-level 4 # 👇这是修改iptables的log默认存放目录，默认是放在/var/log/message下的，当然你这样做，默认里还是有的，无所谓了，这里多一份也OK，通过这里的日志分析，就会知道其实一个web页打开，就会涉及很多个sessions了，就可以和这里所学的浏览器的默认6个持久连接联系起来，不过呢6个也是他说的，不要当真，就算是真的也不要较真。 kern.warning /log/iptables/iptables.log 👇以下截图是流量LOG的说明截图，哈哈，当时就发现了一个网页会有多个tcp连接。 这里面还涉及专业术语，前面没有提炼出来： 1、单个tcp连接的持久化，里面支持多个http应答，不管http是否并发还是串行，这个叫持久连接 persistent connection； 2、一个TCP里，同时发送http请求，也许是有细微的间隔不过近似于并发(我看图猜的)，这个叫管道机制，TNN的真TM不会起名字。但是没办法，可能有的教程里还真这么些的。英文叫pipelining，真TM不会起名字，pipline是gitlab runnerr做CI/CD的流水线啊，操，老外起名字的水平肯定是不行的。 不过老早就http2.0了，这些参数也发生变化了，总之了解一下。 缓存工作原理，前面章节讲过LRU算法，涉及过期时间。 视频举例提到是jd的图片预览，但是实测并不是206而依旧是200的返回码。 问问gpt 虚拟主机，主机名，不过现在都是基于head头转发啊，niginx就可以啊，这也是个老技术了。 可见👆keep-alive也不一定是件好事，关键点就是长连接不释放，占用资源。 SPDY就是私有变公有HTTP2 推消息啊，所有个性化广告就是这么推过来的哦。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"29-HTTP协议和APACHE/3-httpd软件工作模型.html":{"url":"29-HTTP协议和APACHE/3-httpd软件工作模型.html","title":"第3节 httpd软件工作模型","keywords":"","body":"第3节. httpd软件工作模型 URI URI: Uniform Resource Identifier 统一资源标识，分为URL和URN ​ > URN: Uniform Resource Naming，统一资源命名 ​ 示例： P2P下载使用的磁力链接是URN的一种实现 magnet:?xt=urn:btih:6605589.....890EF888666 ​ > URL: Uniform Resorce Locator，统一资源定位符，用于描述某服务器某特定资源位置。 两者区别：URN如同一个人的名称，而URL代表一个人的住址。换言之， URN定义某事物的身份，而URL提供查找该事物的方法。URN仅用于命名， 而不指定地址 额，看来URI真的很多啊，哈哈哈 其实URL使用是最多的，URI里面虽然包含了很多，但是URL是最最多的，所以通常人们提到屌丝的时候，大概也不是什么赞美，哎哎哎，通常工作中URI和URL其实是混为一谈的，不必较真。但是你自己说出来的就得是URL URL组成 URL格式： ://:@:/;?# scheme:方案，访问服务器以获取资源时要使用哪种协议，比如https://，这就是scheme，ftp://也是schema，不同的scheme其实就是不同的协议，自然后面的port也就不同。 user:用户，某些方案访问资源时需要的用户名 # 基本不用 password:密码，用户对应的密码，中间用：分隔 # 基本不用 Host:主机，资源宿主服务器的主机名或IP地址 port:端口,资源宿主服务器正在监听的端口号，很多方案有默认端口号 path:路径,服务器资源的本地名，由一个/将其与前面的URL组件分隔 params:参数，指定输入的参数，参数为名/值对，多个参数，用;分隔 # 名/值对，就是在说 键值对的形式 query:查询，传递参数给程序，如数据库，用？分隔,多个查询用&分隔 # 这个比较常见，👇cat是类型的简写。 上图👆?cat=670,671,672实际上就是mysql的select * from goods where cat='670,671,672'; frag:片段,一小片或一部分资源的名字，此组件在客户端使用，用#分隔 这个是跳转，比如业内跳转到下面的内容。 ​ 当点击M的时候👇，就等价于在URL后面补一个#m，这就是frag片段，可用来直接跳转到下面的特定行。 &就是select里的and，翻译成DQL就是select * from goods where cat='670,671,672' and ev='exbrand_华为(HUAWEI)^' and cid3='672'; 这里还涉及%2C就是,逗号 然后%5E好像是^脱字符， 网站访问量 IP(独立IP)：即Internet Protocol,指独立IP数。一天内来自相同客户机IP地址只 计算一次，记录远程客户机IP地址的计算机访问网站的次数，是衡量网站流量 的重要指标； ​ 需要注意的是，IP不代表一个用户，一个公司可能出口就是一个IP。 PV(访问量)： 即Page View, 页面浏览量或点击量，用户每次刷新即被计算一 次，PV反映的是浏览某网站的页面数，PV与来访者的数量成正比，PV并不是页 面的来访者数量，而是网站被访问的页面数量; ​ PV,是页面刷新一次就算一次， 叫页面点击量，我觉的前提是你的业务不涉及自动刷新功能吧。 UV(独立访客)：即Unique Visitor,访问网站的一台电脑为一个访客。一天内相 同的客户端只被计算一次。可以理解成访问某网站的电脑的数量。网站判断来 访电脑的身份是通过来访电脑的cookies实现的。如果更换了IP后但不清除 cookies，再访问相同网站，该网站的统计中UV数是不变的 ​ 这里涉及cookies标识用户身份，按上文描述，IP就算换了，缓存还在cookies还在，用户身份不变的，这其实好理解，类似缓存一样，身份老的cookies一直在那呢，所以server还是认为同一个用户。 网站统计：http://www.alexa.cn/rank/ 网站统计实例 网站访问量 web服务请求处理步骤 一次完整 的http请求处理过程 1、建立连接：接收或拒绝连接请求 ​ 三次握手后，发http请求过去，对方收到后去判断是否为合法用户，来判断是否接收和拒绝。 2、接收请求：接收客户端请求报文中对某资源的一次请求的过程 ​ 接收请求会有多种响应模型，统称为web I/O响应模型👇： ​ Web访问响应模型（Web I/O） ​ 单进程I/O模型：启动一个进程处理用户请求，而且一次只处理一个，多个 请求被串行响应 ​ 多进程I/O模型：并行启动多个进程,每个进程响应一个连接请求 ​ 复用I/O结构：启动一个进程，同时响应N个连接请求 ​ 实现方法：多线程模型和事件驱动 ​ 多线程模型：一个进程生成N个线程，每线程响应一个连接请求 ​ 事件驱动：一个进程处理N个请求 ​ 复用的多进程I/O模型：启动M个进程，每个进程响应N个连接请求，同时接 收M*N个请求 就是讲了一个服务器的进程-线程，处理单-多用户请求的情况。 前面提到的tcp连接里多个请求并发，和这里的关系就是tcp连接就是用户连接请求。 web访问响应模型 上图不好说对不对，至少apache不是(b)，apache如果用上图解释，需要将(b)改成多进程I/O结构，然后apache是使用(a)/(b)/(c)三种的，默认是(b)多进程就对了。 (c) 就是单进程 通过\"连接复用器\"来对接多个连接请求，思路如下👇： 不知道这个连接复用器是否是多线程还是指的是事件驱动呢？好像是的，连接服用可能也是事件驱动的一种应用场景，什么应用场景--就是网络连接的事件驱动下产生的连接服用器这么一个效果。俺是这样理解了~ ​ ①这里涉及一个情况：就是一堆用户的请求连接过来，server需要从DB里调用数据资源取到后才能给到用户响应response，所以不是每个用户的请求都能够立即响应的，也不需要立刻响应的，就算该进程就为一个用户服务，也会存在 调用资源没法立即响应的情况。所以这有了连接复用器的 发挥空间。 ​ 同样再梳理一遍：连接复用器 也叫 连接池 ，这个pool里就接收用户发起的请求，比如来个7个用户(7剑夏天山)，但是立即需要处理的请求不是所有的用户，用户的响应，需要server去磁盘上或者db里找到对应的文件数据资源。这些资源在server找到之前，是不需要和回应用户的，此时只需要和用户保持连接(长连接)就行，你看这不就是event MPM的监听线程干的活嘛。 ​ 比如针对某个用户的网页请求，server已经把对应的网页文件已经从磁盘上调度/加载到当前的进程里了，此时就可以回应他了。 ​ 所以server回应用户，涉及后端的 数据查找-网页文件合并？-加载到进程？ 大概这些步骤吧。 结合下图👇理解一下，accept函数调用后，就接收client的connect，开启一个新的socket就是一个连接，通过这个socket连接来响应用户请求，这一个连接就是上面讨论的用户请求，一个socket通常不会独享一个进程，都是多个socket通过连接复用器来共享一个进程的。 在后面讲到nginx的时候，会展开说，一个进程并发给多个用户响应，背后有一些复杂的I/O模型，涉及 阻塞、非阻塞、复用多路复用。 (d) 其实就是(c)的进一步，比如nginx，会开一个主进程，然后有几个颗CPU，就开几个子进程，然后每个子进程来讲，就是一个(c)结构。每个子进程再开多个线程*然后为多个用户提供响应服务。 *再开多个线程*：*不过图中提到的都是线程，而不是进程，这一点也有不同，我猜可能是这样，niginx为例，①主进程一个②几个CPU开启几个子进程③子进程再开启多个线程，此时细化到线程才对应(c)的单线程I/0 连接复用器结构。参考https://cloud.tencent.com/developer/article/1931083* apache是(b)结构，多线程I/0结构；一个用户请求过来就开启一个线程为其响应，所以资源消耗主要是内存消耗比较大。apache并发所以上不去，虽说上不去，但传统行业基本也够用(用户量没有互联网公司的业务大一般情况)，互联网不行nginx才行。 3、处理请求：服务器对请求报文进行解析，并获取请求的资源及请求方法等相 关信息，根据方法，资源，首部和可选的主体部分对请求进行处理 元数据：请求报文首部 HEADERS 格式 name:value 示例： Host: www.magedu.com 请求的主机名称 Server: Apache/2.4.7 HTTP常用请求方式(方法)，Method GET、POST、HEAD、PUT、DELETE、TRACE、OPTIONS 估计是这里一堆东西，不过好像一些关键字变了👇： 比如，GET 方法，然后结合URL里指明的页面，此时server就会去找这个URL所指的资源，于是进入下一步4-访问资源。 4、访问资源 服务器获取请求报文中请求的资源web服务器，即存放了web资源的服务器，负责向请求者提供对方请求的静态资源，或动态运行后生成的资源 资源放置于本地文件系统特定的路径：DocRoot # apache的默认路径找页面资源咯 DocRoot ---> /var/www/html # 默认的页面根路径。 /var/www/html/images/logo.jpg http://www.xxxx.com/images/logo.jpg # images/log.jpg就是去httpd的根路径下找。 web服务器资源路径映射方式： ①docroot ②alias ③虚拟主机docroot ④用户家目录docroot 找到磁盘上的网页文件后，要构建 \"响应报文\" 于是进入下一步 5-构建响应报文。 apache的documentation入口官网竟然没了，不过可以直接进去 https://httpd.apache.org/docs/ 5、构建响应报文： 一旦Web服务器识别出了资源，就执行请求方法中描述的动作，并返回响应 报文。响应报文中 包含有响应状态码、响应首部，如果生成了响应主体的话，还包 括响应主体 1、响应实体：如果事务处理产生了响应主体，就将内容放在响应报文中回送过 去。响应报文中通常包括： ​ 描述了响应主体MIME类型的Content-Type首部 ​ 描述了响应主体长度的Content-Length ​ 实际报文的主体内容 2、URL重定向：web服务构建的响应并非客户端请求的资源，而是资源另外一 个访问路径 永久重定向301：http://www.360buy.com 临时重定向302/307：http://www.taobao.com 不过这个也变成301了，需要重新找一个 https://blog.csdn.net/idwtwt/article/details/90692773 👈这个有时间再研究吧，不研究也行，知道有这个回事就行。 3、MIME类型： Web服务器要负责确定响应主体的MIME类型。多种配置服务器的方法可将MIME类型与资源管理起来： ​ 魔法分类： Apache web服务器可以扫描每个资源的内容，并将其与一个已知模 式表(被称为魔法文件)进行匹配，以决定每个文件的MIME类型。这样做可能比较 慢，但很方便，尤其是文件没有标准扩展名时 ​ 显式分类： 可以对Web服务器进行配置，使其不考虑文件的扩展名或内容，强 制特定文件或目录内容拥有某个MIME类型 ​ 类型协商： 有些Web服务器经过配置，可以以多种文档格式来存储资源。在这 种情况下，可以配置Web服务器，使其可以通过与用户的协商来决定使用哪种格 式(及相关的MIME类型)\"最好\" ​ 看不懂，没关系，GPT来帮你👇其实就是上文的表达方式不太通俗易懂。 “当我们访问一个网页或下载一个文件时” 有个好的开头就能让你 立刻理解整段中心思想 6、发送响应报文 Web服务器通过连接发送数据时也会面临与接收数据一样的问题。服务器可能有很多条到各个客户端的连接，有些是空闲的，有些在向服务器发送数据，还有一些在向客户端回送响应数据。服务器要记录连接的状态，还要特别注意对持久连接的处理。 ​ 对非持久连接而言，服务器应该在发送了整条报文之后，关闭自己这 一端的连接。 ​ 对持久连接来说，连接可能仍保持打开状态，在这种情况下，服务器 要正确地计算Content-Length首部，不然客户端就无法知道响应什么时候结束了 7、记录日志 最后，当事务结束时，Web服务器会在日志文件中添加一个条目，来描述已执行的事务。针对这些日志分析，有可能帮助你进一步了解用户画像，从而更好的规划网站内容，啥不理解？打个比方，如果网站内容都是卖货的，就分析日志多多进好卖的商品呗。 http的7个处理过程弄完，下面看看I/O方面的情况，用户的访问其实涉及很多I/O：磁盘I/O、网络I/O、内核和进程之间的交换。 图中👆是涉及浏览器、服务器和硬盘、网口、内核、socket buffer这是网络缓冲区就在内核中、http应用程序的各方I/O和数据交换。 1、用户的浏览器打开一个网页的请求 2、请求的数据包发送到了服务器的接口，就放入接口缓冲 3、然后 内核就从接口缓冲区里拿到数据，放到内核的网络缓冲区(socket buffer)里。 4、然后进一将请求数据交给应用程序httpd这个服务。 5、httpd软件查看请求，一看是个GET方法用户要得到index.html文件 6、由于用户的进程是无法直接访问磁盘的，所以httpd软件发送一个指令给内核(要知道软件本身也不能直接访问磁盘，都是通过内核去间接打交道的) 7、内核收到指令后内核通过DMA--直接内存和磁盘的交互，这里参见 内核就通过DMA直接把磁盘的index.html文件读入到内存中的内核缓冲区？然后再把index.html文件复制到http进程的内存空间里。 ​ 得到这个文件后，还得在外层封装响应头部，再发送给内核的buffer缓冲区--socketbuffer。 ​ 再发给网卡缓冲区，再发给用户的浏览器， ​ 再记录日志。 HTTP服务器应用 http服务器程序 httpd 应用程序服务器 ​ IIS .asp ​ tomcat .jsp ​ jetty 开源的servlet容器，基于Java的web容器 ​ Resin CAUCHO公司，支持servlets和jsp的引擎 ​ webshpere(IBM), weblogic(BEA), jboss,oc4j(Oracle) 市场占有率统计 www.netcraft.com https://www.netcraft.com/resources/?type=blog # 以前在news现在放到blog路径里了。 https://www.netcraft.com/blog/august-2023-web-server-survey/ nginx不仅仅可以做web服务器，还可以做强大的反向代理。 apache只能作为web服务器。 HTTPD介绍 httpd ​ 20世纪90年代初，国家超级计算机应用中心NCSA开发 ​ 1995年开源社区发布apache（a patchy server） # 补丁服务器，我国也有一个补丁墙-三个佛，哈哈~ 都很牛逼~ ​ ASF: apache software foundation ​ FSF：Free Software Foundation 特性 ​ 高度模块化：core + modules # 核心+模块，都是这样的，包括linux(内核+各种模块) ​ DSO：Dynamic Shared Object 动态加/卸载 # 这些模块 安装/卸载 比较灵活，灵活个屁，据说是编译的时候加进去的。 ​ MPM：multi-processing module多路处理模块 # 这里排版空间不够写道下面去👇下文内容 APACHE不仅仅是HTTPD咯，点开See All Projects可见N多ASF旗下的开源软件。 往下翻，可见tomcat也是他家的，一切都是从apache开始的。 kafka也是，牛逼 hadoop、HBase 也是~ 这些都是大数据里的东西 大数据运维这个岗位，和正常linux应用运维不一样，大数据有开发、运维两个方向。 大数据运维可能就涉及HBase，hadoop的搭建？都是一些专业大数据分析相关的软件维护了吧。 ​ 上接前文跳到前文，MPM：multi-processing module多路处理模块 # 这个和前面的web访问响应模型很像啊跳转前文 MPM工作模式 prefork：多进程I/O模型，每个进程响应一个请求，默认模型 ​ 一个主进程：生成和回收n个子进程，创建套接字，不响应请求 ​ 多个子进程：工作work进程，每个子进程处理一个请求；系统初始时，预先生成多个空闲进程，等待请求，最大不超过1024个 worker：复用的多进程I/O模型,多进程多线程，IIS使用此模型 ​ 一个主进程：生成m个子进程，每个子进程负责生个n个线程，每个线程响应一个请求，并发响应请求：m*n event：事件驱动模型（worker模型的变种） 这个图中没有唉。 一个主进程：生成m个子进程，每个子进程负责生个n个线程，每个线程响应一个请求，并发响应请求：m*n，有专门的监控线程来管理这些keep-alive类型的线程，当有真实请求时， 将请求传递给服务线程，执行完毕后，又允许释放。这样增强了高并发场景下的 请求处理能力 ​ httpd-2.2：event 测试版，centos6默认 ​ httpd-2.4：event 稳定版，centos7默认，现在马上都用rocky-linux9这一类的linux，httpd早就默认yum下来就是2.4了。 https://cloud.tencent.com/developer/article/1931083 https://httpd.apache.org/docs/2.4/zh-cn/mpm.html # 看原版算了 进一步看图 prefork MPM，pre fork就是预先开好fork子进程得意思 注意，这个模式下，一个子进程里只会开一个线程，所以一般就说一个进程服务一个用户请求了。 Prefork MPM: 预派生模式，有一个主控制进程，然后生成多个子进程，使用 select模型，最大并发1024，每个子进程有一个独立的线程响应用户请求，相 对比较占用内存，但是比较稳定，可以设置最大和最小进程数，是最古老的一 种模式，也是最稳定的模式，适用于访问量不是很大的场景。 优点：稳定 缺点：慢，占用资源，不适用于高并发场景 worker MPM worker MPM：是一种多进程和多线程混合的模型，有一个控制进程，启动多 个子进程，每个子进程里面包含固定的线程，使用线程程来处理请求，当线程 不够使用的时候会再启动一个新的子进程，然后在进程里面再启动线程处理请求，由于其使用了线程处理请求，因此可以承受更高的并发。 优点：相比prefork 占用的内存较少，可以同时处理更多的请求 缺点：使用keep-alive的长连接方式，某个线程会一直被占据，即使没有传输 数据，也需要一直等待到超时才会被释放。如果过多的线程，被这样占据，也 会导致在高并发场景下的无服务线程可用。（该问题在prefork模式下，同样会 发生） event MPM，在worker PMP基础上多了回收线程机制，我估计就是前文提到的\"事件驱动\" 不过上图PPT是将事件驱动放到 复用I/O结构里，实际上复用多进程I/O结构里也会有事件驱动，其实apache的event MPM就是在复用的多进程I/O模型基础上又加了事件驱动模型， 我认为 所谓的 连接复用器，其实就是 事件驱动模型。 event MPM：Apache中最新的模式，属于事件驱动模型(epoll)，每个进程响应多个 请求，在现在版本里的已经是稳定可用的模式。它和worker模式很像，最大的区别在 于，它解决了keep-alive场景下，长期被占用的线程的资源浪费问题（某些线程因为被 keep-alive，空挂在哪里等待，中间几乎没有请求过来，甚至等到超时）。event MPM中，会有一个专门的线程来管理这些keep-alive类型的线程，当有真实请求过来 的时候，将请求传递给服务线程，执行完毕后，又允许它释放。这样增强了高并发场景 下的请求处理能力 event只在有数据发送的时候才开始建立连接，连接请求才会触发工作线程，即使用了 TCP的一个选项，叫做延迟接受连接TCP_DEFER_ACCEPT，加了这个选项后，若客户 端只进行TCP连接，不发送请求，则不会触发Accept操作，也就不会触发工作线程去 干活，进行了简单的防攻击（TCP连接） 优点：单线程响应多请求，占据更少的内存，高并发下表现更优秀，会有一个专门的线 程来管理keep-alive类型的线程，当有真实请求过来的时候，将请求传递给服务线程， 执行完毕后，又允许它释放 缺点：没有线程安全控制 上图👆没有提到，前文说的，client请求过来，server从后台捞出数据/文件之前的一个不响应处理，只是维持连接，我觉得可以把这个动作逻辑加到上图的 请求1过来，交给工作线程A处理，但是捞出数据/文件之前，都不会响应，此时这个工作线程A可能去干别的事也是有可能的，期间该线程A向内核发起数据查询后，就将连接交给监听线程，然后等内核通过DMA拿到数据/文件后放入内核缓冲，此时线程A才会重新从监听线程接管之前的连接，将HTTP响应回给用户。event也好，连接复用器也罢，其实正式有了监听线程，才有了单线程多路复用的可能，所以这个监听线程和工作线程之间的协同合作就是连接复用器，就是event MPM机制 ​ 补充一下：线程的一个释放和阻塞问题，涉及①工作线程处理完请求后就会释放线程②处理着请求呢，等待DMA将数据调入缓冲呢此时 是否不是阻塞，是否也存在该线程干别的工作去了，这个调度是否也是内核还是event MPM机制调度的呢？然后③就是空请求不会分配线程；以上就是的事件调度机制event可以做到高并发。 本质上还是 时分复用+用完立马释放+和空请求不分配 这么一套优化动作。 ​ 其实你要说学技术有什么用，学这么细有什么用，反正配置文件一个单词就搞定了。你要知道，上面这套东西是人家apache自身的处理逻辑，当你不用apache的时候就没有这套逻辑了，你自己开发的脚本、软件，如果面对这些场景的时候，此时就有处理思路，这就是学习的本质！对吧，无非是内核函数的调用，线程的分配，连接的维持，这些拆分开来，确实可以想象代码模块可能没有我们想象的那么难，特别是写C的那帮吊人，吊人在这里是羡慕嫉妒恨的意思，想当初C就上课听考试还能70，现在啥都不会了。 其实，别人问你apache的三个MPM多路处理机制，你可以两句话搞定他咯 ①http2.2之前默认是预fork，开启子进程，单一线程，资源占用大，内存消耗高； ②http2.4之后默认是事件模型，子进程里有监听线程和工作线程，监听 负责分配任务给工作线程和绑工作线程维护与客户的连接；工作线程就拼命干活。 ③此时你可以反问，工作线程如果从监听线程接手了一个请求，该请求需要从本地捞取的数据文件量大，此时I/O处理时间长，你反问：工作线程是阻塞态还是干别的活，如果是干别的活去了，这个调度机制是系统内核本身的行为还是apache event事件机制实现的。 所以说apache高并发不行，顶多C10K( connect 10 000，1w的连接)，其实可能是停留在老的版本也就是prefork这个机制上的，我严重这样认为，所以高并发apache也能支持的。 只不过niginx的牛B之处，估计在于 人家支持复杂的反向代理，L4的也OK，所以这就集成了F5和WEB SERVER的功能，你说谁还用apache呢，所以不要用并发去踩人家apache，而要准确的认知。 至于你说结果对了就行，那就没有推导能力咯，用错误的论据得到正确的结论，下次呢？ 然后这里提一个点应用上的：为什么传统公司用apache的prefork，因为event MPM存在一个问题就是，如果一个子进程里的某个工作线程 DOWN了，那么可能会导致整个子进程都DOWN了--影响同一个子进程下的其他线程。 那个，我怎么感觉不一定呢，对吧，哦某个工作线程吊死占用大量资源？也许吧。 所以大家的共识是：追求稳定性用prefork，呵呵，整个共识也许是不对的，早些年的共识吧，这里表示怀疑。 然后APACHE MPM其实最终还是要读官方的 https://httpd.apache.org/docs/2.4/zh-cn/mpm.html 举个例子 默认机制其实很多的👆，不是PPT上仅仅的三种就完事了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"29-HTTP协议和APACHE/4-httpd2.4常见配置1.html":{"url":"29-HTTP协议和APACHE/4-httpd2.4常见配置1.html","title":"第4节 httpd2.4常见配置1","keywords":"","body":"第4节. httpd2 apache的文件构成了解 rockylinux9上apache直接yum安装的版本 安装httpd软件时的依赖 其中重要文件，比如conf文件都在httpd-core-xxx这个软件包里。 /etc/httpd 就是apache软件的工作总目录 服务service文件就不在httpd-core-xxx这个包里了，而是在httpd.x86_64这个里 关于modules ①/etc/httpd/modules 模块目录 conf.modules.d这个一看就是conf配置文件的模板不是这里要梳理的/etc/httpd/modules这个模块 还有/usr/lib64/httpd/modules/这个 模块 其实是一样的 /etc/httpd/modules时/usr/lib64/httpd/modules的软连接。 /etc/httpd是作为apache的总目录，后面一些配置上，都以这个目录为相对路径的。配置上直接写的就是相对路径，而相对的就是这个目录。 mod_auth_basic.so模块的作用：用来验证用户名密码的一个基本功能；一般来讲用户登入POST提交用户密码，对比DB里的记录，一致则验证通过，而这一套东西涉及软件开发的过程(...这就涉及软件开发了？不就是POST对比DB库嘛，先估且认同这个观点吧，估计是涉及DB和POST处理吧，正常django好像挺简单的处理这块)。而mod_auth_basic.so这个模块就可以对某一个网站做基本的用户名密码认证。 apachectl 启用关闭的二进制程序，不过一般用systemctl start httpd。 /usr/sbin/httpd是apache的主进程，systemctl start httpd启动的就是这个。 然后apachectl 这个cli也看下效果👇 然后/var/www这个耳熟能详的目录 是在httpd-filesystem-2.4xxxx这个包里了， 刚安装好的httpd的配置检测看看 Syntax OK，语法OK，然后提示消息说的是FQDN的事情，没啥后面会配置。 跟pptp，l2tp的服务一样，vpn服务跑起来针对针对某个用户踢下线，就可以找到用户拨入接口的进程PID； 我的意思就是针对进程的ID，通常服务软件们会自己动态生成xxx.pid文件来保存的，不然ps aux 去看可能有很多不好区分--就比如上图的ppp0、ppp1如果这些很多，ps aux |grep ppp 也不知道哪个接口时哪个进程的。 httpd的配置 [root@node1 ~]# cat /etc/httpd/conf/httpd.conf |grep -Ev '^ *#' ServerRoot \"/etc/httpd\" # 这就是根，所有的配置目录的相对路径都是相对这个目录的 Listen 80 Include conf.modules.d/*.conf # 包含的辅助配置文件，这里就是相对路径了，/etc/httpd/conf.modules.d/了。 User apache Group apache ServerAdmin root@localhost AllowOverride none Require all denied DocumentRoot \"/var/www/html\" AllowOverride None Require all granted Options Indexes FollowSymLinks AllowOverride None Require all granted DirectoryIndex index.html Require all denied ErrorLog \"logs/error_log\" LogLevel warn LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b\" common LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %I %O\" combinedio CustomLog \"logs/access_log\" combined ScriptAlias /cgi-bin/ \"/var/www/cgi-bin/\" AllowOverride None Options None Require all granted TypesConfig /etc/mime.types AddType application/x-compress .Z AddType application/x-gzip .gz .tgz AddType text/html .shtml AddOutputFilter INCLUDES .shtml AddDefaultCharset UTF-8 MIMEMagicFile conf/magic EnableSendfile on IncludeOptional conf.d/*.conf # 由于当前文件也就是/etc/httpd/conf/httpd.conf里的东西太多，所以建议后面的配置都放到conf.d/下面。 不管怎么说先备份这个主配置文件 HTTPD常见配置 httpd配置文件的组成： 主要组成 ​ Global Environment ​ Main server configuration virtual host 配置格式：directive value # 指令 值得格式，类似字典得键值对。 Directive 不区分字符大小写 Value 为路径时，是否区分大小写，取决于文件系统 官方帮助 ​ http://httpd.apache.org/docs/2.4/ 配置的格式都是类似key value的方式 listen的写法 IP-address 不写就是绑定所有IP。 IP地址本地不存在，httpd都不会让你起来 修改监听端口需要重启服务的，reload不行👇 不是所有的配置都能reload，进而无需重启服务， 将来就可以做两个网站，一个80，一个8080。可以使用虚拟主机技术。是VPS嘛？感觉不是.. ServerTokens 👇下图可见server字段就暴露了apache的版本和操作系统。 使用curl看头部信息 ①好像-i和-I一样，忽略大小写了；其实不对的，-I 是大写，看下图案例 ②就是curl -i可以查看head头部信息。其中就有server字段显示你的apache版本和操作系统。 我把/var/www/html/index.html 拿掉--就该改成xxx 然后呢就有两个现象，① -i和-I不同 ②403报错 然后403页面报错，是页面不存的一个报错，了解下👇，当然也可能是页面文件没有访问权限。这里就是index.html没有创建导致的报错403。 还有一个点，就是如果你修改了index.html的默认位置--/var/www/html/index.html，那么修改后的路径也要让apache这个id有至少读的权限吧。 修改index.html文件无需重启服务，刷新页面就行了 回到serverToken的内容，ServerToken就是用来掩盖真实的server字段信息的。 根据参数说明，可知httpd默认用的full，就是把信息显示全了，然后透露信息最少的是Prod[uctOnly]，方括号是可以省略的意思，linux通用写法。 其实prod暴露的信息也没必要，最好啥都不暴露，这就不是httpd自带的功能了--或者源码修改这些信息(字符串Apache搜一搜改一改就行了)然后编译安装；如果不改源码，就需要前端通过调度器来过滤掉。 修改配置文件推荐不要动/etc/httpd/conf/httpd.conf这个主配置文件 恢复一下之前的listen的修改 /etc/httpd/conf.d/下新建test.conf，编辑所需指令和值 再补一个servertokens 需要重启服务的 👆这张图就看清楚了-i和-I的区别，-i就是include包含头部和页面内容。-I就是仅仅头部。 然后servertokens可见，已经生效了，只有Apache这个简略信息了。 持久连接 Persistent Connection：连接建立，每个资源获取完成后不会断开连 接，而是继续等待其它的请求完成，默认关闭持久连接 断开条件：时间限制：以秒为单位， 默认5s，httpd-2.4 支持毫秒级 副作用：对并发访问量大的服务器，持久连接会使有些请求得不到响应 折衷：使用较短的持久连接时间 设置：KeepAlive On|Off ​ KeepAliveTimeout 15 测试：telnet WEB_SERVER_IP PORT ​ GET /URL HTTP/1.1 ​ Host: WEB_SERVER_IP 前面的章节讲过http1.1版本默认就支持persistent connection的👇 官方说是默认开启的，当然2.4版本的http的，自然也是http1.1协议。 如果知道是否开启了\"持久连接\"--通过telnet 80然后GET方式进行测试持久连接 ①持久连接就是一个tcp连接里，可以处理多个用户请求 ②测试方法： 上图👆就是用telnet开启了80的端口TCP连接--算是一个长连接，然后这里有keepalive timeout的超时(默认是seconds秒为单位，也支持ms毫秒)。 这是👆KeepAliveTimeout Directive的参数解释，这里要注意默认是5秒钟，然后5秒钟的超时表现在什么点上呢，就是你输入host 3.3.3.3 回车发送了个GETrequest出去后，开始计时的，也就是wait for a subsequtent request befor closing the conn。怎一个subsequent了得~随后的，所以两个请求之间超时时间就是keepAliveTimeout。 除了这个keepalivetimeout，还有一个参数设置是 持久连接里 资源下载次数(get其实就是下载页面了) 到了也可以断开。通过poe.com问一下就找到了 测试下👇 👆上图可不是keepalivetimeout 30秒超时哦，没到呢；这是maxkeepaliverequests 2，2次下载超了的限制。 然后补充说明一下，测试方法👇 其实就是类似curl以及浏览器的行为，通过抓包可见 curl一样，就不抓了，然后host其实是可以随便写的。 DSO：Dynamic Shared Object 动态的共享对象，也就是说http的模块是支持动态加载的。 上图其实还有很多，modules，通过ls 去看更方便 这些模块是否都加载了，可以通过👇httpd -M来查看，可见105个模块，加载了93个。 👇这张图就说明了，确实是ls /etc/httpd/modules的模块mod_auth_basic.so加载后通过httpd -M可见的👇 关于模块加载也是由配置的，同样在主配置文件/etc/httpd/conf/httpd.conf里可找到👇 👆说明要进到/etc/httpd/conf.modules.d/下看 👆这样就就看到了LoadModule加载哪些具体的模块了，这就是105个模块，加载了93个的原因。 注释后，再次看看加载模块的数量 果然少了1个 记得恢复该模块的加载，后续要用 所以这也是加载和卸载模块的方法👆 总结如下： DSO： Dynamic Shared Object ​ 加载动态模块配置，不需重启即生效 ​ /etc/httpd/conf/httpd.conf ​ Include conf.modules.d/*.conf ​ 配置指定实现模块加载格式： ​ LoadModule 模块文件路径可使用相对路径：相对于ServerRoot（默认/etc/httpd） ​ 示例：LoadModule auth_basic_module modules/mod_auth_basic.so 动态模块路径： /usr/lib64/httpd/modules/ 查看静态编译的模块 httpd -l # 看的是静态编译的模块区别share的，是绑定的模块。 查看静态编译及动态装载的模块 httpd –M # 看的是全部的包含：share(共享)模块和static(静态其实可以叫独享)模块 然后static模块是无法通过之前上文讲的 配置文件去 卸载，是httpd服务本身自带的默认就有的。 真要想\"卸载\"也可以---就是源码里去掉，重新编译。不过估计卸了也就没法用了，这个自带的估计是core核心的东西。httpd软件组成本身就是核心core+模块modules。 MPM(Multi-Processing Module)多路处理模块 前文讲过prefork，worker，event，这里讲如何进行配置 以前：要进行该处理方式的切换，需要重新编译，因为是绑定在core代码模块里面的。 现在：但是从centos7开始(奇了怪了，难道不是从httpd的某个版本开始嘛，怎么还是从centos版本开始呢)，也支持配置文件的方式来切换了。 默认处理方式是：以前好像是prefork--就是一个主进程-多个子进程-每个子进程里只开一个线程；现在不是了，现在好像是event(可以去/etc/httpd/conf.modules.d/00-mpm.conf可见就是event)，有3680独立的子进程，还有几个了很多线程的子进程。 不过通过ps auxf看到的只显示到子进程。线程就不显示了👇 ps -ef一样 然后进程数不是限死的，如果进程数不够，依然会增加的。不过这里涉及线程，不知道具体的增加细节了，可能先增加线程吧。 没有讲是Unix到底是默认哪个？ 看配置文件，果然是event👇 修改方法，官网查看路径 妈的，哈哈，查个屁，/etc/httpd/conf.modules.d/00-mpm.conf里写的好好的，你上面截图的时候看不到，靠哦，什么眼神。 这里注释原来的，取消所需的就行了 重启后，再pstree -p 应该就没有花括号--也就是没有线程了，结果事与愿违 👆还是有一堆线程啊，一个进程里开了好多线程，奇怪了。不过更像worker。 再切成worker 发现worker开启的线程要多了去了，和event一样多。 所以，我觉的，新版本的prefork也不是原来的prefork了，不是一个进程里就开一个线程了，确实可见开了但开的不多，相比而言 线程开启的数量远远小于event或者worker。 通过pstree -p 是无法区分worker和event的，event是每一个子进程里的多个线程中有一个是监听线程，这是里面程序调度的事情了，无法在pstree中查看的。 不过我好奇的是为什么现在的prefork不是单纯的一个子进程里仅仅一个线程了。 测试mpm的event和prefork的并发能力worker同理 记得修改页面文件的访问权限 测试CLI -C1000就是1000个并发，同时去server上看看并发量；-n 是发动的request个数。 ps aux看到的是进程涨了很多，原来就是几个，现在是258个 pstree看到的都是线程数，1296个了已经。 等了一小会👆结果被 server端reset了，说明请求量太大了吧，减少请求量继续测试 在👆上图ab测试的过程中，多次在server端查看并发情况👇 PS：上图👆提到的此时 restart 会等蛮久，我觉的不一定是ab的并发回收慢造成的，本身httpd服务重启就慢，有时候快，过半的时候感觉都是慢的，从前面不断的重启动作得出的经验这是。 好，第一个prefork mpm的并发结果有了，主要就是看Request per second，每秒处理了3个请求。这是平均测试结果。 再修改为event mpm来测试下 多次查看进程数和线程数没有变化，说明该释放的都释放了，而且6个进程和209个线程基本上就是event的初始开启的量了。 然后ab 命令走一波 然而这里仅仅看requests per second发现event反而只有3.08，要小于perfork的3.13哈哈，怎么可能，不过综合开进程和线程的开启数量，event是要远远小于perfork的。从这个角度来看就是event要更加节省资源的。换句话说如果相同资源的消耗下event的并发处理就会远远高于prefork了。 另外为啥event模式系统服务没有多开写进程和线程，让处理量上一个台阶，这就不知道了，可能服务判断处理的速度还行无需增加资源开销。 然后上图👆测试期间，并发没啥大变化，结束了5分钟左右，也不见回收1个线程 搞不懂。。。哈哈，继续往下看吧，这里先这样，并发测试 偶然看到这篇 https://lvwenhan.com/tech-epic/500.html 需要自行研究下的感觉👆 然后，这个URL只是该作者的一篇，全部的高并发在这里https://github.com/johnlui/PPHC，挺牛逼。不是我现在该去看的，不过像innodb、分布式db、K8S，这些确实是我的方向，所以这个blog我感觉确实是大佬级的。 并发数超了-client的socket打开数 说是收到ulimit -a里的打开文件数限制 连接数，最终就是打开的socket文件，linux一切皆文件。 调大也没有用，可能是不止调这一个地方。 哦，操~~~！ 是CLI敲错了，并发-c 1010 大于总数 -n 1000了，这算乌龙吧~ 搞错了~再来一遍👇 调大socket 文件上限，确实就可以了 并发超了-server端的进程数 在server端改为perfork进行测试 大页面文件的并发，结果就是被server reset了，超出了server的承受能力--就是进程数超了，大页面文件下载的慢，进程占用时间长，回收的慢，不够用了。 改成小文件就可以了，所以下面研究下server的处理上限问题，小文件测试的时候，server的进程数是够用的，因为回收的快，小页面打开(下载)处理的快，进程回收的快，所以够用 prefork 的 配 置 StartServers # 这是一开始的ps auxf |grep httpd 默认开启的进程数 perfork默认是开5个进程数 然后线程开的也不多 StartServers 8 # 初始开启8个进程 MinSpareServers 5 # 最少保留5个空闲进程 MaxSpareServers 20 # 最多保留20个空闲进程 连起来的意思就是，8个初始进程随着用户请求变多，会开启的越来越多； 但是至少保留5个空闲的进程不分配出去，比如从8涨到100进程，那么会开105个。 当用户请求下降了，那么比如100个用户请求大文件，100个进程开着，随着请求处理结束，这个100进程会回收80个，留20个空闲，因为最多20个。 我是这么理解的👆 ServerLimit 256最多进程数,最大20000 2000或者20W可能和MPM模型有关 MaxRequestsPerChild 4000 子进程最多能处理的请求数量。在处理 MaxRequestsPerChild 个请求之后,子进程将会被父进程终止，这时候子进 程占用的内存就会释放(为0时永远不释放） 然后修改一下配置 然后重启服务 然后看下对应的参数是否落实 修改MaxClients为2560发现确实调大了，而且该参数官方网站上是找不到的，可能是早期版本的参数在2.4里没有写，但是依然有效。 然后MaxClients 2560这个调大了以后，StartServers 2000这个初始的子进程数就对了2000个就OK了 以上是间隔3-5分钟多次键入的，👇间隔2分钟多次输入的命令，说明此时子进程数和线程数涨停👇。 这个参数是httpd 2.2版本里的参数。 由于此时进程数调大了，再次用ab测试一下 这是👇本篇 上文的 内容--之前ab测试的结果 这是👇调大参数后的测试结果👇，肯定就可以了因为server初始化了2000个进程，这里ab测试时1500个并发，而且2.4的prefork虽然2000个进程，但是同样也开启了线程1W+了都，肯定支持1500个tcp连接了。 同样此时内存消耗也大：server端👇 看下现在的测试结果👇，确实不再报错，但是内存要够的，好在我的内存还行给的。怎么判断给的内容够不够，简单，free -h 看看涨不涨不就行了，没变化就是够用了--上图就是内存：2.8G用了。 进程数也涨了写，线程同样涨了，说明2000个初始子进程StartServers还不够，因为m.txt太大了 半天10分钟过去了， 1500个并发就瞬间消耗1500个tcp，然后要访问2000次，m.txt又太大，下载耗时长tcp得不到释放，从第一个tcp建立到最后一tcp释放，历经了一共1500*2000=30 00 000 次的tcp连接。前面页面加载完m.txt就会得到释放吧--感觉会释放-不过释放的条件可能时timeout和上面设置的4000个请求数MaxRequestsPerChild。 总结 ServerLimit和MaxClients都是限制最大并发连接的参数，都需要改的。 worker MPM的参数 threads 一般表示线程，python里的import 也是这么表示的 MinSpareThreads 最小空闲线程 ThreadsPerChild 25 就是 每个子进程里开25个线程，直接看GPT的解释吧👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:52 "},"29-HTTP协议和APACHE/5-httpd2.4常见配置2.html":{"url":"29-HTTP协议和APACHE/5-httpd2.4常见配置2.html","title":"第5节 httpd2.4常见配置2","keywords":"","body":"第5节. httpd2 继续上一篇的配置 主站点页面-DocumentRoot 定义'Main' server的文档页面路径 DocumentRoot “/path” 文档路径映射： DocumentRoot指向的路径为URL路径的起始位置 示例： DocumentRoot \"/app/data“ http://HOST:PORT/test/index.html --> /app/data/test/index.html 注意：SELinux和iptables的状态 这里修改站点的页面存放路径，还需要保证该路径的文件的访问权限。注释掉👆上图的主站点配置。 下面对其进行修改👇，还是使用单独的配置文件，不对httpd.conf这个主文件进行修改 权限👆没问题 selinux没问题👇 修改配置文件， 必须是文件夹，且其下放index.html 重启OK 但是curl 发现不行 这就意味着页面一样是apache的默认页面了，因为index.html打不开 可是对应的文件夹的进入，和文件的可读权限都有的啊，奇怪了 但是就是403 Forbidden了 所谓授权不是linux系统文件的授权，而是httpd服务本身配置文件里要明确授权，所以还是配置文件里的配置不全。 同样写道单独的配置文件里去 ---------------这样就OK👆了------------------ 然后取消/data文件夹的所有权限，哈哈 其上图不用重启服务，因为修改的os操作系统的权限，而不是httpd服务的配置文件，👇证明下： 如果是DocumentRoot下的子目录的index就是补一个路径就好了👇。 如果子目录不在DocumentRoot下呢 1、用软连接行不行：👇可以的，没得问题 那为什么文件默认就是index.html，是否可以修改呢？可以，在配置里面找到关键词DirectoryIndex👇 定义站点主页面 DirectoryIndex index.html 增加一个 上图👆就说明了，配置文件的写法：DirectoryIndex m.txt index.html是优先级从左到右的。 如果m.txt和index.html文件都不存在，那么这个报错的页面403 Forbidden又是哪来的呢？ 在/etc/httpd/conf.d/下有一个叫welcome.conf的配置文件，其中就有403对应的页面👇 这里的/.noindex.html就是相对于DocumentRoot主页根路径来讲的。 但是没找到这个文件，增大俺的狗眼仔细看看上图 找找看，Alias啊，/.noindex.html全文里出现了2次，还不清楚嘛~(/≧▽≦)/ 删掉这个welcome.conf文件，观察页面 重启服务后👇 所以修改一下 页面一样效果👇 站点访问控制常见机制 可基于两种机制指明对哪些资源进行何种访问控制 ​ 两种机制：客户端来源地址，用户账号 ​ 哪些资源：文件系统路径、URL路径 文件系统路径： ①针对文件夹②针对文件③写正则匹配 ... ... ... URL路径： ... ... 示例： # FilesMatch就是REGEX正则 通配符 # 应该是Files 开头就是通配符 # Location就是URL # LocationMatch就是url里的regex正则 具体的写法-带上源 概述 中“基于源地址”实现访问控制 (1) Options：后跟1个或多个以空白字符分隔的选项列表 在选项前的+，- 表示增加或删除指定选项 常见选项： Indexes：指明的URL路径下不存在与定义的主页面资源相符的资源文件时，返回索引列表给用户 FollowSymLinks：允许访问符号链接文件所指向的源文件 None：全部禁用 All： 全部允许 示例： Options Indexes FollowSymLinks Options FollowSymLinks (2) AllowOverride 与访问控制相关的哪些指令可以放在指定目录下的.htaccess（由AccessFileName指定）文件中，覆盖之前的配置指令 只对语句有效 AllowOverride All： .htaccess中所有指令都有效 AllowOverride None： .htaccess 文件无效 AllowOverride AuthConfig .htaccess 文件中，除了AuthConfig 其它指 令都无法生效 Indexes：指明的URL路径下不存在与定义的主页面资源相符的资 源文件时，返回索引列表给用户 就是访问页面不存在，返回当前路径下的列表，页面显示出来的文件夹和文件都可以点击，类似yum源网站的点击浏览一样 然后页面访问是默认的报错页面 删掉该文件也没用！ 找找官网说明 这个模块也加载了啊 搞不懂，继续折腾，发现还是welcome.conf搞的鬼，这样就可以看到了，其实就是👇这里得options -Indexes。 - 减号就是禁止啊，禁用了目录索引功能。 然后dir1不现实，dir2可以的，研究下dir1为啥不显示 搞不懂dir1为啥不显示了，之前可以的啊 就是dir1好好的不行了，哈哈 👇结果发现是软连接得问题，但是之前可以得啊！ 权限OK👆，找到元婴了，👇就是开启indexes就不支持软连接了，奇怪了，先记着这个点 👆同时注意：curl 不同于浏览器http://192.168.126.130/dir2 , 浏览器这样回车自动就是dir2/ 补一个斜杠的/； curl 没斜杆/有问题的。 来一张总结图👇，结论就是 options indexes会导致ln -s 的403 Forbidden。 如果要把软连接也显示出来，需要再配置一下 FollowSymLinks：允许访问符号链接文件所指向的源文件 当然，welcome.conf里的-Indexes要去掉的 补上对应参数后就可以显示软连接了👇 .htaccess ①同样用来针对语句的参数，比如Options Indexes，不过 ②不过，需要在配置文件里开启功能AllowOverride ALL ③.htaccess是优先于配置文件的，不然也不会叫AllowOverride允许覆盖了。 ④.htaccess本身是不会被看到的，因为在主配置文件里有如下配置👇： 当前状态👇 去哪里创建.htaccess整个文件呢，去accessFileName，整个文件其实不是单一的路径，而是一个递归检索的路径。 所以就在documentRoot里创建就行了 上图👆漏了一个AllowOverride All，这就是针对.htaccess的生效开关。 然后就可以了 阶段总结一下 下面开始学习(3)基于IP的访问控制 基于IP的访问控制: 先做一个拒绝所有的conf文件的访问 这里files 就是通配符，filesMatch才是正则表达式了，嗯~ o(￣▽￣)o上面的通配符写错了哦，改下👇： 重启服务后，xxx.conf文件就看不到了， 将通配符改成正则 打开上图注释，就会实现拒绝访问 包含.conf的文件，当然regex要写好一点，优化下 这样就实现了regex的写法，参见上文 进一步实现针对192.168.126.1这个IP，不能访问beijing/index.html，其他都可以 mkdir beijing echo this is beijingbeijing > beijing/index.html ls beijing/ 配置acl后的效果： 其他都OK 由于只是针对192.168.126.1的禁止访问/beijing/index.html，所以其他IP可以访问 改为拒绝整个子网 然后研究下优先级和默认行为 先上结论： ①RequireAll 搭配 all granted 然后拒绝谁，这是黑名单写法 ②RequireAny搭配all denied然后放行谁，这是白名单写法。 ③宇宙法则之--从上到下匹配，被打破了，这里明明all granted还可以下面deny，明明all deny了还可以下面permit所以这里打破了从上往下的宇宙法则。 👆上图的划红线出 文字表达是摸棱两可的，不要按他的思路理解。 就理解成RequireAll是黑名单机制，RequireAny是白名单机制，就好了啊，还折腾啥呢，浅测一下👇 看来不能注释掉，哈哈 改成RequreAny就不行了 可以大概判断出来，就是固定写法当作就行了， any就是任意，任意就是任意一个OK就OK，所以是白名单； all就是所有OK，针对某个拒绝，所以就是黑名单了。 白名单写法👇 两个日志 可能会奇怪log不是应该通常放到/var/log/下吗，怎么在/etc/httpd/下呢，其实都是对的，这里人家用的软连接 这种拒绝访问也是记录到错误日志里的👇 关于FQDN的提示处理 首先，这不是个报错 同样也会在错误日志里存放，当然error_log叫错误日志不一定就单单存放错误信息。 优化处理下，很简单 没啥实际作用吧应该。 再看看访问日志access_log的格式 👆上图定义了LogFormat两种格式，分别命名为combined和common，然后access_log(CustomLog就是用户日志) 调用的格式是combined。 看看是不是这么个格式 官方解释👇https://httpd.apache.org/docs/2.4/mod/mod_log_config.html#logformat https://httpd.apache.org/docs/2.4/mod/mod_log_config.html#formats %h Remote hostname. Will log the IP address if HostnameLookups is set to Off, which is the default. If it logs the hostname for only a few hosts, you probably have access control directives mentioning them by name. See the Require host documentation. %l Remote logname (from identd, if supplied). This will return a dash unless mod_ident is present and IdentityCheck is set On. %u Remote user if the request was authenticated. May be bogus if return status (%s) is 401 (unauthorized). # 这是基于basic的验证，上一章节提到的mod_auth_basic.so干的活吧应该是。 %t Time the request was received, in the format [18/Sep/2011:19:18:28 -0400]. The last number indicates the timezone offset from GMT %r First line of request. # 👇对应下图的\\\"%r\\\"也就是log里的\"GET /beijing/ HTTP/1.1\" %s Status. For requests that have been internally redirected, this is the status of the original request. Use %>s for the final status. # 说的是%s，但是其实用的是%>s。 这里涉及重定向，比如先从page1，301到page2，然后再200；估计%s就是301了，%>s就是200啦，这个后面可以尝试一下。 这个200 403 301 这些数字本身就很大程度上反映除了 页面请求的结果，比如200是页面拿到了，403是禁止访问了，301是重定向。这个和shell里的exit 1000 退出时候的返回1000这个数字一个道理啊；也和echo $?看到的0是cli执行成功和非0执行失败一个道理啊。 %b Size of response in bytes, excluding HTTP headers. In CLF format, i.e. a '-' rather than a 0 when no bytes are sent. 23是对，200请求OK后23字节就是对的，然后403是禁止了199就是那个报错的页面啦-人家是199个字节 200 23 怎么理解👇 不过23Bytes其实是去掉了head头的吧 403 199 怎么理解👇 继续----- 在httpd日志里看到花括号{}就表示头信息，这里LOG里记录了head信息里的 Referer--从哪个站点跳转过来的信息，以及User-Agent用户浏览器信息。这些都是%{xxxx}i 代表的head里的键值对信息，具体用哪个你就写哪个就行了。 关于referer浅测一下 html语言 你好 这是跳转测试你好啊 借鉴上面的跳转写法，做两个页面来实验 此时到test2.html第二个页面就可以看到referer的上一个跳转过来的链接了👇 产生test1.html会有referer的原因有点奇怪，好像是偶尔从test2.html页面返回后退回来F5一下弄出来的。不过有时候出不来，搞不清楚， ​ 噢噢噢，我知道，是这样的，人家写了是从http://192.168.126.130/ 上一个页面来的，所以答案就清楚了 从👆这里点进去不就是httpd默认给你弄出来的跳转嘛， 再开一个机器192.168.126.131做一个图片页面， 然后在192.168.126.130上做跳转referer过去 写错了哦👆上图http://192写错了，导致👇 修改之 这就是盗链，如何发现盗链呢，就是去server上看referer 问：是否可以通过在server上写iptables -A INPUT -s 192.168.126.130 -j REJECT来防止盗链呢？ 答：该方法实现不了，因为跳转的实际效果是让用户重新下载一个新的内容，是用户去访问而不是130这个referer的上一个链接地址去访问。 所以要防止盗链就要继续学习咯... ... 再一个，如果是referer里显示的是百度的IP，说明什么，也是盗链么？应该就不是了，这是广告费没白给，哈哈，或者人家一搜，就你家的网站，总之十有八九算是好事了，这种情况。 关于日志的阶段性总结PPT放着了，随便看看，不看也行，图个完整性 然后日志的一个访问IP排序 字符集 iconv -l # 查看所有字符集 通常可以看得到网页的字符集的 带不带s https的s 也不一样 不过针对使用curl百度，不显示字符集👇 浏览器是有的 改一下字符集看看效果 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:52 "},"29-HTTP协议和APACHE/6-多虚拟主机实现.html":{"url":"29-HTTP协议和APACHE/6-多虚拟主机实现.html","title":"第6节 多虚拟主机实现","keywords":"","body":"第6节. 多虚拟主机实现 目录访问的别名 别名在welcome.conf里也见过的，上一篇将/.noindex.html的时候就遇到过👇 开始实验 mkdir /app/forum echo 'thisi is /app/forum/' > /app/forum/index.html 用户通过 curl 192.168.126.130/bbs/ 访问到/app/forum/ 当然你说用跳转，没必要，这里用alias别名 当你还还说用软连接，没必要，这用别名 光写别名还不行，👆还需要补上别名的那个目标地址的访问权限。 问题来了①我待会使用curl 192.168.126.130/bbs/去访问，/bbs这个文件夹我感觉不用存在，感觉正确。 ②/app/forum在os层面也是需要apache可读的。 总结，页面URL里的路径不一定在DocumentRoot下 curl http://192.168.126.130/dir100，实际上就是打开dir100/index.html页面 ①这个dir100文件夹确实在DocumentRoot下，且apache配置文件里有给到访问权限。 ②软连接，不过软连接所在的路径应该也需要在配置文件里给到访问权限 ③别名，就是上文的alias，这个种dir100文件夹 都无需创建。 ④跳转应该也算一个，具体见上一篇文章。 基于用户名密码方式控制访问 之前上一篇讲了基于源IP地址来控制访问已经实现了，这里接着讲基于用户名密码 要支持用户名密码方式控制访问，需要确认开启auth_basic模块，当然默认就开启了 要实现的目标：用户访问页面的时候，弹出对话框让其输入用户名密码。 cd /data/www mkdir admin echo 'this is /data/www/admin/index.html ...' > admin/index.html 需求：访问主站点无需验证，访问admin/index.html的时候要验证。 比如admin里放的是后台管理的页面，所以需要验证才能访问。 而用户名密码肯定不是系统的用户，而是httpd服务的，这种用户就称之为虚拟用户。不是真实的OS的用户。 这种虚拟用户，在不同的服务里都存在，除了这里的HTTP，还有FTP，等，都是服务自身的用户认证，无关于系统用户。 使用htpasswd来创建http的用户名密码，第一次敲这个命令的时候需要-c指定存放用户名密码的文件的。后续不能使用-c，否则原来的文件就被覆盖了。 由于http页面认证的用户名密码也算是一种配置，所以就放到/etc/httpd/conf.d/配置文件路径下。 找出下图👇错误之处 当然第二次别加-c就对了，然后 ① .httpuser前面的.就是好习惯，用户名密码文件用隐藏的就很不错 ②就是交互式的密码2次就很不爽，找一下👇 好创建三个用户吧，重新来一遍 htpasswd -c .httpuser user1 ... 输入密码 ... 再次输入密码 htpasswd .httpuser user2 ... 输入密码 ... 再次输入密码 echo 'cisco' | htpasswd -i .httpuser user3 # 非交互式类似于echo 'cisco' | passwd --stdin user1 果然和passwd很像，同样也加了盐，所以一样的密码，hash值就不一样了👆。apr1可能是类似哈希算法的类型。后面一段$xxxxx$应该就是盐，$分割除了3段，最后一段就是加了盐一起的哈希结果。 这样用户名密码就有了，下面就是在配置文件里写上 调用验证，然后到指定的文件认证就行了。 使用.httpuser密码文件的方法1：写到配置文件 主站无需认证👇 admin路径需要认证 上图没有体现出来AuthName \"xxxxx\" 内容。估计现在没几个浏览器支持这种显示了。 只有输入Require user 里指定的 admin用户名密码才行 👆上图一共3条记录，前两条是弹窗用户名密码输错了的记录401就是未授权的意思，第三条是正确的用户名的记录而且在记录的开头有显示具体的用户名。 另外curl也支持输入用户名密码的方式，不过就不是交互式的了👇 针对 .httpuser里的所有用户都可以访问这个文件夹 使用.httpuser密码文件的方法2：.htaccess 进一步复习下上一篇的.htaccess文件，就是上述配置可以移到这个文件里 先去配置文件里 允许.htaccess覆盖，当然这里选择仅覆盖authconfig就是认证相关的； 然后再将原来配置文件里的4行配置删除，配置到访问路径--也就是需要认证文件夹下 上图修正下，既然已经在这个路径下了，肯定就不需要写Directory了， 修改这些路径下的文件，无需重启服务，验证OK👇 用户组授权的配置 上图👆有错误，httpd -t也检查不出来；①漏掉了AuthUserfile XXXX 路径指定；②随后一行少了group参数。 修改👇 验证效果，符合按组授权👇 前面就介绍了http的访问控制里的基于IP和基于用户名密码的控制，下面学习一下两种方式的选择组合 远程客户端IP和用户验证的控制 浅测一下👇 1、首先我打算把配置都写道页面资源的路径下的.htaccess里 2、于是我就要去到配置文件里去写上\"允许覆盖\"的参数 3、然后去页面资源路径下编辑.htaccess文件 有点问题，就是Satisfy All没有生效！没有做到 双重验证。就是输入用户名后就能访问了，理论上是要备ip 禁止访问的。 这里就会担心这个.htaccess里是否可以这么写，查查官网发现.htaccess支持Satisfy All的 context的意思就是配置在哪里， 不过写法有点变化 IP控制用的是Allow，自然有deny。 然后呢发现 然后发现 结论： ①all确实是 用户认证 + IP acl 都要满足；any就是任一个满足就行。 ②原来那套配置\\XXXXX\\ 在这里貌似不行。 ③尝试写一个白名单出来 OK~ 实现用户家目录的http共享也就是web访问 同样要支持家目录的web访问，要有模块预加载 关于模块加载，见前文👇 下面就是将 用户的 家目录分享出来 修改权限也没用 恢复原来的权限700👆后继续实验👇 待会使用这个ming1的家目录 找到配置文件userdir.conf 修改对应内容 修改如下 这样就①开启了userdir的共享；②家目录共享不是说所有文件，而是要指定家目录下的某个文件夹共享，如上图的public_html共享。 重启服务后，浏览器访问http://192.168.126.130/~ming/即可 这里要记得修改/home/ming1的权限，只要通过facl给到apache就行了，因为用户访问文件夹，其实是通过httpd服务进程区访问的，而httpd都是apache用户运行的。 修改权限 给了apache进入ming1家目录的权限就行了 同样注意一点就是facl设置以后的ll 注意 必须是/~userDir/的写法 加一个验证 👇注意：缓冲会导致弹不出验证窗口：①常规浏览器需要清缓存才能弹；②无痕需要 关闭所有无痕再打开新的无痕才能保证没有缓存；如果有一个无痕没关，则缓存还在。 其实写一个.htaccess就要有一个习惯思维--就是是否可以写进去，如果不行就写道directory里。啥意思 上下文可以写的地方有👇 像有的配置就写在别的地方👇 再一个 这样试试看 结果也是正常的👇并没有说上面的授权全放就全放了，下面没有不起作用。挺好~ 要实现这种哪哪都生效的效果，是不是 代码里 ①将所有配置合并②找出最严格的配置③类似rib里的最长匹配。 ServerSignatrue 和结合serverToken一起梳理知识点 浅测一下 再删掉.htacess做一下放心对比动作 我感觉.htaccess挺好，因为它不是配置文件，无需重启服务就能生效；但是它又能做配置文件里的配置，就很香了。 status页面，可以用来做网站的监控 依赖于这个模块👇 干嘛的呢，就是获取apache的工作状态，默认关闭的。 又可以塞到.htaccess里了，哈哈，开心 当然，你要用directory一样的效果，不过要配置，同样DocumentRoot下的.htaccess或者配置文件里配置： 这倒是个看MPM的方法，默认果然是prefork。 理解下上图👆 __ __ __.................这些 表示的是进程状态，很多下划线和...说明进程太多了 _ 就是等待连接 . 就是打开slot但是没有线程来处理 发现一个问题，得到3种status的配置方法 就是setHandler server-status，这个写在两次， ①配置文件里的 只能是DocumentRoot下带/status访问，就是http://192.168.126.130/status，也不影响http://192.168.126.130/，这就是index.html的访问不受影响。 也不能写成\"/www/data/status\"，不会生效，所以只能放到documentRoot下来访问。 ②写道url路径文件夹下，但这种写法，就是把index.html给覆盖了，就是你访问http://192.168.126.130/~ming/ 后面带或者不带或随便写都是 打开的status页面了 ③其实可以这么玩 去掉配置文件里的相关配置 记得重启服务，验证一下 然后在DocumentRoot下新建一个status文件夹，然后进去编辑.htaccess，里面写一个setHandler 这样这个status文件夹里所有的页面统统不生效，你也无需创建其他任何页面，该文件夹就一个作用--setHandler server-status，显示服务器状态。 这样也自然也不会影响documentRoot的index.html 然后针对这个status页面限定只能特定IP查看 写到配置文件里也行 重启服务后，测试 虚拟主机 host就是你访问的域名/IP，记录在报文的host字段里了，该字段属于head，http的报文头。这是client发送请求的时候携带的，当server收到一看就知道了你访问的是啥了，所以这就是LB的转发最佳实践，而不是基于desti-端口也不是基于dest-IP。 之前用telnet 测试写过host👇，那会host是随便写的，因为没有涉及server检查host做负载均衡的机制，也就是没有做这里的虚拟主机来依据host转发的技术。 下面开始实验 首先、规划三个网站，简单就是三个文件夹页面就行了。 上图👆有错误，index.thml改成index.html才行。 虚拟主机-基于三个不同的des ip 然后编写配置文件，documentRoot、directory、access_log独立开来、virtualhost IP 重启后，access_log_asite文件就生成了，然后在写2个 通过域名访问简单加个host 就可以了👇 但是时间太慢了 浅查一下：结果一查就查了2小时，这里直接回头来写结论，load确实高，但不是load的问题，是allow from any，不用用any的原因，排障过程往下慢慢看。 负载太高了，如何判断高呢，依据如下👇，所以上图👆4 C，load 不能超过12。 然后load 三个值分别是1分钟、5分、10分钟的均值。 解决下，注释掉进程开启过多的配置，只是对于我这个pc的workstation的VM来讲太多了 等一会，等load降下来就可以测试看是否变快了 是快一点，但是也要4s呢，奇怪了不应该啊 难道虚拟主机就是慢？ 昨天也没有这么卡啊，同样的👇cli，估计笔记卡了，但是笔记本资源利用也就45%内存啊。其他都很低啊。 curl www.b.com 卡在那的时候，同时看看I/O也没变化，也不卡啊 浏览器打开，就是用无痕，也不卡，奇怪了，之前负载高的时候浏览器就不卡 TMD curl卡浏览器不卡 是啥什么鬼？ 将除了虚拟主机以外的所有配置全部注释掉，发现竟然好了--速度快了 只要再打开注释，就会变慢，👇 定位具体问题出在哪？ 发现只要这一段打开，就会卡 👆上图结论不一定对，进一步对比，取消上图的框选的部分的注释，然后注释掉虚拟主机部分，发现依然有2-4s的情况 出现频次也不低，发现还是👇这部分问题， 这部分问题不一定是看到的配置出了问题，去/data/www下面ls -a看到 删掉后继续测试，返现速度一直都很快，故障消失了 再补回去，故障又出现了 这里还不是白名单写法，如果是白名单写法应该是 deny from all，这样就是拒绝所有，放行了126.1。 而deny from any拒绝任一，就很不符合逻辑。但是从效果上来讲，就是除了126.1，其他curl都TM会卡。 修改成 黑白名单的坑-1，不能用any：测试过程在上下段落，这里是结论 结论，就是.htaccess的错误的写法(deny from any)导致，这会频繁的出现访问卡顿的情况。就用allow from all，deny from all ，只用all字样就行了。 然后研究下黑白名单 白名单写法，测试有效 但是黑名单失败了， 查资料 图上框出来的还是原因，没框出来的，First, all Deny directives are evaluated; if any match, the request is denied unless it also matches an Allow directive. 这才是原因，所以 直接看表格 https://httpd.apache.org/docs/2.4/mod/mod_access_compat.html#allow 所以这样的👇配置最大的关键点也是疏忽点就是，Match both Allow & Deny 在默认的机制(Deny,Allow)下是放行的，分析下图：whiteList有效是因为126.1和126.131是both命令allow和deny all所以allow，其他没写的Only match deny所以deny了。而下图的黑名单写法192.168.126是同时命中了deny和allow all所以就放行了，没有做到deny 具体IP的效果。 上图黑名单失败，修改一下，去掉deny就行了，这样依然是默认的deny,allow机制，但是deny的126段only在deny语句里，没有说 no match也没有both match。 黑白名单的坑-2，默认顺序(Default: Order Deny,Allow)黑白名单写法，这里是结论，过程见上文 黑名单简单，拒绝什么就些什么，不要写allow all字样！写了就是both match 全放了。 deny from 192.168.126.1 白名单写法 allow from 192.168.126.1 deny from all # 这里底层逻辑与众不同，126.1是 both match，也就是同时满足allow和deny语句的，所以both match是allow放行的，其他没写的IP，就是only match deny的，是拒绝的。 然后statisfy是针对源IP和用户名的一个机制，all就是两种因素认证，any就是(源IP或者用户名密码)任一满足就行。 至此，问题得以解决，什么问题，点这里 恢复load高的场景，修改.htaccess下的allow from any为allow from all 上图不能说明load高，为什么，如果cpu数量多，达不到1个cpu3个load以上就不算高 4个cpu，25个load，平均一个cpu8个，肯定高了。然后恢复配置和修改.htaccess 重启服务测试N次 此时想看之前的故障👇 虚拟主机-基于相同目标IP+不同desti port 把上一个实验的多个IP清掉 修改为端口区分来服务 这就好了，下面测试👇 搞定👆 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:53 "},"29-HTTP协议和APACHE/7-基于主机头的多虚拟主机和实现HTTPS加密.html":{"url":"29-HTTP协议和APACHE/7-基于主机头的多虚拟主机和实现HTTPS加密.html","title":"第7节 基于主机头的多虚拟主机和实现HTTPS加密","keywords":"","body":"第7节. 基于主机头的多虚拟主机和实现HTTPS加密 书接上回，虚拟主机的实现继续，上一篇实现了基于dstIp，基于dstPort 虚拟主机-基于主机头head里的host字段实现负载均衡 原理就是client发送http报文请求的时候，http头部里就携带了访问网站的FQDN 抓包是看不到https的head的，TCP以上都是加密的。 HTTP不加密可以抓到head内容 实验 👇就是统一监听80，80本来在/etc/httpd/conf/httpd.conf这个主配置文件里就启用了， 然后documentroot及其all granted授权，documentroot和directory是一对，根和授权。 最后就是servername转发依据和LOG 搞定👆 疑问：为什么cstie没写servername，结果转发到www.a.com了 估计就是 不写servername，然后用的就是第一个 从上往下找 通过官网查看就会自然知道现在servername还不够，还需要知道serveralais 看个现象 问，为什么192.168.126.130的访问变成了www.a.com了 答，因为www.a.com在前面 问，为什么不是原来的 非虚拟主机的页面了，答：从结果判断，就是虚拟主机的转发配置抢先了 其实就是虚拟主机的80和非虚拟主机的80冲突了，结果就是虚拟主机胜利 80变成www.c.com排在第一个，所以和 冲突了，存在优先级了就。 如果ip+port+servername一起用会如何 说明端口 port 优先> servername 不能！因为www.c.com:81 这个主机头就是带81端口的 上图👆192.168.126.103和192.168.126.130都是配置在本机eth0接口的多个IP地址。 绿色线条表示：命中了虚拟主机的ip+port走的是虚拟主机 红色线条表示：虽然curl的是www.c.com域名，但是会dns解析成192.168.126.130:83，于此同时host主机头是www.c.com:83，所以这个一个请求发到server上apache处理是按照ip+port优先来处理的--没有命中任何虚拟主机的配置于是命中的是非虚拟主机的配置，并不是说看主机头去处理的--如果按照host处理结果就应该是www.c.com了。 这样做也是合理的，毕竟IP可能都打到别的机器上去了，你还看主机头不是扯嘛。①主机头只是请求里的一个字段携带的内容②请求的域名对应的IP才是首要的③域名默认会写到主机头里而已，可修改。④不要瞎折腾，简单规范的配置就行了。⑤apache看到请求处理思路1、看IP+PORT啦，如果没有命中虚拟主机就走非虚拟主机的DocumentROOT；2、如果命中虚拟主机 ，此时在虚拟主机层面匹配还是先看虚拟主机的IP:PORT，如果几个虚拟主机的IP:PORT一样，才会去看host主机头。 好好了，回过头复习了一下，溜了溜了~去继续折腾nginx去。 压缩技术，用的也非常多 server在页面发送到网络上之前，做一下压缩，节省带宽用。穿越网络，到达彼岸用户手上后，浏览器会自动解压出来后进行页面展示。 --- 属于典型的时间换空间的打法，时间--CPU压缩耗时，空间--磁盘和带宽的空间节省。 --- 什么时候用什么换什么，有时候也会用空间换时间，具体就看性价比，怎么划算怎么来。 --- 再一个压缩技术，消耗的是两头的CPU：server压，client解压，用户的资源消耗的提升也是一种拉动内需的手段。 上图👆SetOutputFilter DEFLATE不写也行。 AddOutputFilterByTyep DEFALE XXX这是要压缩的文件类型，一般来讲视频压缩效果不太理想(不过具体也看如果视频本身的画面单一，可能压缩比也高的)。 使用压缩技术，server端依赖的是deflate_module，client依靠浏览器 压缩级别的指定👆 下面实验 首先找一个大文件，用来做页面 是的，关键是curl www.a.com 回车后直接默认就是访问的www.a.com/m.txt，奇怪了不应该是www.a.com/index.html嘛 上图403的原因：正式因为访问www.a.com默认就访问了m.txt，而m.txt没有给r权限，所以就403 Forbidden了。 至于为啥不是index.html就不清楚了。 curl 看server有没有压缩，要用--compressed选项才能确定 好上图就说明没有压缩，原文件多大，curl 就下载了多大---conntent-length 开启压缩 重启服务 浏览器可以看到压缩了👇 curl压缩要用选项 压缩的意义在于 money的节省，举个例子，一个图片没压缩钱是4MB，压缩后400KB，结果忘记开启压缩了，结果对外服务大量用户都是下载的4MB，流量哗哗的就出去了，都是钱啊。 HTTPS 逻辑 实现 1、颁发证书的两种方式 自签名证书，自己给自己颁发，就是私网里自己做一个CA证书颁发机器来实现内部的ssl证书颁发，不过这个互联网上是不认得。 申请购买商业SSL证书正儿八经去相关机构比如亚信、godaddy去申请SSL证书。 申请免费SSL证书这个互联网也认 凡是互联网认得，不管是买的还是免费的，关键点在于\"受信任的根证书颁发机构\"这一随OS安装就存在于电脑上的，手机同理。 这些证书就在了，也就是得到这些著名的CA的公钥了。 所以这些CA的颁发的SSL证书--也就是用他们的私钥加密来文件(这个文件往往就是网站的公钥)--也就是Sca(Psite)，PC-client就可以用已安装的公钥解密了，所以就可以解开了 2、加载httpd的加密模块 上图可知：本身105个模块加载了93，12个没有加载，然后105也没有ssl模块，所以ssl模块需要另外安装。安装就用yum install mod_ssl yum 安装后，就有这个ssl模块了，而且也加载了 👆为什么yum后就加载了，其实yum安装本质上也是添加了配置文件，做了加载的配置👇 配置文件相关 证书和私钥是yum安装的时候自动生成的，具体怎么来的，①要么安装出来的-通过 rpm -ql mod_ssl看看②要么是yum安装前或者前后的脚本跑出来的--通过rpm -q --scripts mod_ssL看看 👆确实是yum 出来的两个证书和key文件。 但实际上都没有我的都没有，省流：因为新版本是mod_ssl模块安装后，httpd启动后就会自动生成localhost.crt和localhost.key。而不是什么安装后脚本了。 视频里老师的有，是老版本的打法。 👆上图确实是老版本的mod_ssl确实存在安装后脚本的，ssl证书和key确实是脚本产生的，但是新版不同。 再找一台机器yum mod_ssl可得出结论，新版本确实没有mod_ssl 安装后脚本，通过下面两个图可见证书和key文件都是启动httpd服务后自动生成的。哈哈新版本和老板的区别咯。 再找一台，就是不安装mod_ssl，启动服务，预判是没有这两个文件的，确实没有👇 所以结论是：mod_ssl模块+httpd服务启动一次，就会自动生成localhost.crt和localhost.key文件了。 不过倒是得到了一个证书生成的脚本：就叫做自签名证书脚本 哈哈，天下脚本一大抄嘛，倒是个不错的思路，抄之 其实就是yum 一下 mod_ssl ，重启一下httpd服务，443就监听了，证书也有了，https也启用了 此时80端口也开着，http协议不加密的服务也开着。所以http和https都可以访问，而且mod_ssl配置文件里面是这么配置的👇 虚拟主机的配置方法，也不会和之前的80服务产生冲突 冲突情况 不冲突的情况 其实mod_ssl本质上也是 虚拟主机不过人家是443，不会和 之前的80冲突，冲突的无非是虚拟主机的80和非虚拟主机的80. curl要注意使用-k选项忽略证书的安全性 将message大文件复制到/data/www下进行ab性能测试 此时主站就变成了 导入证书 证书怎么看，👆这样看不到任何信息， openssl x509 -in localhost.crt -noout -text 该 自签名证书👆，好像无法导入\"受信任的根证书颁发机构\"，颁发者和颁发给的都是 主机名node2，也不知道有没有问题，反正一般都是颁发给某某网站，比如👇 上面是利用mod_ssl+httpd服务启动后自动生成的证书去实现的，下面使用手动内部签名的方式 利用私有CA，实现https 先回顾一下mysql里的ssl加密的所有cli mkdir /etc/my.cnf.d/ssl # 专门放证书信息，利用现成的my.cnf.d文件夹，ssl是创建的 cd /etc/my.cnf.d/ssl ①生成CA的私钥： openssl genrsa 2048 > cakey.pem # 以前是专门一个目录，现在简单放一起就行 可能最好加个密，或者改个cakey.pem的权限，安全些。 ②利用私钥生成自签名证书 openssl req -new -x509 -key cakey.pem -out cacert.pem -days 3650 ③生成master的私钥和证书申请文件 openssl req -newkey rsa:1024 -days 365 -nodes -keyout master.key > master.csr # 利用一条命令生成私钥文件master.key，并利用该key生成证书申请文件。 注意！1024得改成2048，否则mysql起不来。可能是之前用得2048的CA私钥吧。 ④有了证书申请文件，就可以签名了--也就是颁发证书 openssl x509 -req -in master.csr -CA cacert.pem -CAkey cakey.pem -set_serial 01 > master.crt 然后SSH里也涉及CA，下面是SSH里的操作，也是本次httpd的证书操作，一样的。 1、建立CA：genrsa生成ca私钥--cakey；利用cakey自签名证书--自己给自己颁发证书 (umask 077;openssl genrsa -out private/cakey.pem 4096) openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -out /etc/pki/CA/cacert.pem -days 3650 /etc/pki/CA/serial # 存放下一个颁发证书的序列号，0F改成01从第一个号开始分 2、搞一个证书申请文件 mkdir /etc/httpd/conf.d/ssl cd /etc/httpd/conf.d/ssl (umask 066;openssl genrsa -out httpd.key 1024) #1024可能有问题msyql那会的经验告诉我要4096保持一致 openssl req -new -key httpd.key -out httpd.csr 3、针对申请文件进行颁发证书-也就是签名-也就是用CA的私钥进行加密 openssl ca -in /etc/pki/CA/httpd.csr -out /etc/pki/CA/certs/httpd.crt -days 100 scp /etc/pki/CA/certs/httpd.crt root@httpdServer:/etc/httpd/conf.d/ssl # 把证书复制到server上 scp /etc/pki/CA/cacert.pem root@httpdServer:/etc/httpd/conf.d/ssl # 把ca自己的证书也复制倒server上，此举相当于windows预加载了受信任的根证书文件。 实验开始👇 在CA上： 实测👆上图的umask并不会影响到openssl genrsa生成文件的权限，可惜了，(umask 077;mikdir/touch)倒是ok openssl x509 -in cacert.pem -noout -text # 查看证书内容cli 👆对的，上述信息都在的。 在服务器上： 把httpd.csr证书申请文件传到CA上 在CA上进行签名 将证书复制到server上 将CA证书复制到server上 然后去到server上 👆httpd.key就是私钥，httpd.crt就是证书-也就Sca[Pserver]-也就是经CA私钥加密后的server公钥文件。 最后就是在server上的ssl配置文件里使用上面得到的cacert.pem根证书，httpd.crt证书，httpd.key私钥 CA的证书先不配置，看下是否就会报不安全的证书信息 名字改一下好了，结果还是报错，是不是1024和4096不匹配啊，应该不是，仅仅1024太短了，后面改成了2048就好了CA还是4096的长度。 通过err_log看看，重启服务的动作会生成如下错误日志 回过头去修改httpd.key的1024长度为2048试试 就是index.txt里有记录了，就是CN、beijing、admin@ming.com这些申请信息已经有了，重复了冲突了 结果，就牛逼的OK了 屡试不爽啊，myslq里和httpd里，然后httpd其实就是ssh的ssl一样的用法，所以CA ssl证书里的2048这种长度一定要一致啊，具体来讲就是CA的key2048，那么server的key也要2048，等等，一致肯定没问题，但是上述实验CA的key是4096哦，server的key最后2048也行的，就是不能1024，可能是1024的长度被弃用了。 测试-1-目前没有加载CA的证书呢，只是server有了自己的证书 观察证书的信息，就是我们上面申请的信息 注意实验的时候不要点击 继续前往，否则就没有对比效果了 注意细节，没有导入CA证书的时候，证书 详细信息里 的层次结构是看不到的 待会导入证书后，就会看到多了颁发给：www.a.com 的信息 ，而不再是上图👆的颁发者和颁发给都是ca.ming.com了 上图👆这里层次结构不全，是因为我们没有加载CA证书，在httpd的/etc/httpd/conf.d/ssl.conf里 加载一下 重启服务后，httpd网站的ssl证书里的层次结构就全了-就是谁下面的谁的证书。 这页没啥变化👆，主要是第二页 详细信息 但是为什么层级里都是ca.ming.com啊，难道不是www.a.com，稍等我知道了 再改一下，将httpd.csr重置， 注意上图index.txt里由于本次颁发是www.a.com和之前的ca.ming.com不冲突了，所以不会报错 看看是不是www.a.com了👇 issuer，\"颁发者\" subject，这里指 \"颁发给\" 重启服务 这下就顺眼鸟 导出CA证书，放到client PC上去，导入倒PC的 受信任的根证书机构 里去 如何导入呢，修改cacert.pem为caert.pem.crt，就是在linux里pem后缀就认了，但是windows里还得再加个crt后缀 注意上图左1的 \"证书状态\"是 不受信的。 此时，再看证书状态就没问题了👇，这里只是系统层面的判断，浏览器层面只完成了一半。 但是浏览器不管的，还是说 不安全，我怀疑浏览器就是不认私自颁发的证书了。 https://support.huaweicloud.com/ccm_faq/ccm_01_0098.html 如何解决浏览器的不安全提示 下了个火狐，人家明确说了自签名，不认，啊哈哈，牛逼。 那么问题还在吖，自签名证书就没办法让浏览器认了吗？ https://blog.csdn.net/a735131232/article/details/80526859 试试这招👆不具体，再看看别的教程 https://blog.51cto.com/u_296714/5754713 这篇👆貌似可以解决，但是我就想用openssl生成的ca自签名证书呢 导出后加个.crt后缀，在windows里打开看看 发现没有 \"使用者可选名称\"啊 搜索 openssl如何生成这个选项，准备重新颁发 https://blog.51cto.com/u_11508007/5674376 # 他这个只是配置文件写对了。# 一切OK后补说明：但是颁发的时候也要加上v3_req选项，他就没讲。而且只适用于自签名和颁发，csr文件无需这么操作。这些都是后话，回顾写道这里的，可以不看。 修改openssl配置文件 备份 cp -a /etc/pki/tls/openssl.cnf /etc/pki/tls/openssl.cnf.bak1 修改vim /etc/pki/tls/openssl.cnf 取消req下被注释的第2行 删除req_distinguished_name下的0.xxx 的标签，把0.xxx的0. 去掉 在[ v3_req ]下新增最后一行内容 subjectAltName = @alt_names 新增 alt_names,注意括号前后的空格，DNS.x 的数量可以自己加 上图写错了，DNS.1改为*.a.com和DNS.2=www.a.com 重新走一遍所有证书的操作-第二次NG CA上👇 cd /etc/pki/CA mkdir certs mkdir crl mkdir newcerts mkdir private mkdir ssl (umask 077;openssl genrsa -out private/cakey.pem 4096) openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -out /etc/pki/CA/cacert.pem -days 3650 touch /etc/pki/CA/index.txt # 存放已经颁发的证书信息 echo 0F > /etc/pki/CA/serial # 存放下一个颁发证书的序列号，0F改成01从第一个号开始分 server上👇 mkdir /etc/httpd/conf.d/ssl cd /etc/httpd/conf.d/ssl (umask 066;openssl genrsa -out httpd.key 2048) #1024可能有问题msyql那会的经验告诉我要4096保持一致 openssl req -new -key httpd.key -out httpd.csr scp /etc/httpd/conf.d/ssl/httpd.csr CAServer:/etc/pki/CA # 把csr申请文件传到CA上，在CA上根据csr文件来颁发证书，也就是对其加密。 CA上👇 openssl ca -in /etc/pki/CA/httpd.csr -out /etc/pki/CA/certs/httpd.crt -days 100 scp /etc/pki/CA/certs/httpd.crt root@httpdServer:/etc/httpd/conf.d/ssl # 把证书复制到server上 scp /etc/pki/CA/cacert.pem root@httpdServer:/etc/httpd/conf.d/ssl # 把ca自己的证书也复制倒server上，此举相当于windows预加载了受信任的根证书文件。 这里调了默认的[]里的值也是在配置文件里 后略了，但是没有 使用者可选名称 ！！ 查询原因 终于有了 知道了自签名和颁发的时候才会使Subject Alternative Name有效 openssl req -new -x509 -nodes -out cert.pem -keyout key.pem -config /etc/pki/tls/openssl.cnf -days 365 -extensions v3_req 这是自签名，我可能需要的是ca颁发的时候，或者server生成csr的时候做出来 使用者可选名称--也就是subject Alternative Name，实际测试时ca颁发或者自签名的时候才会起作用 CSR文件里无需做任何配置，下图👇是错误的，通过CA那边修改配置文件立马生效可知 证明上图时错误的 所以要让server服务器的ssl证书里有subject Alternative name出来，就要在ca颁发server证书的时候①修改openssl的配置文件②颁发的时候带上v3_req版本参数 然后要让浏览器不在说不安全就要①加载ca根证书到\"受信任的根证书颁发机构\"②网站ssl证书里要有Subject Alternative Name。 重做证书-最后一次-ok了 CA上👇 cd /etc/pki/CA mkdir certs mkdir crl mkdir newcerts mkdir private mkdir ssl (umask 077;openssl genrsa -out private/cakey.pem 4096) openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -out /etc/pki/CA/cacert.pem -days 3650 touch /etc/pki/CA/index.txt # 存放已经颁发的证书信息 echo 0F > /etc/pki/CA/serial # 存放下一个颁发证书的序列号，0F改成01从第一个号开始分 server上👇 mkdir /etc/httpd/conf.d/ssl cd /etc/httpd/conf.d/ssl (umask 066;openssl genrsa -out httpd.key 2048) #1024可能有问题msyql那会的经验告诉我要4096保持一致 openssl req -new -key httpd.key -out httpd.csr scp /etc/httpd/conf.d/ssl/httpd.csr CAServer:/etc/pki/CA # 把csr申请文件传到CA上，在CA上根据csr文件来颁发证书，也就是对其加密。 CA上👇 openssl ca -in /etc/pki/CA/httpd.csr -out /etc/pki/CA/certs/httpd.crt -days 100 -extensions v3_req scp /etc/pki/CA/certs/httpd.crt root@httpdServer:/etc/httpd/conf.d/ssl # 把证书复制到server上 scp /etc/pki/CA/cacert.pem root@httpdServer:/etc/httpd/conf.d/ssl # 把ca自己的证书也复制倒server上，此举相当于windows预加载了受信任的根证书文件。 一下是所有完整过程： 在CA上👇 在server上👇 在CA上👇 在server上👇加载ca证书、server证书和server的key 把ca证书拿到PC，加上.crt后缀，导入 受信任的根证书颁发机构 无痕模式 一定要关闭所有无痕窗口再重新打开，不然确实有缓存的，然后结果就是搞定啦 这个在浏览器里显示为 \"证书主题背景的备用名称\" 导出后补上.crt打开可见为\"使用者的可选名称\" subject 在证书里 显示为 颁发给、使用者、主题，所以只是翻译的问题，subject是本来的名字 上图👆这就是 网站浏览器不再 提示 不安全的原因，一定要①加载根证书②server证书也就是网站证书里要有使用者可选名称--subject Alternative name，这个参数怎么来说的，就是CA颁发的时候加上-extensions v3_req，颁发者(CA颁发或者自签名)的openssl配置文件里也要改的。 搞定搞定~~~ 所以要让server服务器的ssl证书里有subject Alternative name出来，就要在ca颁发server证书的时候①修改openssl的配置文件②颁发的时候带上v3_req版本参数 然后要让浏览器不再说不安全就要①OS层面：加载ca根证书到\"受信任的根证书颁发机构\"--这一步只是让ssl证书导出后打开显示为 “该证书没有问题”；并不能让浏览器认为安全，浏览器认为安全的判断依据是，判断\"网址\" 和 \"使用者可选名称也即是subject alternative name\"里的东西 一致 ②浏览器认为安全的判断依据：网站ssl证书里要有Subject Alternative Name，且该参数里的内容包含网站地址 \"多个域名不用再购买多个证书了\"这话有点吊啊，回头看看公司的ssl证书是怎么买的。(￣▽￣)\" 以前浏览器看这个 上图证书导出来 主体背景 就是 \"使用者\" 翻译问题 现在看这个 上图证书导出，证书主体背景的备用名称 就是\"使用者可选名称\" 删除根证书要到mmc里删除了 这就删除了 删除 \"受信任的根证书\"后，再次打开看看 一样不安全，所以①导入ca证书；②subject Alternative Name里有网站地址。 浅排一下打开www.a.com慢的问题 真TMD高，回头研究下为什么是每个掉用户，load这么高， 直接降低就是修改配置文件，把几个初始化多线程关掉 重启服务后，逐渐就降下来了，估计是参数设置不合理，没有匹配 VM虚拟机的资源。 然后PC 浏览器打开就秒开了，呵呵呵，之前都是很慢 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:54 "},"29-HTTP协议和APACHE/8-http的安全加固和重定向.html":{"url":"29-HTTP协议和APACHE/8-http的安全加固和重定向.html","title":"第8节 http的安全加固和重定向","keywords":"","body":"第8节. http的安全加固和重定向 书接上文的一些其他注意点 测试基于https访问相应的主机 浏览器验证https，呵呵 openssl 验证https主机 openssl s_client [-connect host:port] [-cert filename] [-CApath directory] [-CAfilefilename] 其中-cert filename应该是client的证书，这个不用写，一般server也不会验证client证书。 上图是拼接出来的哦，无缝衔接，一字不拉，牛逼不~ 如果不带ca根证书 -CApath . -CAfile cacert.pem 就会error的哦 没有上图的绿色ok了，顺带一提绿色是MobaXterm自带的着色。 以上就是原来绿色ok处的对应截图，现在都是error和 Verify return code 19报错了，原来是Verify return code code 0 ok。 curl验证https curl 带上根证书进行访问https网页 理解下目前实验页面显示状况 为什么https://www.a.com和http://www.a.com内容一样 ①首先按https://www.a.com走的是/etc/httpd/conf.d/ssl.conf里的虚拟主机 而此处没有定影DocumentRoot的路径，,所以去看之前我们定义的/etc/httpd/conf.d/test.conf，要定义DocumentRoot，自然/etc/httpd/conf/httpd.conf下的DocumentRoot要注释掉的 其实我感觉规范的做法就是虚拟主机也配置一遍自己的，不管是否利用默认的，都配一边方便维护。 然后②看下http://www.a.com走的是/etc/httpd/conf.d/test.conf里的虚拟主机 浏览器里输入的是http://www.a.com，所以先看80，再看head里的host是www.a.com所以走的是/data/www/asite下的xx.txt，如果没有xx.txt就是index.html了 所以结合①和②，https和http都是一个页面啦 有人说：https不能说配置多个\"基于主机头的-虚拟主机\"。我按他的思路理解是因为https加密范围是包含了head字段的，虚拟主机处理的时候再ssl卸载之前了，这也是我猜的，尴尬的点在于server具备卸载ssl证书的能力，也就是可以看到主机头，为什么不能针对多个https站点用基于head里的host来实现路由呢。除非 废话不多说，上实验 为了便于对比，将原来的https://www.a.com的页面改成index.html不再使用m.txt 这不是好好的嘛 www.b.com复制的www.a.com虚拟主机的配置，证书要改，否则 subject alternative name里的没有www.b.com网站地址，不能认为安全的👇 只不过www.b.com没有颁发证书而已。搞一个 ①生成csr文件 ②ca颁发一下 发现没有Subject Alternative Name，所以要去/etc/pki/tls/openssl.conf下去配置一下 然后就有了 回到server上配置好www.b.com的证书文件的加载，同样和www.a.com的一样，在虚拟主机里配置，在/etc/httpd/conf.d/ssl.conf里配置 只需要改一行👆 PC上补一个hosts记录 然后就好了啊 所以通过实验不是发现了https的多站点，可以基于head主机头host字段进行配置多个虚拟主机啊。 我就觉的server自己有能力卸载ssl证书-解密，而且卸载的证书和servername都在一起配置的👆，为啥不能基于主机头进行https的多虚拟主机配置呢，对吧。实验证明就是可以的。 下面实现http跳转https 首先知道一个chrome的默认行为， chrome浏览器会自动给你补上https的，比如 使用联想浏览器输入a.com 使用chrome输入a.com，只有chrome会干这事，其他浏览器不会说 输入xx.xx.com自动用https:// 使用火狐，结果发现火狐的证书好像识别可能不是OS的 这样就加载进去了，火狐果然不是用的OS的受信任的证书颁发机构。 然后火狐浏览器就不再报不安全了 然后火狐的行为 输入https://www.a.com或https://www.a.com/都一样。 然后正儿八经开始做server的http重定向到https 区别上面说的Client的chrome的默认https行为，防止误判 两种重定向：301和302👆，通过语句里的[status]区别配置 Location：http://www.jd.com就是301重定向过去的网址。 活久见http跳http，浏览器错误显示了吧 jd就是两次跳转，一次301 从http://www.360buy.com/跳到http://www.jd.com，一次302从http://www.jd.com跳到https://www.jd.com; 也就是第一次是 不同网站的http跳转-301；第二次是相同站点的http跳https。 好像后台自动给你转，就是一次请求就行了，好像也是有此类技术的。 实验 访问www.a.com跳转到www.b.com 将上面的访问都跳转到https://www.b.com 再把https://www.a.com也重定向了 这样就实现了http://www.a.com跳http://www.b.com;https://www.a.com跳https://www/b.com 这样就实现了http://www.a.com跳https://www.b.com curl -L 的说明，curl默认行为就是只发一次请求，-L就会follow redirects，就是会跟随重定向们，多次也没问题 如果你再在上图上地址栏里敲一下回车，由于是chrome就会导致你访问的其实不再是http://www.b.com而TMD的是https://www.b.com了然后就会走别的虚拟主机了，重定向的配置就不生效了，或者别的地方的重新向了，我的配置如下 好烦呐，翻来覆去的，(￣▽￣)\"，反正这里记住，处理思路就是类似网络转发：PHB，per-hop-behavior，这里也是一样一跳一跳的路由转发查询就行了。 淘宝用的301永久重定向，其他大多数用的是302临时重定向。 然后就是搜索引擎比如百度，就不会抓取301重定向前的地址页面了，302的跳转前页面还会抓的。因为301永久重定向被被认为是废弃的地址了不用了，也就不会去这个页面抓取内容了。 不做虚拟主机进行ssl跳转 恢复实验环境①移除/etc/httpd/conf.d/test.conf②恢复/etc/httpd/conf.d/ssl.conf里的配置，证书相关保留③恢复/etc/httpd/conf/httpd.conf里的配置 修改如下👇直接在主配置文件里补上重定向的配置就行redirect temp / https://www.a.com 最多跳转50次 产生这样的跳转的原因不太清楚，解决方法是换一种配置方式 RewriteEngine on RewriteRule ^(/.*)$ https://%{HTTP_HOSTS}$1 [redirect=302] 重启服务后生效 HSTS： ①HSTS:HTTP Strict Transport Security 解决如下问题： 除非第一次就给你劫持了，否则还是比较安全的，靠的是本地缓存，如果缓存被清空或者到期或缓存时间设置较短，还是存在安全风险的具体逻辑如下： HSTS -- HTTP Strict Transport Security 使用该技术的案例👇。其他网址看了下，用的不多。 老化时间是1小时，也就是重定向缓存1小时 ②HTST preload list 是Chrome浏览器中的HSTS预载入列表，在该列表中的网站，使用Chrome浏 览器访问时，会自动转换成HTTPS。Firefox、Safari、Edge浏览器也会采用这个列表 这里和chrome浏览器自动给你补Https的行为还不是一回事。 ①chrome你输入www.baidu.com会自动给你补https://的 注意这里不存在重定向的，要输入http://www.baidu.com才会有302出现 当然curl 也不会触发302，有点奇怪， ②chrome里的hsts列表，这是preload预加载的，你输入http://www.bing.com内部就给你转成https://www.bing.com 使用无痕单开模式输入http://bing.com，不要加www哦，因为上图的found清单里并没有www.bing.com只有bing.com 然后通过F12查看是不是直接就是https出去了 ③就是各种重定向了，301、302、307之类的 但是加上www，由于不在chrome浏览器的hsts清单里所以还是走的服务的重定向不过这里是307 当我在无痕里输入http://www.bing.com的时候，通过F12看到的是307，内部重定向。 把字符集改一下 没改过来！ 好像是开启重定向就会这样，下图👇是不用虚拟主机方式的写法，会造成循环重定向，这里仅仅测试重定向对字符集的影响。 重定向都关了就好了 这些cil进一步研究可以查看官方手册https://httpd.apache.org/docs/2.4/mod/mod_rewrite.html#rewriterule 正向代理和反向代理 图片不多说了，补充要给一般正向代理往往会加一个缓存功能，就是出口代理缓存服务器，比如城域网出口想必是有缓存的。比如内部的Nexus私网原站(pip源、yum源、npm源、docker源)，然后client都将源指向nexus代理服务器，所有的基于各种源的软件安装都会从nexus代理走，然后凡是从nexus下载过的软件都会在nexus本地缓存起来，这样，下一次其他人通过该代理下载就会不用再去互联网请请求了。 同样理论上也是可以在代理服务器上做acl，针对某个用户拒绝代理服务也是可以的。 反向代理服务器，通常就是一个服务器 服务不好 N多个用户了，才需要多个服务器分担。所谓服务不好主要指用户量大了，或者用户地理位置分散-服务器需要有就近服务需求。 类似myslq读写分离的调度器 正向代理，加速、缓存 反向代理，均衡、调度 正反都是欺骗用户，欺骗不太好，用户也知道有这些东西，或者叫都是代理了server服务来着。 正向代理软件，squid老牌正向代理软件，web cache咯http://www.squid-cache.org/有需要再研究吧 反向代理如那件，nginx较多，apache有但是用的很少，还有LVS、还有F5、HAproxy。 实验-apache的反向代理 一般会将背后真正的提供服务的服务器叫做real server👇 再设置一下反向代理服务器 apache的反向代理-常规 ProxyPass \"/\" \"http://192.168.126.130/\" # client访问 我\"/\" 转成 身后 realServer 130 ProxyPassReverse \"/\" \"http://192.168.126.130/\" # 身后realServer回包我，将130转成我自己\"/\" 👆就是中间代理，两头欺骗，不好意思我又用了欺骗一词。因为我找不到更简单词用来替代了。承接？也不明显。用的好就是透明代理，恶意的就是欺骗了吧。 记得重启131的httpd服务 然后就可以测试了，测试之前打开realserver和proxy的log client上不知道真正的服务器s👇：访问的是proxySer proxyser上知道真正的c和s👇：看到的是真正的client IP realSer上不知道真正的c👇：看到的是proxy访问过来的 apache的反向代理-特定URL ProxyPass \"/images\" \"http://www.example.com/\" ProxyPassReverse \"/images\" http://www.example.com/ 找个image测试 修改proxySer的配置，只针对特定url进行转发过去 测试，一般不转发ok 测试，特殊url转发ok 测试，特殊url转发ok apache的反向代理-虚拟主机处配置 这个就有点像nginx了哦，呵呵，基于主机头的负载均衡，后端用端口区分，也可以用nodes的ip区分，貌似可行。 ServerName www.magedu.com ProxyPass / http://localhost:8080/ ProxyPassReverse / http://localhost:8080/ 浅测一下 配置proxySer 修改windows这个client上的hosts 解析到proxySer上 测试负载均衡 还不错啊，貌似用apache一样做反向代理，不过nginx用的比较多，apache用的少，所以就是那家饭店吃饭人多去哪家就是这个朴实无华的道理。 Sendfile机制-属于零复制技术里的一种 先复习 一直看到 然后上图👆结合下图理解一遍👇 4次用户态和内核态的交互如图👇 图中的④未标注，图中的socket buffer到httpd，是内核态到用户态的切换，但是应该不是③的write()返回。 简单理解就是确实存在4次左右的用户态和内核态的交互。这样就发现数据的传递存在不必要的处理过程--数据本来就在内核态里非要从用户态绕一圈没必要，于是sendfile就来了。 简单来讲就是👇下图的绿色箭头，不过socketbuffer好理解，协议栈难不成算到接口的队列缓存里的？ apache默认就启用了该👆sendfiler机制。 该技术不仅仅属于apache，nginx里也有。 sendfiler技术又名零复制，其意思就是没有将数据从内核空间复制到用户空间。 具体零复制，还涉及一些别的技术。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:54 "},"29-HTTP协议和APACHE/9-http协议详解和相关工具.html":{"url":"29-HTTP协议和APACHE/9-http协议详解和相关工具.html","title":"第9节 http协议详解和相关工具","keywords":"","body":"第9节. http协议详解和相关工具 HTTP协议 HTTP请求报文 \"实体主体\"，比如POST上传的数据内容，比如上传文章。正好我用confluence的api上传文章看看 难道所谓的\"实体主体\"就是上图的JavaScript Object Notation，不过这部分内容确实是我上传conf的文章。 上图既有请求报文，也有响应报文，>就是请求， HTTP响应报文 HTTP报文语法 GET POST用的较多，HEAD就是只看头的请求，通过curl -Iv可见👇 关于HTTP的响应码的说明查看方法 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status/200?utm_source=mozilla&utm_medium=devtools-netmonitor&utm_campaign=default 304缓存，F12里有Disable cache开关。我一般都是勾选的，防止缓存带来的误判。 有人缓存就清2个礼拜的，但实际缓存可以存在长达1年之久👇 Cookie cookie是键值对，里面存放了用户信息 ①用户访问服务器的时候，服务器生成cookie值，是key,value键值对，比如user=bob,goods=tesla ②server通过set-cookie这个首部字段信息，发给用户 ③用户的浏览器就收到了， 然后存着，下次访问就会自动在请求报文中携带cookie,user=bob,goods=tesla ④网站服务器一看，这个人又来了，这就实现了HTTP无状态的用户信息连贯性。 胖cookie：早期什么信息都网cookie里放，造成cookie东西太多，带宽占用大。 cookie的有效期：应该也是set-cookie服务让用户浏览器缓存的时间。 会话级的cookie：就是类似mysql的会话级的变量一样，只针对当前会话生效；这里的会话级的cookie就是浏览器一关，cookie就没了。 好像有的登入ID就是用的session级别的cookie。 通过userid来简化cookie携带的内容，id对应的内容放到server端的DB里，这样信息在网络中的传递就少了很多。 说到信息的传递和验证，这块玩的花的玩的6的我接触到的还是cisco的ASA课程里其实不仅仅是ASA就是security方向里用的比较多，印象中有一个ISN、还有DOS防护的一些技巧，当然抗D肯定不是这么玩咯。 再一个session id，这个比如jd都没有登入，但是两次页面刷新，购物车里的东西还在，说明不是基于用户ID来保持购物车里的信息的，这里应该就是基于sessionID来的。 session ID 10000在DB里存在购物车里的商品信息。 sessionID也是放到set-cookie发给用户浏览器的，其实说白了就是因为http长连接也长不了多久，http会话不会保持，http本身设计就是短链接，所以需要有一个东西来做多个tcp连接的承上启下context的作用。不管session ID也好，userid也罢，都是打标 用标，然后标的信息完整信息放到server的DB里作为节省带宽的玩法。 cookie也不会随浏览器关闭而关闭 cookie也可以进一步查看的 不过再详细好像现在版本直接看不到了，之前点进去可以看到类似浏览器F12里的cookie值得。 https://blog.csdn.net/u011781521/article/details/87791125 找到一个chrome浏览器得缓存 cookie找到了，这是chrome浏览器cookie的存放路径 发现是SQLite格式 传到linux里看算了，windows 和linux 一样要安装sqlite https://zhuanlan.zhihu.com/p/99643229 https://juejin.cn/post/7111861277751771173 yum -y install sqlite select * from cookies;结果一堆乱码，操 一堆乱码，唉 算了，不看了，可能是字符集要修改一下👆 1、2、3、4就是一个session1存入到浏览器上了，这里涉及负载均衡和身后的服务器nodes。session1是对应在特定的node节点的。 如果5请求过来，假设负载均衡没有做session保持，那么就有可能将请求路由到其他的node，而新的node上没有之前的信息，比如登入信息，购物车里的商品，这样原来的页面就没了。 解决误区-不做会话保持，为了保持会话，基于某个源IP-1就路由到特定node上，理论上OK的，因为该源IP都是和一个特定node进行通信的，所以会话一致都在--就是用cookies里存放session ID标记就行了；但是如果IP是PAT身后一堆PC呢。这样针对这一堆IP就全部负载分担到某一台node了吧，可能造成一台node负担重。 解决方法-基于sessionID,nodes1通过set-cookies打上sessionid1000给A，A就存入缓存。然后负载均衡就基于该session1000进行转发。 这种方式也存在某一个node承担太多的情况，也不能实现均衡。 解决方法-session复制,随便调度到哪个node都有会话同步，消耗内存大-因为整体上来看，session是每台机器都要保持。 最佳方案-session服务器，主流软件redis。用户访问网站了产生session了，session信息不放在web服务器上，统一放到redis服务上。因为session大家共用的，所以也无需上图复制。 redis特点：基于内存的，速度快；但是不适用于数据持久化，重启就没了，持久化还得靠数据库比如mysql。 比如用户登入密码，放到mysql里。 cookie是开发，java、PHP开发人员大概去具体实施的，动态页面才需要cookie吧，因为静态页面通常页面数据不大，在http的超时时间内拿掉就行了，不过太大最好也要cookie了吧。 动态页面需要更多的交互也就是会话。 JAVA里的sessionID：JSESSIONID PHP里的sessionID：PHPSESSID 浏览器如果禁用了cookies，就无法记录sessionID，就无法保持会话了，一些应用就失效了。 php配置cookie👇 vim /var/www/html/setcookie.php 安装php yum -y install php systemctl restart httpd # 安装完php后要重启httpd 差不多👆上图的时间确实就是从unix元年1970-1-1 00:00:00开始的，通过man date可见 下图是第二天上午敲得命令， 时间相差4个小时，差不多 在这纠结时间不如找到head头里有date字段的看看，验证下图 诺，真要掌握了so easy的时间OK的。 这样上图的1697584010就终于搞明白了， 至于下面的1697583766是request，client请求的时候的时间是client打上去的time()就是这个时候 而上面的1697584010就是server回应的response的time()打上去的时间是满了4分钟的样子，这个可以猜测很可能就是一个client的时间和linux服务器的时间的误差，哦否则c-s之间的请求/响应也就是1s钟了不得了，实验环境啊；所以此项细节研究到此为止，呵呵；不要在细节，要在意细节。话都是你们说的，但是事情原本是什么样的就该什么样的对不对~ 针对上图titlesb，修改一下配置文件 这下超时时间1小时就对了👇 注意上图Expires/Max-Age，一个是session也就是会话级的cookie，一个是1小时超时。 然后重新打开浏览器，但是不要再访问这个192.168.126.130页面 可见就也给cookie了不过这个cookie是啥，现在看不到了，以前浏览器版本点击去就有的，不过可以换浏览器看，或者这样看 cookies文件正在被使用，C:\\Users\\oneye\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Network 在这个路径下的Cookies文件咯，说被使用，其实就是chrome浏览在使用中，关闭浏览器就好了。 然后就看到浏览器里(仅显示1个Cookie-看不到内容)里面的内容了 7列13342160425029725 是expres_utc，通过date可得出，得出个鬼啊，你算算多大，呵呵 搞不懂，value TEXT竟然连sb字符都不显示的嘛，加上现在浏览器设置里都看不到cookie的具体值，只能在F12里看，是不是从安全出发做了限制了。 links和curl简单用法 有些网站不让看源码，其实下都下来了怎么会看不到呢 网站的文章不错，但是不让你复制粘贴可以这样 wget简单使用 wget 是-O 大的，可不带 ；curl 是-o 小的下载必带 如果页面是一个bash脚本，就可以这样写 优化输出 curl -s xxx |bash -s其实也是有std的，所以可以|管道符到bash 去运行。 还可以用wget的std打印和管道符的用法👇 不过还需要进一步优化-q静默一下 就干净了👆。 所以上面就是一个很好的，将脚本share出去的方法，比如shell/python就可以用这种方式，不过得有web服务咯。这种适合简单场景无需传递参数的情况。 很多这个安装脚本都是这样玩的，比如 玄学上网一键安装脚本(科学的尽头是玄学嘛，你懂的)。 不过我更多脚本share出去一般推荐这样，才可以传递参数👇，然后使用curl去和该api交互 curl -X POST -H \"Content-Type: application/json\" -H \"Data_Type:msg\" -d \"{\\\"name\\\": \\\"Alice\\\", \\\"ip\\\": \\\"130.1.1.11\\\"}\" -s http://192.168.11.77:5000/api | python -c \"import sys; print(sys.stdin.read().encode('utf-8').decode('unicode_escape'))\" 当然eip_rib_add是一个python脚本函数。再一个上图脚本后台运行就行了，nohup clixxx > /dev/null & 敲完后exit退出保证后台运行，或者screen -S xxx开启后台运行clixxx然后直接关闭该窗口就行了--具体用法参见本blog的screen章节。 wget也支持限速 --limit-rate= curl也支持很多协议不仅仅是HTTP https://curl.se/docs/tutorial.html user-agent伪装 修改user-agent这个翻译叫用户代理，其实就是使用的什么方式访问server的，包括浏览器，本质上就是个注释👇 这就比较多了👆，一般都是浏览器访问，user-agent里携带的都是浏览器的表示，不过Mozilla需要了解下 网景 Netscape 值得了解 伪装从哪个网站跳转过来的 这里就涉及字段解释，这个没记错就是在配置文件里有 上文又讲，这里略 然后basic好像加不加一样啊， 帮助信息里也讲了this option is usually pointless。 curl --basic其实是修改认证方式的👇 除了systemctl还有一个启动关闭服务的工具 http对访问日志的处理 日志越来越大，达到一定大小就切分，然后按序命名，然后再生成一个新的access.log文件。 而且默认就是有切分的 具体用法 https://www.apachehttpd.com/programs/rotatelogs.html # 破网站，换成下面的👇 https://httpd.apache.org/docs/2.4/programs/rotatelogs.html 配置到这里👇去： 修改报错记录 修改，\"|/usr/sbin/rotatelogs\" 最前面的|是将原来的log通过管道符传递过来，需要注释掉原来的CustomLog行 这配置是ok的啦，不过你别想看到效果， 不信你看下 如何才能看到呢，往下看咯，curl一下有新的日志生成才会有触发 然后要注意httpd里的默认时间是UTC的 需要修改offset 480就是480分钟，也就是GMT+8啦，然后5就是5秒中一次 生成日志--但是你真要是等5秒钟结果是没有新的文件的，必须是满足5s后且+有新的access日志产生。 http压力测试 弄一下Jmeter https://jmeter.apache.org/download_jmeter.cgi https://www.jianshu.com/p/6bc152ca6126 问题记录 1、JAVA安装后就可以打开jmeter.bat了 https://www.java.com/zh-CN/download/manual.jsp Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"29-HTTP协议和APACHE/10-httpd源码编译安装.html":{"url":"29-HTTP协议和APACHE/10-httpd源码编译安装.html","title":"第10节 httpd源码编译安装","keywords":"","body":"第10节. httpd源码编译安装 确实有：https://httpd.apache.org/test/flood/ 但是搜不到👇 APR是统一的调用OS的接口，便于开发。而apache也就是httpd软件就是基于APR开发的。 随着开发项目的越来越多，就需要APR这种接口、工具、或者引擎之类的专项小组，这样规模起来了，就需要统一接口开发小组来节省整体的开发成本。 其实软件是否基于什么软件通常都可以查的👇 讲这个就是要明白，编译安装的时候，要注意apache(httpd)的版本，和APR的版本时候适配。 一般yum安装的版本都相对较旧一点 官方有新的且稳定的版本，也不是就要安装最新的，要安装稳定的。判断依据有①该版本发布时间通常晚于最新1年？②和最新的隔1-2个版本。 理论上要先编译安装较新的apr和apr-util软件，再编译安装httpd的较新版本。 也可以把三个软件的源代码拷在一起，一起编译就行了。 方法一：逐个编译，这样就要注意依赖关系 比如👆apr-util依赖于apr，所以--with要知名apr的安装路径。 --with-apr和--with-apr-util就是依赖包的指定，这些依赖都是已经安装好的。 方法二：一起编译 --with-included-apr就表示httpd里的源码包里已经有了par的源码包了。 下面实验快开始，用rockyliux9.2弄得，思路还是和上面一样 1、找源码包 https://httpd.apache.org/ 2、找APR包 https://apr.apache.org/ 好像没有看到各个apache版本和apr的一个对应表，但是有推荐版本，所以猜测用这些就行了。也许人家软件向下兼容的。 3、下载 4、解压 安装bzip2 yum -y install bzip2 tar解压，通过history查看和awk过滤的比较优的方法如下： awk的思路就是：第二列是tar并且不包含字符*，然后去掉第一列序列号啦，再去掉行首行尾的空格。over~👇 [root@server httpd_new]# history |awk '$2==\"tar\" && $0!~\"*\" {$1=\"\";print $0}' |awk '$1=$1' |sort |uniq tar xvf apr-1.7.4.tar.bz2 tar xvf apr-util-1.6.3.tar.bz2 > /dev/null tar xvf httpd-2.4.58.tar.bz2 取history里的需要cli的方法①： 方法②： 合并一下cli 必要时可以去掉行尾 优化下 将两个awk合并一下 得到最屌的cli 再来一个更吊的cli 妈的我在干嘛...... 以上就得到了之运行的几个tar命令，(￣▽￣)\" 5、合并 把apr和apr-util的东西复制到httpd源码指定目录下。 cp -a apr-1.7.4 httpd-2.4.58/srclib/apr cp -a apr-util-1.6.3 httpd-2.4.58/srclib/apr-util 6、安装一些常规依赖包 yum -y install gcc pcre-devel openssl-devel expat-devel; yum -y groupinstall \"Development Tools\" 7、编译安装 cd httpd-2.4.58 ./configure \\ --prefix=/app/httpd24 \\ --enable-so \\ --enable-ssl \\ --enable-cgi \\ --enable-rewrite \\ --with-zlib \\ --with-pcre \\ --with-included-apr \\ --enable-modules=most \\ --enable-mpms-shared=all \\ --with-mpm=prefork make && make install 上图是找到cpu核数，然后并发编译的 编译的过程也会自动记录下来 8、进入编译指定安装路径下查看一下安装的情况，并添加PATH变量或做软连接 cd /app/httpd24 echo 'PATH=/app/httpd24/bin:$PATH' > /etc/profile.d/httpd.sh . /etc/profile.d/httpd.sh 或者用ln -s /app/httpd24/bin/apachectl /usr/local/sbin/ 但是要注意如果当前已经安装了httpd其他版本，还需要去掉其bin文件调用优先。 但是整个bin文件夹 链过去是没有用的，必须是二进制文件在$PATH下不能子目录。 awk的继续优化之去除前几列和空格的方法 ①软链接是文件还好👇 ②软链接是文件夹就一定要小心了 9、启动服务 apachectl start 但是查看不行， 通过修改systemctl的启动配置修正 👇这是yum安装的httpd的systemctl的配置，参考修改 修改之👇 systemctl的配置修改后需要reload加载一下👉：systemctl daemon-reload，再启动服务 死活起不来，这里不太好排查，但是ps aux可以看到 找到个这个，用的是daemon用户，而不是apache这个用户，不过也没关系，①有该daemon用户②httpd可以使用该用户执行 当服务启动的时候 status显示超时 继续排查通过错误日志可见， 可以看到 caught SIGWINCH，shutting down gracefully的情况。 改回apachectl start发现能起来，就是ss -tlnup 等3s就能看到80端口打开了，说明使用这种方式启动服务没问题。 然后去ps auxf看看 尝试将systemctl配置文件里的启动cli改成上图的试试 还是不行，算了换种方式吧，systemctl 有问题实在不行就换成脚本的方式👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:51 "},"30-实现LAMP架构/30-实现LAMP架构.html":{"url":"30-实现LAMP架构/30-实现LAMP架构.html","title":"第三十章 实现LAMP架构","keywords":"","body":"第三十章 实现LAMP架构 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"30-实现LAMP架构/1-实现LAMP应用phpmyadmin.html":{"url":"30-实现LAMP架构/1-实现LAMP应用phpmyadmin.html","title":"第1节 实现LAMP应用phpmyadmin","keywords":"","body":"第1节. 实现LAMP应用phpmyadmin Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"30-实现LAMP架构/2-LAMP应用部署.html":{"url":"30-实现LAMP架构/2-LAMP应用部署.html","title":"第2节 LAMP应用部署","keywords":"","body":"第2节. LAMP应用部署 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"30-实现LAMP架构/3-实现FCGI的LAMP架构.html":{"url":"30-实现LAMP架构/3-实现FCGI的LAMP架构.html","title":"第3节 实现FCGI的LAMP架构","keywords":"","body":"第3节. 实现FCGI的LAMP架构 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"30-实现LAMP架构/4-编译安装LAMP环境部署应用01.html":{"url":"30-实现LAMP架构/4-编译安装LAMP环境部署应用01.html","title":"第4节 编译安装LAMP环境部署应用01","keywords":"","body":"第4节. 编译安装LAMP环境部署应用01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"30-实现LAMP架构/5-编译安装LAMP环境部署应用02.html":{"url":"30-实现LAMP架构/5-编译安装LAMP环境部署应用02.html","title":"第5节 编译安装LAMP环境部署应用02","keywords":"","body":"第5节. 编译安装LAMP环境部署应用02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"31-日志管理rsyslog/31-日志管理rsyslog.html":{"url":"31-日志管理rsyslog/31-日志管理rsyslog.html","title":"第三十一章 日志管理rsyslog","keywords":"","body":"第三十一章 日志管理rsyslog Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"31-日志管理rsyslog/1-日志管理实现远程日志.html":{"url":"31-日志管理rsyslog/1-日志管理实现远程日志.html","title":"第1节 日志管理实现远程日志","keywords":"","body":"第1节. 日志管理实现远程日志 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"31-日志管理rsyslog/2-通过loganalyzer展示数据库中的日志.html":{"url":"31-日志管理rsyslog/2-通过loganalyzer展示数据库中的日志.html","title":"第2节 通过loganalyzer展示数据库中的日志","keywords":"","body":"第2节. 通过loganalyzer展示数据库中的日志 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/32-网络文件共享服务.html":{"url":"32-网络文件共享服务/32-网络文件共享服务.html","title":"第三十二章 网络文件共享服务","keywords":"","body":"第三十二章 网络文件共享服务 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/1-文件共享服务FTP01.html":{"url":"32-网络文件共享服务/1-文件共享服务FTP01.html","title":"第1节 文件共享服务FTP01","keywords":"","body":"第1节. 文件共享服务FTP01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/2-文件共享服务FTP02.html":{"url":"32-网络文件共享服务/2-文件共享服务FTP02.html","title":"第2节 文件共享服务FTP02","keywords":"","body":"第2节. 文件共享服务FTP02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/3-FTP常见配置和基于DB文件的虚拟用户.html":{"url":"32-网络文件共享服务/3-FTP常见配置和基于DB文件的虚拟用户.html","title":"第3节 FTP常见配置和基于DB文件的虚拟用户","keywords":"","body":"第3节. FTP常见配置和基于DB文件的虚拟用户 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/4-基于mysql的FTP的虚拟用户和NFS服务介绍.html":{"url":"32-网络文件共享服务/4-基于mysql的FTP的虚拟用户和NFS服务介绍.html","title":"第4节 基于mysql的FTP的虚拟用户和NFS服务介绍","keywords":"","body":"第4节. 基于mysql的FTP的虚拟用户和NFS服务介绍 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/5-基于NFS共享服务器.html":{"url":"32-网络文件共享服务/5-基于NFS共享服务器.html","title":"第5节 基于NFS共享服务器","keywords":"","body":"第5节. 基于NFS共享服务器 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/6-NFS网络共享和自动挂载.html":{"url":"32-网络文件共享服务/6-NFS网络共享和自动挂载.html","title":"第6节 NFS网络共享和自动挂载","keywords":"","body":"第6节. NFS网络共享和自动挂载 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/7-SAMBA共享服务实现01.html":{"url":"32-网络文件共享服务/7-SAMBA共享服务实现01.html","title":"第7节 SAMBA共享服务实现01","keywords":"","body":"第7节. SAMBA共享服务实现01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/8-SAMBA共享服务实现02.html":{"url":"32-网络文件共享服务/8-SAMBA共享服务实现02.html","title":"第8节 SAMBA共享服务实现02","keywords":"","body":"第8节. SAMBA共享服务实现02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"32-网络文件共享服务/9-利用infotify和rsync服务实现实时同步.html":{"url":"32-网络文件共享服务/9-利用infotify和rsync服务实现实时同步.html","title":"第9节 利用infotify和rsync服务实现实时同步","keywords":"","body":"第9节. 利用infotify和rsync服务实现实时同步 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/33-linux防火墙.html":{"url":"33-linux防火墙/33-linux防火墙.html","title":"第三十三章 linux防火墙","keywords":"","body":"第三十三章 linux防火墙 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/1-linux防火墙基础.html":{"url":"33-linux防火墙/1-linux防火墙基础.html","title":"第1节 linux防火墙基础","keywords":"","body":"第1节. linux防火墙基础 工作案例-ftp 开发内网 ftp server，办公外网ftp client 此时要知道如下概念 1、ftp 主动被动，就是第二通道里站在server角度说的，server主动20去连client，和server第一通道里告知了随机端口，然后第二通道server被连到这个随机端口。 2、ftp 如果over tls 加密字段范围 3、一般作为硬件防火墙SSG比如，不管client 是主动还是被动 访问server、ssg只需要放行 c--到--s的 21端口就行了。 会自动inpsect 第一通道里的数据(ftp不能加密)，找到主动的20-链接的pc的随机端口；找到被动client--链接--server的server随机端口。 实操 一句话总结：就是FTP要明文，policy是需要放开21，其他理解下就行了。 ftpclient软件一般默认就是走的PASV被动模式，也就是第二通道， 1、中间的硬件防火墙比如ssg要看得到的--也就是FTP要明文，否则无法自动给你放行 2、SSG看到了不管是 被动模式访问server的随机端口，还是主动模式server20访问出去的随机目标段端口，只要SSG看到了，都会自动给你放行的，不用管。 3、只不过一般主动，还需要放行PC 这个clinet的windows的防火墙的进来的流量。 同样改为主动，ssg也会默认给你放开内网server:20----> 外网PC 随机端口的策略的，不过PC的windows防火墙要关掉。 如果加密，TCP以上都是加密的 不加密，就是tcp以上不加密，才能看到 被动也好，主动也罢的 随机端口。 被动的包看看顺便👇 主动模式的包随便看看👇 当然这个是windows的防火墙没放行47532 放开就好了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-10-25 10:20:05 "},"33-linux防火墙/2-iptables防火墙实战.html":{"url":"33-linux防火墙/2-iptables防火墙实战.html","title":"第2节 iptables防火墙实战","keywords":"","body":"第2节. iptables防火墙实战 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/3-iptables防火墙扩展模块实战.html":{"url":"33-linux防火墙/3-iptables防火墙扩展模块实战.html","title":"第3节 iptables防火墙扩展模块实战","keywords":"","body":"第3节. iptables防火墙扩展模块实战 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/4-iptables防火墙扩展模块实战和策略优化.html":{"url":"33-linux防火墙/4-iptables防火墙扩展模块实战和策略优化.html","title":"第4节 iptables防火墙扩展模块实战和策略优化","keywords":"","body":"第4节. iptables防火墙扩展模块实战和策略优化 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/5-网络防火墙和SNAT原理.html":{"url":"33-linux防火墙/5-网络防火墙和SNAT原理.html","title":"第5节 网络防火墙和SNAT原理","keywords":"","body":"第5节. 网络防火墙和SNAT原理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/6-网络防火墙DNAT实战和端口重定向.html":{"url":"33-linux防火墙/6-网络防火墙DNAT实战和端口重定向.html","title":"第6节 网络防火墙DNAT实战和端口重定向","keywords":"","body":"第6节. 网络防火墙DNAT实战和端口重定向 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/7-firewalld新式防火墙实现.html":{"url":"33-linux防火墙/7-firewalld新式防火墙实现.html","title":"第7节 firewalld新式防火墙实现","keywords":"","body":"第7节. firewalld新式防火墙实现 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"33-linux防火墙/8-firewalld实现防火墙功能.html":{"url":"33-linux防火墙/8-firewalld实现防火墙功能.html","title":"第8节 firewalld实现防火墙功能","keywords":"","body":"第8节. firewalld实现防火墙功能 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/34-LinuxVirtualServer.html":{"url":"34-LinuxVirtualServer/34-LinuxVirtualServer.html","title":"第三十四章 LinuxVirtualServer","keywords":"","body":"第三十四章 LinuxVirtualServer Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/1-LVS工作原理和NAT模型.html":{"url":"34-LinuxVirtualServer/1-LVS工作原理和NAT模型.html","title":"第1节 LVS工作原理和NAT模型","keywords":"","body":"第1节. LVS工作原理和NAT模型 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/2-LVS工作原理DR等三种模型.html":{"url":"34-LinuxVirtualServer/2-LVS工作原理DR等三种模型.html","title":"第2节 LVS工作原理DR等三种模型","keywords":"","body":"第2节. LVS工作原理DR等三种模型 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/3-LVS和NAT模型实现.html":{"url":"34-LinuxVirtualServer/3-LVS和NAT模型实现.html","title":"第3节 LVS和NAT模型实现","keywords":"","body":"第3节. LVS和NAT模型实现 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/4-LVS的DR模型实现.html":{"url":"34-LinuxVirtualServer/4-LVS的DR模型实现.html","title":"第4节 LVS的DR模型实现","keywords":"","body":"第4节. LVS的DR模型实现 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/5-LVS跨网段DR模型和FWM多服务绑定.html":{"url":"34-LinuxVirtualServer/5-LVS跨网段DR模型和FWM多服务绑定.html","title":"第5节 LVS跨网段DR模型和FWM多服务绑定","keywords":"","body":"第5节. LVS跨网段DR模型和FWM多服务绑定 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/6-LVS实现健康性检查功能.html":{"url":"34-LinuxVirtualServer/6-LVS实现健康性检查功能.html","title":"第6节 LVS实现健康性检查功能","keywords":"","body":"第6节. LVS实现健康性检查功能 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"34-LinuxVirtualServer/7-keepalived实现高可用性原理介绍.html":{"url":"34-LinuxVirtualServer/7-keepalived实现高可用性原理介绍.html","title":"第7节 keepalived实现高可用性原理介绍","keywords":"","body":"第7节. keepalived实现高可用性原理介绍 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"35-高可用集群keepalived/35-高可用集群keepalived.html":{"url":"35-高可用集群keepalived/35-高可用集群keepalived.html","title":"第三十五章 高可用集群keepalived","keywords":"","body":"第三十五章 高可用集群keepalived Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"35-高可用集群keepalived/1-keepalived实现浮动的VIP.html":{"url":"35-高可用集群keepalived/1-keepalived实现浮动的VIP.html","title":"第1节 keepalived实现浮动的VIP","keywords":"","body":"第1节. keepalived实现浮动的VIP Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"35-高可用集群keepalived/2-keepalive实现LVS的高可用性.html":{"url":"35-高可用集群keepalived/2-keepalive实现LVS的高可用性.html","title":"第2节 keepalive实现LVS的高可用性","keywords":"","body":"第2节. keepalive实现LVS的高可用性 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"36-高性能WEB服务nginx01/36-高性能WEB服务nginx01.html":{"url":"36-高性能WEB服务nginx01/36-高性能WEB服务nginx01.html","title":"第三十六章 高性能WEB服务nginx01","keywords":"","body":"第三十六章 高性能WEB服务nginx01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:56 "},"36-高性能WEB服务nginx01/1-IO五种模型和select与epoll工作原理.html":{"url":"36-高性能WEB服务nginx01/1-IO五种模型和select与epoll工作原理.html","title":"第1节 IO五种模型和select与epoll工作原理","keywords":"","body":"第1节. IO五种模型和select与epoll工作原理 apache用的模型，nginx都没用 不管多少核，如果负载超过5就算高了--一般是这么认为？ 再次了解下PIO和DMA，也就是CPU不再参与大量I/O的处理。 PIP模型-数据复制是走CPU的，也就是CPU参与I/O DMA模型：就是CPU只发送指令控制命令--比如disk复制数据到MEM里，执行者是DMA专用硬件(正在干活的设备)。真正的数据是不走CPU走的。 所以现在基本上，CPU都是不参与I/O。 图中圈圈就是DMA来完成copy的。硬件之间的一个数据传递。 3.copy从内核到用户空间就不是DMA来实现的，而是CPU完成的，从整个角度来讲CPU其实是参与I/O了咯。不对，都在整个大的内存空间里，DMA不参与，然后CPU参与的其实也不算I/O吧应该是。 ①I/O是DMA完成，CPU就是下发一个控制指令 ②内核和用户空间的数据交互，是CPU完成的 ③I/O，是外界设备，比如网卡、磁盘，这些和内存里的内核空间交互 ④内存里有内核空间和用户空间 ⑤内核里还有socket Buffer和 内核Buffer ⑥网卡将数据送给内核buffer ⑦socket buffer将数据送给网卡 以上是一些关键点，然后再结合看下👇图 看图说话： 1、数据发起：client 网络请求发给网卡 2、数据流转：网卡将数据复制到内存里的内核缓冲区-DMA技术 3、数据流转：内核缓冲区将数据复制到用户空间-CPU转发，用户空间将数据交给应用程序 4、数据处理：应用程序开始处理数据，根据HTTP的请求，比如GET，去所需磁盘上拿数据则👇 5、数据处理：应用程序不能直接和硬盘打交道，通过system call调用内核去访问磁盘上的数据 6、数据处理：数据从磁盘复制到内核缓冲区-DMA技术，再复制到用户空间交还给应用程序-CPU转发。 7、数据处理：应用程序构建HTTP响应报文，head+DATA之类， 8、数据流转：构建完毕后数据复制到内存里的socket buffer中。 9、数据流转：socket buffer再将数据复制到网卡中 10、数据流转：网卡再将数据发给用户。 什么，我上面有些不应叫数据处理，应该叫流传，好的，你赢了。 11、数据流转：sendFile技术减少了一部分的交换过程，好像有的吹牛逼叫 \"零复制\" 技术，其实还有一个内核缓冲区->复制到->socket buffer里的一个复制过程。 12、可能nginx还有其他 \"零复制\"技术，不仅仅是sendFile技术。 I/O表现为：磁盘I/O和网络I/O，都表现为文件的读取，磁盘I/O就是表现为磁盘上的文件好理解；而网络I/O其实就是socket文件的读取。 I/O: 网络IO：本质是socket文件读取 磁盘IO： 每次IO，都要经由两个阶段： 第一步，DMA完成转发：将数据从文件先加载至内核内存空间（缓冲区），等待数据准备完 成，时间较长，因为要读磁盘，要寻址的。 第二步，CPU完成转发的都在内存中交互的：将数据从内核缓冲区复制到用户空间的进程的内存中，时间较短 I/O模型 要了解I/O模型-据说很复杂的-内核来实现的，就要先了解两个概念①同步/异步②阻塞/非阻塞 同步/异步： 1、这个在mysql里说的是另一回事，是主从复制的同步/半同步/异步。 2、在数据传输的时候，说的是类似军队过桥，步伐整齐划一-危险；和乱步走-安全。落实到数据传输就是 数据传输的同步和异步是两种不同的方式。同步传输是指发送端和接收端的时钟同步，数据按照一定速率和时序进行传输，保证数据的顺序和完整性。异步传输则是指没有时钟同步，数据按照不规则的速率和时序传输，需要使用特殊的控制字符来标识数据的开头和结尾。 ​ 然后L2帧结构就有前导码，这个就是数据传输里的异步模式 3、在I/O里的同步异步，上图可能讲的不太到位。同步：调用者等待被调用者的返回结果，才会继续执行原本程序的后面内容；异步：调用者不等待被调用者的返回结果就去执行原本程序的后面内容，等被调用者什么时候通知调用者，那个时候再处理这个返回结果。一般来讲要和阻塞和非阻塞联系起来讲的，不过...继续往下学吧。 ​ 上图PPT应该是对的，同步和异步就指的是消息通信机制，不要和阻塞和非阻塞联系起来，要联系，但是分开来理解，也要联系起来理解。 ​ 同步：就是调用者--一般就是程序本身啦，要等到被调用者--一般就是函数，的数据结果。是直接坐等DATA数据结果。是你等到包裹投递到手上 。 同步+非阻塞 就是 等-不断去问，不还是啥也干不了吗，动不动就去问，能干别的事？还不是等价于同步+阻塞了嘛。 ​ 也可以理解成：是你在接空中的泡泡落到手上。①手一直摆着接的姿势，此乃阻塞②手会放下，一会抬头看下有泡泡来了就抬手接，所以抬头频率满了会借不到，抬头频率快了也不好，其他事就干不了了，此乃非阻塞。 ​ 异步：调用者不是等被调用者的返回数据结果，而是被调用者主动告知调用者结果来了。是被调用者喊过去拿到DATA数据结果。是你等到了门卫喊你取快递。 同样分为异步+阻塞 ：有人来喊你取快递你还在那边干等着 和 异步+非阻塞：有人喊你取快递，你去忙别的。 ​ 所以有没有喊你这个动作就是同步/异步的区分。正是因为同步/异步的情况才需要阻塞和非阻塞的存在。 ​ 调用者-应用程序；被调用者-内核。阻塞和非阻塞其实就是调用者还是被调用者谁多干活的问题，谁更忙。你希望谁更忙？在nginx服务中，nginx应用程序和内核，谁更忙好一些，nginx提供服务，使用非阻塞将活交给内核处理，自己本身可以面向更多的用户提供服务，支持更多的人高并发访问。 4、其实还有mysql的logbuffer的机制innodb_flush_log_at_trx_commit，也可以理解成同步机制，同步动作存在于内存(logbuffer/osCache/)和磁盘(logFile)。 5、其实还有ext3日志的写法也是同步概念，同步动作存在于log文件和磁盘文件。 但是4、5不要真的划进去，否则概念界限不清混为一谈反而不美。 阻塞I/O模型 思考一个问题：apache为什么解决不了C10K的问题？而nginx可以，这些就涉及底层的工作原理。 recvForm所属的位置：就是一种SYSTEMCALL，通过调用内核来对接Ip+port套接字，来接收数据的，也是应用程序面向身后内核缓冲区进而取数据的接口。 同步非阻塞I/O模型 非阻塞I/O模型其实就是，同步+非阻塞的方式，简单讲就是，调用者不断去问被调用者 如上图👆 I/O多路复用模型 应用程序调用select函数(这是系统调用)，很多访问请求过来，应用程并发调用多个select函数，通过这个函数进而访问内核。 应用程序接收很多访问请求进来，上图app1\\app2\\app3也许理解为一个app的多个并发请求为好？ 应用程序要处理请求，要回包就要构建响应报文，要构建，就要获取响应数据，要获取响应数据就要到磁盘上取，而app不能直接访问磁盘这些硬件，需要通过系统调用systemcall去访问磁盘 这里通过select这个系统调用函数去访问内核，通过内核去访问磁盘，取到数据，当数据取到后就返回\"可读条件\"信号给到recvfrom函数，该函数是面向ip+port用户socket的，它就会将数据从内核空间里复制到用户空间完成响应报文构建进而回给 内核里的socket buffer，再有buffer交给网卡。 返回可读条件 说明有提醒消息有人喊你取快递的，就是异步了。但是该模型受限于select IO多路复用（IO Multiplexing) ：是一种机制，程序注册一组socket文件描述符 给操作系统，表示“我要监视这些fd是否有IO事件发生，有了就告诉程序处理” IO多路复用是要和NIO(Not Blocking IO)一起使用的。NIO和IO多路复用是相对独立的。NIO仅仅 是指IO API总是能立刻返回，不会被Blocking；而IO多路复用仅仅是操作系统提 供的一种便利的通知机制(就是异步了)。操作系统并不会强制这俩必须得一起用，可以只用IO 多路复用 + BIO，这时还是当前线程被卡住。IO多路复用和NIO是要配合一起使 用才有实际意义 # 理解这段话，抓住IO多路复用就是有通知就是异步，BIO和NIO就是程序调用函数后阻塞在那。 IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，就通知该进程。 # 这不就是异步咯，只不过就是优化升级的异步通知。 多个连接共用一个等待机制，本模型(#这里应该加一个IO多路复用+NIO)会阻塞进程，但是进程是阻塞在select或者poll这两个系统调用上，而不是阻塞在真正的IO操作上。 用户首先将需要进行IO操作添加到select中，同时等待select系统调用返回。当数据到达时，IO被激活，select函数返回。用户线程正式发起read请求，读取数据 并继续执行 阻塞发生点：select调用、poll调用、数据从内核复制到用户空间的过程 信号驱动I/O模型 异步I/O模型 这个就是全程不阻塞了。 总结：五种I/O模型 wait for data👇① 和 copy data 👇② I/O模型的具体实现 说apache并发1W差不多了，为什么达不到C10K，而nginx可以2-3W都没问题，可以就可以在select、poll、epool三种方式上。 遍历就是 数组(linux里数组是列表/字典的统一称呼，列表其实就是下表是1 2 3...的字典） 的一个遍历。 IO效率，拿select举例，就是select遍历操作的时间复杂度O(n)--随着资源越多性能越差，是线性规律。 epoll不是遍历方式，而是回调。IO效率不变都是O(1)。 select存在最大连接数：X86是1024 和 X64是2048 ，这是内核里的自带的FD_SETSIZE就是一个进程最多打开的文件数目，如何在内核文件里找到这个FD_SETSIZE这个文件 wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.5.8.tar.xz tar xvf linux-6.5.8.tar.xz > /dev/null cd linux-6.5.8 find linux-x.x.x/ -type f |xargs grep \"FD_SETSIZE\" epoll最优秀，nginx用的就epoll 水平触发LT、边缘触发ET 如果一个文件描述符fd变成就绪态了--比如上面有数据了-比如socket收到一个网络请求这就是fd文件里有数据了。此时就会告知进程哪些fd刚刚变成了就绪态，ET边缘触发就是通知一次；LT水平触发就是通知多次。 LT和ET都是IO多路复用的两种模式，LT模式下，当一个文件描述符就绪时会一直通知进程，直到该文件描述符被处理；ET模式下，当一个文件描述符就绪时只会通知一次，需要进程自己保证及时处理。LT模式下可能会导致进程不断地被唤醒，而ET模式则可以减少系统调用次数，提高效率。 零拷贝 mmap技术，是epoll的技术，一般也可以将epoll理解成select和poll的增强版。 mmap也算是一种 零拷贝 技术，之前学过一个sendfile技术。 原始数据复制操作 图中copy过程是DMA完成的👇也不是，是copy下半截-从kernel到hardware才是DMA的活，上半截从用户空间到kernel是CPU的活。当然下半截CPU也是发送了控制指令的，干活还是DMA来做的。 有个问题，之前学到就是网卡将数据复制道Kernel缓存里；socket缓存将数据复制到网卡里。不知道是不是这样的。。。所以上图的箭头都是单向的。还需要补上磁盘文件活网络适配器到kernel缓存的箭头咯。 会导致，数据包在用户态和内核态 频繁复制， SENDFILE也是一种零复制技术 apache和nginx都支持该技术👇 但是有些场景下sendfile是不推荐的，是不合理的，比如上图的磁盘文件不是本地磁盘，而是NFS挂载过来的，此时就不要用sendfile技术。 NFS挂载过来本质上还是走的网络，按这个说法，不就是一样可以用sendfile嘛，还是说要走过用户空间过一遍NFS协议才能解开数据包？可能是的，也许是这个原因就不能用sendfile了。 MMAP：Memory Mapping也是零复制技术 映射过来，省去了内核空间和用户空间的数据复制，但是我觉得只是不走文件复制，物理层面还是要走用户空间和内核空间之间的连接的吧，只不过这个连接在都在整个内存里，具体的表现形式是什么？ 上图的CPU-COPY是从内核缓存复制到socket缓存，这里将的复制可能正如GPT所说不是文件系统层面的复制，所以要节省很多开销。 DMA辅助的SENDFILE 因为是内存和硬件网卡的交互，所以是DMA来sendfile了。 然后这种技术是需要硬件支持的，不具备通用性可能。 mmap和sendfile是软件级别的，更加具备通用性。 总结，nginx采用的是epoll，apache使用的是select，所以性能nginx更好，不过要去官网求证一下 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:55 "},"36-高性能WEB服务nginx01/2-nginx编译安装和功能介绍.html":{"url":"36-高性能WEB服务nginx01/2-nginx编译安装和功能介绍.html","title":"第2节 nginx编译安装和功能介绍","keywords":"","body":"第2节. nginx编译安装和功能介绍 nginx介绍 http://nginx.org UDP的反代貌似可以用来做内网的DNS负载。 1、场景：用户从两个出口出去，奇数、偶数，分配到不同的出口。但存在奇数和偶数出口对调的情形。 2、DNS：DNS1和DNS2自然也是奇数偶数配置，就等价于两个线路的用户，自然也是和用户一致。 3、1 和 2 就保证了奇数用户有一个DNS1奇数的IP；偶数用户就有一个偶数的DNS2。 4、此时真正需要干的活就只有(上面的都不需要做啥，默认就有了顶多配置飞塔的ECMP基于源IP出去，和两个DNS服务器)；就只有将奇数用户的DNS请求转发到DNS1，将偶数DNS的请求转发到DNS2 5、此类需求之前通过BIND可以实现，但是BIND的负载LB实在太LOW了--因为不支持reg正则表达式，所以无法支持辣么多的IP的奇偶区分，纯手工嘛！我出家吧~哈哈，所以这个需求是否在nginx这里能够得到圆满的解决呢，拭目以待... nginx的server字段里竟然不是nginx，volt-adc又是啥？nginx.org看看 http://tengine.taobao.org/ http://openresty.org/ 都是自家用自家的啊，哈哈 据说追求极致性能的网站可能用这个openresy软件，官网的性能测试可以达到2W。 事件驱动event-driven、异步IO(aio)、mmap和sendfile apache那会也学习过事件驱动模型，就是多进程里开多线程，线程里有监管和工作线程。 上一篇讲nginx用的是epoll--具有信号驱动I/O模型的一些特点，然后这里又提到aio(异步IO)，问问GPT：👇 模块化设计，较好的扩展性 一般就是编译的时候做加减，编译好以后，不对啊，apache好像是可以灵活安装的。当然具体安装可不是简单地在配置文件里补一行就行了的，卸载模块倒是可以注释掉。 模块如果已经安装，那么加载/卸载就可以👆去到配置文件里打开/注释。 模块如果没有安装，就需要安装的 支持热部署：不停机更新配置文件、升级版本、更换日志文件 nginx软件升级的时候，用户连接挂着老版本，等用户所有连接访问自然结束自行断开后，就会去连新版本进行业务访问了，所有用户都切到新版本后，老版本的进程就销毁了。 ​ 是不是可以换个说法👇 nginx软件升级的时候，存在新旧两个版本同时对外服务、以及很多当前的\"连接\"和请求。连接挂着旧版本的不受影响，新请求则使用新版本去服务。等所有\"旧版本连接\"也就是用户访问结束后，新的连接就转到新版本上去了。等所有旧版本的连接都断开了，就销毁旧版本的进程了。 # 这就是平滑升级了，也可以算到业务层面的一个平滑切换。 # 平滑机1 如果又HA切换的内容就打上 平滑机标签 那会不会存在 用户A的一个浏览器里多个页面访问着新旧两个版本的nginx呢，或者一个页面先后访问了两个版本的nginx呢，其实这个可以测试的后面学到了通过实际测试可以看到，也要通过curl -I xxx去看一下。 不过实际工作中好像是这种，LB后面多个的机器，然后将一台机器下线(其上用户就断开重连并调度到其他机器上去)；然后对其升级，升级后再加入调度，观察用户； 然后就如法炮制一台台升级，仅升级一部分比如说一半的机器，继续观察业务，稳定后全部升级。 # 平滑机2 低内存消耗： 10000个keep-alive连接模式下的非活动连接，仅需2.5M内存。 非活动连接：就是连上来了，但是没有数据传输。这个类似AP可以带多少个用户一样，256个其实是非活动用户。 LVS调度算法里有一种计算最短连接，非活动连接/活动连接 比例达到256。也就是一个活动连接对资源的消耗相当于256个非活动连接。这个点我后面整理LVS的时候注意下我是从apache跳过LVS直接看nginx的。 所以nginx可以用epoll来维持上万个连接--非活动连接？，有数据请求过来了--有数据传递了？-从活动连接变成非活动连接？，epoll才会激活服务。 # 其实就是apache里学的mpm event模型类似吧。 nginx架构 一个主进程+若干个工作进程 一个worker进程，利用epoll监控上万的连接 worker可以对接支持：web服务器，应用程序服务(比如PHP FPM 独立服务监听9000端口)，还支持memcached（提供会话信息，类似redis）,但是没有提到对接redis，将来对接redis如果不能直接对接，也可以通过应用程序来间接对接。比如PHP这个应用程序里面装redis插件模块。session和应用程序相关，让应用程序对接就行了。 作为nginx来讲不需要太多关注应用程序来做的事，apache可能会多做一点类似应用程序要做的事。 nginx通过fastCGI来对接应用程序，而应用程序再去对接session的问题。 proxyCache，就是说nginx支持反向代理的同时也支持缓存。不需要跑到后端服务器上拿数据，直接交给用户就行了。 master管理workers的，load conf，激活woker，no-top update不间断升级。 master复制监听端口，监管worker进程，worker进程负责处理用户请求。woker进程是以普通用户的身份来启动的。master进程应该是以root身份启动的。这一点和apache一样 好像有个一个说法：只有root超级用户才能创建1024以内的监听端口，好像我之前验证过这个说法在rocky-linux里不存在了。 不支持DSO就要编译 worker一般随CPU内核数，几个cpu就配置几个worker。 nginx模块从1.9.11开始支持DSO，分为 安装自带的模型模块、标准模块-人工加载的、和三方模块 标准模块也分为 HTTP、邮件、stream模块 stream是TCP的反代，一般来讲HAproxy和LVS在这方面用的多一些，nginx后来才支持，应该也还行的。 可能nginx用的最多就是http的反代、fastCGI的反代较多一些。 说是重点掌握前三个，其实tcp/udp对我来讲才是重点，或者叫情怀，反正比iptables DNAT用起来更舒服，也更加适合给应用运维用或者和他们沟通。还据：说什么LVS做TCP/UDP代理性能更好，HAPROXY做TCP/UDP代理功能更强。好吧~ fastCGI桥接PHP等众多语言的APP、uWSGI是python做的网站(比如django做的网站、flask做的网站)、 https://zhuanlan.zhihu.com/p/354037327 👈简单了解下 WSGI 的官方定义是，the Python Web Server Gateway Interface 源码包； 预编译包-就是yum类的安装 上图的baseurl可以尝试打开看看的，$releasever 就是自己的centos版本，我的是rocky-linux就用centos就行了，然后$basearch就是x86_64架构，也可以缩到前面看整体的 版本也是对的 关掉最新的nginx.repo源，再info看下 可见nginx是在epel源里的，然后是版本也还行。 也可以编译安装，这个用的也很多 简单了解下👇 编译安装实例：👇，与上面的略有区别什么log估计用的就是默认的路径，而模块多了一些。 useradd -r -s /sbin/nologin nginx ./configure --prefix=/apps/nginx \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-pcre \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module make && make install chown nginx.nginx -R /apps/nginx/ 我这边就用官方的yum源安装1.24.0的了，老规矩yum 后rpm看看 nginx不像apache一样，rpm -ql一看一堆模块，nginx的模块都是直接打包到nginx主程序里了。 虽然也有modules文件夹，但是其实是空的 那么问题来了，DSO这种从1.9.11开始支持的模块都在哪里编辑，从而支持安装卸载的呢？可能就是这里，只不过这里目前是空的而已。 systemctl start nginx后(要保证80没有被占用)，修改一下主页面的内容，无需重启，无需reload， 据说高手不用systemctl启动，哦，牛逼，高手一般怎么弄啊，高手用主程序二进制启动👇恩，真牛逼，受不了。 而且nginx这个启动后默认就是后台启动的。 nginx源码编译的时候到底加了哪些选项或者模块 你yum也是yum的人家源码编译好的啊，所该段标题描述没毛病。 nginx和httpd检测配置文件的语法 所以好习惯就是，①先检查(nginx -t检查，人眼检查)②再重启服务，你要知道原来服务启动的着，你重启如果起不来影响就大了，原来只是重启一下也就是1秒不到的时间，现在就不止咯。 nginx -T应该是整合所有的conf文件并检查配置，不仅仅是主配置文件里的内容了 差了8行，哈哈，减去2行测试，就是6行，没找到~ 找到了，就是nginx -T的两个作用①除了测试所有配置文件以外和-t小t一样的作用②还可以合并所有配置文件，并且在模块开头会注释写明调用的哪个模块，一共6行咯。 源码编译安装 也用稳定版 curl -O https://nginx.org/download/nginx-1.24.0.tar.gz tar -xvf nginx-1.24.0.tar.gz yum install gcc pcre-devel openssl-devel zlib-devel useradd -r -s /sbin/nologin nginx # 这里有个说法：就是最好统一id，便于管理，比如用户通过nginx上传资源到后端NFS服务器，如果ID不统一，可能权限出问题？不懂！估计具体问题遇到再看了，emmm。。。 cd nginx-1.24.0 ./configure --prefix=/apps/nginx \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-pcre \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module echo $? # 看下是不是0也就是true，表示configure OK make && make install # make -j xcpu个数x && make install echo $? chown nginx.nginx -R /apps/nginx/ # 这条不用加，默认编译安装后自动修改所有者的，除非全是root。 ll /apps/nginx/ # 看看 tree /apps/nginx/ ln -s /apps/sbin/nginx /usr/sbin/nginx #就一个二进制文件，所以直接软连接就行了 vim /etc/rc.d/rc.local /usr/sbin/nginx chmod +x /etc/rc.d/rc.local 就一个二进制文件，没必要加PATH变量了，如果是一堆bin文件可以添加PATH，如果是一个就ln -s xx就行了 开机启动👇简单处理，如果要用service可以复制一个yum安装的service文件改改。 修改主页面 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:56 "},"36-高性能WEB服务nginx01/3-nginx全局配置和性能优化.html":{"url":"36-高性能WEB服务nginx01/3-nginx全局配置和性能优化.html","title":"第3节 nginx全局配置和性能优化","keywords":"","body":"第3节. nginx全局配置和性能优化 nginx的配置文件 nginx的配置 上面一段是全局配置，包括，nginx软件以谁的身份来运行，worker进程是几个， 下面的http打头的段，是泛指http，包括http，已经fastCGI-这个其实不是http但是是web服务也和http相关，所以也放在这里；但是后面做tcp的反代后单独块了不在http块里了。 然后语句里面还有嵌套，就是类似字典里还有字典。不同的指令只能出现在不同的语句块中。 比如上图👆的sendfile on，能够出现在什么语句块里，就需要通过官网来了解 然后配置文件，好习惯其实就是规范配置，是一个网站一个配置文件独立存放的，不是堆在一个主配置文件里的。 yum 安装的 nginx的主配置文件还是有点东西的👇 编译安装的主配置文件好像少了的意思👇 cd /apps/nginx/conf mkdir conf.d 稍后配置文件都放到这个conf.d里 总结PPT ​ 看上图👆default默认user就是叫nobody的这个用户，结果编译安装的时候ps auxf看到的是nginx用户，原因就是编译的时候指定了用户了 看看yum安装的 看看PID的路径 yum 安装的 我在想，编译安装的是不是都写到conf里了，应该不是，否则conf不可能那么短的 然后就是conf应该优先于nginx -V看到的配置选项咯，实验过了，是的。 include包含的文件 load_module 模块加载文件 下面看看和性能相关的配置 worker进程数一般推荐和cpu核数一样，不过这也是默认的行为 auto意味着自动根据CPU核数来配置 实验的时候：新增CPU后，保证worker_processes为auto后，nginx -s reload如果nginx的worker进程数没有上来，就需要nginx -s stop 和 nginx，重启生效了。 CPU有L1 L2 L3三级缓存 lscpu可见，L3缓存时所有CPU共享的，L1和L2是针对一个CPU来讲的。 L1有指令缓存和数据缓存 看下跑在哪个CPU上，也就是nginx应用程序和CPU的绑定关系 先看下前面的内容，反正学习的过程中如果存疑，①搜前文②GPT③谷歌 前文关于L1 L2 L3 的说明和如何查看和绑定进程到CPU，当然nginx应该有自己的方式 继续看nginx的worker进程和cpu的关系，然后进一步做cpu绑定。为什么要绑，也不是一定要绑，就是绑了以后L1 L2这两个缓存本来就是CPU专用的，CPU一飘，L1L2的缓存内容就用不了了，所以从找个角度考虑，是需要绑的。 ps axo pid,cmd,psr 目前四个worker运行的CPU情况如下👇 使用ab并发请求，观察是否存在cpu飘的情况：注意ab测试的url要带上http以及最后的/斜杠。 果然cpu开始飘了 下面就绑一下worker进程到各个cpu吧 worker_cpu_affinity 00000001 00000010 00000100 00001000 # 假设有8个cpu，4个worker就这么些 worker_cpu_affinity 0101 1010 # 就是worker2个进程，worker1绑在cpu0和cpu3上；worker2进程绑在cpu1和cpu3上。 有几个cpu，就些几个bit，比如00000000代表第0个CPU，00000001代表第1个CPU。从0计数 我是4核就这么写👇，注意结尾的分号要写的。 这样就worker1绑在cpu0，worker2绑在cpu1，worker3绑在cpu2，worker4绑在cpu3了👇，再次使用ab压力测试，发现此时CPU不飘了就。L1 L2 利用率就上来了，否则CPU一瓢1 2两级缓存就没了。 不过只是warn告警，服务不会停掉，只是绑CPU可能有点问题。 绑定OK后，CPU0就绑在worker1上，CPU1绑在了worker2上，但是不是说CPU0就只能为nginx服务，还可以被别的应用程序使用的。 worker进程的优先级nice ps axo pid,cmd,nice worker的优先级 官网写错了 就算写1000最大也就19(也就是最低优先级是19) worker进程打开fd数要和ulimit一致 看下系统内核本身的fd并发限制--是单进程打开文件的限制 虽然nginx就开了4个worker线程， 但是一个worker就能应对大量并发号称无上限，因为使用的是epoll模型，所ab高并发测试的时候响应灰常快。 永久调整ulimit的值 系统里调整 PAM文件和security文件看看 nginx里调整 在nginx里直接修改配置文件就行 1、所有worker进程总的并发数 2、每个worker进程的并发数 use mothod 不用指定，默认就是epoll应该也是最好的方式了。也不一定如果是nginx跑在linux上默认就是epoll，如果跑在windows上默认就是select。官方就是说默认使用最有效的方式，应该是这么理解的了。 上图👆得到的并发上限是多大，假设它的硬件资源是OK的，1024是一个worker线程的并发数，auto就看lscpu里几个，如果是4核，那么1024*4=4096的并发数。 一般nginx作为web服务器3W基本到顶了，比apache强也不是说没有上限的。apache一般提到的C10K也就是1W咯。 accept_mutex 默认on不会惊群 虽说accept_mutex on是一种优化，不过EPOLL也不需要这种优化。。。感觉最好还是改成on。 mutl_accept默认是off的，如果on，并发就更强了，不知道是否推荐打开。 然后图中写明了事件驱动相关的配置，说明配置的语句块是在event里的，具体官网可查确实是events里配置 然后multi_accept针对kqueue是无用的，kqueue是和epoll和select对等的机制 所以MAC用户如果本地测试可能就要小心了。存在连接处理机制的不同。 调试和定位 nginx命令一敲默认就是后台运行，是通过daemon来配置的off就是前台运行了 master_process on; 就是以master和多个worker进程的方式来运行，off就是只有一个单进程了也就是master。 error_log file，里面除了error日志，还可以带上debug(但是要nginx -V |& grep with-debug 有东西才行，说明编译的时候开启了debug，配置文件里无法打开debug只能编译里启用。)，就好比msyql的慢查询日志里面还可以带上是否用索引的日志。 上图是实测结论，下图是找到的官方依据 就是相对路，相对于--prefix=/apps/nginx/的，所以结论是对的。 还有关于access_log，关注一下关键词combined，这个在httpd里也是出现过的，代表着log的格式， 同样按此思路去找一找nginx的combined的格式定义在哪里的。 默认的log_format就是combined，然后combined的格式如下👇 不管什么日志，access_log也好，error_log也罢，都是要分开来，跟server语句块走的，就是虚拟主机的log各自分开来。 nginix到现在接触到的优化点: worker_process(线程数跟CPU走) worker_cpu_affinity(cpu亲缘性绑定) woker_connecctions(单worker的并发数) 还有log放到server块里。 还有👇 accept_mutex on; # 就是防止惊群效应。 multi_accept on; # 每个worker线程可以同时接收所有新的网络连接 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:56 "},"36-高性能WEB服务nginx01/4-nginx实现web服务器01.html":{"url":"36-高性能WEB服务nginx01/4-nginx实现web服务器01.html","title":"第4节 nginx实现web服务器01","keywords":"","body":"第4节. nginx实现web服务器01 nginx的http的语句块配置 主要是ngx_http_core_module这个模块 MIME参考文档： https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Basics_of_HTTP/MIME_Types 不写include，就写type同样放到http语句块下 default_type是有默认值的 但是通常yum和编译都会给你改成 这个，这个就是除了明确的类型以为，浏览器不能识别了就，此时统统提示下载。 测试下default_type application/octet-stream的效果 如果是无痕模式的浏览器，会直接弹 另存为 的对话框，也是下载的动作。 通过curl 看看头部 curl 不加-I 貌似还是直接以文本打开了就 注释掉使其使用默认的text/plain格式 看下php文件的识别 默认就是不支持的 写一个php文件来实验 由于不支持.php所以打开该URL默认就是下载该文件的，对比之前apache的效果，我有两个apache①个yum安装②一个编译安装，其实不管是yum还是编译都不支持php文件的浏览器直接打开，只不过yum安装的apache，后来又yum安装了php，于是 这就找到了cli的出处啦，正式因为yum install php才有了httpd配置文件里的哪个php.conf且放在了conf.d下，才有了httpd浏览器打开了php文件。所以，以此类推现在nginx也要这么搞一下。 但是我现在的nginx是编译安装的，所以我需要尝试蒋这个php.conf复制到编译的路径下 没用php里的ngnix关键字的文件里都没有text/html .php之类的字眼，简单网速搜了下nginx对php的支持不想apache一样简单。具体操作参见以下教程👇 https://blog.51cto.com/928004321/1744675 https://www.php.cn/faq/476519.html tcp_nodelay on; 可优化报文确认频次，提高传输效率 据说是当 数据报文比较大的时候拆包传输了就，此时就一下发好几个包，然后再确认一次就比较合理了。 其实数据报文小的时候不涉及拆包，同样也能提高效率，只是数据量小，资源消耗小，系统也好硬件也罢能对付得了，数据量大的时候资源消耗大了，此时就会从各个方面优化，才需要tcp_nodely on多包确认一次。 tcp_nopush on; 可优化报文合并提高，提高传输效率 默认是off不显示的 也可以利用\"include conf.d/*.conf\"去单开一个文件，我理解就是include是在这个语句块也就是这个缩进层级下的，所以新建的xxx.conf里的内容也只反映到这个语句块层级下的。 只是个显示而已。 server_tokens on|off|build|string; string是商业版的nginx才支持的。 build参数就不是简单修改了的， 所以build是编译的时候，通过--build=xx.xxx.v1001，然后配置文件里写server_tokens build;才会有效果的。 或者修改源码文件里的关键字段；server_tokens on或者off都能改，改的地方不同👇 server_tokens on;修改👇 改成： server_tokens off;修改👇 改成： 当然改完还需要重新编译了 那么就重新变一下看看 编译之前看下二进制主程序的哈希值 担心软连接的哈希，确认下，不是哦，就是源文件的哈希 开始编译，ctrl+r调出历史命令回车就行👇 那个此时之前的nginx服务还在，不会停，因为你用的之前的nginx二进制启动的服务，你重新编译其实也就是对这个二进制进行变化吧，所以之前已经启动了的服务不受影响，服务程序继续对外提供服务。 此时想用nginx -s reload看到效果是不可能的，因为我们修改的/apps/nginx/sbin/nginx二进制程序里的东西，不是配置文件，所以需要重启服务后才能看到效果 然后修改server_token为on，reload配置文件就可以看到效果了👇 server在很多模块里都有 下面梳理core模块里的server server就是虚拟主机 listen PORT|address[:port]|unix:/PATH/TO/SOCKET_FILE listen可以写port、ip+port、unix套接字的路径。但是写unix socket就只能本机访问了。 default_server：默认虚拟主机，就是没有命中的统统用这个虚拟主机来响应，这里在apache里是不用设置，拍最前面的virtualHost就是默认的。 配置server实验 默认的不用动，利用include xx/xx/conf.d/*.conf写到独立的配置文件里去 默认的页面路径yum安装的话在/usr/share/nginx/html/下，新建两个到/data下 新建配置文件，listen不写默认就是80 做一下dns解析，dns的意义①，否则域名无法翻译成IP，无法在网络中寻址路由可达，②同时请求的域名也会写道http头里的host字段里，作为nginx处理转到具体虚拟server的依据--server_name。 如果访问的站点不命中server字段，那么就是走的default_server了。而默认主配置文件里好像也没有明确配置default_server，所以按他说法就是第一个，而主配置文件里没有server，就去conf.d/下找第一个文件里的server👇 人为修改为site2为default_server 修改一下配置文件的名称，把default.conf拍到第一个去 然后设置site.conf里的site2为默认server 主机头也就是server_name可以多个用空格分隔 listen ip :port 和server_name就等价于apache里的虚拟主机ip:port和servername。匹配逻辑可预见的一样 还是先看ip和port，如果ip和port一样再看servername。 匹配不中的，自然就默认走default_server，即使没有配置default_server，其实也是存在的，就是第一个server。 匹配顺序：精确>通配符(左>右)>正则>default_server root页面路径 location 没有定义location的时候，所有的访问url都是server下的root路径下的目录。 用了location可以单独将某个url指向别的不再root下的路径。 而看起来location的值也不一定必须是一个目录，试试文件：结论👆上面test是文件夹所以curl的时候test/不带/就报错，带上/就表示test/index.html；👇下面test是文件，所以curl的时候test带上/就报错，不带就是文件处理了，呵呵烦死了。 就有点搞啊 test是出于url的中间位置的，应是location的匹配规则中的一种。 上图301挺神奇的，好像目录不带/就会报301， 既然是301那么加上-L就行了 为什么会这样，官方也给出了解释 location的路径拼接👇 这个等于，其实就是除去fqdn对应的root，www.site1.com去掉就行了，如果等于location里的path，那么www.site1.com就对应到/opt/testdir/ 这个location里定义的root根了。 就是nginx作为应用程序处理请求URL的时候不区分大小写，但是如果nginx跑在windows上就真的是不区分大小写了，但是如果跑在nginx上由于nginx的文件系统文件名是区分大小写的，所以还是会区分的最终。 location用的非常多，比如现在做一个动静分离 比如各种图片就属于静态资源 上图就是意思一下，实际动态文件php|jsp|asp不是简单的放在这里的，而是后端的反向代理发给后边能处理jsp的比如是PHP也就是fastCGI的服务器地址 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:56 "},"36-高性能WEB服务nginx01/5-nginx实现web服务器02.html":{"url":"36-高性能WEB服务nginx01/5-nginx实现web服务器02.html","title":"第5节 nginx实现web服务器02","keywords":"","body":"第5节. nginx实现web服务器02 比如统一把静态资源放到/static/下，或者将后缀写全了进行转发。 alias和root，替换和拼接 root是拼接，而alias是看①如果访问的url里的有alias的path比如上图的about，那么就把整个url替换成alias的path。于是www.site1.com/about/index.html就访问了/opt/testdir/index.html了。 加不加/就是为了xx/xxx/zz/和xx/xxx/zz效果一样因为浏览器会自动重定向类似curl -L的效果 上图注意location里是/about/，用户请求的www.site1.com/about里是不包含/about/的所以不会走location的转发，而是走的非location的也就是/data/site1/的转发路径，具体见error.log 而官方的解释很清楚了已经 就是/user/和/user你不想合成一个URL，是可以指到不同的站点URL上去的。而一般情况都是使用/user不带/这样，用户访问/user和/user/其实都是一个站点，因为/user用户访问呢会自动301到/user/上去的。 上面讨论了location里路径的结尾带不带/的区别，下面讨论alias/root的转发路径的结尾带不带/的区别 总结下 修正理解 上图总结的处理问题不大，但是需要补充 用户行为： 1、curl www.site1.com/about这是访问about文件 2、curl www.site1.com/about/这是访问about/index.html文件 通常都会location里写成/about而不是/about/来应对，这样用户不管最后带不带/实际效果都是一样的，因为不带/也会匹配location然后301到/about/。 location匹配 1、写成/about和/about/是有区别的，首先用户访问的url里必须完整包含，比如location写的是/about/但是用户行为里是/about，就不会命中了； 2、结合用户行为，如果是/about/那么就是访问的/about/index.html此时 \"alias\" 就会将/about/的url整个替换成alias的路径，并在最后补上index.html；此时如果alias那里没有写成文件夹而是写成/xxx，那么整个url就有问题了，就变成了xxxindex.html中间会少了/。 不过root不会出现这个问题，因为root是拼接的，会拼接成xxx/about/index.html alias/root转发处理 1、alias是整个替换只要用户行为里包含了location里的比如/about那么整个url都会替换掉alias的东西，当然如果用户行为最后带/会自动补上index.html的。 2、root是拼接，问题不大，总之实测一下，留意就好，实际测试oK就行，不要纠结了。 index 上图👆有一处地方要改，就是location /about/改成lcoation /about，这样curl -L www.site1.com/about 就能利用301重定向了。 error_page code 找不到404 Not Found哪来的，因为配置文件里404是注释的， 我好像找到了，👇应该就是ngx_http_special_response.c文件 具体调用和最终应用到页码不知道什么弄得，不过这里由明确得html的语法，知道是源码里的404已经做好了就行了。 好像只是简单做一个error页面不用写location啊 location在处理error_page时的注意点 1、root没毛病 root的最后/写不写都行，因为时location 和 root 拼接的，location里带了/了 2、alias有点细节要注意 注意由于重定向的存在也就是location转发的存在，这些重定向的动作是不会日志中看到的，日志中只是最终的一个结果。 这里盲猜location = /404.html 被alias转成了/opt/testdir/，然后没有给你自动补一个index.html 总之也实现了alias和root的404location转发。 场景：浏览器劫持 输入www.site1.com/xfsdfaf后跳转的地址被修改了 为了防止这种浏览器的不良行为，应该如何应对，现在来讲，可能也就是你自己的配置问题，没有对应的文件存在👇比如这种：就会让别人有机可乘。当然chrome不会，可以将404报错改成200这种正确响应码。 当然chrome不会 try_files file ...uri; try_files file ... =code; 这就是类似什么呢，类似这个 网工看了就懂咯 配置下try_files 测试效果就是访问 http://www.site1.com/images/ajpg http://www.site1.com/images/b.jpg http://www.site1.com/images/default.jpg 都OK 但是访问 http://www.site1.com/images/xxx.yy 整个uri不存在，就对应到try_files 的$uri整个变量的文件不存在，于是就转到default.jpg上去。 思考👇 总结就是：内部重定向的/images/default.jpg，也会再次命中location的alias，于是 http://www.site1.com/images/default.jpg被alias成http://www.site1.com/data/images/default.jpg了？？？只能这么理解了啊。 如果/images/ keepalive_timeout 这个65怎么体现在测试中： 通过telnet 80 并发送一次get可以感受到时间的长短，操作主要事项①get一次得到页面内容后②不要再做任何操作。此时可以观测到配置的时间。 注意这两个参数是不同的 keepalive_time是一个链接可以存活多久 keepalive_timeout是一个client请求的链接，空闲时间可以存活多久。 可见启用了keepalive_timeout但是没有说多少秒。 写道head里的是10秒，实际是65秒。 持久链接断开的情况 1、keepalive_timeout时间到了。 2、请求的资源个数超了，比如一次长连接允许请求文件为3个。测试👇 禁止那种浏览器使用长连接 keepalive_disable none | browser ...; 手机上各种浏览器 向客户端发送超时时长 send_timeout time; 向客户端发送响应报文的超时时长，此处是指两次写操作之间的间隔时长，而非整个响应时间过程的传输时长。 服务器响应发送时间间隔，也就是两次写操作的间隔，这里写，我的理解是 server要构建response的，自然是写内容的要。 上传文件的限制？ client_max_body_size size; 请求报文中实体的最大值,默认1M，超过了就报错413 client_body_buffer_size size;上传的文件是有buffer的，超过buffer大小默认16K，就会暂存到磁盘上的下面client_body_temp_path指令所定义的位置。 client_body_temp_path path [level1 [level2 [level3]]]; 设定存储客户端请求报文的body部分的临时存储路径及子目录结构和数量 目录名为16进制的数字；用hash之后的值从后往前截取第1、2、3级作为文件名 比如： client_body_temp_path /var/tmp/client_body 1 2 2 # 三级目录1 2 2，分别用1个字符，2个字符，2个字符。 1 1级目录占1位16进制，即2^4=16个目录 0-f 2 2级目录占2位16进制，即2^8=256个目录 00-ff 2 3级目录占2位16进制，即2^8=256个目录 00-ff 👇计算下这样三级命名文件夹的好处--就是用上传的文件的哈希值作为分层目录，这样能够支持1048576个子文件，每个子文件里如果放1个文件也就是100w个文件了。好处就是如果整个100w放在一个文件夹里ls都能卡死，分层放置，查找自然就快了。L1目录顶多16个，L2目录256个，L3目录256个也是。 上传服务器配置生产案例： location /upload { client_max_body_size 100m； client_body_buffer_size 2048k; client_body_temp_path /apps/nginx/temp 1 2 2; … } Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:56 "},"37-高性能WEB服务nginx02/37-高性能WEB服务nginx02.html":{"url":"37-高性能WEB服务nginx02/37-高性能WEB服务nginx02.html","title":"第三十七章 高性能WEB服务nginx02","keywords":"","body":"第三十七章 高性能WEB服务nginx02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:57 "},"37-高性能WEB服务nginx02/1-nginx常见配置.html":{"url":"37-高性能WEB服务nginx02/1-nginx常见配置.html","title":"第1节 nginx常见配置","keywords":"","body":"第1节. nginx常见配置 对客户端进行限制 下载限速 准备下载的资源 下载速度 限速成功👇 限制method limit_except method ... { ... }，仅用于location 限制客户端使用除了指定的请求方法之外的其它方法 method:GET, HEAD, POST, PUT, DELETE，MKCOL, COPY, MOVE, OPTIONS, PROPFIND, PROPPATCH, LOCK, UNLOCK, PATCH limit_except GET { allow 192.168.1.0/24; deny all; } 除了GET和HEAD 之外其它方法仅允许192.168.1.0/24网段主机使用 这里的Not Allowed应该不是没有放行，而是网站不支持的意思，证明如下👇 配置 翻译：限制所有，除了GET和OPTIONS以及GET放了就带了HEAD，IP方面全部包含。 这才是没有放行的报错👇 而这个不是没有放行，而是网站不支持的现象👇 文件操作优化的配置 aio on | off | threads[=pool]; ​ 是否启用异步io，默认off directio size | off; ​ 当文件大于等于给定大小时，同步(直接)写磁盘，而非写缓存，默认off location /video/ { sendfile on; aio on; directio 8m; } open_file_cache off; open_file_cache max=N [inactive=time]; ​ nginx可以使用缓存来加快文件访问速度，可以缓存以下三种信息： ​ ①文件元数据：文件描述符、文件大小和最近一次的修改时间 ​ ②打开的目录结构 ​ ③没有找到的或者没有权限访问的文件的相关信息 ​ max=N：可缓存的缓存项上限，达到上限后会使用LRU算法来管理 ​ inactive=time：缓存项的非活动时长，在此处指定的时长内未命中的或命中次数少于open_file_cahce_min_users指令所指定的次数的缓存项即为非活动项，将被删除。 这里面就体现了缓存的一个使用逻辑：①缓存的空间多少能存多少数据量数据量是条数来定义的吗？②存多久，不用就删，不用的标准是什么，用的少就删，用的少的标准是什么。这里就有一个指定时间。所以上面的逻辑就是答案了。 open_file_cache_min_uses number; ​ open_file_cache指令的inactive参数指定的时长内，至少被命中此处指定的次数方可被归类为活动项。默认为1，1就是等价于LRU算法了--只要命中一次就将优先级提为最高。可以改大点。 open_file_cache_errors on | off，是否缓存查找时发生错误的文件一类的信息，默认off ​ 用户经常访问错误的页面，可以开启这个选项 open_file_cache_valid time，缓存项有效性的检查频率，默认值60s ​ 检查到了非活动项，就会删除了。 ngx_http_access_module 类似acl 该模块，可实现基于ip的访问控制功能 allow address | CIDR | unix: | all; deny address | CIDR | unix: | all; ​ 上下文所属：http，server，location，limit_execpt ​ 自上而下检查，一旦匹配，将生效，条件严格的置前。 示例： location /about { # 这玩意会301重定向为/about/然后走root拼接，就是 root /data/nginx/html/pc; # 拼接为/data/nginx/html/pc/about/index.html index index.html; deny 192.168.1.1; allow 192.168.1.0/24; allow 10.1.1.0/16; allow 2001:0db8::/32; deny all; } 调整一下顺序 ngx_http_auth_basic_module 该模块基于用户访问控制，使用basic机制进行用户认证 auth_basic string | off auth_basic_user_file file; ​ location /admin/ { ​ auth_basic \"Admin Area\"; # 提示语句，弹出对话框里的信息 ​ auth_basic_user_file /etc/nginx/.ngxpasswd; # 你看，和httpd一样，密码都是用.xxx隐藏文件来做一般 } 用户口令文件： 1、明文文本：格式name:passswd:comment 2、加密文本：由htpasswd命令实现 ​ httpd-tools所提供 好像和httpd也就是apache一样？肯定一样了，都是一个工具httpd-tools提供的 按这走一遍咯👆 或者这样更简单 注意-c 只有首次才能加哦 以后就这么配置👇 配置以下使用该用户密码 去掉index test.html才会使认证生效👇 不对，不对，不去掉也可以的，这里实验效果生效可能慢了 要注意这个basic认证的密码，是明文的，抓包可见 抓这个口的报文 这就是为什么要采用https的原因 apache那会讲过status统计页面，nginx同样也有 这有什么用呢，就是将来可以curl xx/xxx/status，就能快捷地获得各个指标，联动报警了。 location /nginx_status { stub_status; allow 127.0.0.1; allow 172.16.0.0/16; # 自己修改成所需要地白名单 deny all; } ab打一下 注意：第二行参数地值也就是2、3两行 是累计地值 最后一行是当前的值。 第三方插件-echo echo就是变量 显示出来比较方便可能。 这是没有echo功能的，需要安装插件 https://github.com/openresty/echo-nginx-module 然后下载nginx源码 https://nginx.org/en/download.html 然后编译的时候，把echo插件加进去 ok👇 图中error只是摸粑粑的着色 然后make -j 4 && make install报错👇 处理方法 完整的编译选项如下👇 ./configure \\ --with-cc-opt='-fPIE -fPIC' \\ --with-ld-opt='-pie' \\ --prefix=/apps/nginx \\ --user=nginx --group=nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-http_perl_module \\ --with-pcre \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module \\ --add-module=/usr/local/src/echo-nginx-module make -j 4 && make install 创建测试配置文件 看上图不行哦👆，nginx -t看到的是nginx.conf test OK，其实没有检测test.conf的，因为主配置文件里没有指定includ xxx 加一下 这就是效果了，然后浏览器是由于mime.types应该没有识别所以就统一下载 下载下来打开就是 优化下指一下default_type为text/html;👇 ok，如果注释掉echo，就是index.html内容了 简化配置👇 如果用echo，其实不必要指root的什么文件也无需存在的。 同样echo插件可以打印变量 解释上图👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:56 "},"37-高性能WEB服务nginx02/2-nginx第三方模块echo和自定义json.html":{"url":"37-高性能WEB服务nginx02/2-nginx第三方模块echo和自定义json.html","title":"第2节 nginx第三方模块echo和自定义json","keywords":"","body":"第2节. nginx第三方模块echo和自定义json nginx变量 同样可以利用echo显示出来吧 好像br才能在浏览器中换行 linux cli里还是有点问题 要加引号的 然后br只是针对浏览器的，linux curl无需br的 然后还要关注这里一直看到的 ?id=1&core=100其实就是$args的变量的结果，用在sql语句去后端查DB的。 如果你不清楚nginx的站点的目录，但是你又知道配置文件的路径，可以这么做 cookies的显示需要curl里带上才行的 正常访问网站，server会构建cookie发给你，下次你浏览器就会自动携带这个cookie了-这是浏览器自带的行为。 cookie的用途，将来可以根据cookie来进行调度，session会话，前端有个调度器，如果轮询就不知道轮到那台node去了，之前的信息就丢了，所以可以根据cookie或者说这里的user，如果是ming这个用户，就往同一台node上转发。这就不会造成会话丢失。 但是，cookie是7层内容，LVS不支持。 整体上判断nginx的性能要地域lvs，这是再说四层转发的功能和lvs的四层转发。 自定义变量 变量之间赋值：不过要联系到python里的列表赋值和字典赋值这种 指针类的 要小心。 如果是http，就可以重定向到https。但好像也不用echo一下 判断一下 ，多此一举。 访问日志的格式 和httpd也就是apache一样。 log_format 指令只能在http模块下定义；但是调用log_format指定的格式变量就不一定了 也是①先定义格式②定义完个时候，再引用这个格式。 apache也是一样的格式 还有用户认证的记录，这是之前讲过的 日志如下👇 log_format 指令只能在http模块下定义；但是调用log_format指定的格式变量比如下图的main就不一定了 每个服务器，也就是server都应该有自己的访问日志，区分开的，所以自然是server模块下可以使用access_log来调用log_format格式，进一步产生自己的独立的日志。 自定一下log的格式和产生的地方 http_referer是从哪个url跳转过来的。 上面新的日志格式就改完了，下面就去虚拟主机那里调用就行了。 access_log off;就是不记录这个server块，虚拟主机的日志。 重启服务日志文件就有了，不过是空的，访问一下，日志文件里就有内容了👇 要注意 - 有的是 格式排版，有的是没有数值。 要注意，上图的14就是index.html的文件大小 大小会有出入，文件越大可能出入就出来了。 /favicon.ico是当你访问站点的时候，会自动的找你的图标，这是自动的浏览器行为，结果没有就是404找不到了。 然后可能更提倡用json格式，因为有key value，不像上面的一行一个全是内容，一些值就不清楚具体是什么意思了。 但是用json格式保存，就意味着你查看不是 行记录了，人工查看就不是很舒服咯，对吧。 重启服务后，访问，看下日志 格式化下 关于日志或者大文件的清理方法 1、> 2、cat /dev/null > # 一些shell不支持 上面个的>重定向的方法，就用/dev/null垃圾箱的方式来做 3、别用echo，echo是有一个字节的，是一个换行符 浏览器的行为，会找图标 扣一个图标下来用用 找到站点的根目录，将图标放进去 浏览器的行为 curl就不会，因为curl是cli不是图形界面，不存在图标啊 解决方法就是补一个图标文件 或者访问这个favicon.ico文件的时候就不记录日志，这有点掩耳盗铃哦，也许不是，哈哈 图标默认就是在站点的根，这里上图是指定了具体位置了。 缓存 这图👆是说的文件的缓存 而下图👇是日志的缓存，也是LRU算法，也是max、inactive、min、valid这些参数。 open_log_file_cache on; # 启用缓存 max=N 记录多少条缓存 inactive=time 多长时间内 该缓存 没有被访问，就认为过期了；而且在inactivetime时间内 还要访问min_uses=N个次数才认为是活动的缓存。反之这段时间内该缓存没有达到min_uses次则缓存删除了应该就。 valid=time 多长时间检查一下缓存情况，就是检查一些信息是否过期了。 nginx做下载页面，类似yum源之类的 1、将sr0光盘挂到站点目录下，直接是不能访问的 这里的一个排查思路，就是umount后，写一个index.html文件，再用浏览器打开看看如果可以就说明路径啊，配置啊都没问题，问题是文件夹里没有index.html文件。其实不必测试了，知道这么个思路就好了。 好了，现在将使用ngx_http_autoindex_module模块 涉及选项 autoindex on|off 开关咯 autoindex_exact_size on|off 一般就是off显示粗略的不对的文件大小，主要是没啥大用，on还要实时计算不好，对资源消耗方面。 autoindex_localtime on|off 使用on就是用本机时间，然后本机时间就用ntp同步了。 autoindex_format html|xml|json|jsonp 显示格式 这不就是可以用来当yum源咯👆 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:56 "},"37-高性能WEB服务nginx02/3-nginx的压缩和https加密实现.html":{"url":"37-高性能WEB服务nginx02/3-nginx的压缩和https加密实现.html","title":"第3节 nginx的压缩和https加密实现","keywords":"","body":"第3节. nginx的压缩和https加密实现 压缩 apache也支持压缩 这是👆httpd的压缩篇章 nginx -t查看编译的配置选项 配置参数如下： gzip on|off 开关 gzip_comp_level level 压缩比修改 1到9，9 最高压缩比，9也是最耗CPU的 虽然是1-9，但是压缩的效果也不会差太多 gzip_disable regex ... 匹配客户端浏览器类型就不执行压缩，比如gzip_disable \"MSIE[1-6]\\.\"; gzip_min_length length 压缩的起点，什么样的大小才开始压缩，比如＞100k的才开始压缩，100Bytes就别压了，压了也没啥效果。 gzip_http_version 1.0|1.1 是支持的http版本 gzip_buffers number szie 压缩的时候启用多大的缓冲区 gzip_types mime-type ... 针对什么类型的文件进行压缩，默认包含text/html的格式支持，不要写，写了反而报错。 gzip_vary on|off 如果启用压缩了，是否在响应报文首部插入\"Vary：Accept-Encoding\" 解释：👇就是说server要插入这个标记，代理那边就会存储两个版本的web页，一个压缩一个不压缩，以此来响应支持和不支持压缩的client浏览器。当然c-ser直连的话就看client那边浏览器是否支持压缩了支持-server就发送经过gzip压缩的页面版本。 gzip_proxied off|expied|no-cache|no-store|private|no_last_modified|no_etag|auth|any ...; nginx做反代的时候，后端服务器将响应报文发给nginx，nginx再回给用户的时候，是否启用压缩，就靠这个参数来做的。 off 就是nginx返给client的时候不压缩 expied|no-cache|no-store|private|no_last_modified|no_etag|auth|any ... 后端服务器响应报文中带有Cache-Control字段，然后这个字段里的值 写的是这些的时候，就启用压缩。 配置下 同样，可以了解下gzip的适配模块 gzip 的总开关 默认就是off的。其他的就是on后的一些选项 观察这个大文件，看下压缩的效果 curl可以看到Content-Lenth大小就是文件的大小👇👆 既是curl带上压缩功能，也没有，因为server那边没有开启 然后server开启压缩功能 然后curl看压缩要启用压缩功能的，不启用是没效果的 curl带上压缩选项，应该是成功了，只是不再显示Content-Length了。 所以说压缩比差不多就行了👇 SSL证书，加密 之前httpd也弄过，自签名证书还挺有意思的，实现了client的安全访问的关键点。 然后购买证书这块的情况 阿里云上有全套的购买申请流程，可以学习下，但是买不一定，需要看哪里性价比高。 apache做ssl，server上是配置3个文件：ca证书、服务器证书、服务器私钥 nginx做ssl，服务器证书、服务器私钥 实现ssl是用的ngx_http_ssl_module模块 ①ssl on|off， ssl开关，不推荐，建议使用listen指令，和httpd一样 listen 443 ssl httpd就是listen 443 https ，不是一个意思嘛。 ②ssl_certificate_file 和 ssl_certificate_key file 服务器证书文件和私钥文件；从供应商购买后会得到两个文件，一个服务器证书文件，一个私钥key ③ssl_protocols 支持的ssl加密协议，默认就行了 ④ssl_session_cache 加速服务器响应速度的吧应该是， ⑤ssl_session_timeout 客户端链接可以复用ssl session cache中缓存的有效时长，默认5m 下面开始实验 还是使用自签名证书咯 之前http的时候用的openssl还是比较多步骤的，现在利用一个Makefile文件来弄，其实就是Makefile封装好了函数，脚本简化成了cli了。 首先你需要一个Makefile文件👇 https://exampleconfig.com/view/openssl-centos7-etc-pki-tls-certs-makefile ①下载Makefile ②生成自签名证书 上图漏了Common Name，就是要写网站域名的👇 但是现在有个问题，每次重启服务都有问题，就是ming.key这个私钥文件是加密的， 所以还需要给它解密一下👇 [root@server tls]# openssl rsa -in ming.key -out ming.net.key 这样就得到不加密的私钥了 将两个文件ming.crt和ming.key放到nginx的配置路径下，再看下权限是否为600 证书文件有了，就可以改nginx的配置文件了 然后打开浏览器 这不就来了嘛 但是不安全没法搞，因为需要下载ming.crt证书到PC，然后导入到受信任的根证书颁发机构 既是导入了可以肯定也是不行的，因为通过 openssl x509 -in ming.crt -text -noout发现这种Makefile的方式并没有X509v3 Subject Alternative Name这个选项，所以导入也没有用的，Makefile需要优化下。 尝试优化Makefile文件 搞定 哈哈哈，修改Makefile和处理方式如下 1、makefile的修改 这里补充一个取消xxx.key生成的时候输入密码的要求，也就是不加密了 去掉-aes128就行了 2、openssl的配置文件修改 3、重新make 一下 make ming.crt # 一键生成ming.key 和 ming.crt openssl rsa -in ming.key -out ming.net.key # 去掉ming.key的密码 rm -rf ming.key # 删除带密码的key mv ming.net.key ming.key # 后面使用不带命名的key，重命名一下 mv ming.key ming.crt /etc/nginx/ssl/ # 移动到规范目录下，供nginx调用 [root@server tls]# cat /etc/nginx/conf.d/site.conf server { listen 443 ssl; server_name *.site1.com; root /data/ssl/; ssl_certificate /etc/nginx/ssl/ming.crt; ssl_certificate_key /etc/nginx/ssl/ming.key; ssl_session_cache shared:sslcache:20m; ssl_session_timeout 10m; access_log /data/ssl/log/site1.access.log access_json; } systemctl restart nginx 4、导出ming.crt 5、导入PC 打开浏览器就ok拉 但是通过这个可发发现，现在ssl浏览器验证果然不再看站点了，只看备用名称。 你看哦 这里👆写错了，其实没关系 肯定正常要一样的 这里故意写不一样，就是验证只看Alternative Name 然后导入ming.crt进入PC ssl跳转，ngx_http_rewrite_module 先说if return是返回301这个code，并重定向到性的url 上图漏了80监听，否则会实现ssl跳转 ok了就，不过更多的不用return，而是用rewrite来做重定向。 curl验证 👆看的很清楚跳转了 然后echo的测试要用之前的编译安装的nginx 将include移到后面就行了，主配置文件 子配置文件👇 OK了这样，第一个if重定向到https，然后浏览器继续发起https的请求，然后就命中第二个if，然后就会调用echo得到👇页面了： 官网的if例子 可以知道if可以控制，比如我的站点不让你curl 比如163肯定就做了if 不让你curl， 但其实curl依然可以用，因为163限制curl是通过if来匹配必须要有浏览器，而curl自然可以模拟浏览器 👆curl通过-AIE来模拟IE浏览器，加上-L支持重定向就行了。一般就可以绕过网站对curl的限制了 测试if user_agent 改一下不要方括号 这样就if区分了curl👇 看下agent怎么自定义出来的👇 看下301 302 307 区别： 1、301是永久重定向，是可以缓存的，所以会出现from disk cache。 2、 结果第二次变成307 from disk cache了。。。 302就是临时性的，一般现在趋势是全站加密，可能301反而更多一些。 307是浏览器内部跳转，不访问页面了，这个怎么关掉呢，就是现在我测试都是 第一次 301 第二次 然后就307 from disk cache 我想做出出来 第一次 301 第二次 301 from disck cache HSTS是啥 HSTS是 、、 307，导致的内部网站打不开 1、背景 www.sw.com 和 sw.com是我内部用的一个域名 都解析到了192.168.0.9，是内部的soft站点，套了nginx，只监听了80 但是有一天突然 我的笔记本 访问www.sw.com或sw.com的时候打不开，无痕可以👇，无痕不用本地的HSTS列表所以可以。 题外话：就是这里有一个307 把443重定向为80的动作，和本段研究内容无关。 进一步发现：即使手动输入http://www.sw.com也会307到https://www.sw.com，**页面直接打不开👇**，是一个报错页面 红色的www.sw.com其实就是被307到了https://www.sw.om也就是 浏览器地址栏 里的 https👆 然后手动输入http://sw.com，**网页打开不全👇**，原因就是http://sw.com里的部分图片，框架，走的还是http://www.sw.com；而http://www.sw.com就会被307到https://www.sw.com，多了个s。 2、原因 就是HSTS导致的 1、浏览器里的 chrome://net-internals/#hsts 可以查到www.sw.com存在的，于是就是会被浏览器直接给你在浏览器内部做了307了，很骚啊 删掉就可以了 2、为什么其他电脑的 chrome://net-internals/#hsts 里没有www.sw.com 分析：可能是www.sw.com这个，不是分析了，我实测了一下，就是你用代理访问了www.sw.com就会在 chrome://net-internals/#hsts这个里面生成www.sw.com的HSTS 找到原因了，并且是可以复现的故障现象，very nice 进一步分析，就是Strict-Transport-Security，就是这个互联网的www.sw.com的服务器端给了这个回应选项导致的。 这一点前面的章节里也有提到，不过没有这次故障处理后得到的逻辑清晰 所以要复现故障 1、打开代理 2、访问www.sw.com # 拿到HSTS 3、关闭代理 4、访问www.sw.com # 得到网页直接打不开的 故障现象1 5、访问http://sw.com # 得到网页打不开不全的 故障现象2 再研究下上面说的题外话，为什么我内部的www.sw.com会443跳80： 分析判断： 1、收到307 临时 重定向，就会转到location的URL上，那么local 的url哪里定义的？还以为什么会收到307 临时 重定向 ？ https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status/307 location 首部 从浏览器可知就是http://www.sw.com👇 我觉的location 首部 这里找找，也许默认就是server name 和 listen的组合也就是http://www.sw.com👇 为什么会产生307 临时重定向 无痕测试 1、输入sw 按ctrl enter，也就是访问https://www.sw.com 2、但是server那边不管是accesslog和errorlog都没有看到443的访问，也正常，因为443的流量就进不来，没监听啊 3、所以这个是浏览器自身的行为，一定是，瞬间就调到80了👇 有可能就是它的回退既然要到处https，万一不行，总要回退吧，我认为就是这样的。 就是浏览的自身行为👇 10.9压根没有HTTP的报文回过去，在80 connect 联通之前。所以\"307 Temporary Redirect\" 我判断是浏览器的自身行为---当 ①你输入的是www.sw.com的时候由于未指定http还是https，所以它(可能就是https everywhere) 优先让你用https， ②但是多次被拒绝后，回退到http了； ③如果你直接输入https://www.sw.com，由于明确制定了，所以就打不开啦； ④如果你直接输入http://www.sw.com，由于明确指定了http，所以就没有类似https everywhere的事啦就不会有\"307 Temporary Redirect\"啦，直接就打开啦。 多日以后的一个截图👇补到这里佐证 另外，无痕 里 两种 输入方式，普通模式里一样 1、www.sw.com 不是每次都能看到优先https，很多时候是没有说给你先https去443连接的，就直接http了。 # 这是因为这种行为，浏览器会参考历史数据的，删掉涉及sw的历史记录，www.sw.com 就会和 👇下面的方式一样了，也能看到优先HTTPS了就。 2、输入sw，然后ctrl + enter，每次都是优先https # 这种行为不会，就直接补上HTTPS了优先了。 历史记录 不等于 缓存 ！ 无痕有历史url的证据👇 一旦这些历史url在，也是www.sw.com能够自动补出来，www.sw.com就会直接使用http://www.sw.com去访问，因为历史数据里成功的就是http吗。而输入 sw 再ctrl + enter 就是快速提交内容，而且通过测试多次测试多次频繁测试可以说这种就是不会被历史数据影响的手段。就好像ctrl+r 一样是不用缓存刷新一样。 无痕开两个，第二个无痕访问第一个无痕的url没有意义了就，等于不是无痕了，所以无痕测试的时候要关闭所有浏览器窗口包括无痕 这里不演示了就，多次体验过了已经。 最后总结 307 Temprorary Redirect 307 Internal Redirect 浏览器的输入手法 无痕测试注意 最终都是浏览器内部的行为，但Internal才是和server交互出来的内部配置，而Temprorary才是真正内部就有的只需要server拒绝浏览器多次请求就行，是一种回退类似。 307 Internal Redirect，是网站server回应里的Strict-Transport-Security键值将浏览器的HSTS列表里[chrome://net-internals/#hsts]加上了，所以你怎么输入都会让你直接走HTTPS 307 Temprorary Redirect，是浏览器自身行为，是一种回退行为，特别是443被拒后的80回退 回退是回退，这里307 Temporaray Reirect是 httpUpgrade到https 上图👆是多日以后的一个作证图，补到这里。 浏览器由于内置机制，当你输入www.sw.com，未指定http还是https的时候，会优先https的，但是也要和输入手法结合在一起的👇 ​ 1、浏览器里的历史记录会影响url输入的行为，比如有www.sw.com的记录，是http://www.sw.com,那么你输入www.sw.com就会走http:了 ​ 2、但是地址栏里输入sw，后ctrl + enter就不会 被历史记录的影响。此时就会优先Https，然后多次被拒后才会307 Temprorary Redirect回退到http 无痕测试只能一个无痕窗口，否则还是会利用其他无痕里的缓存。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-10-12 18:18:32 "},"37-高性能WEB服务nginx02/4-nginx的rewrite模块实现.html":{"url":"37-高性能WEB服务nginx02/4-nginx的rewrite模块实现.html","title":"第4节 nginx的rewrite模块实现","keywords":"","body":"第4节. nginx的rewrite模块实现 rewrite 先看下官网的例子，关注下last 这种是服务器内部的跳转，client是看不到的，是没有什么301 302 的，直接就是server内部重定向好了直接给你200的。而不是return你301让你重新去访问一个新的url的。 通过https://www.site1.cm/test1/可见直接是200返回码，内容是server自己rewrite成了/data/ssl/site2/inedex22.html了，上图的第二个location匹配到的 这点就和return 301 httpxxxx不同了，301是告诉用户重新访问发起的。这个rewrite只server自己内部处理后直接返给用户的。节省了c/s交互吧。 👇echo会抢了rewrite的 反正last不推荐放到location里 👆上图是官网的说法，我反正这么测试👇没看到什么10次循环500code的结果 所以我认为👇 总之：last就是下一个location匹配；break就是不继续匹配了 好像上图的说法也不对，哈哈，原因看下图👇 可能echo优先级高于rewrite吧，所以echo启用后，访问xxx/xxx/test就默认行为重定向301到xxx/xxx/test/然后 echo优先级高，于是直接echo出来了。 而访问xxx/xxx/test/，按理说也是命中location /test，但是此时echo又没有生效！奇怪了 两次现象的区别就是第一次有一个301，而直接访问/test/是没有301的。就这个区别了，所以我认为： 对于last： /test 301到/test/就会echo优先 /test/ 直接访问echo就会rewrite优先，又或者就是rewrite更希望没有重新向过的行为。 对于break和last一样的逻辑 1、如果存在echo，那么echo就会优先，和last的区别就是 /xxx和/xxx/ 一样都是echo优先了 2、不存在echo，就是常规理解了，就是跳出继续的location匹配了， 我认为，这里写的就是一坨，而且echo是插件，没有弄好一些细节，这块技术落地的时候要测试好你自己的业务场景的。 redirect，http跳转https 第一个301，默认的/xxx跳转/xxx/行为 第二个302 就是conf文件里的redirect导致 curl 里的log要看到框框里的重定向里的完整url需要这么写，要注意request_uri和uri还是有区别的！ 比如/xxx/是request_uri，而uri确实/xxx/index.html。 ssl的基于域名进行转发 1、给site2.com创建ssl证书相关的文件 ①修改openssl配置文件里的dns为*.site2.com 上图改错了，必须改成*.site2.com才行，否则证书无法验证通过！ 好了两个文件有了 配置好虚拟主机，基于域名的 然后就实现了👇 由于上面的site2.com没有写成*.site2.com通配，所以ssl验证失败，这里改一下，顺便用一个ssl证书搞定两个网站 去掉Makefile的xxx.key加密选项和添加v3的dns选项的👇 ssl证书两个虚拟主机就用一个样的 然后看最关键的浏览器 通过sz site.crt导入到本地电脑的桌面上，然后双击导入 受信任的根证书颁发机构 就行了 👆这就实现了一个证书搞定多个域名啦，自然也是一个ip+80对应不同域名的。 据说nginx支持这种ssl的多域名对应一个ip+port是因为TLS SNI功能 TLS SNI就是TLS的sever name indication（sni) 访问错误页面时的处理 需要利用request_filename变量来判断访问的页面不存在 这就实现了访问不存在的网页跳转到首页了，不过要和80转443共存，可能需要这么修改 这是80转443的过程，里面也涉及一个不存在的页面(https://www.site1.com/)这个页面本来是应该存在的啊，奇怪了？这么写把原来的默认/就是访问/index.html的行为给搞没了。 这是不存在页面转的过程👇 奇怪的点看看日志👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:57 "},"37-高性能WEB服务nginx02/5-nginx反向代理实现.html":{"url":"37-高性能WEB服务nginx02/5-nginx反向代理实现.html","title":"第5节 nginx反向代理实现","keywords":"","body":"第5节. nginx反向代理实现 防盗链 盗链就是一个网站调用另一个网站的资源 构建盗链 site1站点盗链site2站点里的一张图片 这是两个之前做的站点👆就利用这个现成的做实验 这就实现了盗链， 很明显存在两个问题①乱码②图片不能缩放了， 这是site1和site2 log 合在一起咯，盗链行为其实是，站在被盗方的角度去防止，所以log拆开来，去开被盗站点site2的log 可以看到的，所以就可以防止盗链行了的 防止盗链的解放方法：ngx_http_referer_module模块来弄 大体意思就是先定义出哪些是合法的referer，比如百度过来的referer说明你广告费没白出。 两个参数 valid_referers none|blocked|server_names|string ...; # 用来定义合法referer的值的 none：就是referer为空 判断为合法。一般直接访问某个网站referer都是空的。自然要写none的。 none在浏览器里F12是看不到的吧 blocked：就是referer里写的东西不是常规的uri，这个没见过还 server_names：本网站内部跳转的，就会在referer里带上自己的主机名，都是自家人啊。 arbitrary_string：任意字符串，用通配符的方式。 regular_expression ：也是任意字符串，用regex来做了，格式就是~开头代表regex。 ​ 例如： ~.*.site1.com 配置防盗链肯定在被盗站点上配置了👇 valid_referers none block server_names *.site2.com site2.* ~\\.site2\\. ~\\.google\\. ~\\.baidu\\.; if ($invalid_referer) { return 403 \"Forbidden Access\"; } 然后如果是从baidu跳过来的就不禁止访问，因为十有八九就是广告的钱啦👇 测试：修改site1为baidu 修改本地hosts文件 C:\\Windows\\System32\\drivers\\etc 下👇 上图配置是不行的，因为http://www.baidu.com/daolian.html是80跳443的也就是跳到https://www.site1.com的， 然后就会去访问https://www.site1.com/daolian.html 然后图片的访问里就有 这样就不是从www.baidu.com跳过去的了， 测试的话倒不用改配置文件，直接访问https://www.baidu.com/daolian.html 这样就可以了， 当然这里ssl不安全的原因也很简单啊，就是域名和ssl里的 备用名称一致。 如果就要实现http://www.baidu.com/daolian.html能够打开呢 可以，单独写baidu就行了 开始学习nginx的反向代理咯 正向代理后面也要学习，应用在比如手机的ssl报文抓包要抓到内容的，做ssl解密的。后话这里记一下~ 据说nginx的反代还可以做健康性检查，这点比LVS强，LVS好像没有健康性检查，据说哦。 而且nginx可以做应用层和TCP层的反代，不过速度没有LVS快据说。LVS好像工作在内核层的？ 模块支持 1、proxy、fastcgi、uwsgi这些都是http类的，所以名字都是ngx_http开头的。 ​ 1.1 、 proxy就是nginx对接http原本协议、fastcgi就是nginx对接php协议、uwsgi是nginx对接python协议 ​ 1p.1.1、编写动态页面，可以用PHP、python、java、 ​ 对于java，如果后端是一个tomcat类似的，这个就用ngx_http_proxy_module来支持了。 所以： ​ 前后端是java-tomcat之类，就用ngx_http_proxy_module了 ​ 前后端是python，比如jango，就用ngx_http_uwsgi_moduel了 ​ 前后端是php，就用ngx_http_fastcgi_module了 ngx_stream_proxy_module就是tcp的反代，不知道是否支持udp哦/ 反向代理分为同构和异构 异构、同构，就是client请求的协议和nginx身后的协议是否一样。 用户请求，可能是浏览器的80/443，也可能是mysql的客户端3306。 一个应用场景 就是程序员还是喜欢把ip地址写死在代码里，所以就让他们写成nginx的反代IP，然后后面的DB也好服务也罢这些IP可以变动的，反代IP相对固定唯一区分端口就行了，除了这个点，还有个好处，就是代码里可能都是也给nginx的IP然后端口区分就行了，即使nginx反代IP多个也是统一在nginx上的相对集中便于管理的。 一个简单架构 很正常的一个操作：就是image和网站，也就是http://images.xxx.com和http://www.xxx.com分开来部署的。 上图的http://images.xxx.com将来就只放静态图片，静态资源。很多网站都是这么部署的。这样也是就是不同资源在不同服务器上，也能实现并行下载的效果--因为一个浏览器向同一个域名发起请求最多支持5、6个。也就是说一个网站里放5、6个资源顶多再多也就是要二次请求下载了。而拆成多个网站就并发效果好。而且我认为拆成多个针对多个用户也是并发效果更好吧应该。 上图架构说明： 1、用户访问域名比如http://www.xxx.com里面涉及的图片就是其他站点URL了。网站域名就解析到左边的FW的公网IP，图片域名就解析到右边的FWIP，这里显然不是最佳实践，最佳实践是CDN的动静分离。 2、然后FW做DNAT映射到后面的VIP，这个里的VIP可以是LVS来做，然后LVS做TCP/UDP的反代，然后HTTP的就走TCP反代到nginx，再由nginx进行反代。这样无非就是四层的流量会走内核LVS快一点，但是其实HTTP就会多了一层LVS，所以这里可以将LVS和NGINX并排做成一个层级的，LVS给TCP/UDP服务，而nginx給HTTP服务。 3、fw的ha，fw自身解决，比如HSRP，比如juniper的nsrp。 4、lvs和nginx的ha就是依靠通用协议keepalive。 5、反代(lvs/nginx)接收vip进来的流量负载分担到身后内网的服务器，比如web网站这种动态资源站点比如image这种静态资源站点。 6、服务器本身还会去后面找DB，DB还需要做集群，这样可实现HA；或者用读写分离，如果是读写分离就是前置调度器，而调度器也要HA，同样读写分离调度器2个也需要用keepalive做HA的。 keepalive是一个通用的东西，实现浮动IP都可以用。 7、静态页面可以用NFS服务器挂一下就行了。然后用rsysnc+I notify来实现实时备份。https://cloud.tencent.com/developer/article/1373541 具体实现 ngx_http_proxy_module模块反代 做反代在192.168.126.132上配置👇 反代对于用户来讲，是看不到真实的server IP的； 后端服务器认为是谁在访问呢 调度器也就是反代nginx上肯定是可以看到真实的用户IP的👇 看下telnet www.site1.com 80的效果：只有192.168.126.130上也就是调度器上由TCP连接 后端server192.168.126.132上是没有的 原因就是，nginx的反代写的的http而不是telnet，人家是看协议的。 telnet测试http的方式👇，可见请求后没有断，应该能说明反代和后端都支持长连接吧。 反代上的log👇 server上的log👇 针对特定的文件夹(uri)进行反代调度-动静分离 相当于动静分离了👆只要是图片后缀的就调度到192.168.126.132上，只要是api路径的就调度到133上 搞个图片👆 👆133的页面内容，但是反代里写的是 会导致访问http://192.168.126.133/api，而该文件是不存在的。 所以创建所需的文件夹api， 或者api干脆就是个文件也行 访问图片的就会调度到192.168.126.132上👇 注意 如果写成 location /api { proxy_pass http://192.168.126.133/ # 实际访问的就是http://192.168.126.133/index.html了，此乃替换。 } location /api { proxy_pass http://192.168.126.133 # 实际访问的就是http://192.168.126.133/api；或者是http://192.168.126.133/api/index.html 此乃拼接 } location /api { proxy_pass http://192.168.126.133/index.html # 实际访问的就是http://192.168.126.133/index.html。此乃替换 } 修改后端服务监听的端口 好了👇 如果server那边用iptables drop，没有响应会导致tcp三次握手都失败，而且是没有回应，这样client端看到的就是504报错。 所以502和504的区别：502server可能没有监听倒是会明确告诉你一个信息的，504就是超时了。 以上就是实现了初始的调度，但是调度还要涉及 健康检查和往健康的server上调度的功效。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:57 "},"38-高性能WEB服务nginx03/38-高性能WEB服务nginx03.html":{"url":"38-高性能WEB服务nginx03/38-高性能WEB服务nginx03.html","title":"第三十八章 高性能WEB服务nginx03","keywords":"","body":"第三十八章 高性能WEB服务nginx03 38001 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 13:00:50 "},"38-高性能WEB服务nginx03/1-nginx反向代理缓存和IP透传等功能.html":{"url":"38-高性能WEB服务nginx03/1-nginx反向代理缓存和IP透传等功能.html","title":"第1节 nginx反向代理缓存和IP透传等功能","keywords":"","body":"第1节. nginx反向代理缓存和IP透传等功能 之前一节，接触了反代，但是发现两个问题 1、用户ip没有真实的反映到后端服务器上 2、调度没有真正的实现 如何让后端服务器得到真实的用户IP nginx反代的数据交互4个过程，②和④nginx是可以修改的 要实现RS(real server)也能获得真实的c(client)的IP信息，可以将这个信息在nginx上添加到请求报文的头部里通过②发个RS。 关于请求头，除了浏览器自己加的东西，也可以自定义一些(nginx 反代的时候或nginx做server的时候或者httpd做server的时候都可以吧)。 这些都是键值对，nginx上自定义 添加什么key:value都是可以的。 proxy_set_header X-Real-IP $remote_addr; X-Real-IP自定义的键，值就是用变量$remote_addr，这个在配置log格式的时候就见过的 然后key的配置位置，http、server、location都行，我们就放到location里 在反代nginx也就是192.168.126.130的nginx的配置中定义一个key，并使用remote_addr变量作为value👇 这样配置以后，只是添加了head头部里的一个叫做X-Real-IP的key，value是client ip，要想real server也就是后端真实的服务器记录实际的ip，还得修改对应的log字段吧。否则默认也是看不到的。 在后端服务器上，这里就是192.168.126.133上 的httpd配置中log里调用该变量。 固定格式 %{xxx}i 其中192.168.126.1就是真实的用户IP，儿192.168.126.130就是nginx反代。 这个X-Real-IP是自定义的，随便写的。 如果存在多级代理，remote_addr不好用了就 注意这里有一个变量教X-Forwarder-For，然后没经过一次反代，就会添加到proxy_add_x_forwarded_for变量里，类似列表的append。 第一个加进去的就是真实的客户端IP，然后后面的都是反代的IP。 类似这样： 然后X-Forwarder-For也是自定义个的名称，随便换名字都行；关键是后面的$proxy_add_x_forwarded_for是固定的变量，当然规范还是用通用的名称便于维护。 然后 ①、整条链路上所有的代理nginx上都要做这个配置，才能完成client,proxy1,proxy2的信息传递和累计。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; ②、最后的real server上加X-Forwarded-For变量到日志里就行了。 LogFormat \"%{X-Forwarded-For}i %h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined 缓存功能 C1第一次访问某个资源，nginx反代会像real server请求；并缓存该资源 C2也访问这个资源的时候，就命中了该缓存资源，于是nginx不会再向后端请求，直接将对应资源返给C2。 实验缓存的效果 1、既然是缓存，就要找一个大点的页面做测试 2、ab命令测试 在192.168.126.133上的api目录下弄一个大文件m.html并可读👇 在192.168.126.130上curl 可见👇 同样在130上ab测试 👆以上是未开启中间反代缓存功能的结果，c--nginx---server ①首先proxy_cache_path指定的path就是存放缓存的地方 ②levels=levels就是1:2:2这个和之前上传资源的存放是一样的 然后缓存的使用，也分内存和硬盘空间， ③key_zone=proxycache:20m # proxycache是自定义的缓存名称 用来调用的，然后20m是指内存中缓存的大小，主要用于存放key和metadata（如：使用次数） 比如，存放的key是什么，比如就是url，然后对url进行哈希运算，运算之后放到缓冲区proxy_cache里 比如，metadata元数据里存放了访问了多少次。 ④inactive=120s； 缓存有效期，应该就是磁盘上的文件失效自动删除，我理解。 ⑤max_size=1g; 磁盘上缓存文件也即是path里的东西最大占用空间。 ⑥proxy_cache zone | off; 开启缓存，默认是off。 zone就是写上面定义的keys_zone名称--proxycache完成开启缓存，调用在：http、server、location模块里。 ⑦proxy_cache_key string; 对什么内容进行缓存呢，对key键的内容做缓存的，默认的proxy_cache_key的值是 $scheme$proxy_host$request_uri;来做缓存的。 scheme是协议的变量；proxy_host是代理服务器的主机头；request_uri就是请求的uri了。 组合起来就是完整的url了吧。然后针对这个组合的信息做哈希，如果哈希一样，就认为缓存信息就一样。$scheme$proxy_host$request_uri;这个信息作为key进行哈希，然后去哈希值的1:2:2也就是倒数1位作为文件夹/倒数2-3位作为子目录/倒是4-5位再作为子目录，然后将页面html的内容存进去。 这样的话，下次再访问同样的url的时候(内容中存着呢)，就按照同样的哈希值进到对应的1:2:2目录下找网页内容就行了。然后直接放给用户。 此时就总结下：其实很简单，就是key_zone=xxxx:20m；这就是内容中定一个zone名词叫xxxx，然后里面通常就存放访问的url和调用的次数，如果有人请求的url命中内存中放的url，且根据你定义的响应码如果是200 之类的就利用缓存，此时就会到对应的哈希目录里去找网页内容，而这个对应的哈希目录就是levels=1:2:2方式建立的。 ， ⑧proxy_cache_valid [code...] time; 比如proxy_cache_valid 200 302 10m;就是针对200、302缓存10分钟。 上下文也就是配置模块写在http中。 当出现错误的时候，意味着后端服务器不能响应，此时可以使用过期缓存来应答给用户。虽然后端服务器down了，但是曾经缓存过这些资源，所以还是可以让用户看看的。 所以，这里的\"过期\"是指和后端real server相比而言是过期的缓存，不是指inactive=120s那个失效时间？ 哪些方法启用缓存 默认是GET和HEAD都是被缓存的。 下面开始实验 proxy_cache_path只能在 http模块下进行配置 其中proxycache代表整个缓存定义信息(proxy_cache_path的内容) 接下来还需要引用这个缓存定义proxycache，引用可以在三个地方👇，这里使用server虚拟主机里去配置。 proxy_cache proxycahce;就是调用了上面定义的缓存信息。 不过当前还没有缓存信息 触发下 这个就是用户访问的url的哈希值作为文件名；倒数第1个d，倒数2-3位就是3a，倒数4-5位就是9d，整体就是1:2:2来利用哈希值来创建目录的。这样做的好处就是大量级的文件将来可以快速索引过去，如果堆在一个文件夹里ls都能半天才出来。 该文件的内容就是网页。 此时缓存有了，ab 压测再看👇无缓存的效率 然后缓存方面会有一个问题， ①rm -rf m.html # 删除页面文件 ②cp /etc/fstab /xx/xxx/xx/m.html # 创建一个不一样内容的页面文件，但是名字一样 ③curl 看看结果还是只来未删除的页面内容， ④原因：因为文件名称没变，url没变，哈希没变，所以还是利用的老的缓存。 ⑤处理方法：缓存时间不要太长，超时后，页面重新请求就对了；或者手动清理一下缓存。 后端服务器信息的隐藏和透传 比如上图的ETag要隐藏起来，ETag是后端服务器负责生成的。访问不同的url的ETag值也不一样。 上图是在192.168.126.130 反代上测试的，其实和client用户那边差不多了 然后下图👇是在后端real server上curl的ETag是一样的，可见ETag就是real server生成的，反代nginx也不会修改的，就是透传到用户那边的。 如果不希望用户看到ETag，就可以在反代上配置proxy_hide_header Etag; 加以隐藏。 有些客户端看不到的，默认就看不到的，也可以用prox_pass_header field;加以透传。 添加一些自定义的头部信息或尾部信息 上面的是修改，就是隐藏和透传，现在是添加自定义的key:value ①缓存利用状态信息添加到头部里 add_header X-Cache $upstream_cache_status; X-Cache自定义的key名词，可以改成其他的，虽然不推荐改。 然后第一次是MISS表示没有命中缓存，第一次也没有缓存啊 第二次就HIT了 三个512一大的分配就好，不一样大，会导致：一会分配内存，一会回收内存，反而性能不好 以前httpd支持php那会是打了补丁，也就是新加了模块，让apache具备了处理php的能力。这种方式不如专门的fastcgi独立软件甚至是独立服务来处理php。 nginx没有像apache那样的模块来处理php，同样是通过fastcgi的方式来支持的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:57 "},"38-高性能WEB服务nginx03/2-nginx实现fastcgi反向代理.html":{"url":"38-高性能WEB服务nginx03/2-nginx实现fastcgi反向代理.html","title":"第2节 nginx实现fastcgi反向代理","keywords":"","body":"第2节. nginx实现fastcgi反向代理 概述 用户还是访问页面，还是http请求，不过请求的内容变成了php文件，也就是程序代码了，是动态资源。 现在nginx反代收到这个php的http请求，本身是处理不了的，需要通过fastcgi协议把php请求发送给后端的php服务器，类似fpm-php服务器，默认监听9000端口。 这种就是异构了👇 搭建一个fastcgi的real server 1、物理上可以将反代nginx和real server放在一台服务器上； 2、也可以分开放。 总结下，一两句简单点就是：fastcgi是快速的网关接口，所谓快速就是 类似nginx的master和worker父子进程的一个处理效率；所谓网关接口，就是web服务和php程序代码的一个通信协议。 实验：反代+real server放在一台机器上 先搞一个fastcgi的服务器： 是epel源里的 需要安装php-fpm作为fastcgi的服务器，还需要安装php-mysql用来连接数据库。 不过现在好像叫php-mysqlnd了，装之 监听的端口没写，而是socket文件，还不是tcp/ip端口。不过由于反代和fastcgi server也就是php-fpm在一个机器上，还想不用改，就用socket文件还不错。 然后listen.allowed_cliants 就是127本地来连接。 需要修改的是user = apache 改为user = nginx，因为php-fpm这里配合的是nginx不是apache。 要用nginx作为服务的启动账户，也会开启类似httpd的worker进程，master进程。这里nginx就是work进程的用户身份。 不改就会存在问题，比如nginx传一个文件过来，php用其他的账号，可能就会存在权限不够的情况。 修改user 和 group 为nginx 重启服务后，work进程的用户就是nginx了，当然master进程还的是root 由于是socket文件，所以ss -tlnup 就没必要看监听端口了 然后php程序放在哪？nginx的站点root路径？往下看，可以不在一起的 由于上图的虚拟主机没有配置root，所以报错 补上root ok 将php程序放到别的地方， 下面要让nginx收到http请求访问php程序的报文后，nginx要知道去找php-fpm服务，然后该服务(从socket文件或者9000端口收到后)还得知道去哪找php程序，也就是上图的路径。 1、首先nginx反代上要有fastcgi模块：ngx_http_fastcgi_module 2、然后要配置fastcgi服务器的地址：fastcgi_pass address；fastcgi和反代在一起就指向本机，不过是127.0.0.1还是本机的IP地址需要明确一下。 3、指定类似index.html的php资源，一般就是index.php，指定cli为：fastcgi_index index.php; 4、fastcgi和nginx之间的数据交换，需要用一些参数，这些参数是固定格式的，引用就行：fastcgi_param parameter value [if_not_empty]; 所以看看fastcgi_params参数👇，直接引用就行： 参数的使用是需要在配置文件里引用这个文件的，具体就是location里配置 include fastcgi_params; fastcgi_params里就是一个文件，里面内容上面也讲了。 然后fastcgi_param看起来就是自己定义的一些其他参数咯，比如 SCRIPT_FAILENAME脚本文件名，$document_root就是 root /xx/xx定义的根路径；而fastcgi_script_name就是脚本名称。 上图也写明了，只有访问 ~* .php$的请求才发送到fastcgi_param参数指定的路径/data/php下的$fastcgi_script_name这个文件。 关于$fastcgi_script_name具体是啥，可以在log中配置上，然后去log里查看 上图👆fastcgi_pass unix:xxxx是因为php-fpm配置文件里也是这样写的 如果上图写成listen = 127.0.0.1:9000那么nginx的配置location里一样要写成fastcgi_pass 127.0.0.1:9000; log里配置这个$fastcgi_script_name变量 可见fastcgi_script_name为php.index，其实就是用户访问的php程序的名称，通常就是url最后一个字段。 nginx+wordpress wordpress是php写的，所以nginx也是用fastcgi去对接的，其实wordpress就相当于一个fastcgi服务器了。 搞定db 安装这个👆，启动db服务后，创建数据库和用户以及授权👇 确认下 上图/16写错了，改成192.168.% 在远端一台机器上测试联通性，ok👇 wordpress数据库准好了，接下来弄wordpress程序 搞定wordpress程序 直接github拉到自己规划的目录里就行 https://github.com/WordPress/WordPress.git 不过wordpress的版本也要php的版本适配的。 github上也提到了推荐的版本 php是7.4以上，mysql是8.0以上 我用的mariadb-server直接yum的，mariadb的版本和mysql有一个平行分叉点是5.5吧，后面mysql又经过了几个5.x的小版本就直接跳到了8，而mariadb则是跳到了10好像。 所以一般来讲，这里可以认为mariadb10.11满足了wordpress的db要求--mysql8.0以上的这个要求。 利用模板创建wordpress配置文件并修改 然后注意下wordpress文件夹的权限，比如某些文件夹的上传权限wp-content这个文件夹就是接收上传的。而权限是开给nginx这个用户的，因为之前针对fastcgi服务也就是php-fpm已经修改为nginx了 略微改一下php的访问路径 注意上图红框里写错了，大小写不对 修改为root /data/php/WordPress; 重启nginx后，发现访问有点问题，👇是空白页，而且是302重定向 重新git clone不修改wp-config.php就是这样👇 浏览器里输入http://www.site1.com/index.php，就会跳到 不过图片之类的不对，就是一些静态页面包裹css，图片之类的，好像没显示出来。 一下就是除了.php以外的文件，应该都属于静态文件 修改wp-config.php继续空白页，继续研究下 打开wp-confi.php里的debug看到502 也去看看错误日志 看到too big header，增加buffer 再次访问，发现原来是存在大量报错导致buffer不够用👇 网上说 https://stackoverflow.com/questions/70040287/php7-4-preg-replace-compilation-failed-unrecognised-compile-time-option-bi 然后我的是rockylinux 没有libpcre2，但是有pcre2 于是果断yum之 再记得重启php-fpm 此时再次浏览器访问http://www.site1.com/index.php就好啦 当然是存在302跳转的。 这里可以简单做一个动静分离 改之 这样xx.php的还是走location去 /data/php/WordPress/下找 而不是xx.php的就去/data/site1/wordpressStatics/下找 再次访问http://www.site1.com/index.php看看 没得问题👇 然后优化成访问http://www.site1.com直接就是上图的页面，也就是说index 设置为index.php了。 安排~具体配置如下 尝试在root下新建一个index.html看看，因为index index.php index.html是从左到右先找先得。 好了 这样就实现了输入http://www.site1.com，注意上图我输入的时候最后是没有/的，是nginx自己302补了一个： 所以index的php文件要有，才能跳到location 的php里 继续排错吧，哈哈 看图是http://site1.com/wp-admin/的403，我基本就认为是该路径下没有index.php和inde.html 还是因为我做了动静分离，所以这个URL走的是下图的框框里的路径 而该路径下是没有index文件的，补一个就行了，如果坚持要动静分离的话。 其实如果不做动静分离也没这么些毛病。 再次登入就OK啦👇，上图的echo 只是满足一下让他有个跳转到location里。而location里的root /data/php/WordPress/下的wp-admin下是有各种php文件的，关键是有index.php 创建文章，上传图片的时候报错 看error log 所以你认为到底是哪个root呢？哈哈，上图是referrer从http://www.site1.com/wp-admin/post.php?post=8&action=eidt跳过来的，姑且认为在下面的root里面吧 所以去/data/php/WordPress/wp-admin/看看 其实想想也知道，这些多层级的文件夹，肯定不用手动创建的嘛，就是动静分离后一些程序没有生效导致的。 这里手动创建目录，然后接着报错变了 继续报错，可能是php程序没有打赏alt attribute，我猜是动静分离后一些程序没有运行起来。 得，我不弄动静分离了，合起来算了 果然好啦 其实很好理解，你强行做的动静分离，人家一些php没有办法起作用了。 php的一个参数项 在fastcgi的配置里，也就是php-fpm服务的配置里 不过和apache的status冲突了，大家都是根路径带一个/status，不知道访问哪一个了，改之 然后测试fastcgi服务器是否ok的 以上完成了①status页面②ping的页面，这是php-fpm也就是fastcgi的设置， 下面做nginx的配置，将这个两个/fpm_stats /ping的页面 路由到fastcgi服务上去才行啊 上图的locaiton 一行 ~*表示正则忽略大小姐， ^表示主机头后面的部分也就是www.site1.com后面的部分以/开头的。 ping pong~ xxxxxx/fpm_status?full查看详情👇 所谓详情就是php-fpm服务的所有进程都给你显示出来了 将nginx和php-fpm分开来 独立安装php-fpm和php-mysqlnd 上面是合在一起的，用的socket文件 ①php-fpm的配置，也就是监听 ②nginx的fastcgi对接 合在一起，也可以改成listen = 127.0.0.1:9000 以及 nginx的 fastcgi_pass 127.0.0.1:9000 分开来也没啥好说的，无非就是mysql独立、php-fpm独立、nginx独立，用ip port互相连接就行了。 一些信息操作的记录下 安装php-fpm的yum源除了默认EPEL，还可以用REMI仓库，REMI也是依赖EPEL源的。 找到所需的资源 yum install https://mirrors.tuna.tsinghua.edu.cn/remi/enterprise/remi-release-9.rpm 或者下载下来后， yum install remi-release-9.rpm 可以用rpm -ql看看installed包里的内容 说白了就是给你安装，也就是自动创建了yum源文件 找到这个版本较高的php-fpm了就 再一个确认下 激活之 发现无所谓了 一样的remi源和remi-safe一样的，估计就是一个更加safe咯。 除了安装这个php-fpm还需要安装这个软件和mysql互通的软件php-mysqlnd 装之 yum -y install php83-php-fpm php83-php-mysqlnd php-fpm是通过php-pdo或者php-mysqlnd来支持 连接数据库的 配置一下php-fpm 需要在php-fpm服务器上创建nginx用户和用户组，创建之前去nginx机器上看下uid，然后php-fpm创建的时候要保持一致。 groupadd -g 989 nginx useradd -r -u 989 -g nginx -s /sbin/nologin nginx 如果只是有nginx用户名，不保持ID一致会咋样，我怎么感觉没必要id一致呢，试试 上图的注释方式不适合php-fpm的配置文件，需要将#改成;实现注释，否则报错，如下面stauts 放行所有人连接过来，其实可以写nginx的IP地址会不会安全点👇，结果写错了应该写130而不是134，然后这就导致了502报错了见下面的502报错页面 状态页面也改一下： 改完就启服 这是之前的报错已修正👇 通过status看到具体错误的地方，就是注释#改成;就行了 放置好php程序 把项目down下来，直接git clone url就行，网络不给力就down zip吧 修改一下wordpress的配置文件 修改wp-confi.php连接数据的信息 这里的数据库也是独立的 测试下顺便 所以现在 nginx是192.168.126.130 php-fpm是192.168.126.135 mariadb是192.168.126.134 都分开咯 修改nginx的fastcgi的配置为远端php-fpm服务器 502报错很可能就是php-fpm里没有放开nginx的访问 修改一下 好了就 然后怎么查看php的版本 之前的看法是错误的好像 首先php -v的命令是依赖于php-cli这个包，而这个cli看到的版本其实是php-cli的版本。 status里看不到版本唉 干脆写一个version_look的php页面吧 没问题 再转一个php-cli瞧瞧，看看到底php -v行不行 果然php -v 只是仅仅看php-cli 这个命令函工具的版本的，和php-fpm没有关系 然后这里有一个php-fpm的配置参考 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:57 "},"38-高性能WEB服务nginx03/3-nginx反向代理实现负载均衡及调度方法.html":{"url":"38-高性能WEB服务nginx03/3-nginx反向代理实现负载均衡及调度方法.html","title":"第3节 nginx反向代理实现负载均衡及调度方法","keywords":"","body":"第3节. nginx反向代理实现负载均衡及调度方法 nginx动态资源缓存 接上文，fastcgi的缓存利用和之前讲的基本一样 之前的缓存是nginx针对静态页面的缓存👇 现在要梳理的是nginx针对动态页面方面的fastcgi的缓存支持 对比下 ①fastcgi_cache zone |off # 调用keys_zone=xxx起的名字xxx:20m ②fastcgi_cache_key string # 就是哪些部分作为哈希缓存，就是所谓的key string ③fastcgi_cache_methods GET | HEAD | POST ...; # 哪些方式定义缓存 ④fastcgi_keep_conn on | off ; # 收到后端realserver后是否立即关闭连接，推荐启用长连接 ⑤fastcgi_cache_valid [CODE ... ]time; # 不同的响应码的缓存时长 fastcgi缓存的具体配置 主配配置文件里修改一下👇 然后调用这个定义的keys_zone:fcgicache 对比测试：未开启的效率 对比测试👇 响应速度明显提升 这是缓存信息👇 这就是缓存的效果，速度大大提升，问题--动态页面静态化了；什么意思，就是命名时php程序的访问，结果变成了缓存的静态资源应答，页面的更新明显时滞后的，所以使用的场景一般就是博客文章、论坛这些不要实时的业务了。 另外，上面时php程序的服务，用fastcgi对接； 如果是java程序，比如tomcat的，就直接nginx代理就行就是http协议i对接。 如果是python程序，比如django框架，就需要用uwsgi_params，其实nginx也是交给类似uwsgi的服务，nginx把请求交给该服务，然后该uwsgi服务再转给python程序。就类似nginx发给php-fpm，php-fpm再转给wordpress这个php代码一样。 调度功能 终于学到这一章节啦，调度才是网工普遍关心的点，也是从F5联系过来的吧。 upstream模块，可以把后端realserver分组起名，nginx就往这些分组上调度，每个组里一般就有好多服务器，此时就不再固定的只转发到一台机器了。 配置逻辑上 👇下图是一个tcp 反代，配置在stream层级下的，如果是http反代是写到http模块里的。 ①先用upstream定义出一组服务器定义出来，后端serverS的ip+port ②然后把这些ip和端口，起一个统一的名字 ③接着往这些分组上调度，用proxy_pass调度到upstream定义的分组名称上 ④stream的上下文是main，换句话讲 stream是配置在全局最顶层的； ⑤upstream是配置再stream或http下的。 一些参数看看 先配置http的反代调度 简单讲就是proxy_pass不再指向某个固定url，而是一个定义好的组名，组内有N个url。 一般来讲 配置上，一个是主配指文件/etc/nginx/nginx.conf 和 其他子配置文件/etc/nginx/conf.d/xxx.conf。 前置放http，后者放独立的server虚拟主机。 所以按照这个思路，我就把upstream要去主配置文件里配置，而不是折腾到子配置文件里了对吧，否则冲突了就。 准备两个后端服务器，并配置好测试页面 然后在nginx的主配置文件里的http层级下，定义好虚拟主机组 然后在子配置文件里，配置调度 上图的index.php要删掉，否则还是访问到之前实验的php文件了。 重启服务，测试 发现没有轮询，memebers，而且即使192.168.126.134 httpd服务down掉了，还是显示134，说明缓存起作用咯。 确实开启了缓存 看下缓存文件， 此时2分钟时间到了，缓存失效了，再次curl发现轮询调度到另一台机器了也就是135 再次开启134的httpd，这样134和135的apache服务都开着了， 然后测试如下图 发现轮询是存在的。 干脆关闭缓存 再次测试轮询 也是ok的。上面已经实现了自动调度了， 再看下调度的算法的细节 首先，默认就是wrr调度机制 wrr就是Nginx 调度算法中的 \"WRR\" 全称是 \"Weighted Round Robin\"，即加权轮询算法。 测试，通过配置权重，观察curl的返回站点的页面，看调度比例是否改变。 配置之前，先看下上图默认wrr 大家一样的权重，调度基本是均匀的。 测试发现还真是3:1的调度比例 除了weight分担比重外，还有最大连接数 max_conns=10 # 最大连接数10个 max_fails=10 # 10次nginx连接后端realserver失败就标记位不可用，默认为1次 fail_timeout=15 # 上面的max_fails=10次连接不上(默认10s)，判定为不可用之后，15s会重新判定。就是认为人家不可用，也只是暂时，正所谓3日不见，当刮目相看。一周不见自然永远不见~~古语有云(我说古语就是古语)3分天注定7分靠打拼，3分颜值7分作，作死的作~，3分靠颜值，7分靠人品，品德的品！你品~细品，细细品~ 灰度发布 down # 灰度升级，标记的状态realserver就不会被调度了。此时就可以对这个太down的服务器进行服务升级，因为不被调度到，所以不影响业务。 注意修改 upstream里的某台realserver为down后，你不能说重启nginx的哦，这样所有连接就断了业务就断了，你只需要nginx -s reload重载配置文件就可以了。 reload不会影响正常的业务，除非你的配置文件down掉的就是提供服务的那个realserver才会断，当然如果有调度的空间还是可以的(所谓空间，就是能调度到其他正常的node就只是瞬断了，TMD不会瞬断，看下图验证) reload测试 这是调度1:1 然后修改配置文件 再reload 看长连接是否断开 telnet www.site1.com 80后挂在那里 然后修改配置文件 然后reload，业务不断 发现 1、1：1 调度的情况下，cmd 先挂上去，保证调度到的是192.168.1126.135 2、然后修改192.168.126.135为down 3、然后nginx -s reload 4、发现nginx的ss -nt 不会断，然后cmd里也不会断。 5、分析，nginx服务没断，cmd的长连接是和nginx-192.168.126.130保持的，所以nginx不重启就不会断，然后reload后即使135这个ip down，也不会影响啊，因为那个是nginx和后面realserver的连接，那个本来默认就是瞬断的，你要测试的是修改为长连接后的配置调度写down，reload后的长连接是否会down，然后这个实验上面并没有体现。上面的实验很搞笑，只是nginx和client的长连接仅仅受nginx服务影响，不会收到nginx身后的realserver影响。除非改长连接。 灰度升级的常规方法：①先把某个realserver下线，也就是upstream 里的server配置为down，然后nginx -s reload，就不会调度到这个机器上了；②然后就升级这个机器就行了，升级好了就测试一段时间OK后③去掉配置文件里upstream层级里的realserver的down关键字就行了，然后nginx -s reload让新机器参与调度。 sorry server backup 就是俗称sorry server，将服务器标记位备用，其他所有服务器都不可用之后就会使用这个backup标记的机器，一般就是将nginx反代本身作为sorry server， 再配置一个8080的虚拟主机 nginx -s reload后，就可以测试了效果了， 主配指里的http快定义好realserver组名 再到子配置文件里 定义好server，写明location里的proxy_pass作为反代，协议和组名的调用。 然后实验，就是关闭realserver看sorry server 然后恢复业务测试下 注意，nginx的10k只自己做webserver，反代是socket tcp/ip套接字的socket是支持不到10k的连接的。LVS可以达到上百万级别的连接量并发量吧应该叫。这个nginx的tcp/ip反代也就是支持3w多的并发。这里还有个逻辑就是nginx是基于tcp/ip socket的也就是ip+port所谓端口也就是1-65535了，所以理论上就是6w而已，然后lvs是基于内核的不是走socket的所以不受这个65535的限制可以达到上百万。所以一些大业务量就是前面顶一个LVS，后面再分业务场景用nginx做反代。总入口是用LVS，下面分散的是用nginx的。nginx的一些功能是LVS做不到的，比如应用层的调度。 调度算法 LVS是有10种调度算法，nginx也有N种，后面可以对比总结一下。 wrr nginx默认是wrr调度算法，1:1调度，上面测试过了已经，有几台就依次调度几台。比如5台realserver就是1:1:1:1:1咯 ip_hash ip_hash：源地址hash调度算法，说白了就是同一个客户端ip就往同一个realserver上调度。下面测一下源地址hash也就是ip_hash的调度效果。 ip_hash是基于/24的网段来LB的，所以以下实验一直到更换/24网段才测出来效果👇 好像测试下来要是192.168.126.0/24的client访问都是调度到135的机器上去了，下图为证 其他同网段的IP一个个试下来都调度到一样的realserver 135上了 我还以为是134挂了呢，结果并不是👇 这样，给nginx再加一个网卡设置成桥接到物理网卡，然后换办公网的N个IP段来curl 这样nginx就是192.168.56.0/24段的，然后外面我找56段也找25段不行再找其他段来测。 经过测试终于在192.168.11.77上得到调度到134的现象，不容易啊👇 所以正如网上讲的👇 使用ip_hash指令无法保证后端服务器的负载均衡，可能有些后端服务器接收的请求多，有些后端服务器收到的请求少，而且设置后端服务权重等方法将不起作用。所以，如果后端的动态应用服务器能够做到SESSION共享，还是建议采用后端服务的SESSION共享方式代替Nginx的ip_hash方式。 wrr才能是weight设置生效啊，ip_hash自然不吃weight这一套啦。 这种ip_hash负载分担一点都不均衡(只是看/24的不看/32)，和LACP的链路捆绑也就是端口聚合的负载相比，均衡的效果差远了。这种还不完全是奇偶负载均衡，但也是比较均衡的。 和飞塔的ECMP相比也差远了，👇这个是标准的IP负载均衡，比如192.168.25.10从一个线路出去，改成25.11就会换一个线路了，这个效果就是奇偶负载均衡。 好了继续看别的吧 least_conn has key [consistent] 刚才上文的ip_hash就是针对client的ip做hash来进行调度，不过调度的差异化不明显，结果就是负载分担不均衡。 而这里的has key是针对key进行hash运算的。 也就是说 hash $remote_addr就是等价于ip_hash咯 hash $request_uri 就是针对访问的站点的url进行哈希，也就是不同的站点调度到不同的realserver上。 # 类似LVS的DH算法 测一下hash $request_uri ①修改调度策略 ②做一堆页面出来 我现在是192.168.126.134和192.168.126.135两个IP作为realserver，所以在这两台server上分别做10个页面 for i in {1..10};do echo `hostname -I` Page $i > /var/www/html/test$i.html;done ③测试 不同的client访问10个页面，调度的结果都是一样的，说明就是依据request uri调度的，访问同一个目标url，就是往同一个realserver调度的。 好，如果是以上的调度算法，会有会话保持的问题，也就是说用户访问http://www.site1.com/test1.html是一个realserver，然后访问http://www.site1.com/test5.html又是一个realserver，那么一些保留的会话信息就没了。比如cookies，登入的信息，如果基于uri调度到别的机器，那么login就没了。还比如php文件的访问，php是程序了，程序运行依赖的一些环境换了realserver可能也存在问题，比如之前的cookies肯定就没了。 解决思路有： ①session服务器前文提到过，可以过去复习下，就会知道redis的作用，cookie的设置以及time()的计算方式，哈哈，不要怕花时间就可以再次捡起来这些有用/无用的知识了，天地无用~ 上图的章节里提到了这么几个方案： 1.1 session缓存，就是session id放到cookie里面来玩的， (session就是server存放的表示某个会话的一堆信息，但是id就是代表啦，可以认为session id就类似数据库表里的主键，比如id 1000代表登入网站的用户名，购物车里的东西等等信息。) 。 # hash $cookie_name; 这样写就行。 ​ 但是cookie有很多，所谓cookie就是很多个key:value键值对，hash写的时候要写清楚是哪一个cookie的key。 1.2 session复制，所有的realserver互相同步也就是复制会话，肯定不可取。 1.3 session服务器，就是用redis来做session服务器，这个是很好的方案，不过login的信息要拆分到mysql里去做数据持久化的。redis是基于内存的，不是持久化存放的。 补充以下cookies 像这种👇就是一个包里有N个cookie的，每个cookie就是一个键值对。 如果前端调度器也就是nginx反代，就需要明确指定依据cookies里的哪个cookie也就是键值对来调度。 要知道client发动的请求里会有一大堆cookies的。 这个👇sticky指令确实能够实现基于cookie来调度，不过是商业版是收费的，需要用hash指令实现。 hash实现就免费啦👇 再查查变量就能用起来啦。 hash $cookie_name; 顺便瞧瞧redis的功效 ②继续说session id的事情 client请求发过来，server那边的php或者java程序 可以轻松做到 根据不同的client分配不同的session id。 测试下sessionid的事情 测试1：使用curl -b 结合 log 的方式 产生和显示cookie 用log查看session 首先cookiename的落地写法其实是 cookie变量名称，比如你要用sessionid这个值，就写成cookie_sessionid，具体如下 这里log里显示出cookiesessionid的值，注意cookie后面的sessionid才是你要设置的键值对里的键。值你就设置个123456测试下。 然后curl产生cookie，注意键值对要是sessionid这个变量 最后就能在log里看到 测试2：使用server的php程序打上cookie和log查看的方式 log格式还是和上面一样这个是nginx反代192.168.126.130的log配置 架构： 130(nginx) -----134 | 135 这两个realserver php需要nginx的fastcgi对接，这里就使用192.168.126.135:9000的php程序来打上cookie 注意：这里不需要135上开启http，只需要fastcgi对接的php-fpm服务也就是9000端口服务就行了。 然后用浏览器打开，记得使用无痕 第二次访问就有cookie了，当然这里log里只是显示了cookie里的sessionid值，其实也可以都显示出来的。 浏览器里看看再 补充截图 与之对应的log里由于配置了log 所以log里就看到三个cookies了 其中前两个cookies的都是会话级的，会话没了就失效了 最后一个cookie是1小时老化的。 测试3：使用echo进行显示cookies，当然打上cookie可以用curl -b或server端的php或者java程序来弄 要使用echo，需要使用： ①echo插件 ②nginx的编译的时候带上echo插件 以下是实验过程记录 ③echo使用的时候需要补上default_type text/html来防止浏览器自动下载 echo的详情见前文 1、下载两个包：echo插件和nginx源码 2、安装依赖 3、编译安装nginx，且带上echo插件 解压 开始编译安装 上面的cli会导致make编译的时候报错，修改为👇 编译并安装成功后 停掉当前机器的httpd，启用nginx 上图其实错了，server_name *site1.com;写法明明是错的，但是为什么ngintx -t检查ok呢，因为主配置文件里压根没有引用到这个子配置文件，所以语法检查就没有检查到这里，所以这里要注意的。 修改主配置文件里的引用 纠错上图 再次nginx -t就看到报错了 看着基本ok了，测试一波 测试现象1：日志路径是日志的路径，调度的路径是调度的路径 浏览器打开http://www.site1.com 观察日志 所以，就是反代的nginx提供的服务，你请求过来访问http://www.site1.com ，匹配到我的server块对吧，也就是server_name *.site1.com 80端口的，自然就是路由到/varxxx/www/html/index.html啦，所以log里就是这么记录了，没毛病的。 然后由于配置了proxy_pass http://websrvs；所以继续调度，日志是日志，调度是调度，各回各家，各找各妈。 然后看日志是调度到135的嘛，所以135上就是看80是nginx编译的那个程序提供的，于是就是路由到/apps/nginx/html/index.html啦。所以页面就是 也是符合文件里的内容的 现象2：后端server上的子配置文件未生效，应该说本地访问生效，远端通过nginx访问无效。 直接上结论：用户直接访问192.168.126.135就是请求直接落到192.168.126.135的，不管是域名还是IP。都是找对应的配置文件里的server块。而用户访问nginx反代，不管用户是域名还是IP访问的，nginx反代访问后端192.168.126.135都是用的ip，因为你配置的就是IP。这才是根本👇 基于上面的结论，所以才会又下面的测试现象： 所以server虚拟主机的路由，一定要看真正发到server上的请求是什么，而不是client发出来的是什么，因为client--->nginx--->server，nginx里可能反代发送过去的就不是你client发送的样子。比如client发送的域名，而nginx反代转过去的是IP。自然命中的server块是不一样的。 再一个，上图的localhost看来也是可以涵盖192.168.126.135个IP的，不是说localhost仅仅是127.0.0.1咯。这个要纠正一下了吧。不要矫枉过正，就是纠正过头，localhost就是127，不要瞎理解，这里localhost在nginx里只是可以处理监听的所有本地IP的作用而已。 言归正传，开始测试cookies 既然c--n---s，n发过去的都是ip，所以就在主配置文件里配置吧 curl -b 测试 一个cookie就是一个-b 调度cookie 几人上面测试cookies是可以抓到的，那么就用hash $cookie_name来调度 下面就通过curl -b 测试 固定的sessionid是不是都是固定调度到同一个realserver上了呢。 不过实际情况就是server给client打上sessionid了，nginx然后利用这个sessionid来区分对待不同的client和server。 此外还有keepalive 参数 keepalive 连接数N; 为每个worker进程保留的空闲的长连接数量，可节约nginx端口，并减少 连接管理的消耗 client-------nginx----------varnish1 | 2 | 3 | 很多个varish ---- 后端很多个realserver hash $request_uri 的解释 hash ( /a.html ) = 128bit % 6 => 0 -5 # varnish的权重是1 2 3 ，所以整体权重就是6，hash值对6取模，也就是0 1 2 3 4 5 个结果。 varnish 1/6 0 # 1/6就是hash % 6结果-0就 调度到 这个台varnish varnish 2/6 1,2 # hash % 6 结果是1，2 就往这里调度 varnish 3/6 3,4,5 # hash % 6 结果是 3 4 5 就往这里调度。 如果varnish挂了一台，或者后端realserver增加了，varnish跟不上业务量了，再加一个varnish 此时调度分母变成了10，意味着hash ( /a.html ) = 128bit % 6 变成了 % 10，此时取模的结果全变了，意味着所有缓存全部失效！此时缓存穿透，压力全部给到后端服务器了，业务瞬间爆了。因为业务基本都是靠缓存提供快速响应的，缓存没了，靠服务器是提供不起来的。varnish可能就承担了80%的业务量，后端realserver只是20%顶多也许。 varnish 1/10 varnish 2/10 varnish 3/10 varnish 4/10 这就是一致性hash算法产生的背景。 下章继续 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"38-高性能WEB服务nginx03/4-nginx四层代理功能和tengine编译安装.html":{"url":"38-高性能WEB服务nginx03/4-nginx四层代理功能和tengine编译安装.html","title":"第4节 nginx四层代理功能和tengine编译安装","keywords":"","body":"第4节. nginx四层代理功能和tengine编译安装 非常经典的一致性hash算法 ①背景问题产生的原因：接上篇，上篇交代了该算法产生的背景，是缓存穿透。所以才有了这么个 \"一致性hash算法\" https://www.xiaolincoding.com/os/8_network_system/hash.html#%E5%A6%82%E4%BD%95%E5%88%86%E9%85%8D%E8%AF%B7%E6%B1%82 可以看看这篇介绍，别人写的👆 简单总结下 ②为了解决上述问题(①里介绍)，于是做了如下优化 hash (/a.html) = 128bit % 2^32 = 0 ~ 2^32-1 # 一个url就是某一个值，无数个类似的值就组成了一个0~2^32范围的区间,这区间他们说看成一个环，好，先这么着，依你~ 环你大爷，为啥环？先理解只是不吵，我估计就是取模的数据模型可能就是不断的各种被除数都是逃不出这个余数的圈圈咯。 varnish 1 hash(192.168.126.101) % 2^32 # varnish缓存的ip也一样落在上面的0~2^32区间里 varnish 2 # 这个节点权重是2，也是生成2次随机数👇，这两个随机数也落在区间里的2个位置。 hash(192.168.126.102 + random_1) % 2^32 hash(192.168.126.102 + random_2) % 2^32 varnish 3 # 这个节点权重是3，也是生成3次随机数👇，这两个随机数也落在区间里的3个位置。 hash(192.168.126.103 + random_1) % 2^32 hash(192.168.126.103 + random_2) % 2^32 hash(192.168.126.103 + random_3) % 2^32 一句话总结：调度的节点 落在 区间里；请求的url也落在区间里。把区间看成换于是就是这样了👇 大概意思就是黑圈是url，绿框就是varnish节点，都是通过哈希取模的算法落进去的。 然后调度的时候就是，顺时针，某个黑圈就是url咯，于是这个url的请求就调度到顺时针最近的一个绿框也就是varnish。 ③虽然优化，但是还是存在问题： 调度上存在不均衡的情况，而且是经常性的。称之为\"倾斜\"，下图就调度倾斜的厉害咯👇 好，问题就是后端5个节点如何均衡调度呢？ 解决方法就是虚拟节点，大量的虚拟节点充斥这哈希环，于是就错落在了url落脚点周围了，就均衡了。 那如何生成虚拟节点呢，答案就是 利用权重 varnish 1 hash(192.168.126.101) % 2^32 # varnish缓存的ip也一样落在上面的0~2^32区间里 varnish 2 # 这个节点权重是2，也是生成2次随机数👇，这两个随机数也落在区间里的2个位置。 hash(192.168.126.102 + random_1) % 2^32 hash(192.168.126.102 + random_2) % 2^32 varnish 3 # 这个节点权重是3，也是生成3次随机数👇，这两个随机数也落在区间里的3个位置。 hash(192.168.126.103 + random_1) % 2^32 hash(192.168.126.103 + random_2) % 2^32 hash(192.168.126.103 + random_3) % 2^32 本来 1 2 3 就是权重，就是生成几个落脚点，现在改成1000 2000 3000，这样权重的调度比例还是1：2：3没变，就是3个varnish缓存节点的调度比重不变，但是1000 2000 3000，就是分别会有1000个随机数来生成1000个哈希环上的落脚点。 hash(192.168.126.102 + random_1) % 2^32 hash(192.168.126.102 + random_2) % 2^32 ... ... hash(192.168.126.102 + random_1000) % 2^32 这样就实现了虚拟节点。1000个落脚点，其实都是varnish的节点映射。 讲了这么多，实现起来，一个单词consistent就搞定了。 hash $request_uri consistent; 对比下lvs的调度算法： 静态和动态 静态4种，rr，wr，sh，dh 静态的4种，nginx都是支持的，wrr默认的也可以写权重也就是等价于LVS的RR和WRR 然后nginx的hash 写变量 hash $remote_addr; ip_hash就是SH，hash $requst_uri;就是目标地址不过是url了不是DH，不过意思是一样的，都是基于目标调度--就是什么目标调度到什么机器上。 6种动态：LC、 最少连接LC，这个nginx也有 leat_conn，而且nginx加个weight值也就是wlc了。 最少加权连接WLC，上一行已经说过了，nginx也是有的 以及都是基于连接数和负载的一个调度，不过LVS都是基于四层的，并不涉及应用层，而nginx可以。 ​ ​ nginx做四层调度 之前http应用层调度用的是ngx_http_upstream-module，现在做L4的调度，用的是ngx_stream_core_module模块。 一些版本需要yum -y install nginx-mod-stream 来手动安装stream模块，当然一般情况/etc/nginx/nginx.conf主配置文件里是包含了这个模块的 所以http的反代-调度是写在http语句块下的， 而，L4的反代-调度是写在stream语句块下的 1.9才开始支持L4的反代 点击去 就是2015年4月29 开始支持的咯 开始实验-tcp代理db 1、搞两个 mysql做realserver 创建数据库便于nginx反代调度后 client测试 知道连的哪个realserver。 2、nginx反代配置基于tcp协议也就是L4的调度 stream语句块是写在main下的，也就是和http 就写到主配指文件里，放到最下面，和http平级，自然就是main层级了。 重启nginx就会看到本地就会监听3306，这样才能反代哦 这里也可以写成0.0.0.0:3306 顺手测下端口连通OK的👇 这就好了，client测试 发现调度到了132这个DB上了 多试几次也会调度到133，因为是least_conn，就两台realserver都没有连接的。 试试权重 测试OK👇 开始实验-tcp代理redis 1、安装redis，并修改监听所有端口 这里简单用下redis，后面单独开章节 在后端realserver，两台 上都安装redis，当然得有epel源。 yum -y install redis 以前看到yum redis会有依赖包，现在看不到了 修改配置文件监听所有端口，👇删掉也一样的效果 redis集群搭建后面再弄。 测试下，本地测试就行 可以看到redis cli里自带帮助信息的。 最简单的就是设置key value，键值对。 写，读👇 这是134的一个redis的键值对👆， 再设置133的redis里的一个key value，区分开来方便测试效果 发现默认要本地配置，远端默认还不行 改本地配置👇 或者取消proctected mode 上图取消肯定不安全咯，推荐去配置文件里写明bind ip就行了--就是监听自己的IP就行了。 3、配置nginx 反代 到redis去 但是发现默认说好的1:1的调度，默认权重=1嘛。但是没有看到134的redis的key value出现 调整调度测试下👇 继续观察 👆说好的1:3也没看到。 要exit退出测试才能看到调度效果，继续测试 调度也不是立马生效的好像 要退出才能看到1:3权重效果 然后redis是比member cache强，Memcache是基于内存的，redis是可以内存也可以放到磁盘上。 注意也可以用UDP。 超时间也关注下 consistent就是前文讲的哈希一致性算法，不过这里是针对的源IP计算的咯，所以再来看这图 这里的node--也就是后端realservers以及键--就是hash $xxx，xxx是requset_uri还是remote_addr基于你怎么写。所以均匀的调度还真可能就是依赖这个consistent关键词了，呵呵。 nginx 不能这里用下划线_低版本？ 调用的地方自然也不能咯 如果用下划线会有问题 据说 测试👇 修改为下划线 也没问题，高版本修复了可能👆。 估计就是低版本里的_下划线，代码里没有匹配好比如regex写漏了？ nginx的二次开发的版本 https://tengine.taobao.org/ 比如 ngx_http_concat_module模块的效率 https://tengine.taobao.org/document_cn/http_concat_cn.html 该模块类似于apache中的mod_concat模块，用于合并多个文件在一个响应报文中。 安装测下Tengine 下载最新的试试看 curl -LO https://tengine.taobao.org/download/tengine-3.1.0.tar.gz tar xvf tengine-3.1.0.tar.gz cd tengine-3.1.0.tar.gz 编译前，需安装依赖 yum install gcc pcre-devel openssl-devel zlib-devel 编译前，需要添加nginx用户 useradd -r -s /sbin/nologin nginx 编译的时候考虑下stream模块是否支持，当然支持了，2015年的nginx-1.9.0支持stream的，tengine-3.1.0都是2023年了，没问题~,其实如果不支持会报错的，同时也可以这么看下是否支持 ./configure --help |grep stream 看看是否有--with-stream字眼。有就支持咯 ./configure --prefix=/apps/nginx \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-pcre \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module make && make install ln -s /apps/nginx/sbin/nginx /usr/bin/nginx nginx 启动就行，如果报错，就按提示解决报错 确认是否支持stream模块👇 然后就ok了 尝试启用concat，但是由于编译的时候没有加上这个模块，所以还是不支持。 结果发现开发版3.1.0不支持唉 换 妈的，2.3.1支持的，他这个分开发版本和稳定版，3.1.0是开发版，还没有详细的明细 就首页有说，难不成大版本1就是稳定版，2就是开发版？ 重新下载 curl -LO https://tengine.taobao.org/download/tengine-2.3.1.tar.gz 操，有个屁 再换 curl -LO https://tengine.taobao.org/download/tengine-2.1.2.tar.gz 有了，操，但是这个版本没有stream模块， ./configure --prefix=/apps/nginx \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-pcre \\ --with-http_concat_module 报错和处理 参考：https://www.linuxquestions.org/questions/slackware-arm-108/gcc-7-x-compile-issue-with-nginx-4175608107/ 继续编译，继续报错 处理方法，注释掉这行， 再次编译，再次报错 妈的，所以tenginx的concat到底行不行，为了一个concat，结果高版本的tenginx里没有concat，用低版本里结果stream不支持，而且openssl也要降版本，操。参考：https://blog.csdn.net/qq_39720249/article/details/84655501 curl -LO https://www.openssl.org/source/old/1.0.1/openssl-1.0.1u.tar.gz tar xvf openssl-1.0.1u.tar.gz cd openssl-1.0.1u ./config --prefix=/apps/nginx/openssl-1.0.1u make && make install 再重新编译tenginx试试看咯 编译报错 给你个openssl目录 ok 继续make fuck 报错是openssl.o 下错版本呢了，继续 curl -LO https://www.openssl.org/source/old/1.0.1/openssl-1.0.1o.tar.gz cd openssl-1.0.1o tar xvf openssl-1.0.1o.tar.gz cd openssl-1.0.1o rm -rf tengine-2.1.2/openssl # 删除之前的openssl目录 mv openssl-1.0.1o tengine-2.1.2/openssl cd tengine-2.1.2 ./configure --prefix=/apps/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_stub_status_module --with-http_gzip_static_module --with-pcre --with-http_concat_module --with-cc-opt=\"-Wno-error\" make && make install 还是openssl.o的报错， 可能是旧版的openssl没有删除导致的。 https://juejin.cn/post/7106429674942627854 按这个来一遍再 yum -y install remove nginx yum -y remove nginx cd /usr/local/ ; curl -LO https://tengine.taobao.org/download/tengine-2.1.2.tar.gz tar xvf tengine-2.1.2.tar.gz cd tengine-2.1.2 ll ./configure --help |grep concat ./configure --help |grep stream yum remove openssl openssl-devel cd .. curl -LO https://www.openssl.org/source/old/1.0.1/openssl-1.0.1o.tar.gz tar xvf openssl-1.0.1o.tar.gz cd openssl-1.0.1o ./config --prefix=/opt/ldkjdata/nginx/openssl-1.0.1o make && make install 这是OK的 cd ../tengine-2.1.2 pwd vim src/os/unix/ngx_user.c ./configure --prefix=/apps/nginx --user=nginx --group=nginx --with-openssl=../openssl-1.0.1o --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_stub_status_module --with-http_gzip_static_module --with-pcre --with-http_concat_module --with-cc-opt=\"-Wno-error\" make -j 2 && make install 最后一步报错openssl的问题还是，反正解决要么正面搞定openssl，要么用3.x.xtenginx安装dso的concat模块。 方案二：就用最新的然后利用--add-module=结合单独下载模块 到这个网站下载concat模块 https://github.com/alibaba/nginx-http-concat 然后下载最新的tenginx 3.1.0，再编译的时候加上这个模块就行了 git clone https://github.com/alibaba/nginx-http-concat.git git clone https://github.com/vozlt/nginx-module-sts.git cd tengine-3.1.0 ./configure --prefix=/apps/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_stub_status_module --with-http_gzip_static_module --with-pcre --with-stream --with-stream_ssl_module --with-stream_realip_module --add-module=/root/nginx-http-concat make -j 2 && make install OK鸡巴开了 虽然ok了，但是我有一个疑问啊，就是为什么 1、官网分开发稳定版和稳定版，首页稳定版只显示到2013年，什么鬼 2、开发稳定版里最新的竟然不带concat模块，还需要去11年前的github库里下载，什么鬼 下载stream模块去试试tengine-2.1.2，不行就算了 因为编译，所以找了个源文件， https://github.com/vozlt/nginx-module-stream-sts 还是用tengine-2.1.2来弄试试 ./configure --prefix=/apps/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_stub_status_module --with-http_gzip_static_module --with-pcre --add-module=/root/nginx-module-sts --with-cc-opt=\"-Wno-error\" --add-module=/root/nginx-module-stream-sts make dso_install # 注意这里不直接make make && make install # 然后再make 还是一样的报错，不过stream模块好像可以这么加。 openssl，算了不弄了。就上面的方法搞定就行了。 nginx优化 默认的Linux内核参数考虑的是最通用场景，不符合用于支持高并发访问的Web服务器的 定义，根据业务特点来进行调整，当Nginx作为静态web内容服务器、反向代理或者提供 压缩服务器的服务器时，内核参数的调整都是不同的，此处针对最通用的、使Nginx支持 更多并发请求的TCP网络参数做简单的配置,修改/etc/sysctl.conf来更改内核参数 fs.file-max = 999999 表示单个进程较大可以打开的句柄数 net.ipv4.tcp_tw_reuse = 1 参数设置为 1 ，表示允许将TIME_WAIT状态的socket重新用于新的TCP链接，这对于 服务器来说意义重大，因为总有大量TIME_WAIT状态的链接存在 net.ipv4.tcp_keepalive_time = 600 当keepalive启动时，TCP发送keepalive消息的频度；默认是2小时，将其设置为10分钟， 可更快的清理无效链接 net.ipv4.tcp_fin_timeout = 30 当服务器主动关闭链接时，socket保持在FIN_WAIT_2状态的较大时间 net.ipv4.tcp_max_tw_buckets = 5000 表示操作系统允许TIME_WAIT套接字数量的较大值，如超过此值，TIME_WAIT 套接字将立刻被清除并打印警告信息,默认为8000，过多的TIME_WAIT套接字会使 Web服务器变慢 net.ipv4.ip_local_port_range = 1024 65000 ，比如nginx 往后端发送连接的源随机端口 定义UDP和TCP链接的本地端口的取值范围 net.ipv4.tcp_rmem = 10240 87380 12582912 定义了TCP接受缓存的最小值、默认值、较大值 net.ipv4.tcp_wmem = 10240 87380 12582912 定义TCP发送缓存的最小值、默认值、较大值 net.core.netdev_max_backlog = 8096 # backlog不是log是队列 当网卡接收数据包的速度大于内核处理速度时，会有一个列队保存这些数据包。 这个参数表示该列队的较大值 net.core.rmem_default = 6291456 表示内核套接字接受缓存区默认大小 net.core.wmem_default = 6291456 表示内核套接字发送缓存区默认大小 net.core.rmem_max = 12582912 表示内核套接字接受缓存区最大大小 net.core.wmem_max = 12582912 表示内核套接字发送缓存区最大大小 注意：以上的四个参数，需要根据业务逻辑和实际的硬件成本来综合考虑 net.ipv4.tcp_syncookies = 1 与性能无关。用于解决TCP的SYN攻击 net.ipv4.tcp_max_syn_backlog = 8192 这个参数表示TCP三次握手建立阶段接受SYN请求列队的较大长度，默认1024，将其 设置的大一些可使出现Nginx繁忙来不及accept新连接时，Linux不至于丢失客户端发起 的链接请求 net.ipv4.tcp_tw_recycle = 1 这个参数用于设置启用timewait快速回收 net.core.somaxconn=262114 选项默认值是128，这个参数用于调节系统同时发起的TCP连接数，在高并发的请求中， 默认的值可能会导致链接超时或者重传，因此需要结合高并发请求数来调节此值。 net.ipv4.tcp_max_orphans=262114 选项用于设定系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如 果超过这个数字，孤立链接将立即被复位并输出警告信息。这个限制指示为了防止简单的 DOS攻击，不用过分依靠这个限制甚至认为的减小这个值，更多的情况是增加这个值 工作案例-zabbix-agent 监控记录 1、主动、被动 见https://blog.51cto.com/u_15094852/2968778 参考一下https://blog.51cto.com/shone/5333216 然后记录我的关键配置 ser端 用被动，也就是 “zabbix 客户端”，键值 这里写的是 ping，其实就是agent上的配置，往下看 上图👆的30s也就是说server端30秒去取一次数据，正因为agent是被动模式，所以server-->agent取才会又30s一次的情况，如果是agent是主动模式server agent先安装 zabbix的安装还是要注意版本的，举例 agent和zabbix版本要统一,否则server端检测项起不来，即使agent那边zabbix-agent -t ping 回车有数值 agent安装走官网zabbix.org下载repo源文件就行： https://www.zabbix.com/download?zabbix=6.4&os_distribution=centos&os_version=9&components=agent&db=&ws= agent端配置 agent上测试👇，其实agent上最好写成这种，但是server上我不会传参 添加agent的监控项后，要记得重启agent服务，否则zabbix server认不到。 然后agent里配置文件Server要写的，不然tcp 10050 会瞬断 工作案例-fail2ban 1、依赖firewalld 会用到的cli firewall-cmd --list all # icmp要放了 firewall-cmd --zone=public --add-rich-rule='rule protocol value=\"icmp\" accept' --permanent # zabbix-agent 被动10050要放了 firewall-cmd --zone=public --add-rich-rule='rule family=\"ipv4\" port port=\"10050\" protocol=\"tcp\" accept' --permanent firewall-cmd --reload 2、安装配置fail2ban yum -y install fail2ban cd /etc/fail2ban/ # 配置文件里只需要在sshd下添加一行 enabled = true 就行 # 其他惩罚机制也在这个conf文件里，默认就好了。 vim jail.conf ... [sshd] enabled = true ... # 注意fail2ban生效本质上是在firewalld里添加一条deny来着 systemclt restart fail2ban Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-29 14:48:50 "},"38-高性能WEB服务nginx03/5-keepalived实现反向代理的高可用.html":{"url":"38-高性能WEB服务nginx03/5-keepalived实现反向代理的高可用.html","title":"第5节 keepalived实现反向代理的高可用","keywords":"","body":"第5节. keepalived实现反向代理的高可用 nginx的高可用怎么做，用keepalive做，keepalive的脚本来是新对nginx的HA。 通过脚本，监控Master/Slave的状态，进一步调整各自的优先级，从而抢占VIP，实现HA的故障切换，听着就和VRRP、HSRP差不多了。结果keepalive的配置文件一看全TM是vrrp，哈哈哈~ 后面还会学习HA Proxy 反向代理软件。也就是说： ①keepavlie + nginx 实现 nginx的HA ②keepalive + ha proxy 实现ha proxy本身的HA ③其实说白了任何服务之间都可以用keepalive做HA了，不过服务L4和L7里L7大多数用nginx了。 ④总之，keepalive经常配合调度器(nginx、haproxy、sql读写分离器也行吧) 一块使用。 ⑤其实keepalive通过实验就能知道他就是机器级别的VIP，就是网工知道的VRRP。可以说keepalive做两台机器的IP HA也就是VIP，然后nginx监听在这个VIP上而已。是这么个逻辑 ⑥keepalive里不仅仅有vrrp，还有lvs吧，而vrrp有v2和v3的。v2的心跳是秒级 ，v3的心跳是厘秒级的默认是100厘秒也就是1秒 搭建 1、准备两台web服务器也就是realserver 2、搭建master/slave 两台keepalive yum -y install keepalived 计划虚拟地址--VIP 192.168.126.100 vim /etc/keepalived/keepalived.conf global_defs 里修改一下 notification_email { xx@xx.xx 收件箱 } notification_email_from 发件箱，或者仅仅,就是个发件地址，邮箱其实不存在也可以。。 smtp_server 127.0.0.1 本机的smtp server要启用咯 postfix router_id xxx 每个服务器起个自己的名字，比如keep1 下面vrrp_instance VI_1 这种就是vrrp的实例，用来实现VIP浮动IP的。 virtual_router_id xx 写个编号比如66，注意两台都写66，虚拟成一个路由器 prioity 不动就用默认的100 authentication { auth_type PASS auth_pass xxx 写两台的协商密码；据说密码即使设置了，抓包也是明文的。怀疑~ } vitual_ipaddress { a.b.c.d vip和端口也可以直接写成👇 192.168.126.100/24 dev eth0 label eth0:1 } 再往下就是virtual_server a.b.c.d 443 { 这些就是LVS了，L4的调度了，本次实验只是用keepalive做nginx的HA，所以这里不管了后面全删掉 } 将此配置文件复制一份到 另一台 keepalive机器上，并略作修改👇；当然两台需要做防火墙、selinux 禁用、时间同步操作 vim /etc/keepalived/keepalived.conf 修改 router_id ka2 state BACKUP 虚拟路由器id必须一样的 priority 80 优先级和master要不同 其他也都一样 到此就配置完了 systemctl start keepalived 就ok了，ping VIP就通了。 参考：https://blog.51cto.com/sparkgo/6127764 VIP的情况： 可能存在自动添加iptables的情况看别人的实验是有的，可能是老版本；我的实验就是开启vrrp_strict就ping不通但是没有看到iptables -vnL。 master上只要keepalive服务起来都会一直看到vip的，backup上的vip会被抢回去，说明keepalive默认是开启了抢占的，就是说： 1、master挂了backup拿到vip此时backup上的vip就出现了， 2、master活了又会抢到vip此时backup上的vip就又消失了。 所以keepalive是秒级的HA咯，如何加快收敛呢？交换机里的vrrp也是秒级的？不过有FRR好像可以加速，说错了是BFD。 方法1：修改源代码，编译咯要 https://blog.csdn.net/hitcompass1/article/details/104054934 方法2：使用高版本的vrrp，不是keepalive高版本是否支持厘秒 https://support.huawei.com/enterprise/zh/doc/EDOC1100112421/6e22bde4 方法3：用BFD联动 https://support.huawei.com/enterprise/zh/doc/EDOC1100112421/54b4f71c 其实方法2 3 都不是keepalive了而是网络设备的vrrp了，所以可以用网络设备来实现加速收敛的vrrp来替代keepalive。这也是个思路，当然前提是你有现成的网络设备而不是另购 方法4：keepalive + BFD https://zhuanlan.zhihu.com/p/582205167 以下是节选 方法5：keepalive + vrrp3 keepalive里的vrrp协议，所以keepalve + vrrp3 https://zhuanlan.zhihu.com/p/582205167 还是这篇，以下是节选防止url失效 ! Configuration File for keepalived global_defs { router_id SLB-SAD script_user root enable_script_security # 检查vrrp报文中的所有地址比较耗时。默认是跳过检查 vrrp_skip_check_adv_addr # 重点是启用vrrp3 vrrp_version 3 } vrrp_script chk_upyun { # 除了心跳检测外，还可以调用脚本做业务上的健康检测 script \"/etc/keepalived/bin/check_vip.sh\" interval 1 # check every 1 seconds # 👈是不是要改小一点才能配得上vrrp v3的厘秒啊，就算这里只支持到秒级，也可以在脚本里写循环做到1s里循环N次来缩短检测进度。 经过下文的脚本实验发现！没用的！脚本里再快，weight -30的动作还是interval 1的秒级，所以主备切换如果是v3的心跳可能确实是厘秒级别的;但是如果是脚本来检测nginx (脚本的整体运行周期以及weight -30) 的主备切换还是1秒的最小值了 fall 1 # require 2 failures for failures rise 1 # require 1 sucesses for ok # weight 值为负数时，当脚本检测失败时，Master节点的权值将是“priority“值与“weight”值之差 weight -30 } vrrp_instance upyun_lb { strict_mode off advert_int 0.03 state BACKUP interface eth3 virtual_router_id 19 priority 100 # 当master和backup角色转换时，触发脚本做业务上的切换 notify \"/etc/keepalived/bin/change_state.sh\" # 这里就是0.03秒就会nofiy了。配合上秒vrrp_script里的脚本循环可以做到高精度的。 track_script { chk_upyun } virtual_ipaddress { 192.168.147.19 label eth3:9 } } # 这一段是可选的，如果和lvs规则就可以调用ipvsadm的转发规则 include /etc/keepalived/virserver.conf 配置中用到了“check_vip.sh”和“change_state.sh”的两个脚本，我们也来简单看下。 check_vip.sh 节选自上面的url 上面配置中只是举例说明，当 ping 丢包严重超过 80% 时，就认为要切换主备关系了。大家也可以根据具体的业务场景做一些逻辑判断，来实现主备切换，以达到高可用的目的。 #!/bin/sh TMP=\"/tmp/bad\" GATEWAY=$(ip ro|awk '/default/{print $3}') LOSS=$(ping -fc10 -s1 $GATEWAY | sed -r -n '/loss/s@.* (.*)%.*@\\1@p') if [ $LOSS -ge 80 ];then echo \"${LOSS}% lost #`date`\" >> $TMP fi if [ -e $TMP ] ;then exit 1 fi change_state.sh 节选自上面的url 当检测到服务器的角色转换时，这个脚本就会调用钉钉报警，并且调整业务上的一些操作。如 sysctl.conf 配置或者 iptables 上的规则，甚至可以配合 LVS 做一些负载均衡的部署。 #!/bin/bash HOME=\"/etc/keepalived/\" LIP=`/sbin/ip addr | awk '/192.168./{gsub(\"/.*\",\"\");if($2!=\"\"){print $2}}'|sort -u|head -n1` VIP=$(awk '/virtual_ipaddress/{getline; print $1}' $HOME/keepalived.conf) URL=\"https://oapi.dingtalk.com/robot/send?access_token=07xxxxxxxxxxxxx\" [ -z $LIP ] && LIP=$VIP ############################################################################ dingding(){ curl $URL --connect-timeout 10 -H 'Content-Type: application/json' \\ -d '{\"msgtype\": \"markdown\", \"markdown\": { \"title\": \"数据中心报警\", \"text\": \"* 报警类别: '\"$1\"'\\n* 报警机器: '\"$2\"'\\n* 报警服务: '\"$3\"'\\n* 报警内容: '\"$4\"'\\n* 报警时间: '\"$(date \"+%Y-%m-%d %T\")\"'\\n\" } }' } ENDSTATE=$3 NAME=$2 TYPE=$1 dingding Keepalived $LIP Change_state \"$ENDSTATE\" case $ENDSTATE in \"BACKUP\") # Perform action for transition to BACKUP state echo \"--- I am $ENDSTATE #`date`\" >> /tmp/keepalived.log sed -r -i '/state/s#MASTER#BACKUP#g' $HOME/keepalived.conf sysctl -w \\ net.ipv4.conf.all.arp_accept=1 \\ net.ipv4.conf.all.arp_ignore=0 \\ net.ipv4.conf.all.arp_announce=0 \\ net.ipv4.ip_nonlocal_bind=1 #$HOME/tunl start exit 0 ;; \"FAULT\") # Perform action for transition to FAULT state exit 0 ;; \"MASTER\") # Perform action for transition to MASTER state echo \"+++ I am $ENDSTATE #`date`\" >> /tmp/keepalived.log sed -r -i '/state/s#BACKUP#MASTER#g' $HOME/keepalived.conf sysctl -w \\ net.ipv4.conf.all.arp_ignore=1 \\ net.ipv4.conf.all.arp_accept=1 \\ net.ipv4.conf.all.arp_announce=1 \\ net.ipv4.ip_nonlocal_bind=1 iptables -L -vn | grep -iqE \"vrrp|112\" [ $? = 0 ] || iptables -I INPUT -p vrrp -j ACCEPT iptables -L -vn | grep -iq \"accept .*$VIP\" [ $? = 0 ] || iptables -I INPUT -d $VIP -j ACCEPT exit 0 ;; *) echo \"Unknown state ${ENDSTATE} for VRRP ${TYPE} ${NAME}\" exit 1 ;; esac 看看GPT怎么回答的，不过我没有测试，参考参考👇 上图的interval 0.1如果没效果，也就是不支持低于秒级的频率，就可以在脚本里写循环来提供检测频率。只要advert_int 0.1 生效就行了。而且advert_int 的频率看起来是可以作用到nofity 后面的脚本的，所以检测和动作一套组合 都可以降低到厘秒级别的。 具体精度实验验证见下文的 vrrp_script脚本章节 3、vip有了就搭建nginx 在两台已经搞定了keepalive的机器上安装nginx yum -y install nginx vim /etc/nginx/nginx.conf upstream websrvs { server 192.168.126.134:8000; server 192.168.126.135:80; } server { location / { proxy_pass http://websrvs; } } systemctl restart nginx 然后就👇 当然我也遇到了问题，就是子配置文件抢了主配置文件的server块，这就涉及优先级了，删掉子配置文件就行了。 反正就这么多配置 第二台发现nginx 同样的rocky 9.0，同样的yum remove yum install结果nginx -v版本不同，研究下，发现 第一台是这么安装的，干脆第二台也这么安装下 yum -y remove nginx yum -y install nginx就行了 总共就加红框里的这么多就行了 好了 这台nginx也调度ok了 windows cmd curl一下，这个测试不能在两台 keepalive上curl，因为backup上没有VIP生效，所以测试就用正规的client测试就好了 此时就已经实现了简单的nginx的HA了。就是，不是基于nginx服务，而是基于机器系统级别的HA也就是实际的 IP down了就切BACKUP了。 如果要让keepalive监控nginx的服务，可以走脚本。 判断进程死没死 killall -0 nginx &> /dev/null;echo $? vrrp_script脚本接口 1、精度 之 脚本运行的频率 interval 结论：interval 1 的1就是最小值了。提高精度就脚本里写while sleep，可达到20ms的精度。 可见是1s一次的精度 先调整echo.sh脚本的显示时间，%3N就是毫秒了，因为%N是纳秒取前3位(从左到右高3位)就是毫秒。 修改0.1👇，没变化， 修改vrrp v3，还是没变化 变通修改echo.sh为循环 虽然没有达到理论上的1纳秒1个，但是可见1秒钟里已经有很多行了，精度已经不低了。差不多在20ms的精度了，已经八错啦。 但是这是脚本里的精度，vrrp_scrpts里只支持到1s👇也就是说即使脚本检测频率很高，但是我weight -20的自减权重让出优先级的动作还是1s，所以解决方案就卡在这里了。 所谓的vrrp v3支持厘秒，只是支持心跳的厘秒级，也就是keepalive本身的 A/S切换而已，不支持通过脚本对nginx或其他应用服务的高精度HA切换。也就是说场景支持仅仅是IP故障，而不是L7的服务故障厘秒级。L7的服务切换仅支持秒级而已。 2、精度 之 心跳的频率同样但不是nofiy通知的频率 advert_int notfiy同样可以调用脚本 但是要明白notify是 主备 切换 的时候才会触发动作， 如何证明心跳是支持厘秒的，通过keepalive的心跳--组播报文 到对方去抓包，抓224.0.0.18组播包就可见 反之master看slave的keepalive应该还是1s一个，要验证下， 两个疑问？ ①协商不是应该V3不能和V2协商成功嘛，或者V3向下兼容协商成V2才合理啊？ ②组播的hello报文不是MASTER/BACKUP 两台机器都 会发送才对嘛，显然不对，只是当前激活的MASTER才会发送helllo报文。 ③实测就是版本不一致可以工作，反正hello报也是单边发送，只是hello包的发送以master为准，但是其他功能就不好说了。当然实际干活不能两边version不一致的。 3、常规实验 之 文件是否存在的脚本 修改 先删掉测试文件，重启服务，是配置文件生效， 创建文件，满足脚本判断所以exit 1 ，1就是报错，报错就是 触发vrrp_script chk_down里的weight -30 于是，通过tcpdump -i eth0 -nn host 224.0.0.18 可见 原来130这个IP发送的优先级减了30变成了70，从而132这个原来的BACKUP以80的优先级抢了MASTER，于是下图就看到了100->70->80的优先级数字。于此同时132这台BACKUP上变成了MASTER就看到了VIP-126.100了。 4、常规实验 之 nginx服务检测 就是脚本改改就行了 无需关注0.1，vrrp_v3都没开，0.1不生效也不报错的 keepalive的两台机器都需要配置一样的脚本，就是脚本一样，配置文件一样 遇到个现象 发现是130的vrrp_strict没注释掉。注释掉就行 保证cmd测试效果 此时停掉一个nginx服务--肯定停掉master的nginx了 在192.168.126.130上stop nginx，如果nginx和keepalive不在同一个机器就需要远程跑脚本了，sshpass之类的也行啊 nofity可以进一步做故障修复 keepalive 通过vrrp_script 检测发现问题，然后切换主备，然后notify 调用修复脚本同时脚本自带信息发送，比如讨厌的dingd nofiy.sh脚本内容👇 记得chmod +x #!/bin/bash # contact='root@localhost' notify() { mailsubject=\"$(hostname) to be $1, vip floating\" mailbody=\"$(date +'%F %T'): vrrp transition, $(hostname) changed to be $1\" echo \"$mailbody\" | mail -s \"$mailsubject\" $contact } case $1 in master) notify master ... # 切成主是否需要进一步做动作 ;; backup) notify backup ... # 切成备是否需要进一步做动作，既然切成备了，说明vrrp_script语句块里的检测到nginx 挂了，所以这要做systemctl start nginx动作去修复的。 ;; fault) notify fault ;; *) echo \"Usage: $(basename $0) {master|backup|fault}\" exit 1 ;; esac vim /etc/keepalived/keepalived vrrp_instances xxx 块里 最后一行加上 notify_master \"上面的notify.sh脚本的路径 master(这是往脚本传递参数$1位置调用)\" # 切成master就执行这一行 notify_bakcup \"上面的notify.sh脚本的路径 bacup(这是往脚本传递参数$1位置调用)\" # 切成master就执行这一行 notify_fault \"上面的notify.sh脚本的路径 fault(这是往脚本传递参数$1位置调用)\" # 切成master就执行这一行 以下就是2个脚本，和配置文件完整的内容 需要安装mail命令的。centos就 yum -y install mailx，rocky就yum -y install s-nail，mail的cli就有了 测试下，由于默认是开启preempt抢占的，但是这个抢占是keepalive之间的prioriy高复活了就抢回来，而不涉及脚本的监控(实验过了是涉及脚本监管的，你脚本监控的是nginx down就weight -30，对比，等你nginx起来了，会自动恢复原来的权重的，这个恢复功能是默认就有的。你不需要就使用nopreempt来实现不回切就行了，但是优先级应该是恢复了的)，所以实验步骤和现象预判为： 1、130 132 两个IP地址 分别前后为MASTER 和 BACKUP 2、130上执行 killall nginx 杀掉nginx后，130 触发chk_down检测脚本，变为BACKUP 3、130变成BACKUP后，触发notify_backup脚本，重启nginx，但此时不会抢回来(这里是猜测，其实是会抢回来的，会抢回来是因为初始手动指定为Master的，这里下文有总结的)。-- 实验到此结束。 因为没有哪个脚本说是nginx起来会怎么怎么样的，不管是chk_down里还是notify_xxx里都没有配置，更准确来讲是notify是切换状态一次触发一次来着，而且你写的是notify_master 这种就是切成master才会触发，而不是直接notify \"\"这种是切一次执行一次，不管是什么切成了角色。 *想nginx活了抢回来，就要再写一个chk_up脚本，来检测到nginx up就将weight 值+30，这样weight就恢复成了100；于此同时还需要辅以notify_master的动作嘛？好像没必要，之前变成BACKUP的时候启用了nginx了，然后chk_up有检测过了确实起来了，才会让keepalive +30重新变成100为MASTER啊。所以ok的思路 而且这种脚本的配置也要考虑清楚，正常时两边MASTET和BACKUP一样的。 4、关于抢占，重启keepalive服务这种就是会发送prio 0 的报文，通知对方自己下线了，呵呵，这种属于重新协商的层面，和抢占无关，preempt抢占是发生在keepalived服务活着的情况下的优先级的上下浮动的时候的M/S的切换，抢占。 结果发现nginx起来后会抢回来， 通过删除notify配置cli，然后测试，发现keepalive的vrrp确实周到，nginx恢复了，会抢回来的。分析逻辑就是vrrp_scritp脚本 监控 nginx down 权重-30，如果监控发现nginx up会自动恢复权重。 preempt关于抢占 1、keepalive服务重启，意味着vrrp协议重新协商，不涉及preempt于否，keepalive活着 优先级的升降才设计抢占。 2、以下配置，两头都 nopreempt关闭抢占，130 那边脚本检测到nginx down就自降30 prio后，由于那头131不抢占，所以还是130 作为master。当然题外话：此时由于本身nginx 没了自然业务就出问题。 130和132都关闭抢占， 此时130上killall nginx，130 prio -30，但是由于132上未开启抢占，所以对于132来讲就不回抢占，好烦啦老说这个词。 如果此时130恢复nginx，自然也没说什么特别的现象，因为从头到尾130就没有将Master交出去过。 如果此时132开启抢占，才是好玩的地方 测试如图，说好的130不抢占，但是其实会抢回来。 到底怎么才能看到130故障恢复后的不抢占效果呢，这个很重要啊，要知道 切一次就是一次故障，你不能说原来的MASTER好了，再切一次，造成二次故障吧。 https://blog.csdn.net/MssGuo/article/details/127336013 扫了这篇，我有点想法 preempt完整总结 1、A/S 初始化协商，如果定义了master，那么在hello的 比如说3个周期内会以手动指定的master来顶定的。这个就是叫做初始的master竞选规则，当然这是我猜的，比如DR/BDR的选举， 2、注意:要使nopreempt参数起作用，初始状态不能是MASTER 3、两边都为backup，preempt才能 在 prio 高的时候 实行抢占的。 1、两边都配置BACKUP，满足不抢占的初始条件， 2、然后一边开启抢占，同时它的prio 要低哦。它最后重启以下keepalive就行了，由于prio低，所以即使开启了preempt，也不会抢，也会让出master。 3、重启开启了抢占的那边 让 配置了nopreept 的那边为MASTER。然后就OK了，再配置检测脚本如果nginx 挂了，就自减prio 30，从100变成70，此时由于对边是80切开启抢占，所以对边抢了过去；等原来的nginx恢复了即使prio变为100，由于没有开启抢占所以不会超出二次故障。 4、如果A挂-->B，B再挂-->A哪怕好了也切不回来了，因为你不想自动切带来二次故障， 5、所以一次切换的，后面需要脚本参与，判定是夜里低业务期间，就脚本自动切回主，这样算不算最佳实践了恩？也许吧，折腾~~ 这是配置 还有👇 然后再完整地看看notify的自动修复思路 补充： 1、配置和脚本要两边一致 2、二次故障，需要优化为一次故障切换和脚本低峰业务回切 我怎么觉的备机上无需做脚本呢，哈哈， 3、两边都BACKUP，两边都prio 100，这样都是一次切换，抢占也开了的情况下 所以进一步优化最佳实践我认为 1、配置 两台一样的配置哦，优先级都一样的哦，都是BACKUP(这里无所谓M/M M/B B/B都行，其实就是通过一样的优先级来实现：M恢复故障后优先级恢复了，但两边一样，所以也是无法回抢的)，脚本都一样的哦 都需要有mail 命令哦 2、测试 初始通过132最后重启以下keepalive让130为master 当前130为master发送hello 下面开始模拟nginx故障 130上killall nginx，prio -30 ，两边都开了preempt的所以对面抢了master，紧接着130又重启nginx了，但是两边prio一样所以不会抢的。 PS：chk_down 导致的-30，后面nginx起来又会自动+回去的，神奇的地方就是这里咯~。 132上killall nginx，prio -30 ，两边都开了preempt的所以对面抢了master，紧接着132又重启nginx了，但是两边prio一样所以不会抢的。 好处就是我出故障我让出master，后紧接着我修复好了，但我不抢了 所以，这tm就实现了一次故障切换，也无需人工什么 低峰业务 去让master抢回原先的也就是130了。因为之前上面那个所谓的最佳实践--就是单边master配置nopreempt---对边preempt+低优先级来实现一次切换，但是如果切过去新的master也出现故障就没法自动回切了--虽然做到了master故障恢复后不抢回来的二次故障--但是万一新的master也出现故障是无法回切的。终于让我找到了最佳方案，就差实践了，实践也一样哈哈哈哈~~~ 他这个有意思的地方就是不像sw的BPDU那里是prio+mac总会有一个竞选出来，不会和linux里的keepalive一样prio一样就是一样preempt开启也不会说再去找一个其他参数判定以下优先级。 3、总结 多测试，反复论证，设计好方案仅此而已 到此 1、nginx 的 ha有了，靠keepalive 2、后端server的健康性调度也有了，靠nginx 此外还有一个LVS里的东西，keepalive之前我没梳理，本篇主要是针对nginx的HA的配置研究，回头有时间再返工前面的一些空白页吧空白页在这里 这是将vrrp_两个实例 合成一个组，要切 整个group一起切走，这样的目的是为了应对NET的场景 就是两台机器，对外虚拟两个ip，就是两个VIP，一个作为内网VIP，一个作为外网VIP的发布IP（而这个外网VIP还是个DNAT，差不多整个意思吧），要切一切到同一台机器上，不过上图是示意图吧，哪有vip dip这种cli关键字的。 然后NAT一般用的也少，LVS里不怎么用NAT 排错20240313 不加-4，就是一致都是5s的dns解析，ping也一样，一旦写了hosts，解析就有了，curl 和 ping就秒出结果一点都不卡，原因如图咯。 处理方式 关闭IPv6好像还是不行唉，但可以肯定是V6导致的 不太好解决，写hosts才稳定解决。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"1-容器技术和堡垒机JumpServer实战/1-容器技术和堡垒机JumpServer实战.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-容器技术和堡垒机JumpServer实战.html","title":"第一章 容器技术和堡垒机JumpServer实战","keywords":"","body":"第一章 容器技术和堡垒机JumpServer实战 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:24 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理.html","title":"第1篇 Docker安装和镜像管理","keywords":"","body":"第1篇 Docker安装和镜像管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:23 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/0-容器技术和Docker特性.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/0-容器技术和Docker特性.html","title":"第0节 容器技术和Docker特性","keywords":"","body":"第0节 容器技术和Docker特性 1、背景 类比集装箱 隔离成本低 docker 只是 一款优秀的 容器技术 ，容器不单单指docker。 docker是结合linux内核开发出来的，不是完全独立开发出来的。将linux内核的两个核心技术加以组合，并封装了额外的功能软件。 docker启动快 build构建----ship传输---run运行 之前docker那本书上有一段敲过实验，可以看到本地build的命令其实可以在远端build然后拉下来运行的。如果不指定就是本地build本地运行，中间是省掉了一个本地到本地的ship过程。 不过这里的ship更多讲的是拿整个build好的镜像到处运行的意思。VM里有迁移的动作，不过也没有docker的ship来的更加频繁。 Any APP ， Anywhere，比如windows构建的镜像，到linux一样运行使用。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:23 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/1-容器和虚拟机特性比较.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/1-容器和虚拟机特性比较.html","title":"第1节 容器和虚拟机特性比较","keywords":"","body":"第1节 容器和虚拟机特性比较 虚拟化技术的对比 简单点，就是主机虚拟化有HYPERVISOR \\ GUSET OS 2层OS层面的虚拟化层， 而容器只有一层容器层。少了一个完整的OS，节省了很多资源。 useSpace 1 2 3用的是HOST OS，而HOST OS就是内核的一些系统文件，共享的系统OS咯。 container层就可以理解为主流的docker 这里我想到一个点，“联合文件系统”，才是关键吧。画图毕竟是画个图，container层其实就涉及联合文件系统才能做到不同的容器之间共用基础文件系统层。当然container可能不仅仅是FS层咯。接着学，这里简单思考下，让上帝笑一笑，接着学，接着奏乐，接着舞。 关于user space 1 2 3 共用HOST OS的情况下，如果space1里的修改了内核参数，是否会影响space 2 3的内核参数。继续舞~~，存在space1黑掉了底层HOST OST的某些关键文件导致space2故障的可能性。 ​ 再一个，官方都说是anywhere运行，所以你linux的docker，在windows上一样运行的，如果HOST OS用的是宿主的，那么linux和windows OS肯定不一样的，那么是如何工作的呢？涉及底层工作逻辑，稍后继续研究... 所以有不少就是在虚拟机里跑容器，就是图二里跑图三👇，虽然这么套着用损耗更大(hypervisor+guestOS+container)哈哈，但是归在灵活啊，所以很多都确实在这么用的。 Bins/Libs：依赖库也是个运行环境，ldd /bin/ls这些也是库，一些依赖如果不全就有问题。 再举例：go语言编译的都是静态库，不是共享库，所以go开发的软件拷贝到其他机器上，比如从ubuntu考到centos就能直接运行的。 size，大小，容器肯定小很多，都少了一个相对庞大的OS层的 startup，启动方面，如果是go这种容器就很快，但是如果是java容器照样慢，OS的启动都可以忽略不计了。 intergration ，继承性方面，比如迁移 容器就很快 容器：好比集装箱 docker，n.搬运工人, 码头工人，搬运管理集装箱的，所以docker就是管理容器的技术，一揽子解决方案的技术。 k8s：来自希腊语 舵手，不是剁手。开船的人好像比码头搬运工有高级了一点。码头工人是在一个码头里工作，舵手是满世界跑，今天一码头，明天一码头。所以k8s听名字就知道更牛。 定位其实 docker和k8s之间是不冲突的，但是K8s好像在抛弃docker，哈哈~所以还是有冲突的，比如不是一家公司的利益冲突，哈哈~。 kvm和docker都是单机游戏：一台宿主机上管理其上创建的所有虚拟机或容器。都是只限于一台机器上的管理。 而k8s是所有宿主上的全管了。 kvm 单机vm管理工具 --- 配合--- openstack 多主机VM管理平台 docker 单机容器管理工具 --配合-- k8s 多主机容器管理平台 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:23 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/2-Docker组成和容器运行规范.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/2-Docker组成和容器运行规范.html","title":"第2节 Docker组成和容器运行规范","keywords":"","body":"第2节 Docker组成和容器运行规范 docker组成 1、Docker主机host，就是宿主，宿主可以是物理机也可以是虚拟机。你是node节点哦 2、docker是C/S结构的，所以还需要一个Docker 服务端 server：就是一个守护进程。 docker属于容器技术类里的一种，docker自己的工作机制就是C/S，而其他容器的工作机制可能就没有什么客户端服务端的情况。 3、docker是C/S架构，就需要client使用cli去调用docker API，docker api就是cli咯，这个就好比mysql-client去操作mysql-server一样的。就是Docker-client通过cli连接docker-server，连接上去再通过docker-server管理容器资源。我们操作docker命令cli的时候其实就是这么一个通过docker-server去连接执行的过程。 4、docker除了上面的c/s和API外还引入了模板思想。这就是Docker 镜像 images 模板：镜像，表现形式就是 文件 ； 静态文件 不运行知识占用一定的磁盘空间而已。 容器：镜像副本，实例，表现为 进程 多实例。 运行了就会还会占用内存空间了。占不占磁盘空间呢，占！因为容器运行起来，基于同一个镜像创建的多个容器的话，那么这些容器各有各的log，可能配置文件也不同，所以磁盘空间也是占用的。 5、docker容器container就是从镜像复制出来的一份运行起来而已。镜像复制很多份(你创建容器的时候就会复制的)，每一份就是一个容器。 6、docker镜像放哪呢，放在仓库里Docker 仓库 registry，方便重复使用。类似yum仓库。harbor是私有内部仓库。 容器的生命周期很短，生产中平均下来也就是1 2天，这个说法对吗？肯定不对啊，我docker跑gitlab，你觉的会是1 2天，1 2年还差不多。 容器的IP不想虚拟机是固定的，所以需要一个服务注册，自动发现的机制，可以实现如何访问容器。不然容器的IP老变，我怎么找你呢 # NFS那里也有注册和发现的机制。 因为你要追求容器的灵活性动态性，所以一整套的方案就要考虑清楚。 ​ 在企业里一般先搭建一个registry，里面事先把需要生成中用到的各种镜像全都放里面，当然这是一个持续的过程，事先不一定的哈哈。 ​ 这些镜像比如有nginx官方做的，ubuntu官方、mysql官方做的各个自家的镜像。都传到互联网公共仓库上了。但官方镜像不一定满足你的需求，可能还需要自己定制。 ​ 读图，client运行命令，就会连接docker守护进程，docker守护进程收到这个cli后帮你去执行这个操作，一般docker守护进程也client运行的机器都是一台了。 ​ docker pull就是从仓库拉取镜像，这里也会涉及本地docker私有仓库以及做代理的事项； ​ docker run就是在pull拉下来镜像了基础上，将其run起来，表现为容器。自然可以基于同一个镜像run出来多个容器。镜像就是静态文件，而容器就是进程了。 ​ docker build就是自己只做镜像，自然就会在本地磁盘上表现为文件咯，然后再执行docker push将自定义的镜像推到仓库里去，便于共用。 ​ 以前我们启服务都是sytemctl start xx了，现在都是docker run咯。不需要再去写什么systemctl的service文件了。 容器怎么隔离的-namespace namespace是内核里的一个特性，自然也是靠内核层实现的。 有很多namespace，不同的namespace隔离不同的资源，比如网络的namespace，PID的namespace等。 MNT Namespace(mount) 不同容器的根/其实是隔离的 IPC Namespace(inter-process communication) 共享内存，消息队列都是隔离的 UTSNamespace(Unix Timesharing System) 主机名和域名的隔离 PID Namespace(process identification) 进程编号隔离，大家都是1号进程，但是互不干扰就是隔离。整个进程树的隔离 Net Namespace(network) 典型代表就是ip地址可以一样的，正因为隔离了才能一样哦 User Namespace(user) 用户账号的隔离，就是这个容器上的root和root组和那个容器上的root和root组不搭嘎。 以上就是利用了内核里的namespace隔离机制实现了类似虚拟机的隔离，只不过虚拟机的隔离是OS级别的，而容器时内核级别的。所以比如一台物理机作为宿主，就直接用linux内核namespace实现了隔离，而不是再在其上安装虚拟机了。 namespace时linux的内核，windows里有吗？有也不一样啊，容器又是如何实现的linux的容器在windows也能跑的呢？ windows上跑容器，其实时先安装了一个linux内核的，所以才能通用。 这些namespace也不是一下子一起出现的，有些时后期开发的，比如linux内核3.8才支持user namespace，不过现在内核都还挺高的。 ​ 容器依赖的用户空间--UserNamespace必须是linux内核3.8才支持的，所以3.8以下的内核就无法跑容器，centos6是2.6的内核不支持是容器的，但是可以升级内核来跑容器。 ​ 用ubuntu的好处就是内核升级快，一些容器的新特性就用起来，其实不仅仅是容器，很多基于高版本内核的特性都是如此。 既然内核的namespace是容器之前就存在的，那么其管理工具也是自然有的 ns enter是进入到namespace里👇 如图👇只是654的PID进程，里面可以看到是划分了多个namespace的。 同样pid为1的进程 普遍发现多了以下namespace，比如time 、time_for_children、pid_for_children、cgroup这些上面都没提到，说明是一些的内核namespace了。 这些新增的namespace可能是较新的技术的依赖，不一定是必须的可能。 下图👇-n是进入了654 PID进程里的network namespace里了。 上图还不是演示的很好，应该用docker来演示，就能看到进入到每个容器里的ip地址都是不一样的。 除了隔离还要限制资源的使用-Cgroup 一个宿主上的若干个容器资源的使用肯定要分配好的，否则容易出现争抢的情况。所以这就需要资源的分配和限制。 Control group就是内核中的另一个重要技术，联系，上面讲了内核里的一个技术namepsace 上面的截图里其实已经看看到了cgroup了，就是干这事的。 通过过滤内核文件就可以知道确实是集成了CGROUP的。 总之一句话：docker运行之所以能够做到 \"隔离\" + \"限制\" 就是依靠内核的namespace和cgroup两技术来实现运行空间的隔离和资源使用的分配和限制。 docker里的namespace和k8s的namespace不是一回事 还有一个 docker的namespace其实就是linux内核的namespace，内核什么版本里有什么名称空间，可能高版本的内核会多几个，但关键的就是上文所讲的那几个。 而k8s的namespace是用户空间里的东西，是用户人为创建自定义的，今天创建明天删除都是可以操作的。 容器管理工具-目前就是docker为主 docker提供了完整的管理工具集 一般就是先制作镜像(把业务的各种服务做成镜像)，然后这些镜像复制出来到该落在的地方，然后跑起来自然就变成了容器。 ​ 镜像里存放的东西就是👇App B和Bins/Libs 依赖的这些环境包括：配置文件、库等。 👇下图理解注意下，严谨点不要理解错了：容器是利用了OS本身的内核，所以下图不是容器的分层，容器里是没有内核的，容器是用的宿主机的内核。kernel层画在这里也是表示一个完整的操作整体示意一下的。 容器是有标准规范的，容器以及生成容器的镜像 ，是表现为一些文件，而这些文件是有格式的，是由国际规范的，简称由国际范的~ 只要遵守 镜像 的国际标准，容器的国际标准，理论上用什么容器工具都而已，不仅仅是docker 阿里巴巴的pouch https://github.com/AliyunContainerService/pouch 红帽的Podman 官网地址：https://podman.io/ 项目地址：https://github.com/containers/podman docker 命令和 podman 命令 的操作是无缝迁移的，就是一样的。不过podman是没有C/S结构的。 但是我发现github里podman的版本更新明显要勤快的多的多， ​ 将来会遇到的问题，容器里的网络排错，连ping都没有，该如何排错呢，以此为例的意思大概就是；容器本质上会极其精简的(除非你自己构建的时候把这些检测工具打进去)，在这个精简的容器里很多常规VM里的操作都是无法进行的，于是排错检查的很多常规手段就失效了。不过ping命令总归要打进去吧，还有ss，ps 都可以打进去的。 规范的事情 docker毕竟是一家公司，如果该公司GG，那么容器管理工具是否就缺失了很重要的一个工具了。 所以OCI指定了规范runtime spec和image format spec只要满足规范，就能够保证容器的可用性(移植性、相互可操作性)。 也许吧 用户都是和高级运行时打交道，然后通过高级运行时去调用低级运行时里面的环境，最终来运行容器。 用户一般就关心高级的，底层的不关心。 目前来说低级运行时，主流的就是runc，docker和其他的都是用的runc(运行时) runc其实是个应用程序，通过runc代码程序来运行容器并管理起来。 runc也是docker公司开发的，后来开源了，大家都在用。 containerd也是docker公司开发的，后来跑到社区里了， K8S从1.24开始抛弃docker，其实是不绕走docker引擎调用containerd了，而是直接调用containerd。 docker 命令去连接docker引擎然后调用containerd，再经过垫片的解耦，调用runc，最终运行容器。 shim的作用好比垫片，解耦了containerd和runc，将来这两个开发是独立的项目，各开发各的，只要能对接shim垫片就行了(就是都遵守shim垫片的api接口)，好比螺丝螺栓之间的垫片。 上面一段文字其实就很不错，就是一些开发思路，不过也会增加工作量。应该是随着项目增大才需要这种垫片的存在。 ​ 安装docker的时候，上图的一套都装上了。 docker info查看可见默认运行时是runc，实际的runtimes是io.containerd.runc.v2 据说有些公司就是作，docker不用非要用其他的工具👇，不过K8S里不是有个什么工具嘛，不用docker这个cli不是正常的嘛 什么意思呢，就是说只安装containerd后面的这些，前面的管理工具也就是cli这方面用其他的。 镜像仓库 1、官方的需要本地作缓存代理，也就是nexus之类的统一管理私有仓库，或者verdaccio这种NPM代理缓存库也OK 2、国内的镜像站点，比如阿里的仓库 3、Harbor：vmware提供的自带web界面，自带认证功能的镜像私有仓库，很多公司在用。 4、image registry：docker官方提供的私有仓库部署工具，无web界面，目前用的少。 容器编排工具 docker是单机的 举例，wordpress要跑在容器上，一般就是拆成多个容器： 1、nginx/apache ： 容器1 2、php ： 容器2 3、mysql ： 容器3 三个容器，启动，之间还有一定的依赖关系，apache依赖于php，php又要连数据库，所以启动的时候，1、先启动mysql，2、在启动php、3、再启动nginx/apache。 如果docker cli来实现，就是从上往下先后敲入👇 docker run mysql docker run php docker run nginx 于是就存在单机编排工具docker compose这是官方实现单机的容器的编排工具。该工具类似ansible ansible的玩法是：简单的就ansible命令临时执行下，一些复杂任务就要写playbook了(ansible-playbook playbook.yml)剧本里都写好了谁先执行，谁后执行了。 docker cli 好比 ansible cli docker compose 好比 ansible-playbook xxx.yml 多个容器用docker-compose跑起来就简单些，而且单个容器用compose跑可能也是不错的选择，因为又类似配置文件的一个存在，不是依靠临时的cli去搞。compose可以理解成脚本化的docker cli。 不过单机上的编排也是不够的，要跨主机进行管理就需要跨主机的容器编排工具 docker swarm 这是官方的，也淘汰了， Mesos+Marathon 也GG了 现在就是Kubernets google领导开发的，内部项目为Borg，且其同时支持docker和CoreOS，当前已成为容器编排工具事实上的标准。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:23 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/3-Docker在线和离线安装多种方法实现.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/3-Docker在线和离线安装多种方法实现.html","title":"第3节 Docker在线和离线安装多种方法实现","keywords":"","body":"第3节 Docker在线和离线安装多种方法实现 安装 https://docs.docker.com/desktop/install/linux-install/ 所需平台： 所需资源： 说是4G内存，其实2G也能安装。工作中肯定不是2G就能OK的。 生产中的服务器，内存也有高达1T的，就是为了跑好多容器的。 安装方法，我们一般不按照destop这种GUI版，正常就安装Engine https://docs.docker.com/engine/install/centos/ sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Install Docker Engine Install Docker Engine, containerd, and Docker Compose: Latest Specific version To install the latest version, run: $ sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin $ sudo yum install docker-ce # 上面一行其实只要敲这一行，安装docker-ce就行了，后面的都会走依赖包自动安装好的。 -ce所谓的ce就是社区版，还有一个-ee就是企业版。 If prompted to accept the GPG key, verify that the fingerprint matches 060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35, and if so, accept it. This command installs Docker, but it doesn't start Docker. It also creates a docker group, however, it doesn't add any users to the group by default. Start Docker. $ sudo systemctl start docker Verify that the Docker Engine installation is successful by running the hello-world image. $ sudo docker run hello-world This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits. You have now successfully installed and started Docker Engine. 使用国内源的安装方法 https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 操作备忘，我之前就有docker，这里需要删掉 ubuntu方面 这种指定版本安装存在的小问题就是，docker-ce你指定了是24.0.0，但是其他依赖包没有手动指定，就会下载最新的了。 docker-ce是install的时候指定的版本，其他未指定的就下载最新的了。 ​ 所以要安装docker 某个老版本的时候，需要严禁一点，就是手动指定各个包的版本。其实也就是图上dpkg -l |grep docker所列出来的各个包 ​ 最新的版本的安装自然就是统一的，就保证仓库的rpm源是最新的直接yum就行了。 这样就保证了ce和ce-cli的版本一致，当然最好还是全部一致。 我就不重新装了，我没啥要求，yum install就行了，要注意docker的各个包版本要一致，不要踩坑。 这是我的版本 容器里安装docker-client，并不是套娃，而是client的命令 server就用宿主机的 client就调用远端的server执行docker cli 场景： 容器里面将来可以执行docker cli连接远端的docker server也就是docker engine来执行 docker操作。 比如👇C1容器里执行docker cli把C2容器给停了，当然C1要通过docker cli 命令连接到宿主上的docker服务进程来执行命令的。 ​ 其实就是本来docker-cli都是在宿主上敲的命令，现在可以在运行的容器里面执行，换而言之，C1一旦安装了docker-cli就可以变相的等价于宿主机上敲docker-cli以此达到控制宿主上所有容器的效果。 ​ docker的命令真正的执行都是docker-server来执行的，docker-cli在哪里敲都一样，你在宿主上敲和在容器里敲都一样的效果。 ​ client和server可以不在一台机器上，网络可达就行了，之前看书的时候也试过cli里可以指定连接的engine也就是server的。 就跟mysql client可以管理本机的mysql db，也可以管理远端的db一样，cli里指定-h a.b.c.d就行了。 那么问题来了该如何在容器里安装docker-cli命令呢 1、通过镜像提前封装进去，可以的，可能就是不太灵活 2、使用官方通用的安装脚本来弄 两条命令就ok了👆 docker的二进制安装-适用于无法上网的机器 然后二进制安装的一个优点👇就是可以明确看到之前的这几个模块， 再写个service文件并做好开机启动 一般就是docker cli就和本地的engine连接就行了，所上图的socket一般也不会修改为让别人机器远程dockercli连过来的ip+port了。除非特殊需求~ 再一个docker是以root用户运行的，因为很多系统底层的资源的权限需要root调用。 二进制安装脚本可能需求按需修改的地方 1、版本和国内镜像源 2、下载的URL，由于是国内的所以路径要修改的 其他无需改动就是下载后，解压，写service 然后运行就可以了 离线安装的时候-也就是内网无互联网访问的情况下安装，就是需要把安装包下载下来，因为docker是go写的，所以都是静态库，比较好安装的，依赖问题少。 脚本在视频目录层的课件目录下有的，要用去那里那就行了👇，以上是补充👆 其实也没啥，就是下载现成的离线包，解压，二进制执行PATH路径都可以直接移动到/user/local/bin下就行，以及service这些常规操作。 别人跟你要要docker 3.0 ，你别当真哦，呵呵~ 也就是说这些文件，拷贝到任何机器上，都可以正常运行的。所以这些复制到容器里面即使那里是极其精简版的linux也照样可以运行。 pstree -p可能也可以看到进程之间的调用，不过不一定全，因为rocky和ubuntu也不太一样，然后就算ubuntu的pstree可以看到👇，也不全，可能是还需要进一步启动容器后才能看全吧。 监听在socket文件上，也就是docker cli通过这个socket来和dockerd服务也好引擎也罢来通信的。 下面就开始学习docker命令了，通过命令学习技术的逻辑和方案的落地，而不是去机械的命令，我反正从不记命令，这也是我写这些文章的底层原因，因为我学了就忘了，连小时候考完试就忘，从来都是如此。但是话说回来，要用的时候忘记了可不行啊，所以还得在这里写笔记以备不时之需，到时候也是需要鼓足勇气耐着性子复习一遍的，反复多次，捡起来的速度就会很快，比如一些投影仪一些智能照明的设备如何在云上操作，他们其实就是只能穿1层NAT，两层就需要在内部的最近的一层NAT里做好DNAT了，至于为什么能够穿一层，那就是之前我还没上传的ssh隧道里的知识了，是我同事遇到了安装师傅的协作要求放行内部AC的DNAT端口，其实就是AC上作DNAT，我就可以立马反映过来其实不是外部发起的流量，而是内部智能电灯发起的ssh隧道，SSH -R 9527:destser:5000 -Nf CloudSer 正因为兄弟我不止一次复习过那片文章，也是我自己记录的，所以才能够快速反应过来，但是cli我是不记得，逻辑我是喜欢的，但是生活往往要放弃逻辑，这又是另外的话题了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:24 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/4-docker命令基本用法说明.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/4-docker命令基本用法说明.html","title":"第4节 docker命令基本用法说明","keywords":"","body":"第4节 docker命令基本用法说明 cli分类 swarm不行了，被k8s替代了，不学。 管理类👇 container 管理容器 image 管理镜像 network 管理网络 volume 管理存储 进一步还可以看docker image 大类下的子命令👇 但是老司机不爱这种分类的cli，更爱直接点比如 docker image ls 就= docker images docker container ls = docker ps docker info 还可以看到docker compose的版本👇 以及 server 里的容器： running运行的几个，paused暂停的几个，stopped停止的几个 docker 跑一个hello world， 1、要知道docker 跑容器，容器靠镜像，镜像哪里来呢 2、本地找找 3、仓库找找 上图就的Hello from Docker！字眼 就说明执行成功了，容器就是运行完就释放掉了，这里又不是当作守护进程来跑的，所以docker ps里看不到了就。 docker pull hello-world 只下载 不run docker run hello-world = docker pull hello-world + docker start hello-world 注意知识容器进程退出了，只是程序退出了，这个容器还在磁盘上。 进程没了，磁盘空间照样占着的会， start 起来后运行完了也会退出的 很重要的一个目录-image下载存放目录 这个目录是默认存放镜像和容器相关文件的，这个目录没了，所有容器 和 所有镜像就都没了。 速度线索尝试总结，我感觉要单拎出来①★：这个目录必须是一个高速且大的磁盘，又高又大，因为所有的images和containers都默认扔在这里，跑起来，硬盘慢也会拖累容器的运行速度。生产中可以找个目录单独挂一个高速的SSD，没预算就算了哦，一起ssd也是ok的，哈哈，缝缝补补有三年也是ok的 彻底删除容器 1、卸载软件：apt purge docker-ce # ubuntu的 centos的“：yum remove docker-ce ? 还是yum history list 找到id后yum history undo xx 卸的还全呢，对吧。 2、删除软件产生的文件：rm -rf /var/lib/docker 学到这里，我又要逼逼两句了，删除A，就是要两步走①删除A②删除A的影响，有首歌叫 你身上的香水味~~，就是人走了，还得通风。HA也是这个道理，不管是线路的HA还是节点的HA，当HA的成员挂了一个，作为HA的底层逻辑同样需要①去掉故障成员②去掉故障成员当前的影响(比如全网ARP，会话缓存、MAC缓存等)③如果故障恢复一样要观察一下是不是真的恢复了稳定了才能将故障点重新加回HA并继续提供服务。 docker info也是可以看到整个默认存放的关键目录的 可以换成成别的目录，比如换到一块牛逼plus的单独硬盘上去，或者将整个目录挂过去都是一样的。 搞一个run后不退出的容器 镜像是基于不同的操作系统做出来的，所以docker run也好pull也罢都是要注意的， 然后还可以到tag里去看细节 👇看着就是不同系统的镜像，docker pull nginx就是下载linux的且是latest的版本。 alpine是一个精简的linux os。是基于某个os制作出来的镜像。区别默认的应该就是deban和ubuntu的os做出来的。这句话怎么了理解哦： 就是docker的分层模型 container层就是docker engine了实现了容器的隔离，通过namespace和cgroup两大组件做到了隔离和资源的分配以及限制。 那么要理清楚一个点：就是 这些版本代表的意思是啥，对分层理解就会加深一点， 根据GPT多说，可以认为她讲的是对的，我们下载的镜像它肯定不是dockerengine ，docker engine是docker server进程，而images名称里的os就是说我整个image利用了底层宿主os的内核(大家都是linux内核啦)，然后我image里还自带一些基于整个内核实现精简os，整个os是半成品os是没有内核的os，GPT称之为用户空间--Alpine linux的用户空间，所以这部分+宿主的内核就构成了完整的os了。 之前学习sendfile这个0复制技术逻辑的时候，还头头是道，其实殊不知当时的整张图就是在OS里聊的，对吧，用户空间，内核空间，这些就是OS啊，唉，蠢笨如我~~~ user Space 1 2 3 就是你下载的image 并且运行起来的 容器，这些容器通过 container也就是docker 引擎 (namespace + Cgroup) 来隔离且限制地使用了HOST OS的内核，但是OS本身还有用户空间，这部分userSpace1 2 3 的容器是没有使用的，他们各自用的自家的image里封装好的自带了os一部分的用户空间。 好了反复拉扯应该靠谱了 容器运行起来就是一个小OS，自己image里的用户空间 + 底层宿主的OS里的内核空间 构成了一个完整的OS，那么是有自己的IP地址的。查看方法docker inspect # 其实还可以inspect 镜像 网络 存储 等所有资源的详细信息，好比一个万能命令。 inspect里是有容器的IP的，ip a是看不到的哦👇 再run一个nginx，ip就是0.3，按需发放的。 · 再把容器启动起来，大小就恢复了 图中N是image的nginx的n简称，N1 N2就是两个容器，各自启动就会各自从image复制一份镜像过来。停止N1后从镜像复制过来的那部分就删了，但是N1的数据还在。 不要用swap，算作内存优化吧 以前的docker info 就是老版本会有一个提示就是swap的限制limit 解决方法就是，关闭swap，内存不够加内存，别TMD用磁盘作内存，到时候速度跟不上麻烦的很！ docker 和 k8s都 要关掉swap！ swapoff -a # 临时禁swap 在/etc/fstab里删掉挂载swap的那行，保存后，并swapoff -a一下。 还可以停止swap的服务 mask就是停掉了，怎么不是stop和 disable呢。 mask掉，start都起不来的， docker0也是网桥 确实下是否是网桥，安装查看工具 如果开一个容器，容器也会生成专门的网卡，容器的网卡就会和这个网桥docker0关联在一起 而且这些容器专门的网卡ip a都能看到，只是IP还需要docker inpsect去查看 这点和kvm一样👆：关联网卡，以及ip地址，docker是172.17 kvm那里是192.168.122；都可以改。后面专门单篇讲网络。 同样和kvm一样的，iptables 规则也会自动生成 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:24 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/5-Docker配置优化.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/5-Docker配置优化.html","title":"第5节 Docker配置优化","keywords":"","body":"第5节 Docker配置优化 docker 工作是基于镜像，而镜像存储的驱动，通过docker info可见是overlay2。 这些存储引擎不管是那种，都是联合文件系统 比如ext4 xfs的文件系统，磁盘挂载到目录，是不可以重复挂载的，也就是一个目录你两个硬盘挂载的时候，前面一个就被顶掉了。 而docker的联合文件系统时可以把多个设备(所谓设备就是硬盘、分区、lvm这些)挂载到一个目录下，其效果就是这个目录下既可以看到设备1上的数据，也能看到设备2上的数据。 ​ 为什么可以采用联合文件系统，就是因为docker的image是多层镜像。 image是一层层构建出来的 容器生成，是复制一份镜像，并在其上又添加了一层writable可读可写层，这层就用来存放容器自己的数据--比如LOG。 docker stop xxx ，就是把复制过来的image层删了，其上的容器子深深的writable层还在的。 容器启动的时候其实是可以修改存储驱动的 不过，想来一般是不会去改的。确认是overlay2就行了。万一不是就换docker版本~ 查看支持的文件系统👇注意aufs是aufs，autofs是autofs，不要瞎搞 docker优化 下载源，其实应该私有库作缓存来弄比如nexus 可以使用阿里的个人镜像加速地址 其实最好是用私有缓存库比如nexushttps://cloud.tencent.com/developer/article/1764866 socket通信-docker-client和server之间的通信 默认走的本地socket文件，无法支持远端的调用。 配置方法👇下图绿框框；但是用的不太多。一般就是本地连接。 该配置文件类似cli里的-H选项，既有socket文件也有tcp，可能就是写两个-H咯 还可以改service文件 先通过修改daemon.json去试下 但是报错了👆 换个思路，去service文件里改改 然后把daemon.json里的hosts那行删了，修改为 再reload 和 restart 此时tcp和socket文件都好了 此时就可以远端client 调用 server 了，也就是docker cli 连接 docker server引起后 调用命令了 然后远程跑一个nginx看看，此时容器都是退出的 去远端windows的cmd里执行远程cli，run一个nginx 然后去server看下docker ps 这就实现了远程执行docker cli 同时curl 瞧瞧，因为是容器里的ip没有暴露出来呢，所以ipsect看到ip后宿主机上curl下👇 要注意docker engine也就是服务端在哪，容器就运行在哪里，客户都的cli只是一个连接调用工具，很好理解，好比mysql 远程执行命令一样。 将来在K8S里的跨主机的远程docker调用可能就是用的这个。目前docker学习阶段还用不到。 看来是查不到就往下一个地址查找。 现在都是systemd了👇 不用改了，以前是这么改的👇也是daemon.json里 还有这个容器和镜像存放的路径-迁移就是复制粘贴该路径 默认是这里👆，这么改👇改到单独的高性能ssd上去，前文讲过了，目录不该直接挂到新磁盘上去，或者该目录这个目录挂过去也行👇 在旧版本的 Docker 中，可以使用 \"graph\" 指定 Docker 存储容器和镜像的路径。但是在较新的 Docker 版本中，该选项已被废弃，取而代之的是 data-root。因此，您可以尝试将 \"graph\":\"/data/docker\" 替换为 \"data-root\":\"/data/docker\"。 修改后的配置文件内容如下所示： jsonCopy code{ \"registry-mirrors\":[\"https://si7y70hh.mirror.aliyuncs.com/\"], \"data-root\":\"/data/docker\" } 修改完成后保存文件，并尝试重新启动 Docker 服务。您可以执行以下命令： bashCopy codesystemctl daemon-reload systemctl restart docker 还可以在service文件里配置 systemctl daemon-reload下，再重启一下systemctl restart docker 此时，会自动创建文件夹，但是docker images里是空的了，docker ps -a也是空的，之前退出的那些容器都没了。 原来的容器和镜像的文件都没过去 max-concurrent-dowload|upload，同时拉去和上传的限制 这个操作会涉及max-concurrent-dowload参数的限制，但不是完全涉及，如果images在本地就不用担心，知识限制了和docker 仓库之间的下载/上传的并发数。 日志的问题 1、容器的日志都是默认放在硬盘上的 2、日志的大小要限制下 3、日志文件也是落在容器和镜像存放的路径/var/lib/docker下的 通过docker info可见，👇当然我的路径改过的 log就在docker root下的container文件夹里的每个容器文件夹里。 限制配置 日志的最大值和滚动覆盖👆 docker日志越来越大把硬盘撑满了，如何解决 1、上图👆限制+滚动日志 2、docker也是在容器和镜像的目录下的也就是/var/lib/docker/，将这个目录挂到单独的磁盘就行了。 3、周期清除无用的镜像和容器👇 curl伪装浏览器，也就是伪装访问方式-A可以-H也行 实验 1、先跑一个nginx 2、然后curl访问这个容器的ip 这就使用curl 伪装了一个用户访问方式--user-agent 3、找到有该user-agent关键词的日志 总之就在这些文件里 很贱的grep -r就行了，不要用-R -R是会继续搜索软连接的，比如上图的第一个No such file or directory就是一个ln -s👇这种搜索一般没必要 将来日志会越来越大，而且不仅仅日志，images和容器也会越来越多，之前的处理方式有一种(是在runner上的操作可能会简单的）， 清除无用的镜像容器慎用 live-restore 如果docker服务重启了，容器会不会重启？不会，docker重启，容器就停掉了👇 反之删除配置文件里的live-restore，重启docker后，所有容器就停了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:24 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/6-Docker镜像搜索和下载.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/6-Docker镜像搜索和下载.html","title":"第6节 Docker镜像搜索和下载","keywords":"","body":"第6节 Docker镜像搜索和下载 镜像管理 比如从网上下载了基础镜像，然后在上面封装了一些网络检测工具， 然后继续封装jdk，tomcat，最后跑容器就是又添加了一层可写层，而可写容器层往下都是镜像层是只读的。 未来打镜像就是采用这种逻辑不断地打，最终看到的镜像其实是多个镜像累积起来的结果。有点像mysql的增量备份，每次增量备份一点点，最终累加起来的才是完整的数据。 分层构建，将来每一部分 都可以独立抽取出来，用来构建其他镜像，也就是说镜像层是可以复用的，正以为分层所以复用起来很丝滑。 根镜像、父镜像、子镜像、子子镜像，大概有人会这么称呼 这个每一层的将来复用，不知道是不是联合文件系统 自己去复用啊，我感觉应该是docker自身能够复用才行啊。而不是用户手动去复用，用户没办法说去复用哪一层吧，没这个操作的空间啊。 通过docker image history nginx查看人家是怎么构建每一层的 镜像有了，创建好了，拉好了，落在本地其实都是表现为一个个文件夹的 最大的文件夹就是overlay2，里面的东西其实就是很多层，overlay就是层的意思，是所有images的层级铺开来的，才能共用，复用。这就是所谓的联合文件系统里的overlay2这种。 删除容器后 删除容器后，发现这些分层的东西就少了很多👇 docker rm删除容器和镜像👆 进一步再删除镜像，后发现overlay2里就空了 也就是说容器和镜像删光了，overlay2文件夹里就空了，现在pull一个nginx看看 overlay2文件夹里的就是分层的数据了👆。一个文件夹就是一层但我没仔细数哦，哈哈，大概如此吧 镜像去哪找 1、hub.docker.com 要用官方的镜像 2、cli，命令行搜索的方式不全，没有上面去网站搜索的方式好，就是去hub.docker.com还有其他的比如谷歌的站点。 自己做的镜像也可以上传上去，只是别人一般是不敢用的。 还有比如k8s的一些镜像，k8s就是谷歌开发的，对于谷歌来讲，docker公司肯定没有他们家的大，所以谷歌的镜像也就没有放到docker的仓库里，而是谷歌自己的仓库里的。 比如下图的这些很多就是谷歌的，但是docker上也有，就是别人上传的方便下载的 3、代理的配置方法 其实就是科学上网咯 上图就是配置你的代理机器，走代理的意思，可惜是错误的👆实测这样配置服务都起不来，其实你完全可以做在路由器上，就不用管这里的怎么配置。 不过这里的配置方式也要知道，方法越多，应用起来就越灵活。 设置配置文件里的配置，还有一个脚本是修改系统层面的代理，同样也会被docker读进去的👇 以上是部分咯，修改代理的方法，三选一就行了👇参考https://blog.csdn.net/peng2hui1314/article/details/124267333 总结如下： 1、网络里的路由器层面直接做路由，略 2、主机层面，的系统代理 3、docker服务层面的代理，就是上面的脚本，其实也就是服务文件里的三行内容，其他都是配套语句呵呵👇 4、容器里面， 5、可惜daemon.json这个文件的配置我没有找到上图图示的配置成功的案例，也许是老版本的配置，类似hosts了吧，不管了。 不过容器里的配置倒是和第3点-容器里的配置一样的，哈哈。 镜像制作 比如java程序需要一个镜像，要做java镜像， 然后java又依靠JDK，又要做JDK层对吧 然后JDK又依赖linux，又要做linux层， 一般来讲，是系统镜像就从官方拉取(系统镜像前面的文章也解释过了，其实是没有内核的用户空间的部分)，而业务镜像才是自己制作。 这就回答了下图👇的问题，用户空间里是有os的，只有os的一部分，然后再结合下层宿主的os的内核 就能构成完整的os了。以此来提供服务的。 然后docker制作镜像的时候，底层不是要linux嘛，一般也不会选择ubuntu，centos、rocky、redhat都不会，因为都太大了，压缩后还要25MB 都太大了，所以会选择一些积极精简的linxu，比如aphine或busybox👇 alpine就是基于busybox开发的 生产中用的比较多的alpine 1、包安装工具 2、alpine的仓库配置 3、用法也不一样 ubuntu的仓库文件 alpine是不同的 ustc是中科大的 alpine也是有完全镜像和极简镜像的，好比centos的all-in-one和mini一样👇 指定版本下载，要去网站看看版本的格式 然后再复制，粘贴就行了 然后网站上看到的镜像是压缩后的，pull下来会变大就是解压了应该👆 cli 用法 只显示镜像id：docker images -q # 操作镜像靠ID是不好识别 批量删除就很方便 xargs -i 就行了；或者直接docker rmi `docker images -q` cli用法，显示镜像和tag： docker image ls --format # 操作镜像靠名称和tag，也可以识别 删除也要加上tag的，否则就是删除就是删除latest这个tag 容器跑着的时候删除image，不会真的删掉 被占用的镜像如果使用docker rmi -f 'IMAGE ID' 这种方式 delete 肯定和写 image name tag不会delete，而且 连untag动作都不会执行。 业务也ok 此时即使停掉容器，删掉容器，那个镜像也不会自动删掉，就成为了一个无名的镜像，也叫dangling镜像 只查看这种dangling镜像的方法，docker images -f 就是filter过滤出来哪些特征的images 删除这种就这样docker rmi -f $(docker images -f dangling=true -q) # 就行啦 或者docker images -f dangling=true -q |xargs docker rmi -f # 也行 不过要停掉占用该镜像的容器，否则还是会删不掉👆 系统中有一个清理的命令docker system 这样不仅仅是没有用的images，其他的停掉的容器和没用的网络，缓存都清了 有些容器停了，但是不是说就可以删的，你就不能用这个命令了！所以这个prune还是慎用！ inpsect是通用型命令 上图的问题就是，images里看到的container应该就是当初这个镜像是通过容器commit出来的，所以会带上容器字眼👇 然后详情如下，容器、镜像、网络 ，都这么看👇 [root@nginxproxy ~]# docker inspect 05455a08881e [ { \"Id\": \"sha256:05455a08881ea9cf0e752bc48e61bbd71a34c029bb13df01e40e3e70e0d007bd\", // 镜像的唯一标识符。两个镜像的内容一样(也就是RepoDisgests一样)，但是构建时间不同或者名称tag不同，id也是不同的。 \"RepoTags\": [ \"alpine:latest\" ], // 镜像的标签，这里表示这是 alpine 镜像的最新版本。 \"RepoDigests\": [ \"alpine@sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\" ], // 镜像的摘要信息，用于验证镜像的完整性。 \"Parent\": \"\", // 父镜像的ID，为空表示此镜像没有父镜像。 \"Comment\": \"\", // 镜像的注释。 \"Created\": \"2024-01-27T00:30:48.743965523Z\", // 镜像的创建时间。 \"Container\": \"4189cbc534955765760c227f328ec1cdd52e8550681c2bf9f8f990b27b644f9c\", // 用于创建此镜像的容器的ID。看来很多都是从容器直接commit提交出来的镜像，而不是docker build出来的咯，可能是~ \"ContainerConfig\": { // 创建该镜像的容器的配置信息。 \"Hostname\": \"4189cbc53495\", // 容器的主机名。 \"Domainname\": \"\", // 容器的域名。 \"User\": \"\", // 容器内命令运行的用户。 \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, // 这些设置控制是否将stdin、stdout、stderr附加到容器。 \"Tty\": false, // 是否为容器分配一个tty设备。 \"OpenStdin\": false, \"StdinOnce\": false, // 这些设置控制容器的stdin。 \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], // 容器的环境变量。 \"Cmd\": [ \"/bin/sh\", \"-c\", \"#(nop) \", \"CMD [\\\"/bin/sh\\\"]\" ], // 容器的默认命令和参数。 \"Image\": \"sha256:9a5ce069f40cfe0f2270eafbff0a0f2fa08f1add73571af9f78209e96bb8a5e9\", // 创建容器时使用的镜像ID。 \"Volumes\": null, // 容器使用的卷。 \"WorkingDir\": \"\", // 容器的工作目录。 \"Entrypoint\": null, // 容器的入口点。 \"OnBuild\": null, // Dockerfile中的ONBUILD触发器指令。 \"Labels\": {} // 容器的标签。 }, \"DockerVersion\": \"20.10.23\", // 创建镜像时使用的Docker版本。 \"Author\": \"\", // 镜像的作者。 \"Config\": { // 镜像配置，类似于ContainerConfig，但用于运行时。 \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/sh\" ], // 容器启动时执行的命令。 \"Image\": \"sha256:9a5ce069f40cfe0f2270eafbff0a0f2fa08f1add73571af9f78209e96bb8a5e9\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": null }, \"Architecture\": \"amd64\", // 镜像的架构。 \"Os\": \"linux\", // 镜像的操作系统。 \"Size\": 7377074, // 镜像的大小（字节）。 \"VirtualSize\": 7377074, // 镜像的虚拟大小。 \"GraphDriver\": { // 镜像的存储驱动信息。 \"Data\": { \"MergedDir\": \"/var/lib/docker/overlay2/bb16c3d711597eff0baab5640f474ef4c8c1c40df0351e0a21343e1362676504/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/bb16c3d711597eff0baab5640f474ef4c8c1c40df0351e0a21343e1362676504/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/bb16c3d711597eff0baab5640f474ef4c8c1c40df0351e0a21343e1362676504/work\" }, \"Name\": \"overlay2\" }, \"RootFS\": { // 镜像的根文件系统信息。 \"Type\": \"layers\", \"Layers\": [ \"sha256:d4fc045c9e3a848011de66f34b81f052d4f2c15a17bb196d637e526349601820\" ] }, \"Metadata\": { // 镜像的元数据。 \"LastTagTime\": \"0001-01-01T00:00:00Z\" } } ] 补充个细节，运行中的容器，删除其使用的镜像，此时rmi -f imageID是不会执行的，rmi -f imageName也只能untag也就是去掉名字 而已，此时image就是none，成为dangling镜像， 于此同时运行的容器image字段就不再是nginx了而是镜像的ID了👇 podman是红帽的docker容器管理工具 习惯很重要-pull的时候指明名称和tag pull的时候要带上版本的，不带就是latest，你今天的latest，和你下一次或者几个月后的latest基本上就不是一个版本镜像了！一定要清楚的哦~ ​ 写上版本，运维清晰，不会出错。 然后比如需求是，用最新的alpine，那你是不是要docker pull alpine，也是不可以的，网站去看下 但是pull之前还得打开浏览器就不delicious，于是找了cli的方式👇 利用xargs 排个版就看的一目了然了 肉眼可见3.19.1 是...最新的。。。 这会又发现不一样的了，20240329 结果比3.19.1还新 那么这个20240329在hub.docker.com上为何看不到，搞不好这压根不是官方的镜像，哈哈。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:24 "},"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/7-Docker镜像导出导入.html":{"url":"1-容器技术和堡垒机JumpServer实战/1-Docker安装和镜像管理/7-Docker镜像导出导入.html","title":"第7节 Docker镜像导出导入","keywords":"","body":"第7节 Docker镜像导出导入 镜像导出和导入-比如互联网下载后导入开发内网去-导出要用name:tag docker save 默认是标准输出也就是屏幕打印了，所以需要-o write to a file ,instead of STDOUT 👆上图是屏幕拒绝了这个输出，其实还是有STDOUT动作，也就是说可以在上图的cli后使用重定向的。 因为save出来是tar包格式，所以要加上.tar tar包只是打包的，没有压缩的这里，所以还需要压缩一下，减少磁盘空间占用 优化命令，就是导出来就是压缩的效果 1、首先使用重定向 2、然后 怎么直接打出来的，大小还少了11KB了👆，呵呵，应该数据没丢哦，哈哈哈；回头导入的时候看一下就知道了。 3、复制到其他docker机器上load load的时候👆-i或 的原因是当初save导出镜像的时候用的是ID而不是名称+TAG，所以源头就错了 即使你想通过tag去补也是不行的，因为你压根就不知道这个ID是什么镜像inspect也看不到了 除非你run起来进去看看是个啥容器，再看看image的哈希值，然后去官网比对哈希值，然后打上对应的版本哈哈 搞错了，再来，导出的时候不带tag就是所有tag达成一个包，带tag就是那个tag的导出咯👇 把一个机器的所有镜像导出导入到别的机器 补充，你也可以指定下载哪个，而不是通过自动判断你是amd64的， 就是没有tag了，自己打咯用上面的docker tag 来打 所有镜像导出 1、也算是一条命令搞定吧 因为压缩要时间的，所以你自己选择具体cli👆 综上所述，推荐的cli是 sort卡住不动的CPU👇 一个sort 100% cpu 啊？不正常。正常，稍微分析下就知道答案了： ​ 卡的原因也好理解--就是docker save xxx 其实就是images的导出内容了，是内容啊，所以很大，一直在sort排序，所以sort处理大量的数据了就，什么你表示怀疑，哦，那你用docker save xxx |cat 看看就知道啦 你说sort忙的过来不，这还是一部分，下面没完没了的乱码 二进制 格式👇 纠正截图👇 删除所有镜像 docker rm -f `docker images -aq` ，再导入就全部回来了👇 这样也行： 还有虽然是tar包，但是解开后是看不到images的，而是一层一层文件，还得是load才行 然后看看tar包里的这些文件吧 一个解包多出这么多，继续 发现👆layer.tar解开里面就是一个精简的 linux的 根目录 / 对比发现，少了boot这个么一个重要的文件，这很好理解，bootfs可不是在容器里，而在kernel里是宿主的内核，容器里只有rootfs层以及以上--这些是用户空间的东西。 然后继续看 这个精简镜像里的bin下的二进制都是busybox这个二进制的别名。 所以这些cat 、 df 、grep、mv、ls各种linux的命令都是busybox这个二进制模拟出来的，果真很busy啊~ 尝试使用该牛逼的二进制，结果发现用不了👇 难道有库依赖，还真有👇 然后busybox容器里的分层文件里，果然有这依赖库的👇 只要把这个依赖复制到宿主的/lib下应该就可以了，因为要在宿主用，宿主找的这种so文件还是会从/lib下去找吧 将依赖复制到宿主的/lib/或/lib64/下，记得退出一下，否则还是识别不到 可见busybox --list 得到具体的指令 我用xargs 做了横向排列，看到了ping 于是busybox用ping 优秀👆 very good 👆上图可见busy的大小是极简的，不仅仅cli少，而且cli里的某个命令的功能也是少很多。但是也够用了👇 补充：打tag的 下载的时候也要指定使用哪里的镜像，否则就是从deamon.json配置的仓库下载，什么没有json文件，那就是默认的官网了。 上图👆是tag打了要上传到服务器的，所以要这么打，到时候，push的时候也就是推送到对应的服务器和目录的。 如果是本地用，就只写个镜像名和tag就行了 总结 1、docker安装，还是要先把仓库配置好的，具体操作很简单，就是 https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 2、离线安装，见 离线安装也分官方脚本和非官方的，哈哈 https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 3、小结下 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 10:39:21 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作.html","title":"第2篇 Docker容器管理和镜像制作","keywords":"","body":"第2篇 Docker容器管理和镜像制作 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:24 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/1-Docker运行容器的常见用法.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/1-Docker运行容器的常见用法.html","title":"第1节 Docker运行容器的常见用法","keywords":"","body":"第1节 Docker运行容器的常见用法 docker怎么管理容器的 概述 互联网上的镜像，不需要登入，直接下载，一般公司私有的镜像才需要login后去下载 COMMAND ​ 可以修改为自己希望执行的命令，以及加上ARG也就是跟上参数，但是加的的cli必须是人家容器里有的命令否则无法执行的。比如上图hello-world容器里只有/hello这个命令，其他的红色框框👆里的cil都是没有的， 所有都是报错的。 # 我就想到其实容器里是否可以封装一个busybox呢，哈哈基本很多常用cli就都有了。我觉得还不错~ ​ 镜像就是没有boot(内核)的根文件系统+app程序文件(mysql、nginx、等各自的程序包括各自依赖的库)。 下个busybox看看多大是否可以后面集成到我工作中的images里 1、首先hub.docker.com搜索latest 2、然后找哈希值一样的版本号， 有同学问，那你为什么不直接下latest，你猜我为啥 5MB不到，可以接受吧~ alpine举例，说明commond的是前台cli，否则就是运行后即退出 ​ 容器运行起来不退出，要求容器运行时对应的COMMAND的是前台执行，而👆/bin/sh是一个后台执行的cli，所以运行后就提出了。类似docker run nginx执行了就是前台占用。 ​ 所谓前台执行就是执行后就霸占了前台，而传统systemctl start nginx这种启动方式在容器里就不能使用了，因为后台执行，容器里执行就退出了。 怎么停不掉啊👆，关闭窗口还是up的，因为没有-it交互，你的ctrl c 发不进去👆 加个-d 放后台，但是不会退出，因为用的cli 是前台就行。 --name选项，指定容器NAME 比如指定容器的NAME方便后面，比如容器之间的互访 随机的名字肯定可读性不好👆 -it交互模式进入容器里，看看文件，看看状态等操作 -i是交互，还不够，还需要一个tty接口，类似交换机的vty -i -t 还不够，还需要交互的sh否则👇 -it 配合什么shell，就是尝试尝试就行了 👆这就进来了 不仅仅有bash也有/bin/sh 只是用了bash交互，run的command还是原来的那个脚本 进来后， 通过👆boot里是空的就知道了，容器用的是宿主的boot(内核)，所以这里只是空的。 这个nginx是基于Debian做出来的，Debian系列的典型代表就是ubuntu了，apt就是yum了 安装ps ps就有了👇 容器里用sed修改为清华源，安装更快，调试更方便 容器安装东西大多数都是调试用的。 sed -i 's|http://deb.debian.org/debian|https://mirrors.tuna.tsinghua.edu.cn/debian|g' /etc/apt/sources.list.d/debian.sources sed -i 's|http://deb.debian.org/debian-security|https://mirrors.tuna.tsinghua.edu.cn/debian-security|g' /etc/apt/sources.list.d/debian.sources 容器exit就真的将容器退出了，即使容器本来时run起来就前台执行的那种，你用-it bash进去再exit，容器就不再时running了👇 如果不想exit退出的时候把本来应该前台执行的容器退出也可以用ctrl p q 组合键来安全退出，不过此时nginx就不再时前台了，而是后台up了，类似于docker run -d nginx了 上图👆不是nginx这种run起来就up的，一样也会被ctl p q变成后台up👇 这下倒学到了一个让hello-world后台UP的方法，赶紧试试👇，不行人家压根就没有shell，所以无法进去。而有shell的倒是可以用这种方式后台UP着。 run -it用的少，大多数都是exec -it来针对运行中的容器进行交互排错 比如run了一个nginx 发现某个问题，需要进去排错，于是docker exec -it进去 hostname只在容器内部生效，不具备容器互通的功效，只是在容器内部可以拿来本地通信。 通过这个hostname和本机的程序通信 容器运行起来后，退出自动删除，临时测试屁股擦的比较干净 只要容器run完退出的，那么就给你删除 ​ 所以，要做守护式容器，就得至少有一个进程是前台永久运行的👆 很多添加前台永久运行的手段： docker run alpine:3.19.1 tail -f /dev/null 👆上图就是前台运行不会退出，防止被ctrl c退出，于是加一个-d，是先保证一个前台cli再放到后台👇 然后前文也说了，这里也复制过来一并看下👇 怎么停不掉啊👆，因为没有-it交互，你的ctrl c 发不进去👆 放屁-it也不行 关闭窗口也没用 只能docker rm -f了 👇 那么问题来了， 1、docker run alpine tail -f /dev/null 无法ctrl c 中断 2、docker run nginx 为啥可以ctrl c 中断 难道nginx的COMMAND里的也就是xxx.sh里是用了-it的？ 不行1：用sh -c 包装也不行，就是不接受ctrl c，不知道为啥 docker run -d nginx -d的后台和nginx的前台 1、-d的后台是docker run 这个命令后台执行 2、nginx容器前台执行，是nginx容器的COMMAND是容器里的cli是在容器里前台 所以要区分开来 一般都是-d 在后台run，服务类的容器肯定要容器里的cmd是前天的。 nginx为例，容器里的cmd是前台执行的，日志(比如访问日志)就是屏幕STDOUT的，docker run -d run在后太后，日志就用cli去看 后台运行-d 的容器log查看也简单docker logs idxxx就行👇 docker logs -f xxx 就很nice 还有传统艺能watch -n xxx 下图是wathc -n 5 docker logs nginx001的效果👇，就是-n 5秒刷一次，也是实时的。 容器的自启动，不是说原来exited，变成了up，而是说up还是up 1、之前学过一个，这是docker 服务重启，容器原本UP的还是UP。 stop 过30s start 都是ok的，就是说容器随docker服务，原来启动的，还是启动的 当然原来是停止的，就不会自启动了。 2、容器服务重启的时候，重启的时候容器的状态是什么 + policy 是什么 ==> 重启引擎后 容器的状态会是什么 no ： 退出了不重新UP， 这是默认值，# 重启的时候容器是退出的（不管是异常还是正常），那么重启服务后，容器还是退出，这就是no。 on-failure[:max-retries] : 异常退出就重新UP，尝试N次。 non-zero exit status，这是$? ≠0 的意思，就是异常退出，状态码不等于0。 # 重启前容器是异常退出的，重启服务后，容器就会尝试起来。 always ： 无论退出码是什么--也就是不管是正常退出还是异常退出--也就是不管认为退出($?=0)还是异常退出($?≠0)都随宿主机器起来而起来； # 重启服务(啥重启机器，重启机器也是对应到重启docker 引擎，搞搞清楚，讲的什么东西，当然整体还是不错，这里讲的很，不好说什么了)的时候容器不管是怎么退出的，是你重启导致退出还是本来就是认为退出的，重启后都给你起来。 与no相对。 unless-stopped ： 重启机器(服务)的时候容器如果是人为退出的，那么启动了就不会给你UP容器，如果容器是异常退出的，比如重启的时候容器还是UP( 那么重启的时候容器就会是异常退出 )，重启后容器就给你UP起来 然后重启机器，此时web003就不会起来了，因为docker run 的是后默认是---restart no的，就是说重启等动作反正导致你退出的，docker引擎起来你容器也不会up的。 通过上面的截图可知web01是--restart always 方式启动的容器 启动后就是👇 所以重启宿主要用--restart always 这种或者至少--restart unless-stoped，而重启docker 服务，原来是up的希望它继续在重启服务后继续up就可以用live-restore配置选项，但该选项也做不到原来退出重启服务后给你起来的，而--restart always就可以👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:24 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/2-Docker容器查看常见用法.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/2-Docker容器查看常见用法.html","title":"第2节 Docker容器查看常见用法","keywords":"","body":"第2节 Docker容器查看常见用法 容器里的root docker 运行的时候，在容器里面，通常是以root运行的。 容器本身的文件还是分层落在宿主机上的overlay2目录里的👇，至于为什么是2个m53.txt这个后面再说，涉及分层结构的梳理。 该文件的所有者也是root 就是容器里的m53.txt所有者是root，容器外面宿主机上看到的m53.txt所有者也是root。 是一个root嘛？容器里的root是用的宿主机上的root嘛？并不是，容器里的root是一个假root就是一个普通用户。证明👇。 而宿主机是可以的👇，所以证明了容器里的root并不是传统意义的root超级用户。 df看不到挂载覆盖的效果，lsblk可以 顺便知道下dm0 dm1这些 所以dm-0 dm-1 这种就是lvm了，逻辑块。 privileged--容器里使用宿主机的root，除非特地这么要求，否则别加该选项。 索然/dev/mapper下面没有lvm的逻辑块，但是要知道这个目录下面其实就是软连接 真正的rl-root其实是连接到dm-0去的 这一点，在宿主机上可以看到👇 所以容器里直接mount /dev/dm-0就行了 然后 此时再到宿主上直接就看到了 这样容器里的某些操作就危险了，比如 此时宿主上的echo就没了 但是发现echo还能用， 忽然反应过来，type 一下看到了builtin，说明什么，说明echo是buildin在/bin/bash里的，是内部命令。 好，mv echo看不到效果，mv /bin/bash吧哈哈 直接GG👇 所以除非生产中有明确要求容器里可以操作宿主机的权限，否则别加该--privilieged选项。 那么如何知道run的时候加了那些选项，比如是否加了危险的provilieged 看全部的run的时候加的选项 不过还不是太明显，不是tmd一目了然，使用一个第三方工具runlike来一目了然。 -p就是--pretty 更清晰点排版 查看容器信息 前面一直在用ps images这些， 这里再补充下-f选项的过滤 https://docs.docker.com/reference/cli/docker/container/ls/ 下图链接有问题，纠正为👆 然后支持--filter选项的又有很多dockers cli ： https://docs.docker.com/config/filter/ -f等价于--filter，就是短选项和长选项的意思，好比-h ＝ --help 然后--format就是不想其他的，它只有短选项。 -f删除所有退出的容器 docker ps -f 'status=exited' -q # 退出容器的编号，删除👇起来方便 docker rm $(docker ps -f 'status=exited' -q) -s查看容器空间占用 容器分层，image共用，所以括号里的是image大小，容器自身的可写层，一般是一些log，占用501B --format格式化，能够提供全面的信息，提供赛选或者排序 按时间排序，显示名称、id、时间 按时间反序👇，找到最近的三个 再跟一个|head -3就行。 关于sort的补充，上图其实是3和4合成一个字符串进行排序的，正儿八经可能不是这个思路，而是先安3列排序，然后再按4列排序，同一天里再按时间排序。 以下是sort的补充，很关键👇 ​ -k3其实不仅仅按第3列进行排序，还会继续对比到尾行，就是3列然后看每行的。这种不清不楚的，没意义，不如下面的明确用法来的靠谱。 以后啊，按哪一列排序就用-k3,3 这就是按第三列排序，-k3,3n就是按第三列且当作数字排序。 所以固定用法👇，先按1列数字，再按2列数字，再按3列字符串排序 所以👇才是正确的cli，虽然三种结果一致 参考文档https://fancyerii.github.io/2019/06/15/sort/ docker top CONTAINER 在外面查看容器里的进程 有时候进去反倒没有top、ps这些命令，在外面反而支持 容器里面的进程ID进程数都是从1开始的，而外面看到的却是3277这种真正落在宿主上的进程ID了。因为容器里跑的进程真正还是跑在宿主机上的。 属于containerd-shim 什么是是shim，前面文章里有讲👇 docker stats 查看容器的CPU MEM等资源占用 docker stats就是看所有容器的资源使用情况 找个命令可以看看net的i/o，block的i/o，以及通过观察到的cpu内存使用情况 结合 后面讲的docker的资源限制 来限制使用上限。👆上图可见现在是没有资源限制的，通过LIMIT那一列可知。 LIMIT没限制，free里total多少，上限就是多少。 如果不限制，将来很可能一个容器就把你资源耗光了。说到限制，肯定就会想到QoS，所以容器的限制能否做成类似带宽的QoS，什么意思，就是所有容器有一个基本的紧巴巴资源额度，上不封顶，但是如果某个容器A除了自己紧巴巴的资源以外也占用了额外的资源，此时如果整体资源富裕没问题，如果有别的容器也run起来了人家连紧巴巴的日子都过不了，所以容器A要立刻让出占用的资源的。这就是QoS里的带宽保障机制。队列机制~ 比如es是java开发的，java启动优化参数👇 比如这里 对于 堆的限制： -Xms64m -Xmx12m就显示了初始值和最大值，如果不加资源就越来越大，想用多少就用多少， docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms63m -Xmx128m\" elasticsearch:7.6.2 注意上图docker stats xxxx是全屏显示的，退出后往上翻滚才会出现多行的效果。 然后再看看不显示内存使用的效果，-Xms -Xmx就是堆内存的限制。内存空间里的堆栈空间。可见虽然LIMIT还是1.708G也就容器限没有限制，但是MEM USAGE使用大小从1.114G降低到了上图的357M，明显是限制住了 # docker 服务没有对 容器限制，但是容器内部自己做了限制。我对你没有要求，你自己对自己要求高 就是这个意思。 docker inspect的一些用法。单拎一个信息出来 因为都是json还比较好处理，比如查看容器IP地址 查看IP地址，好像是 题外话-网络故障处理-could not resolve host ​ “dns是真复杂” -- 这话是以下所有内容写完后，有感而发写在这里的。这还只是client端的dns，之前还学习过bind9。唉要不是底层，谁愿意研究这些东西，耗时间，用到还少，人家奉承你一句大佬，讽刺你也是一句大佬。 啊哈哈，认命~好好干，多读书~ 1、最近总是收到故障 开发隔三岔五说 ： could not resolve host ，比如gitrunner里的pipline走不下去，报错看就是dns解析问题。 2、观察、分析 发现ping也是会卡5s，且很稳定 解决方法就是写host就ok了，但是不能总是些host啊，于是抓包 开两个窗口 同时抓包 发现如图5s后二次dns请求，这不是所有linux都这么倒霉。 # 这里其实分析有问题的👇 进一步研究，发现有个参数optonst timeout 默认就是5s，就是超时5s后重试，可是实际情况是第一次有拿不到IP的情况的，然后5s二次查询是对的，但是很多时候第一次就拿到IP了，还是会二次查询的。 ​ 不知道为啥一定是2次，所以优化就是改为1s timeout就能加快了，就是2次查询间隔缩短为1s，然后通过single-request-reopen将1s也缩减到20ms的间隔。也算解决问题了 解决方法： 错误示范1： 继续优化配置👇，下图注释也有问题，不是肉眼已经感知不到2次dns，而是就是一次，只不过一次里面存在了A和AAAA请求。 同时抓包可见 关于single-request-reopen的分析，正确的结论在文尾，这里仅作过程记录，看看就行。 不加这个参数的dns行为👇还是这个图，注意看两次（一个A和AAAA算一次）dns的源端口都是39125 加了这个参数👇，两次dns的报文的源端口是变化的。 也不行 可以了👇 这是对这个参数的解释 ★进一步优化，最终的解决方法来了👇 于此同时抓包，下图👇，也有不准确的描述，不叫\"时间大大缩减\"，而是就一次查询里发送了A和AAAA。 此时遗留一个问题，就是第二次的dns请求就是拿不到IP地址的，但也是也不影响结果的。 以上梳理存在问题，就是AAAA没有看到，也没有理清楚V6和AAAA的关系。 还有一个问题，上面没有解释清楚为什么是2次dns，为什么很多linux就发送一次dns👇 于此同时抓包 这里是最终结果，以上过程里很多是不正确的，看看就好 首先看这张图，这是ping卡住5s的抓包 为什么存在两次dns查询，因为第一次失败了；其实第一次查询里既有V4也有V6（V6的AAAA也走的v4的socket，因为没有v6的GW），而AAAA没有拿到响应，所以人家判定了超时timeout是对的，这才触发了2次查询，所以关闭IPV6也许也是一个办法，或者防火墙放开IPV6（这个机器是在内网的，请求dns server是要经过防火墙到达外网的）。 # 不是防火墙放开IPv6，而是A和AAAA走的一个socket，FW认为是一个session的重复reply可能就直接drop掉了。但是为什么第二次就不drop了？？这是个遗留问题。 ---------一次ping的完整的抓包，涉及节点的抓包--------- 于此同时自己本地抓包 于此同时dnsserver上抓包，注意dnsserver上时间要快5s，date time 验证过了，上图的13:50:15时间对应下图12:50:21👇 ★我猜测哦，可能第一组A和AAAA的一个socket的时间离得太近了只有0.000010s=10μs，而第二组的A和AAAA虽然共用也给socket但时间相隔有个497737-477340=0.20397s=20397μs 也就是微妙 所以就是dns明明回了A和AAAA，但是由于A和AAAA共用的一个socket，注意都是33867，可能就是这里出了问题，防火墙那边可能基于同样的socket也就是五元组就丢弃了。导致client没有收到，但是为什么第二次又可以了呢！也许有时候5s超时第二次都不行的情况也是粗在的。 ​ 所以处理方法：区分A和AAAA的socket，这样就避免了FW的误判丢包--这点只是猜测。 ----↑-----一次ping的，涉及节点的抓包----↑----- 然后我又抓了两次 当然上图的dns 配置就是 所以可以得出上面的猜测很可能是对的，就是第一次两个dns查询（A和AAAA)间隔时间太短了10毫秒，防火墙干掉了第AAAA的那个回包，为什么是回包而不是去包，因为dnsserver上抓包能稳定地抓到2个请求，并回了两个reply，而client第一次没有看到AAAA地replay就是fw干掉了啊，还能是什么原因呢；系统判定查询里存在失败，所以再次timeout 5s后发起查询，这次系统肯定做了offset这种东西，就是A和AAAA发起的间隔拉开了，一定代码里有这个参数！在0.2s级别所以FW没有干掉，能够跟上这个节奏，我判定还是我的SSG干掉的，不能自己发的自己判断不出来的。 ★★ 我处理的方法是再/etc/resolv.conf文件里添加一行options single-request-reopen 此时抓包就变成了，只发送一次dns查询了，注意一次里既有v4也有v6，不过v6和v4的不再共用一个socket了，也就是不再使用相同的源端口。 结论再次总结： ①因为fw只放行了内网机器到外网dns的ipv4的dns流量，或者压根FW的接口就没有V6地址； # 这种说法也是不对的，虽然是AAAA，但是走的还是v4的socket；就是AAAA是v6的解析查询，但是走的还是v4的socket，就是ipv6的dns查询此时此刻用的是ipv4地址去问的。所以A和AAAA的查询其实在外界看来就是一个socket，所以可能就是fw丢弃了一个reply，一个socket里的两个报文里的太近了，去的时候确实没有被FW干掉，dnsserver也收到了并回了，但是回来两个报文经过FW的时候被认为重复了，干掉了第二个，FW肯定是针对回包有判定机制一个socket的判定机制，去包没有肯定。 ②所以在默认dns查询的时候v4和v6由于使用的一个socket； ③系统基于v6没有响应判定为请求失败，因为fw丢弃了一个socket； ④于是系统进行第二次查询，而两次查询间隔默认是5s； ⑤这就是为什么ping卡住5s的原因； ⑥处理方法，将v4和v6的socket区分开来后，区分后FW自然不会再丢弃，系统不再认为第一次请求失败，于是不再发送第二次请求，也就是没有默认5s的timeout了。于是就解决了故障了。 DNS解析超时排查/etc/resolv.conf里的single-request-reopen参数说明 将👇 options rotate timeout:1 attempts:3 single-request-reopen 添加到/etc/resolv.conf 中 #释义： 循环查询 超时时间1s 重试次数3 只收到一个IPV4应答或者只收到一个IPV6应答，重新开一个socket查询 妈的还是别人讲的好,但是也有细节不对，上面我的一些分析也OK的。 https://www.cnblogs.com/zhangmingda/p/9725746.html 以下是复制出来的内容，复制出来反而图片出来了，好奇怪，哈哈哈 说明： 在RHLE6/CENTOS6的环境里，需要在/etc/resolv.conf添加以下参数options single-request-reopen。具体原因请看下面。 ​ 其实不仅仅是6，7，rocky 9.3 一直都是如此默认就是一个socket。 具体： 一. 在RHEL5/CentOS5/Ubuntu 10.04等linux下，dns的解析请求过程如下 1 主机从一个随机的源端口，请求 DNS的AAAA 记录， 2 主机接受dns服务器返回AAAA记录， 3 主机从一个另一个随机的源端口，请求 DNS的A 记录， 4 主机dns 服务器返回A记录， 二. 如果是RHEL6/CentOS6，交互过程有所不同，如图： 1 主机从一个随机的源端口，请求 DNS的A 记录， 2 主机从同一个源端口，请求 DNS的AAAA 记录， 3 主机接受dns服务器返回A记录， 4 主机接受 dns服务器返回AAAA记录， 三. 上面3,4并没有严格的先后顺序，实际的顺序受网络环境，服务器环境的影响 理论上讲centos6的这种工作机制，效率更高，端口复用度更高，能节省更多的资源。 但是这里也同样存在着一个问题。比如在存在防火墙等机制的网络环境中，同样源目的ip,同样源目的port，同样的第4层协议的连接会被防火墙看成是同一个会话，因此会存在返回包被丢弃现象。如下图。 此时的整个dns解析过程如下： 1 主机从一个随机的源端口，请求 DNS的A 记录， 2 主机从同一个源端口，请求 DNS的AAAA 记录， 3 主机先收到dns返回的AAAA记录， 4 防火墙认为本次交互通信已经完成，关闭连接， # 这个说法很nice，和上面我的猜测吻合 5 于是剩下的dns服务器返回的A记录响应包被防火墙丢弃 6 等待5秒超时之后，主机因为收不到A记录的响应，重新通过新的端口发起A记录查询请求，此后的机制等同于centos5） # 新的端口，我抓包可见并不是新端口7 主机收到dns的A记录响应； 8 主机从另一个新的源端口发起AAAA # 并没有，还是A和AAAA连个查询，源端口还是原来的，只不过间隔时间变长了，从10毫秒变成了0.2s量级。 9 主机收到dns的AAAA记录响应； 我们看到在这个解析的序列里面，dns解析有5秒的延迟发生。所以当用linux系统安装大量远程包的时候宏观上看延迟就非常大了（linux是不缓存dns解析记录的）。 总结： 那么到底options single-request-reopen这个参数的作用是什么的，man 5 resolv.conf的结果如下 #man 5 resolv.conf single-request-reopen (since glibc 2.9) The resolver uses the same socket for the A and AAAA requests. Some hardware mistakenly only sends back one reply. When that happens the client sytem will sit and wait for the second reply. Turning this option on changes this behavior so that if two requests from the same port are not handledcorrectly it will close the socket and open a new one before sending the second request. 一句话总结 就是ipv4和ipv6的请求和在一个五元组里，回包在经过fw的时候被干掉了第二个；所以使用参数将其分开为两个socket。 vim进去的时候需要enter才能进，wr的时候不让wr 进去的时候以为是有.swap缓存，结果不是，wr!强制也不让保存， 结果df一看，磁盘满了 磁盘空间满了，rmi都不灵了，用docker system prune -a 这个暴力的命令吧，不加-a稍微温和一点就是仅仅 -a, --all Remove all unused images not just dangling ones 好了删了550MB后占用率降到99%，此时继续修改docker的daemon.json 然后再补充一下docker里的dns注意事项 1、首先，/etc/resolv.conf是个很牛逼的文件，为什么牛逼上面说了，但是很多人没细究过 第一、其中的dns只能生效前3个 2、其次，要让resolv.conf里的3个dns能够转起来，必须配置rotate选项，否则系统就盯着第一个干，不会用第二个， 以下是截图👇 换种轻松的测试方法 而配置rotate后下；注意别用dig nlsookup来测试，测不到的，用ping，curl这种类似APP的工具来测试，dig和nslookup 只会使用第一个dns。 尝试测试3个nameserver轮询的效果 想到126.223是不是没有53端口监听，所以压根发不出去，所以换一个name server 用ping测一样 而且通的情况下，也会轮询的， 这样好像不太好吧，优先用内网dns，然后再用第二个，不行在用第三个，才是最优吧。 如果是第一个dnsserver解析不到的时候跳下一个dnsserver，这种需求是不对的， 举例：你要将第一个dnsserver配置本地的dns，你就要将dnsserver的上级指向公网，然后第二个配置公网dns； 所以不管是linux还是windows的主备dns，都是当主dns不可达后，才会使用备dns。 3、再次，docker run起来的dns也还是容器里的/etc/resolv.conf里的nameserver，同样也是3个顶多，容器里的resolv.conf哪里来的，①docker配置文件/etc/docker/damon.json 优于的方式结合了 ②宿主的resolv.conf 以下是截图👇 ★容器的dns的各种配置，就看daemon.json是否指定，没有就用宿主的，①nameserver②option选项都是如此。 下图是生产环境的，我不管他们的容器专人负责，但是我外面宿主给你容器也给优化，一样也实现了，而且他们还不知道被优化已经，这就叫阴德--帮了别人，别人还不知道。学易经最重要的就是正，正观念，很多名词 世人不清楚什么意思，比如什么是阴德，这就是阴德；比如什么仁义，什么是小人，都讲不清楚的，而易经里列举了很多例子告诉你怎么定义这些名字，这就叫树立，站得住了你就。 话说回来，docker 里的dns，其实你就用3个顶多了，然后考虑到rotate是通也是会轮询的，所以优先使用内网dns是对的情况下，还是去掉rotate，然后10.2上做好上级查找就行了，关键是single-request-reopen要写上，否则会出问题，这点在前文也交代了。 ​ 不过内网有些是海外的站点，这个可能会涉及必须请求8.8.8.8的情况，理论上你就把有故障海外的域名在10.2上通过dnsmasq指到8.8.8.8上也行比如这样 server=/docker.com/8.8.8.8 # 理论上这么写ok，但是现实是骨感的，因为TMD的docker build的时候，他瞄的开发谁知道他用什么URL。 参考资料 https://blog.arstercz.com/linux-dns-issue-note/ https://blog.arstercz.com/linux-%e7%b3%bb%e7%bb%9f%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86%e5%90%8d%e7%a7%b0%e8%a7%a3%e6%9e%90/ https://blog.csdn.net/qq_56676115/article/details/119389610 人家也是建议用rotate，看到没，网上很多人用这个的，这是折中的方案了， ​ 而且不是说单个nameser故障，我三个server 一个内网，一个8.8.8.8 一个223.5.5.5 ，你说怎么故障，要么第一个所有人都不能用，后两个也不太可能， ​ dns server本身故障不是考虑点，解析不出来A记录拿不到，拿不对才是着眼点，于是rotate还能保证一下。成功率肉眼可见的必须是8.8.8.8解析的那些海外的(少数海外域名)就是1/3，所以开发会有疑问为什么时好时坏，哈哈哈，如果想提高到100%，就得开发告诉我你TMDdocker打镜像的时候都用了TMD什么网站，我给你一条条写道第一个内网dns里，然后去掉rotate对吧。谁签头做这事呢？谁会听你的，懒得折腾。再说了，国内镜像源这个才是最优吧。不止一次说了用国内的，然后内网仓库也搭建了N多了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:25 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/3-Docker容器管理和端口映射.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/3-Docker容器管理和端口映射.html","title":"第3节 Docker容器管理和端口映射","keywords":"","body":"第3节 Docker容器管理和端口映射 docker 删除后的一些情况 docker 删除特定的容器两种方法 docker rm `docker ps -aq` # 这个其实不加-f也可以，大不了UP的报错，就是不太安全感觉 docker rm `docker ps -f status=exited -q` # 这种也ok docker container prune # 官方封装好的cli docker container prune -f # 同上，只不过是不用回答y/n 改名运行中的一样改 测一下 docker rm `docker ps -aq` 退出状态是夹在两组UP之间的， 没问题啊，还是那句话docker rm $(docker ps -aq) 就是不能删除UP的，自然就会把退出的删掉 container = image + data，container停止image层就删了，只留下data，简单来讲是这样，细究就是image不是复制而是共用过来的，meged就是image层的东西 容器停止时不会造成数据丢失的，那么dockers stop后释放的空间是什么，肯定不是image对吧，image也不是说复制一份，可能是隔离的namespace这些隔离程序随着容器起来一个就要做一些隔离代码？ # 说法不对，stop后 落差空间大小在4220KB，答案见下文 m53.tx在容器外面也是看得到的👇 diff就是新增加的文件。 下图就是容器stop后释放的空间👇merged看着像image复制过来一份，但其实空间只有4228KB远远不是IMAGE的4.26MB 日志也是落在外面的，也不会删的 其实容器里的数据都是在/var/lib/docker/overlay2目录下的，所以也不会随着容器stop就没了。 有空看看这个，可以回过头来看， https://zhuanlan.zhihu.com/p/374924046 链接里的截图👇 看分层overlay2里确实变小了啊， 但是docker ps -s看到的变大了，可能原因如下👇 容器数据是否持久？ 注意容器的数据会随着容器删除而删除的，上面是说容器的数据不会随着容器的stop而消失；说白就是overlay2目录里的东西，docker rm掉就没了，stop还在的。如果mysql 容器，肯定不能这样，就要做持久化。 上图的目录只要退出来，这个目录就没了 测的玩 所有images和容器都删掉，此时overlay2里是空的 然后pull一个busybox看看 diff里的东西4220KB字节👆 run起来后，多了两个分层目录 然后空间大小，run起来的空间占用主要在merged目录，看起来就是等于image里的diff目录大小；所以这个容器里的merged目录就是从image里的diff拿来的，所以就是复制了一份image的全量了基本上 container = image + data ，容器起来就是复制了一份image这话没毛病，可能多个容器基于一个image存在共用，这个后面再测试，至少第一个容器看来就是多了merged4220KB，然后等于image的diff4220KB，也基本等于docker images 看到的image大小4.26MB=4260KB。 diff目录是和谁的diff不同，应该是容器起来后和image之间的不同。 然后再stop掉看看空间占用，stop掉merged目录就没了。 UnionFS到底是怎么个共用来着，给个效果看看啊，我自己看的也没看出来👇 container = image + data，就是复制一份image的。 所谓UnionFS联合文件系统，这种复用的情况应该主要是image之间的联合复用，容器方面如图所见就是run一个就复制一份image了，在这个点上是不存在复用的。 容器的暂停vs停止 kill 容器发送信号 docker kill ID # 停止容器 docker kill -s 1 ID # 加载配置文件 如何验证确实加载了配置文件呢--通过top查看比如nginx的work进程，因为加载配置文件master进程不会重启，但是worker进程会重启 类似nginx -s reload用法，nginx -s reload也等价于kill -1 $(pgrep nginx |head -1) 一般第一个就是nginx的master，kill -1就是等价于reload配置文件。httpd一样，这里的docker -s 1也是一样。 docker attach exec进去再退出来，并不会导致容器退出 此时再复制一个窗口，一样可以exec -it进去，但是两个exec 进去的shell是独立的， 现在通过attach可以实现screen -x 的 两个人共享shell的效果 上图由于alpine是①没有退出-it进去的②且默认cli是/bin/sh，所以attach就成功挂上去了，然后窗口时一个tty，共享的试图。 四个窗口也是一样的，也就是复制4个窗口出来然后👇 attach用法 1、偷窥别人当前正在干什么，劝你耗子为汁 2、从安全考虑，怎么禁止别人进来，别run -it就行了，而是用start后，exec -it进去。 进入容器通过exec -it sh 一般是要跟一个cli的，exec 不一定都需要进去，直接执行一条cli就出来也行👇 bash一般可能没有，sh一般是有的 exec能够执行的cli都是容器里的有的，有什么用什么 这样就可以进入容器，进一步排查问题，没run是进不去的。所以docker run [-d] 能够up的都是容器里是前台cli挂着的，然后容器没有前台cli的都是run完就exited--也就是run不起来的，也就是没法exec -it sh进去的。 # 简而言之，exec能进去的都是容器里有前台运行进程的。 容器的端口问题 别人容器时一个nginx，80端口也是容器里的ip的80. 随便瞧瞧run的参数 容器里的IP出来都是走的SNAT，不是说172.17.0.2转成172.17.0.1的SNAT哦，而是容器里出来走的是物理网卡的IP的SNAT。 这个cli比较适合容器里查看本地IP👆 这个172.17.0.2，本地宿主机上是可以访问的，因为路由器是本地指向docker0的。 暴露端口 docker run -P --name web01 -d nginx # 将容器里所有的端口都暴露到宿主的随机端口 docker run -p 80 --name web03 -d nginx # 仅将容器的80映射到宿主的随机端口 docker run -p 80:80 --name web02 -d nginx # 仅将容器里的80映射到宿主的80 docker run -p 192.168.126.130:80:80 -d nginx # 宿主IP:宿主PORT:容器PORT docker run -p 192.168.126.130::80 -d nginx # 宿主IP:随机端口:容器端口 docker run -p 192.168.126.130:8080:80/udp --name web03 -d nginx # udp docker run -p 8080:80/tcp -p 8443:443/tcp -p 53:53/udp -d --name nginx #★多端口 小p指定，大P随机 测试就OK了 然后看日志 docker port web01查看端口暴露 固定宿主的映射端口，前提是外面的端口没有被占用，里面的端口你得知道 里面的端口inpsect可见 外面ss -tlnup 确认没被占用就行了 测试OK docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d --restart=always mariadb:11.3.2 # -e 是传递变量进容器，前提也是容器里支持的变量可以传一些值才行。 docker run -d -p 8080:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php8.2-apache 启动db 此时另一台机器就可以连了 账号也有了： 启动wordpress的前端容器： 所以此时页面走一波 关于数据库主机不能填默认的localhost，因为这是在容器里面，localhost指的是容器里面的本地，外面宿主也好，其他容器也好针对localhost看到的又是各自的本地。 可以写容器的ip，但是不好，因为该ip会变；所以要写宿主的IP和暴露出来的端口。 如果一不小心配成了容器里的ip 下图是配置了172.17.0.2后 返回重配未192.168.126.130发现不行，需要进入wordpress容器里删除对应的配置文件 这文件里的IP改改就行了，干脆直接删除这个文件，这个时候再重新去页面配置就可以了 这个细节：图中wordpress默认也是有一个灰色的，但是其实只是提示而已 这就OK了 容器如何修改监听暴露端口 直接进去改了 然后重启容器，尝试发送SIGHUP信号试图重载配置，但是未成功👇 restart -s 1也不行 进去刚才的配置文件发现端口没改过来，看来要先stop 容器再改了 再次尝试docker stop xxx，然后vim修改未90909，然后docker start xxx还是不行，需要停止docker引擎 尝试上图操作先停止docker，发现配置文件可以改过来了，但是由于容器时restart always了，还是不行，需要重启容器，下面cli可以实现不删除容器，修改暴露端口的需求 systemctl stop docker systemctl stop docker.socket # 这个不需要 docker stop xxx # 这个不配：后面需要重启容器：配置的话：后面需要启动容器，哈哈哈~ vim /var/lib/docker/containers/xxx..xxxaad092/hostconfig.json # 修改暴露端口 systemctl start docker # 启动服务后，还得手动重启容器，否则容器之前没有停可能就保持了之前的状态信息然后 docker restart 9472a68122b9 # 发现必须restart，容器跟随docker服务起来好像不行有点奇怪。不奇怪，因为之前没有停，直接被服务重启了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:25 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/4-Docker容器管理常见用法.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/4-Docker容器管理常见用法.html","title":"第4节 Docker容器管理常见用法","keywords":"","body":"第4节 Docker容器管理常见用法 docker查看日志 概述 docker logs 本质是查看容器在终端打印出来的信息，只要你的容器有STDOUT就会被docker logs在外面抓到，不过一般来讲都是容器里的日志才会在屏幕打印，所以docker logs也就起了个名字叫logs否则正宗点应该叫docker stdout，哈哈。 run一个nginx看看上面说的，通过docker top 可见nginx是daemon off也就是容器里是前台运行的，这样才能保证容器run起来不会停止，然后再docker run -d 将 docker run xxx 放入后台执行--否则容器up后会占用宿主的前台。 简单讲一句话：容器的run起来就是要：里面是前台，外面是后台 解释：里面是前台才能run起来不stop这是容器的固有要求；外面是后台，容器up后才不会占用宿主的前台。 docker logs其实是看的容器里的屏幕打印(stdout和stderr) 测试如下👇 此时就可以通过docker logs看到容器里的STDOUT 证明就是STDOUT而不仅仅是LOGS👇 我本以为正因为nginx将两个日志软连接到了stdout和stderr也就是屏幕打印，才会被外面宿主的docker logs抓到，结果当我直接产生屏幕输出的时候docker logs 并没有抓到。 而且cat也就卡住了--在容器里👇，我知道了，因为你exec -it进去不是之前run的时候的那个tty了，所以exec 进去的stdout在docker logs web01的那个tty是看不到的，echo不进去和cat不出来则是因为①看似echo >> access.log和cat access.log实则echo >> /dev/stdout和cat /dev/stdout 自然是：前置不是一个tty外面docker logs自然看不到，后缀cat /dev/stdout卡住其实就是宿主上单独cat一个道理，什么道理，哟还会抬杠啊，就是cat=cat /dev/stdout是等待STDIN将STDIN的内容输入到STDTOU上 改为run -it方式进去，然后用attach也就是一个tty去测试，理当实现 ： 容器里的屏幕的东西都能在docker logs 看到了。 此时logs就能拿到stdout了，这就上文红色字体所说的一个tty才行。 换一种证明方式👇 正因为tailf -f 是屏幕打印的内容，所以docker logs 就把这个屏幕打印的内容抓出来了， /etc/hosts----tail--->STDOUT----docer logs---->宿主看到了 而不是说/etc/hosts变成了log，哦，不要瞎理解。 ​ 前一个例子，STDOUT由于是不同的tty，所以没法同步信息--一边屏幕的STDOUT---一遍docker logs看到，因为不是一个tty。 ​ 现在一个例子tail -f /etc/hosts，也就是上图你exec -it进去往/etc/hosts里echo点东西，这样即便不是同一个tty：一边exec 进去往这个文件里写东西，一边另一个running的容器里由于前台tail -f /etc/hosts了，只要别的tty往该文件写东西，那么容器的tail -f是会看到的，于是容器本身的屏幕也会有同步的打印。 # 落文件了所以不同tty都是同步的该文件而已。 ​ 而前一个例子不同tty中间可能有文件作为中转桥梁，他们是各自的tty在各自的STDOUT或者STDERR。 docker run --name test01 busybox /bin/sh -c 'i=1;while true;do echo $i;let i++;sleep 1;done' logs一样可见， 还是那句话logs看的容器的tty正式当前你run的那个容器的tty 你若此时exec -it 进去该容器，就是另一个tty了，此时STDOUT由于是不同的tty不会在docker logs里看到，因为docker logs看的是就是那个容器里的tty终端。 然后sh -c 要用单引号👇以下是区别，其实就是你要将里面所有的都原封不动的传递到docker run 里的sh -c里去，所以自然不能解析的，所以必须是单引号，统统是字符串扔进去。 补充：为什么一直是100？因为i=100，双引号展开后为\"i=1;while true;do echo 100;let i++;sleep1;done\" 也就是说容器里实际执行的是echo 100，哈哈哈，你说为啥一直100呢。 docker ps docker看到的容器的日志，在宿主机上也有文件落地的 查看容器日志落在宿主上的文件位置： /var/lib/docker/containers/0axxxx...xxxb471ca/0axxx...xxx471ca-json.log 然后logs越来越多，会导致磁盘空间占用越来越多，常用方法 关于替换掉容器里的原本cli ​ 比如原来容器的cli是一个脚本来启动容器的，现在该脚本被你替换为tail -f xxx了，自然也就不会启动nginx服务了，所以这种情况就要小心了。 虽然外面看 但是里面你把脚本替了，里面端口都没开👇 替代cli的使用场景，一般是用来测试查看的。 容器里的DNS以及hosts文件 类比宿主的hosts本地解析情况 容器里多了一个容器IP，且解析到容器ID。当然127.0.0.1还是老样子 还有个仅仅是验证hosts文件，可以docker run --rm 来做，演示完退出容器后直接删除容器 docker run的时候就新增host解析 host文件添加进去就好了，不过外面也可以 再次理解下-it -d 和cli👇 docker容器里的dns 这点我前文就总结过了，结合工作中的故障案例进行和归纳总结 这里补充其他的点：比如ubuntu的情况、比如除了daemon.json 还有docker run --dns=来指定。 ubuntu的容器的dns 和rock-linux一样也是从docker引擎的daemon.json优先然后再结合/etc/resolv.conf里来的，但是ubuntu的宿主机的/etc/resolv.conf文件里看到的其实不是真实的宿主使用的dns，而是要通过 上图的/etc/resovle.conf是容器里的dns的使用文件，该文件自然也是从宿主机的docker引擎的daemon.json 结合 宿主的dns 来生产的。 ​ 而宿主机的dns👇下图显示了ubuntu的特殊的查看方法（ubuntu的/etc/resolv.conf里看不到真实的dns配置的） 同时还有其他的cli查看👇 dns和options的优先级，从上到下： ①docker run指定的参数优先 ②daemon.json指定的参数其次 ③宿主机的/etc/resolv.conf指定的参数最后 ④指定的参数优先的意思就是，就两个参数其实，一个dns 一个options，比如：run 和 json里都没有指定options，而宿主机的resolv.cnf里制定了，那么就是宿主的配置优先了。 注意上图的一个点：就是10.1.1.1是不会生效的，因为/etc/resolv.conf文件里的namserver只认前三个。测试过了的在前面的章节里--就是本段文字的标题下面的第一张图那边做了测试截图的。 docker run --rm --name web01 --dns=1.1.1.1 --dns=2.2.2.2 --dns=3.3.3.3 --dns-option=timeout:23 --dns-option=attempt:3 --dns-option=rotate --dns-option=\"single-request-reopen\" busybox cat /etc/resolv.conf 这样差不多了吧，单个容器生效，也不影响其他容器，也不用dameon.json，也不用宿主的/etc/resolv.conf，就挺专业，哈哈。 search要删掉 还有一个search xx.xxx一般不怎么用，错了，不是不怎么用，而是一定要删掉，防止乱给你自动添加捣乱。 没啥用，谁指望search来补齐啊；而且可能带来意料之外的故障。 search怎么删，不是去/etc/resolv.conf里删，这只是个动态生成的文件，源头在网卡配置文件、或者nmcli去配置修改。 ubuntu里 centos一样类似的网卡，或者用的是NetworkManager的直接用nmcli去修改就行了 图中ipv4.dns和IP4.DNS的区别，一个是前面配置的，一个时候后面最终生成的，你可以理解成配置文件和最终状态，唉，这些太细了，不管了，总之实际操作的时候带点脑子就行了。不理会这句话也行。 容器和宿主机文件怎么交换 docker cp -a 好比cp -a，保留属性 操作下，跑一个容器up着 然后将宿主机的一个文件，复制进容器里 docke cp -a /etc/issue test01:/tmp/ 拷进去： 拷出来 容器的环境变量 这东西懂的都知道很关键的： 1、cmd里的set，看的是环境变量，可能看的都不全对吧，毕竟仅仅是当前用户的吧；存在软件退出了，网络设置里的代理里也清空了，但是set里发现PROXY变量的存在，这种情况也可能导致网络问题，故障定位就很难，出现过一次，具体细节我忘了，结论就是set里也要观察是否配置了代理，容易忽略这个点。 2、env也就是linux看环境变量的cli。 3、环境变量里的PATH变量，crontable里的写清楚PATH变量. crontable正儿八经还要写 这些都是环境变量 每个容器自己都有环境变量，环境变量的用途和镜像密切相关，有些是与众不同的的环境变量。 再看看mysql的变量 mariadb的变量 还可以传递变量用-e 注意上图mysql如果PASSWORD变量没给进去，是起不来的， 如果用了--restart=always，就会看到容器状态是一直重启中 还可以写到一个文件里统一安置 不过这些例子的环境变量没有具体意义，mysql的环境变量一般这么设置 docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d --restart=always mariadb:11.3.2 去掉env观察 可得👆是password没给。其实是you need to specify one of the following as an environment variable，是三个变量给一个就行了。 然后evn取代了docker-entrypoint.sh的脚本，也报错，可见password变量是在cmd之前就需要的。 ​ 然后设置为随机密码或者空密码：具体-e的传递参数的格式，这是当初创建镜像的作者指定的，要去hub.docker.com去查看mysql镜像的传递参数格式，而不是去mysql官方网站去查看。 然后问题来了随机密码MYSQL_RANDOM_ROOT_PASSWORD到底是多少呢，看logs就行啦~👇 尝试登入mysql，由于没有做端口暴露，所以我用容器的ip去连👇 好，那么问题来了：如何安全的设置mysql的密码， 1、你用随机密码，logs里可见，不安全；所以需要手动进去修改，则算是一种解决方案； 2、√ 你用手动指定-e MYSQL_ROOT_PASSWORD=123456 来指定，结果history里可见，不安全；所以我们要vim mysql.env进去定义变量，chown 600 mysql.evn，然后source mysql.env，然后再-e MYSQL_ROOT_PASSWORD=$VAR就行了。 这样就安全了， 然后再试试用文件传参的方式--env-file👇 上图提示No space left on device，磁盘又够了，删吧 后面会讲怎么制作镜像，让用户使用镜像的人可以传递变量，也就是制作镜像的时候要考虑的事情了。 还有别忘了 清理容器 清理前先看看：可以i用 docker system docker system df 停掉几个up的容器后，现在RECLAIMABLE里看到可以回收的空间了 解释如下 docker system events docker system info = docker info docker system prune 四个-all分别是 所有停止的容器 所有没有使用的网络，网络后面讲 所有没有标签的镜像 所有dangling的build cache dangling就是没有标签的镜像，产生的原因如下👇，就是容器正在用着一个image，你删image的时候还得是用的TAG，则删除了TAG，也就是没有TAG，也叫dangling。注意哦，容器用在是删不掉image的哦，这一点通过ID去删就明显看到了，通过TAG去删也只是去掉了TAG而已。 容器没有UP的，Exited的，image是可以删掉的 然后容器run起来是复制了image一份，否则容器停止后将image删除，为什么容器还能继续UP呢，所以正是因为容器run这个动作就是复制一份image的。 还一个补充一下：docker的PS1 需求，我想修改docker里的提示符 诺，是不是so easy，再找找busybox的PS1变量落在那个文件里就行了 如果实在找不到文件，就用-e的方式传进去也行啊。 搞定~ 但其实，不推荐修改，因为这样你 不注意以为还在宿主机上，不知道已经进容器了，就尴尬了。 案例，使用docker自动部署一个开源运维软件 https://github.com/openspug/spug https://www.spug.cc/ docker run -d --restart=always --name=spug -p 80:80 registry.aliyuncs.com/openspug/spug # 通过镜像名指定的从阿里云下载，走的是https默认。 此时页面可以打开了 还需要初始化一个登入密码 docker exec spug init_spug admin 123456 # 初始化并创建登入账号密码 init_spug是人家容器里自带的脚本，这里也就是当作cli还执行的 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:25 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/5-Docker制作镜像方法说明和手动制作镜像.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/5-Docker制作镜像方法说明和手动制作镜像.html","title":"第5节 Docker制作镜像方法说明和手动制作镜像","keywords":"","body":"第5节 Docker制作镜像方法说明和手动制作镜像 比如开发了一套系统，如何迁移到容器里，如何制作成容器镜像，这就是本章的目标。 手动制作镜像 不推荐 概述 ​ 好比虚拟机的模板制作：手动安装虚拟机、定制(关闭selinux、iptables安全加固、优化内核等基本初始化配置、常用软件包)、关机，以此虚拟机为模板，后续克隆即可。 ​ 然后手动制作容器镜像的话，比如，找一个alpine镜像，启动后，进入容器，进行定制化，安装软件、账号创建、仓库优化等。基于这个定制的容器来生成镜像。 ​ 存在的问题，就是每次镜像需求发送细微变化，你都要进到容器里再次修改后生成。纯手动，不推荐使用，说它纯手动是因为基于交互式方式进去操作这种，脚本化不方便，其实真要说起来，什么操作不能自动化，手动制作镜像的方式也可以改成脚本，只不过有docker build的方式才不会这么弄罢了。 pull镜像 下载ubuntu镜像，啥，tag版本号怎么得知的，hub.docker.com里找lastest然后对比MD5找到版本号。或者skopeo list-tags docker://ubuntu看看也行。 run进去定制 其实镜像的CMD就是/bin/bash，所以run后面的bash可敲可不敲： 至于当前的shell是啥要大写去查echo $SHELL 好了，下面使用我们熟悉的bash来定制， 时间校准下，ss命令、curl、wget、vim 、vi也没有 ubuntu的国内镜像可以修改为👇 或者用清华的https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ 我这边就不改了，就用默认的 apt update下 算了还是用国内的吧 再次apt update 还是有问题 看看这个 https://mirrors.ustc.edu.cn/help/ubuntu.html sed -i 's@//.*archive.ubuntu.com@//mirrors.ustc.edu.cn@g' /etc/apt/sources.list.d/ubuntu.sources 还是老样子 其实是时间不对，容器里的时间是从宿主来的，宿主安装chrony后，再创建容器就行了 # 这话其实不一定对，因为后面还是安装不了软件，我最终的方法是使用默认的不挂在时区文件，反而又成功了看下面段落的再来一遍的独立测试过程 不过时区如果也要改的话就这样 容器时区和时间怎么同步宿主 时区就行，时间自动的 这个其实还是用的宿主的时区，和时间，所以GMT+8要保证宿主机上的设置好先。 定制化安装一些常用工具命令行 apt -y install wget curl net-tools procps psmisc iputils-ping iproute2 vim tzdata tcpdump telnet traceroute tree iotop unzip zip nfs-common lrzsz && apt clean 我让GPT分析了原因， 通过阅读GPT的回答，可知可能正式我用 docker run -it --name test01 -v /etc/localtime:/etc/localtime:ro ubuntu:24.04 只读挂载容器里的/etc/localtime文件，导致tzdata无法修改这个文件。 反正是tzdata设置不了导致的👇以下是再来一遍的 apt安装软件失败和成功的对比-寻找故障点 1、寻找故障点 挂载时区文件 然后发现安装软件到这些都OK， apt -y install wget curl net-tools procps psmisc iputils-ping iproute2 vim tzdata tcpdump telnet traceroute tree iotop unzip zip nfs-common lrzsz && apt clean 以上粗体的软件，安装都依赖tzdata，所以都会涉及localtime文件的覆盖，而localtime当初创建容器的时候就是被宿主机给占用了的，即使没有ro的方式也是被占用的，也无法被覆盖。 2、应对措施 1、时间不对可能导致apt update出错；但没必要修改时区。 2、如果进一步修改时区，就可能导致apt install 的包依赖到tzdata就无法mv覆盖 这真是个大聪明的机制。 3.1、解决方法也有：使用docker cp 将时区文件复制进去，而不是-v挂载进去，这样tzdata安装的时候就可以mv 覆盖这个时区文件了。 3.2 、其实解决方法也可以：不要修改时区，UTC也好GMT也罢，都是准确的时间，无非是可读性不好吧。 以上故障总结 1、apt update报错及解决 答案就是时间不对 尝试复现上面的报错 ①先明确容器的时间哪来的：系统时间的UTC时间，是不看系统时区的，只看UTC对应的时间。 ②再复现报错，date -s 改掉系统时间，然后utc时间自然跟着变，就能看到报错 进一步分析，时间判断机制，人家apt update的时候又文件里写着时间的，对比你的时间不对就报错。 https://linux.cn/article-12666-1.html 根据这个大兄弟提供的思路，我找到了docker里的判断基准文件 通过修改宿主机的时间，无需修改时区，然后直接run容器，就可以apt update成功 chronyc makestep # 手动同步网络时间校准下 2、挂载时区文件导致依赖tzdata包无法安装及解决 首先不挂在时区文件，就不会存在这个问题👇 不映射时区文件tzdata安装ok👇 映射时区文件，导致安装tzdata依赖包出错 处理方法 使用docker cp 将时区文件复制进去，而不是-v挂载进去，这样tzdata安装的时候就可以mv 覆盖这个时区文件了。 👇下图是cp -L将软连接的源文件内容复制进去了，仅cp -a这种是没用的，因为软连接进不去，因为容器里没有软连接指向的源文件。 所以要用-L复制： 此时再次安装 apt -y install wget curl net-tools procps psmisc iputils-ping iproute2 vim tzdata tcpdump telnet traceroute tree iotop unzip zip nfs-common lrzsz && apt clean 就很丝滑~也会涉及tzdata的安装，它会修改一些东西，比如 这些都是tzdata设置的，也不知道他基于什么给我设置成上海时区，也许是从宿主拿的，测试一下 并不是人家tzdata默认时区就是上海，哈哈 将外面宿主的时区改成掉 然后再将容器里的时区改掉👆，最后安装tzdata发现时区还是上海，而且最终信息人家也告诉你了就是默认的上海时区👇 所以关于容器的时间，其实就简单了 1、宿主时间要对就行。 2、容器进去apt updata后，直接安装tzdata时区时间就都OK了，此为无脑操作，也挺香。 容器里一般是没有selinux，防火墙也是关闭的 因为容器里没有getenforce、sestatus这两个确认selinux没开的命令 所以就用文件ls -Z 的方式来看，即使已经关闭了selinux，但是这个文件当初是受到selinux关照的，也会有selinux的信息的👇，同时前面就会有一个点 . 而容器里是没有的 然后将定制好的ubuntu制作成镜像 通过docker commit将容器生成镜像，不要和docker save搞混了，docker save是image的导出以及docker load对镜像的导入。而docker commit是容器生成镜像。 这样镜像就从容器制作出来了👆。 然后就可以基于自己制作的镜像来创建容器了 一般镜像制作 按图的思路进行打各种服务的镜像，以后的应用都不是应用了，全是镜像了。而app镜像全是基于系统环境镜像的，也就是比如APP2镜像之前还得有Tomcat8镜像和JDK8u镜像和系统基础镜像以及原始镜像。 所以镜像自然就会非常多了， 由于都是基于一个base镜像打出来的，所以底层环境，工具、账号开局的都是一样的，账号ID也是一样的，这就解决了NFS如果ID不一致的问题(回头补一个NFS的章节梳理下这个问题) 如果非要按下图👇红色箭头来一步到位的方式来打镜像 也不是不行，只是APP2和APP3都是直接从原始镜像制作出来的，这样环境可能是不统一的，就算一个人做出来的，环境也是基本不一致的，维护不好。 镜像制作出来，不要本地存放，要上传到仓库服务器harbor统一安置，其他机器都从harbor拉去。 用commit这种方式制作出来的镜像，也就是从容器进去手动安装配置的结果，这种方式过几个月你都压根记不起来当初你在容器里做了哪些配置，然后commit出来的镜像里带了哪些功能，维护很不方便。这种方式也无法自动化，因为没有一个文档性的东西把历史操作存起来，都是临时性的操作，不好规范化，脚本化，不好运维。 好处就是commit命令会了，基本就会制作镜像了，哈哈。 生产是不用这种方式制作镜像的。 自动(批量执行)制作镜像 需要一个脚本文件( 好比ansible剧本 )，里面定义了如何来创建定制镜像的详细过程。 该文件自然有自己的格式，语法。也就是dockerfile里的常用10来个cli的学习。以及docker build命令依据dockerfile文件来创建镜像。 docker build -t xxx:v1.0 . # -t 后面跟的是镜像名词和版本tag， .就是相对路径，默认就找Dockerfile这个文件。 docker build -t xxx:v1.0 -f /xx/xx/xx/file001 # -f 手动指定文件，就不使用默认的Dockerfile文件了。 而这个默认dockerfile其实是有固定大小写名称的👇Dockeffile 所以每个镜像都用docker build + Dockerfile来创建的， 这么多镜像，是不是都叫Dockerfile不就冲突了嘛，所以需要用文件夹区分开来。 关于Dockerfile们的存放路径，可以按上图，跟着镜像存放，也可以单独放到独立的文件夹下分门别类 mkdir -p /data/dockerfile/{base/{ubuntu,centos,alpine,busybox},web/{jdk,nginx,tomcat}} 将来这些分类下面就都会有一个Dockerfile 比如你要做nginx镜像，就得先去alpine或者centos制作操作系统镜像。 wifi故障案例 故障现象：开大会的时候，FTP传文件速度只有100KB/s； 故障复盘： 1、笔记本在夹层楼道第二个台阶处连接到1F夹层会议室里的AP后，此时信号处于中接近弱的状态，此时FTP传输文件就是50KB/S， 2、然后将笔记本移动到夹层会议室里，此时WIFI信号开始变强，但是FTP的速度并不会上涨 3、然后删除ftp的任务，重新传输，此时速度才会肉眼可见很快的达到2MB/s的样子。 用户行为导致的故障可能分析1： 1、用户的笔记拿到休息区的柜子上的时候，wifi信号还吊死在别的AP上，导致信号弱。 2、用户打开FTP后进行传输的时候，wifi信号是弱的，此时ftp速度很慢。 3、等到wifi连接稳定(切换到最近的AP)信号变强，但是FTP速度只会随着信号变弱而变慢，并不会随着信号变强而变强。 4、此时需要删除FTP任务，重新进行文件传输，让FTP重新协商。 用户行为导致的故障可能分析2（该故障方式已复盘）： 1、用户在二楼的时候就已经开始用wifi传输文件了，然后笔记本移动到1楼休息区的时候经过楼道wifi变弱了导致FTP速度变慢； 2、等到用户wifi稳定后(切换到最近的AP)信号变强，但是FTP速度只会随着信号变弱而变慢，并不会随着信号变强而变强。 3、此时需要删除FTP任务，重新进行文件传输，让FTP重新协商。 应对方法： 用户到哪里，就断开WIFI重连一下(该操作确保终端就近连接AP)，然后再进行FTP或者其他文件的传输 （ ”然后“ 的道理就是：确保wifi稳定后再传输数据，防止信号不稳定的时候，那时软件传输数据的时候是认为网络通道是狭窄的，等信号强了有的软件可不一定能够快速探测到通道OK了从而给你加速传输）。 国内paypal电信拨号线路突然打不开 1、故障现象 用户所在线路未电信拨号线路，之前可以登入国内的paypal，今天打开很慢一直转圈圈好久才出页面，登入就登不进去 2、分析 首先了解：虽然和本次故障不相干 https://zhuanlan.zhihu.com/p/663322201 其实F12看到www.paypalobjects.com很多URL都是走的的这个域名，但是该域名通过电信线路就打开很慢，加载慢，但肯定不是DNS问题，发现该域名IP是美国IP：内部解析和外部解析一致，也就是说内部并没有做任何dns的设置。 结合联通海外线路一般是比电信要好的这么一个普遍共识，所以切到联通测试发现打开速度杠杠的。 3、处理方法 切换到联通线路就行 至于电信拨号打开慢，登入不上，又不是公司网络问题，理由很简单：ip一直有监控60多天地址没变过，用户之前ok，现在不行，经测试发现确实不行，说明就是ISP自己的问题。此类故障统一切换线路就行了。怎么切？切用户啊，因为paypal涉及很多地址，不可能全部一条条打到联通线路去，只能且用户，并告知用户他的出口线路已切的情况。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:25 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/6-Docker自动制作镜像常见指令说明.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/6-Docker自动制作镜像常见指令说明.html","title":"第6节 Docker自动制作镜像常见指令说明","keywords":"","body":"第6节 Docker自动制作镜像常见指令说明 Dockerfile的编写 官方文档： https://docs.docker.com/reference/dockerfile/ 一行一个指令 # 表示是注释 首先指定基础镜像:FROM cd /data/dockerfile/base/apline touch Dockerfile 1、基于alpine来 明确下较新的版本 FROM alpine:3.19.1 # 从该镜像继承 alpine:3.19.1 又是从拿来的呢？这个就要去官网查看了 可见alpine的父镜像是scratch来着。而且ubuntu、centos、busybox都是from的她。然后jdk，tomcat这些APP又是基于OS的。这个scratch就是所有镜像的祖先，这就是一个空镜像 。 https://hub.docker.com/_/scratch 再看回alpine的Dockerfile 参考人家的alpine制作，其实就是from一个scratch原始空镜像，再ADD一个迷你rootfs就行了这个rootfs其实就是OS的用户空间，而OS的内核空间就是用宿主机的。 ​ 你要自己手动创建一个类似alpine的镜像，就可以定制以下这个rootfs文件 也就是说确实还有进一步缩减的空间。 去看看busybox的Dockerfile，里面rootfs更小。 其实，没必要折腾，直接官方的基础镜像拿来用就行了 既然没必要折腾那么就折腾下吧 # 非常规操作 目标：重新自己做一个最小OS，参考busybox咯 然后下载下来 然后看看是否要修改 还是要加压，因为本来我想直接用tar -r -f 追加工具进去，但是-r不支持压缩格式，单单一个tar包不带gz是可以的 进一步缩减cli文件，再加点比如curl命令 删除就怕这些bin文件之间有依赖关系就不好弄了，先不管，删除没见过的所有bin文件 搞不懂了👇，每删一个就会出现新的文件来占用空间 删掉差不多了，再弄个curl进去试试 打包 定制的busybox.tar.gz反而变大了，呵呵 预览以下包里内容当作检查 注释要单独一行，不能跟再CLI后面 这样就好了 这就是LABEL打进去的标签 然后run就报错啦 因为我把sh删掉了，哈哈哈 重做，只要sh等3个bin文件 打包后，build run一下 可惜curl还是没有把依赖库弄好，搞笑的是ls也被我删掉了，搞得没法ls，然后find也没有，curl到底在哪也不知道。 好了，总之思路就是curl和依赖都要打包，然后过程如上。这就是定制os的思路。一般os还是不自己折腾的。 再来一遍 上面错了，cp -a是复制的软连接， 再来一遍吧 很好报别的依赖库了，所以你知道了OS定制还是挺麻烦的，不像基于别人的os，直接安装工具就好了，比如commit方式最简单， 不过上面的情况，我也可以用docker cp 的方式来处理，不过要处理的就多了去了👇 尝试把这些ldd依赖弄进去 但是继续测试发现，这些库弄进来也把sh的依赖搞出问题了 那么就这个文件不弄进去 对比了原来sh依赖的这个库和curl依赖的这个库发现不一样 应该是curl的依赖把这个libc.so.6覆盖了，导致sh启动不了 用官方的tar包里的源文件，将这个库还原回去，可行~下文方法1就是上面折腾的总结 方法1：从tar包入手的镜像定制，ldd是关键 这种方法就是从scratch开了，呵呵，比较叼，一般不这么用，属于非正规方法。 mkdir -p /data/dockerfile/os/busybox-my-base cd /data/dockerfile/os/busybox-my-base vim Dockerfile FROM scratch ADD busybox.tar.gz / # 这句其实就是解包，解开并解压缩到容器里的/根路径下 LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ maintainer=\"oneyearice \" \\ version=1.0 \\ other=\"value3\" CMD [\"sh\"] 以下是完整的一个os自定义过程 需求，将busybox里的cli只保留sh find ls 并打入curl 找到官方Dockerfile 就在Dockerfile文件的同级目录下找到这个tar包 结果是个软连接，找到正主， 复制Raw进行下载 down下来 解压 并将原压缩包移入上层目录，待会会用里面的lib库做修正 进入bin删除sh ls find以外的所有文件 复制curl以及其依赖的库 此时lib里的东西有一个sh依赖的libc.so.6也被curl的依赖覆盖了，curl的依赖是从宿主复制过去的。所以要还原，否则容器里的sh用不了，连容器都启动不了 覆盖回去 打包 编写Dockerfile build后再run就搞定了 贪狼，不计后果，勇往直前，好吧，就是好累。 以上就是完成了一个busybox的OS定制，打入curl和去掉N多cli的操作。不过一般不会这么做的，这里只是我的一个操作排错的记录而已，存思路验证过程。也算完成了 这里记录下images大小，回头用正规方式将curl封装进busybox，再看看那时的镜像大小 # 宿主机上编译安装curl wget https://curl.se/download/curl-8.7.1.tar.xz && \\ tar -xJf curl-8.7.1.tar.xz && \\ cd curl-8.7.1 && \\ ./configure --prefix=/opt/curl-static --enable-static --disable-shared --with-ssl && \\ make && \\ make install ./configure --prefix=/usr/local/curl --disable-shared --enable-static --without-libidn --without-ssl --without-librtmp --without-gnutls --without-nss --without-libssh2 --without-zlib --without-winidn --disable-rtsp --disable-ldap --disable-ldaps --disable-ipv6 # 以上还不是完全静态编译，复制到容器里还是缺库 我觉得还是应该找一个bin二进制的独立文件，或ldd出来放好，build的时候COPY进去。 方法2：别人的镜像增加一个软件，ldd也是关键 上面的编译老是依赖共享库，算了，我直接ldd弄一个出来得了 cd /data/dockerfile/os/busybox-curl/curl-static # 复制出curl的所有依赖库 ldd `which curl` |awk -F '=>' '{print $2}' |awk -F '(' '{print $1}' |grep -E '/' |xargs -i bash -c 'cp -L {} ./' mkdir lib mv * lib # 复制curl的bin文件 mkdir bin cp -L `which curl` lib 得到👇 然后编写Dockerfile # 使用 BusyBox 基础镜像 FROM busybox # 复制编译好的 curl 目录到镜像中 COPY /curl-static /curl-static # 将curl的bin文件和库合并(不覆盖的方式)进入镜像的bin和lib里 RUN cp -n /curl-static/bin/curl /bin/ RUn cp -n /curl-static/lib/* /lib/ # 设置默认命令 CMD [\"sh\"] 搞定 方法3：下载一个独立的bin文件COPY进去 这种更简单，就是要找到人家一个bin文件就是独立的curl这种，all-in-one吧 https://curl.se/download.html 找到linux系统下的binary 搞定 LABEL指定镜像元数据 镜像的说明：标签、版本、作者等 #一行格式 LABEL multi.label1=\"valuel\" multi.label2=\"value2\" maintainer=\"oneyearice other=\"value3\" #多行格式 LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ maintainer=\"oneyearice \" \\ other=\"value3\" 作者的指令早期是 MAINTAINER # 现在改掉了 LABEL maintainer=\"oneyearice \" RUN命令 alpine里安装常用工具 这些默认没有，现在需要安装一点，这个比上面busybox里安装curl要简单多咯。 先替换为国内镜像源 点击问好得到镜像源替换信息 复制 sed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories 好在一般linux里没有vim但是有sed命令的，alpine容器镜像自然是sed命令的。 上面是清华源的使用，我下面用中科大的 https://mirrors.ustc.edu.cn/help/alpine.html sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories 其实可以先run一个alpine进去手动安装看看，这样比较清楚具体过程是否ok 比如tcping其实说到底就是tcptraceroute写出来的脚本，你自己也可以写啊👇写好了： FROM alpine:3.19.1 LABEL maintainer=\"oneyearice \" RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories # 跟新源后再安装 # 一个RUN产生一层镜像，6个就6层镜像，层越多，效率越低，减少层的方式 RUN apk update RUN apk add curl RUN apk add vim RUN apk add ss RUN apk add telnet RUN apk add tcptraceroute # 将上面的6个RUN合成一个RUN，就不会打6层镜像了 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk add curl vim tcptraceroute iproute2 apk update 类似 ubuntu的apt update 但是完全不是yum update哦，别搞错啦，哈哈。 apk update是yum makecache更新镜像源 apk upgrade是yum update，更新软件 apk确实好比apt，而apt的老版本就是apt-get search一下telnet包在哪？ 也可以用busybox里的telnet busybox瑞士军刀啊，这个叼，很多命令都不用找，直接busybox，不过alpine就是利用的busybox 当然不是FROM busybox而是定制的tar包 telnet 如果不用busybox就用search到的，不过要去掉后面的版本好之类的 FROM alpine:3.19.1 LABEL maintainer=\"oneyearice \" RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories # 跟新源后再安装 # 一个RUN产生一层镜像，6个就6层镜像，层越多，效率越低，减少层的方式 RUN apk update RUN apk add curl RUN apk add vim RUN apk add ss RUN apk add busybox RUN apk add tcptraceroute # 将上面的6个RUN合成一个RUN，就不会打6层镜像了 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk --no-cache add curl vim busybox tcptraceroute iproute2 # 但是如果一层，将来镜像复用还能复用得到吗？如果将RUN的替换镜像源、各个工具的安装都单行来写，将来是不是其他镜像也能更好的复用呢，否则上面虽然整合成一条RUN，但是一个分层，别人如何复用呢？ # 常规使用上来考虑还是写成一个RUN比较好，不同基础镜像的工具也不太可能复用，而且复用里面到底怎么复用的也不是很清楚比如UinonFS(overlay2)。 所以你写在Dockerile里的RUN apk add xx xxx xxx xx 如果前面没有这个包，就直接报错了。 没办法只能先run一个进去安装成功了，再写道Dockerfile里了。 apk没有类似ubuntu里的apt clean这个命令，所以要用apk --no-cache来安装软件，这样安装后安装包就会自动删除。简称不缓存安装信息。减少镜像的空间。 可以参考别人的安装工具集 一个自用的alpine:Dockerfile 经过上面的不断校对，下面就是一个相对完善的Dockerfile啦 FROM alpine:3.19.1 LABEL maintainer=\"oneyearice \" # 跟新源后再安装 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk --no-cache add tzdata && ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && echo \"Asia/Shanghai\" > /etc/timezone && apk update && apk --no-cache add iotop gcc libgcc libc-dev libcurl libc-utils pcre-dev zlib-dev libnfs make pcre pcre2 zip unzip net-tools pstree wget libevent libevent-dev iproute2 vim curl tcptraceroute busybox-extras tcpdump 赶快去试试吧 build 灰常顺利 run起来看看 一些依赖python的可能由于py没装好就报错，其他还是ok的👆这个节后再折腾，无所谓了~ 抓包也是ok的 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:26 "},"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/7-Docker自动制作镜像常见指令说明.html":{"url":"1-容器技术和堡垒机JumpServer实战/2-Docker容器管理和镜像制作/7-Docker自动制作镜像常见指令说明.html","title":"第7节 Docker自动制作镜像常见指令说明","keywords":"","body":"第7节 Docker自动制作镜像常见指令说明 ENV 设置环境变量 注意是shell的环境变量，而不是普通变量 之前 run一个mysql容器设置变量的方法，就是基于EVN实现的👇 当时那个mysql容器里的变量就是通过-e来传递的。 #变量赋值格式1 ENV # 此格式只能对一个key赋值，之后的所有内容钧会被视作其 #变量赋值格式2，此格式可以支持多个key赋值，定义多个变量建议使用该格式，减少镜像层 ENV = =\\ = ... #如果中包含空格，可以反斜线\\进行转义，也可以通过对加引号进行标识；另外，反斜线也可以用于续行。 续行，可还行~ 老师词汇不错~ #只使用一次变量 RUN = #引用变量 RUN $key #变量支持高级赋值格式 ${key:-word} ${key:+word} 去看看官方mysql的ENV配置 发现就2行，也没看到密码的变量，发现有个脚本 找啊找啊，找到了👇 就是说人家还是通过脚本整体去搞定变量这件事了。 下面还是用ENV的方式来定义吧 run起来看下变量是否进去了 ok👆 注意观察CACHED 再次build可见 现在的RUN sed 那一行也是之前做过的，所以这里就是CACHED，利用的缓存了，这就是分层了。 这是build的时候定义的环境变量，如何在后面再去修改呢，-e咯 多个-e就是改多个 ENV的跨阶段有效性 再看个阶段状态，从Dockerfile build成image ENV是有效的，然后ENV还在image RUN 成容器的时候也有效，也就是说ENV环境变量设置跨了两个阶段，其实肯定的，第一个阶段是定义，第二个阶段是应用嘛，但是作为类比，其他的指令就不是了比如RUN 比如CMD ARG等👇 举例RUN指令只在build的时候才有效 RUN只在build阶段有效，RUN起来是不会生效的，证明如下 这小段👆内容呢，其实作为我来讲，压根不会有这个困惑，①build build， build完了Dockerfile里的东西就算用过了，不会再持续生效，这才是常理 ②build的东西持续到run生效，其实ENV只是设置了环境变量，从而再run起来变量当然生效，你不能说run起来ENV这个动作还在，对吧，压根没有的困惑，被解释成了一片所谓的知识，这就是这段文字的我的个人理解。 但是这个图确实有意义，它明确指出了哪些命令在哪个阶段起作用的，除了ENV我认为不应该也出现在run阶段，因为build完了的变量是持续的，而不是ENV这个动作是二次生效的。以下有证明改图的右边的EVN是不存在的，常识性的东西。 变量高级用法 Dockerfile里一样这么用 讨厌的咧~ 变量的作用范围 然后注意两个RUN指令之间没有继承关系，我说的是变量不会继承 👆图中红箭头并不会集成变量，所以结果test里还是ming # 因为第二个RUN应该是独立的，name依旧为空，所以author=ming👇 改成一条RUN就行啦 由于是一个RUN指令里的name，所以能够继承，所以author高级赋值就依据name里的值为123了👇 两个RUN之间不能继承，但是RUN可以继承上面的ENV啊，ENV是环境变量了，肯定OK的，RUN好比函数里的变量属于local本地局部变量。 上图👆奇怪没有看到ENV赋值的语句在build中呈现。 总之ENV和RUN的研究就差不多了，当然-e去覆盖只是覆盖的一摸一样的那个变量，不能说你build的时候z=x+y，然后-e 赋值x，z就变了，这是不可能的，又不是python的列表字典赋值--指针的意思。所以要注意别被人带偏了。上图的-e都是无所谓的参数只是放在这里看看。 然后变量高级赋值的+-补充理解 这里有一个高级赋值的案例，是用在监听socket上的写法 COPY复制指令 把脚本复制到镜像里 copy的时候可以顺带改属性 给执行权限 看下效果 这就在容器里的根目录下生成了test.log文件里面的内容如上👆 COPY的是源文件，必须是 ①相对路径 + 且 ②是Dockerfile的同级或下级目录 ADD复制和解包指令 比COPY多一个直接解包的功能 1、先准备一个打包文件 所以mv xxx ~-/ 就是将当前文件xxx移动到上一个工作目录中 2、然后用ADD指令复制并解压到镜像里 看看alpine官方的Dockerfile也是用的ADD CMD容器启动命令 CMD就是指定一个命令作为容器启动的默认命令。 一般服务性的容器就是CMD是挂前台，这样容器运行后就不会退出 这个前面梳理过了👇 启动服务的时候，将服务最终的前台服务进程写在CMD后面 CMD和RUN的不同 CMD和RUN都是执行命令，但是阶段不同，RUN是build的时候用过就用过了，CMD是容器run起来的时候才会生效的。 也就是说CMDbuild的时候是不生效的，而docker run的时候才起作用，这个确实要注意的。 制作基于alpine的自定义nginx镜像 # 准备相关文件 # 先定义文件目录结构，分门别类base、web分开来 mkdir -p /data/dockerfile/{base/{ubuntu,centos,alpine,busybox},web/{jdk,nginx,tomcat}} mkdir /data/dockerfile/web/nginx/1.26.0-alpine/ cd /data/dockerfile/web/nginx/1.26.0-alpine/ wget https://nginx.org/download/nginx-1.26.0.tar.gz echo Test Page based nginx-alpine > index.html cp ../1.26.0-centos7/nginx.conf . cat nginx.conf user nginx worker_processes 1; daemon off; ... location / { root /data/nginx/html; .... # 编写Dockerfile vim Dockerfile cat Dockerfile FROM alpine-my-base:3.19.1 LABEL maintainer=\"oneyearice \" ENV NGINX_VERSION=1.26.0 ADD nginx-$NGINX_VERSION.tar.gz /usr/local/src/ RUN cd /usr/local/src/nginx-$NGINX_VERSION && ./configure --prefix=/apps/nginx && make && make install && ln -s /apps/nginx/sbin/nginx /usr/bin && addgroup -g 2024 -S nginx && adduser -s /sbin/nologin -S -D -u 2024 -G nginx nginx COPY nginx.conf /apps/nginx/conf/nginx.conf ADD index.html /data/nginx/html/index.html RUN chown -R nginx.nginx /data/nginx/ /apps/nginx/ EXPOSE 80 443 # 这里由于修改了配置文件，配置文件里有daemon off前台运行了，所以直接nginx一敲就行了 CMD [\"nginx\"] # 如果利用默认配置文件就需要手动前台运行 #CMD [\"nginx\",\"-g\",\"daemon off;\"] # 构建镜像 vim build.sh cat build.sh #!/bin/bash #----------------------------------------------------- docker build -t nginx-alpine:1.16.1 . 下面是实验过程，比上面的配置文件少了一个自定义配置文件 还少了 端口暴露，要注意下 build一下 很顺利👆 run一下看看 ok跑起来了👆 测试OK 这样就制作出了一个镜像，同时也是按照 OS基础镜像-----安装常用命令得基础镜像------应用镜像得思路来弄得。 可惜的是上面的镜像没有把log做出来，人家官方的log直接就是能👇看得到下图是官方的log 看看人家官方的日志，关于日志前面有讲过的 分析下为什么我的alpine定制的nginx没有logs 所以其实是少了这个动作应该， 试试，在web01的容器里直接修改看看 好像不行没日志出来👇 去看看人家的好像这么tail也看不到 于是再看看自己的docker logs 有了！！，哈哈，原来上面的做法是对的， 只不过tail 看不到stdout罢了，所以将上面的操作改写成Dockerfile就行了 自定义的nginx-alpine做好日志 1、自己折腾方法1--就不丑 重新build run 就ok啦 而且官方就是这么做的👇 2、人家的做法2-也挺好 你tail access.log 那error.log呢，这不是错误日志没有STDERR了，没有屏幕输出，怎么docker logs查看呢。 所以视频里老师就tail /apps/nginx/logs/* 牛逼哈哈哈也行~ buid后run，再logs看看👇 这里再次复习一问题 docker run运行一个容器，端口映射为8080，一段时间之后忘记当初运行容器的命令，请问如何修改映射的端口为80。 1、首先题目要看到，你说停掉重新run，run的选项你知道嘛？你说就那几个，对，万一人家用了非常规的选项呢。 2、所以处理方法有三， ①就是前文讲过的去修改分层文件里的配 ②或者还可以，在你已经安装了runlike第三方工具的前提下，可以直接看出当初run的所有默认和非默认的选项， ③或者还可以，inpsec自己去一个个看也可以找到当初run的配置选项。 汇总 👆上图的kill好像也不怎么用，然后--restart always肯定要用 不过要注意一个细节就是👇 还有一些问题得能够讲清楚，还得继续学习 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:26 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理.html","title":"第3篇 Docker镜像制作及存储和网络管理","keywords":"","body":"第3篇 Docker镜像制作及存储和网络管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:26 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/1-Dockerfile常见指令用法.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/1-Dockerfile常见指令用法.html","title":"第1节 Dockerfile常见指令用法","keywords":"","body":"第1节 Dockerfile常见指令用法 接上节，浅复一下 FROM：所有镜像都有父镜像，scratch镜像 LABEL：版本信息，功能说明，作者 ENV：环境变量，build后run起来可用，后续run得时候-e可覆盖 RUN：build时候执行得shell命令，比如编译安装，ln -s，配置文件修改，脚本操作 COPY：宿主的文件复制到镜像里，文件得放在build动作的当前工作目录，且使用相对路径（一般就是Dockerfile所在目录，除非Dockerfile 是 -f 放到其他地方了，此时COPY的源文件还得必须是build这个动作的当前目录）。 ADD：比COPY多了一个自动解压缩，然后复制的时候，目标目录可能不像COPY那样需要加上/，来严谨对待，但是我们还是要规范的。 CMD：容器run来后执行的命令，一般用来挂前台保证容器run起来后不会停止的。CMD的要求就是执行程序得是前台的不能是后台的，比如nginx命令通常就是后台的，需要明确指定为后台nginx -g daemon off这种。 同时 alpine 、 ubuntu 、centos 这些系统都是/bin/sh的都是会run起来就Exited了，所以创建web服务、java服务、微服务，这些服务的时候就需要CMD是一个前台的程序。 CMD的缺陷： 配置文件的处理 现在要在Dockerfile里定义好配置文件， 1、先把容器跑起来，然后从里面复制一份conf出来 2、优化下配置文件 涉及的参数如下： worker_processes auto; ... events { worker_connections 10240; } ... ... include /apps/nginx/conf/conf.d/*.conf; vim build.sh #!/bin/sh doker build -t nginx-alpine-self:$1 . chmod +x build.sh ./build/sh 1.26.0-v1.0 run起来然后查看下配置文件是否修改👇 已修改，然后看下conf.d文件夹是否自动创建 并没有创建👇 所以Dockerfile里要创建该目录了 这也是分层多的和分层少的 自己要注意的地方，比如这里由于RUN太长了，前面还有编译的动作，所以要考虑拆成两个RUN就会是的前面的RUN层级可以被复用。 再run起来，看看conf.d是否ok 这样就有了独立的配置文件存放目录conf.d 3、配置server块 3.1 创建子目录编写server块 这样配置文件就有了👆 3.2 还得有一个网页html文件 3.3 编写Dockerfile 肯定是基于上面制作得nginx-alpine-selfv1.3来进一步封装啦，要利用之前得nginx的配置文件，特别是conf.d这个目录得存在。 FROM nginx-alpine-self:v1.3 LABEL maintainer=\"oneyearice \" RUN mkdir -p /data/website # 存放也没html文件的 COPY index.html /data/website/ # html复制进去 COPY www.ming.org.conf /apps/nginx/conf/conf.d/ # 子配置文件，也就是server块文件扔进去 # 由于继承的镜像里有CMD挂前台了，所以这里无需再配置CMD了 这样这些原材料就准备好了 build一下 搞定👆 然后run起来，run的时候要用-p而不是大P，理由如下： 1、容器里的nginx默认就是监听80，但我们现在讨论的是 端口暴露这个动作； 2、-p 小p就是明确将容器里的80暴露成宿主的某个端口； 3、-P 大P就是将Dockerfile里定义的 指定暴露的容器里的端口，暴露成宿主机的随机端口。 然后就好了👇 复习下端口暴露的情况查看除了docker ps 还有👇 哎，不对，html页面内容不对，里面有两个网站的，一个是nginx.conf主配文件里的，一个是conf.d下的子配置文件，明显这个helo helo ----xxx被nginx.conf里的location配置给截胡了。 上图主配置文件的server块里的root再location /下，然后使用的是相对路径，这个相对路径就是当初编译的时候的路径，怎么查看呢👇 所以root html就是在/apps/niginx/html下的index.html文件了 汇总一下 所以被截胡了 所以换个curl方式就可以优先应用子配置文件了👇 然后进一步把IP地址的curl也让子配置文件优先👇 nginx的转发优先级 基本实验需求已实现，但是关于nginx的server块优先级，涉及子配置文件，主配文件，location，还有server块里的ip+port > servername ,servername里的又细化为 以上是模糊的梳理，还需进一步明确优先级。 1、include写在主配置文件的server前面，基本上就是include那里的子配置文件优先看了 ​ # 然后因为80是默认的，写不写都是，所以基本上轮不到主配置文件的server块了 ​ # 然后多个include还是看谁在前 2、然后include所指的子配置文件夹里(一般就是conf.d)就看多个文件之间，以及单个文件内的server块之间的优先了 ​ 2.1 按字母顺序查找 ​ 2.2 单个文件里就按 ​ ①ip+port 最优 ​ ②才是这张图👇 3、然后再实现一个curl ip 走一个，curl 域名走一个需求，方法①代理②配置文件里写map？ Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:26 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/2-Dockerfile常见指令用法.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/2-Dockerfile常见指令用法.html","title":"第2节 Dockerfile常见指令用法","keywords":"","body":"第2节 Dockerfile常见指令用法 动态化处理 但是安图配置后👆还是不行，换一个方法 ①环境变量里预先定义 ②使用envsubst来替换 然后再编写到Dockerfile里就行了 1、删除所有镜像和容器，重新构建 docker rm -f `docker ps -qa` docker system prune -a 2、构建base os：构建alpine自定义镜像 3、构建app 镜像 4、构建app 带传递环境变量的镜像 搞定 换个PORT端口传递进去 ok，nginx的端口和HOST自定义就搞定了👆。 注意事项： 1、使用-e传递参数进容器，不是在build阶段，而是在run阶段，所以想要用这个变量，Dockerfile里要是用CMD指令而不是RUN。 2、然后测试的时候不要 -it sh进去看效果，因为会替代CMD导致Dockerile的CMD不执行了。 3、因为父镜像里有CMD，你子镜像也要用CMD，用屁股想也是要最后一个CMD生效的，不可能多个CMD生效的，否则就不存在-it sh替代一说了，所以要写全，比如nginx -g 'daemon off;' 4、nginx 配置文件里可以写变量，但是nginx自身是不认环境变量的，即使主配文件的main块里写env也不会生效-实验结果就是不生效，虽然官方有这个指令说明。 5、使用第三方插件的gettext包里的envsubst命令来替换就行了。 6、上面的Dockerile里的apk 安装需要改成RUN apk add --no-cache gettext ，实现安装后删除安装包的效果。 进一步思考 上图理解： 首先一般来讲nginx.conf里是不配置server块的，所有子配置文件都放在一起，然后系会展开合并在一起去判断，先按ip和端口、再按server_name，如果两个一样，再按子文件的命名顺序，比如a.conf的www.mimng.org优于b.conf的www.ming.org。 1、nginx的转发优先级：ip+port优先，如果一样，就看server_name，server_name里再细分 2、curl的时候使用ip+port，就会从上往下查找配置文件 3、理解1和2参考👇 好了实现就行了，继续看视频吧。 Dockerfile镜像制作和使用流程 工作案例分享-ip和座位号track 1、背景 员工IP动态获取，存在变动性， 员工座位也是经常换，存在变动的， 如何知道该员工当前准确的IP是什么，座位在哪里？ 2、需求 需要实现ip以及座位的自动校准 3、思路 用户的电脑的MAC是固定的，基于该MAC每天动态的获取最新IP就行了 交换机接口和座位号的对应关系是固定的，基于该MAC在哪个接口下就知道了该员工的座位号了 4、落地 首先需要收集所有员工姓名和MAC的清单：如何实现很多人的信息一键收集呢？ 其次有一个基于MAC快速获取IP的接口：如何编写基于mac信息获取ip及座位号的接口？ 所以： ​ 你需要有一个web前端，让用户填写姓名、座位号，并且适当的做一下限制防止有人乱写。 通过http的字段获取报文里的IP信息，根据IP找到此人的MAC、找到交换机的端口。 ​ 然后将user和mac入库形成user_mac表，将seat、sw port 入库形成seat_sw_port表。 ​ 然后每天或者每15分钟都行，根据mac找到当前的IP，根据mac找到所在的sw port，然后遍历seat_sw_port库，找到所在的seat，这样就得到了一个完整的记录user、ip、mac、seat、sw、port ​ 当然这一条记录也要加上aging老化时间，这样当该员工离职后，就会有一个自动删除记录的机制。 5、具体的实现步骤 信息的收集 web前端的细节本文不涉及，我也不太会，请别人写的，这里说明下具体注意点 1、页面长这样 2、为了防止别人乱填，做以下限制 首先当用户填写姓名和座位号的时候，就会在服务器那头收集三个信息分别是：姓名、座位号、IP地址这三个，并入库infocollection 限制如下： ①座位号格式必须是你希望的格式，是你们公司规范化后的格式，比如'[a-bA-B][0-9]{2}'这么一个规范。 ②如果姓名和IP已存在，则覆盖原来的记录，给用户纠错的空间 ③如果姓名和座位号已存在，则弹出窗口 \"该座位号已绑定，请联系管理员\" 3、以上就收集到了很多员工的三个信息 name \\ seat \\ ip 下面就是针对 这三个信息的处理 信息的处理 然后就根据当前的ip找到两个信息 ①mac地址及其②所在的交换机的端口 >> 此时就需要写一个脚本来实现基于mac、ip、交换机的端口的信息查询。 这里提供了我的三个脚本，按序执行 python3 /pycharm_project_418/switch/getData/getMAC/ssh_netmiko_get_mac.py python3 /pycharm_project_418/switch/getData/getArp/ssh_netmiko_get_arp.py python3 /pycharm_project_418/switch/pickInfo/ipMacPort/client_ip_cam.py 这里可能还涉及早期写的execl表格处理，不用管 下面是对三个脚本的解释说明 ssh_netmiko_get_mac.py说明 ​ 该脚本是用来获取各个厂商的交换机的mac地址表 #!/usr/bin/python3.6 # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") from netmiko import ConnectHandler import time import os import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime def ssh_singlecmd_netmiko(sw, cmd): # net_connect = ConnectHandler(device_type='cisco_ios',host='IP地址',username='用户名',password='密码') net_connect = ConnectHandler(**sw) # current_view = net_connect.find_prompt() net_connect.enable() # 进入CISCO或H3C的特权模式，针对H3C可能不灵，但目前是好的。 # 执行命令，返回结果为字符串，赋值给output result = net_connect.send_command(cmd) # if \"confirm\" in output: # output += net_connect.send_command_timing( # \"\\n\", strip_prompt=False, strip_command=False # ) net_connect.disconnect() fileName = f'{sw[\"ip\"]}' os.chdir(\"/switch/mac_port\") save = open(fileName, 'w') save.write(result) def get_mac(): ###################### # 将yaml文件转成字典 # #################### os.chdir(\"/pycharm_project_418/switch\") currentpwd = os.getcwd() print(currentpwd) with open('swList.yml') as f: swInfoList = yaml.load(f, Loader=yaml.Loader) # 将yaml文件转成数组，里面是字典，每个字典是一个sw # 去掉核心交换机172.16.13.254,不要在这里去了，第一步就是大而全，其中处理，后面在过滤吧。 # pool = ThreadPool() pool = ProcessPool() ip_port_error_list = [] # 最终结果 # 格式化为netmiko需要的几个字段 for swInfo in swInfoList: # pprint(swInfo) swInfo.pop('community') swInfo.pop('location') pprint(swInfo['ip']) # pprint(swInfo) if 'cisco' in swInfo['device_type']: print(swInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(swInfo, 'show mac address', )) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'hp_comware' in swInfo['device_type']: print(swInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(swInfo, 'disp mac-address',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'ruijie_os' in swInfo['device_type']: print(swInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(swInfo, 'show mac',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) pool.close() pool.join() if __name__ == '__main__': get_mac() ############################### ## 外网核心交换机和外网楼层交换机 # ############################### - ip: '192.168.11.1' device_type : 'hp_comware' username: 'xxxx' password: 'xxxxx' secret: '' community: 'snmp@xxx' location: '-1F机房' - ip: '172.16.12.1' device_type: 'cisco_ios' username: 'xxx' password: 'xxx' secret: 'xxx' community: 'office@xxx' location: '-1F机房' - ip: '172.16.13.1' device_type: 'ruijie_os' username: 'xxx' password: 'xx' secret: 'xxxx' community: '' location: '-1F机房' ssh_netmiko_get_arp.py说明 用来收集内外网GW上的信息 #!/usr/bin/python3.6 # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") from netmiko import ConnectHandler import time import os import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime def ssh_singlecmd_netmiko(sw, cmd): # net_connect = ConnectHandler(device_type='cisco_ios',host='IP地址',username='用户名',password='密码') net_connect = ConnectHandler(**sw) # current_view = net_connect.find_prompt() # print(current_view) # 显示当前所在试图一般为用户模式 net_connect.enable() # 进入CISCO或H3C的特权模式，针对H3C可能不灵，但目前是好的。 # current_view = net_connect.find_prompt() # print(current_view) # 显示当前所在试图一般为用户模式 # 执行命令，返回结果为字符串，赋值给output result = net_connect.send_command(cmd) # if \"confirm\" in output: # output += net_connect.send_command_timing( # \"\\n\", strip_prompt=False, strip_command=False # ) net_connect.disconnect() # print(result) timeNow = datetime.now().strftime(\"%Y-%m-%d\") # 不要时间了，直接IP命名 fileName = f'{sw[\"ip\"]}' os.chdir(\"/switch/arp\") save = open(fileName, 'w') save.write(result) def get_arp(): ###################### # 将yaml文件转成字典 # #################### os.chdir(\"/pycharm_project_418/switch\") currentpwd = os.getcwd() print(currentpwd) # with open('gw.yml') as f: gwList = yaml.load(f, Loader=yaml.Loader) # 将yaml文件转成数组，里面是字典，每个字典是一个sw # pprint(gwList) # pool = ThreadPool() pool = ProcessPool() ip_port_error_list = [] # 最终结果 # 格式化为netmiko需要的几个字段 for gwInfo in gwList: # pprint(swInfo) gwInfo.pop('community') gwInfo.pop('location') pprint(gwInfo['ip']) # pprint(gwInfo) if 'juniper' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'get arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'hp_comware' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'disp arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'cisco_ios' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'show arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'ruijie_os' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'show arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) pool.close() pool.join() if __name__ == '__main__': # 做成函数是为了将来在页面做成按钮，实现动态实时获取，现在每天写到文件，后面再弄DB之类的。 get_arp() ############ ## 外网GW # ######### - ip: '192.168.11.1' device_type : 'hp_comware' username: 'xxxx' password: 'xxxx' secret: '' community: 'snmp@xxxx' location: '-1F机房' - ip: '172.16.12.1' device_type: 'cisco_ios' username: 'xxxx' password: 'xxxx' secret: 'xxxx' community: 'xxx@xxxx' location: '-1F机房' ############ ## 内网gw # ######### - ip: '2.0.1.2' device_type: 'juniper' username: 'xxxx' password: 'xxxx' secret: '' community: '' location: '-1F机房' - ip: '172.16.13.1' device_type: 'ruijie_os' username: 'xxxx' password: 'xxxx' secret: 'xxxx' community: '' location: '-1F机房' client_ip_cam.py说明 先说明一个前置脚本swIp_mac_port.py #!/usr/bin/python3.6 # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") from netmiko import ConnectHandler import time import os import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime import re import os def sw_ip_mac_port(): # 从pyConsole的/swith/提取arp和mac文件，该文件是每天一份18点15出 os.chdir('/switch/mac_port') # 去掉外网核心、内网核心、楼栋互联的100和101不统计这些非接入PC的交换机 with os.popen('ls |grep -Ev \"192.168.|172.16.13.254|172.16.12.100|172.16.12.101\"', 'r') as p: # with os.popen('ls |grep -Ev \"172.16.13.254\"', 'r') as p: info = p.read().strip() # print(info) # 格式化cam表，去掉无用的，做成字典 swS_ip_cam = {} thisSw_vlan_mac_port = {} for i in info.split('\\n'): # 一个代表一个sw if i: # 因为上面做了p.read().strip()，所以这里算是多余的吧。一般不会出现i是None了。 sw_ip = i.split('_')[0] # print(sw_ip) # with os.popen(f'cat {i} |grep -i DYNAMIC', 'r') as a: # 端口开启了mac-security后client的mac就是static了，此处需要优化 with os.popen(f'cat {i} |grep -Ei \"dynamic|static|Learned\" |grep -Evi \"cpu\"', 'r') as a: # cam = a.read().replace('DYNAMIC', '') cam = re.sub(r'(?i)dynamic', '', a.read()).strip() # (?i)这是正则的写法忽略大小写的意思 cam = re.sub(r'(?i)static', '', cam).strip() cam = re.sub(r'(?i)learned', '', cam).strip() cam = re.sub(r'(?i)aging', '', cam).strip() # cam表格去掉无用的 # print(cam) thisSw_vlan_mac_port = {} unit_info = { } count = 1 for m in cam.split(\"\\n\"): z = m.split() # print(z) try: if len(z[2:]) > 1: port = z[2] + z[3] else: port = z[2] if sw_ip == '192.168.11.1': unit_info = { count: { 'vlan': z[1], 'mac': z[0], 'port': port, # 这个可不能顶层赋值了啊，我的好习惯变成坏事了。或者顶层附一个else的值，再取消else，无语。 } } else: unit_info = { count: { 'vlan': z[0], 'mac': z[1], 'port': port, # 这个可不能顶层赋值了啊，我的好习惯变成坏事了。或者顶层附一个else的值，再取消else，无语。 } } thisSw_vlan_mac_port.update(unit_info.copy()) count += 1 except BaseException as e: print(sw_ip, e, '十有八九是cam表没取到') # pprint(thisSw_vlan_mac_port) thisSw_vlan_mac_port = { sw_ip: thisSw_vlan_mac_port.copy() } # pprint(thisSw_vlan_mac_port) swS_ip_cam.update(thisSw_vlan_mac_port.copy()) # print('===========================') # pprint(swS_ip_cam) return swS_ip_cam # 到此就得到了每台sw的cam表，但是注意没有去掉上行口信息，下面就开始去掉上行口信息。 # 首先得获得每台得上行口,思路来了，每个字典里去掉上行信息，这个需要配合arp表，即去掉网关IP的mac对应的端口，这个端口出来后，所有从这个端口学到的mac都删掉，就得到干净的CAM表。 # 所有真就不应该在mac初始环境去处理，不好，原则是尽量少的登入SW。 # 下面转到clientIP_mac_port模块 if __name__ == '__main__': z = sw_ip_mac_port() pprint(z) 输入的样子如下👇 下面就是最后一个脚本client_ip_cam.py # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") import time import os import json import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime import re from switch.pickInfo.ipMacPort.swIp_mac_port import sw_ip_mac_port from openpyxl import load_workbook os.chdir('/switch/arp') with os.popen('ls', 'r') as p: file_name = p.read() client_mac_ip = {} for i in file_name.split(\"\\n\"): if i: gw_info = i.split(\"_\")[0] grepFilter = 'grep -E \"[0-9]{1,3}\\.\" ' with os.popen(f'cat {i} | {grepFilter}', 'r') as r: context = r.read() # print(context) for m in context.strip().split(\"\\n\"): # print(m) if m: client_ip = m.strip().split()[0] client_mac = m.strip().split()[1] client_vlan_actual = m.strip().split()[2] client_aging = m.strip().split()[4] # 这里面要赛选出一个mac多个ip的条目，并取出aging大的那一条 if 'eth' in client_vlan_actual: client_vlan_actual = client_ip.split('.')[-2] client_aging = m.strip().split()[5] if client_mac in client_mac_ip: if int(client_aging) > int(client_mac_ip[client_mac][-1]): # 如果aging最大，也就是最新就覆盖之前的mac ip vlan # print(client_mac, client_ip,client_vlan_actual,client_aging) # print(client_mac, client_mac_ip[client_mac]) client_mac_ip[client_mac] = [client_ip, client_vlan_actual, client_aging].copy() else: client_mac_ip[client_mac] = [client_ip, client_vlan_actual, client_aging].copy() # 这里补上arp里的vlan才是对的 # pprint(client_mac_ip) ''' 生成client_ip_mac[client_ip] = [client_mac, client_vlan_actual].copy() # 这里补上arp里的vlan才是对的 ''' client_ip_mac = {} for k, v in client_mac_ip.items(): client_ip_mac[v[0]] = [k, v[1]] # pprint(client_ip_mac) # exit() ''' 得到用户ip和mac信息 ''' # pprint(client_ip_mac) # 第一步找到client们的网关的MAC dev_net_sw_mgmt_gw = '172.16.13.253' # 内网交换机的管理段网关 official_net_sw_mgmt_gw = '172.16.12.254' # 外网交换机的管理段网关 os.chdir('/switch/arp') # 找了两个接入sw用来查看GW的MAC access_sw_for_find_dev_net_sw_mgmt_gw = '172.16.13.1' access_sw_for_find_official_net_sw_mgmt_gw = '172.16.12.1' with os.popen(f'cat {access_sw_for_find_dev_net_sw_mgmt_gw} |grep {dev_net_sw_mgmt_gw}', 'r') as p: z = p.read() dev_net_sw_mgmt_gw_mac = re.findall('([0-9a-f]{4}\\.[0-9a-f]{4}\\.[0-9a-f]{4})', z)[0] # print(z) # print(dev_net_sw_mgmt_gw_mac) with os.popen(f'cat {access_sw_for_find_official_net_sw_mgmt_gw} |grep {official_net_sw_mgmt_gw}', 'r') as p: z = p.read() official_net_sw_mgmt_gw_mac = re.findall('([0-9a-f]{4}\\.[0-9a-f]{4}\\.[0-9a-f]{4})', z)[0] # print(z) # print(official_net_sw_mgmt_gw_mac) # 第二步根据网关MAC，找到各个sw的cam表的什么呢？找到该MAC对应的端口即为上行口，去掉所有该口学到的MAC，就得到了用户MAC。 sw_mgmt_gw_mac_list = [dev_net_sw_mgmt_gw_mac,official_net_sw_mgmt_gw_mac] ''' 这就得到交换机上行口的获取方法（交换机管理IP的网关的MAC，必然是从上行口学来的。） ''' # print(sw_mgmt_gw_mac_list) swS_ip_cam = sw_ip_mac_port() # 这里调用了会改变PWD，要注意下 # pprint(swS_ip_cam) sw_upLink_port = {} for k, v in swS_ip_cam.items(): # print(v) upLink_port_list = [] for m, n in v.items(): # print(n) if n['mac'] in sw_mgmt_gw_mac_list: # print(k, n) upLink_port_list.append(n['port']) # print(upLink_port_list) upLink_port_list.sort() upLink_port_list = list(set(upLink_port_list)) sw_upLink_port[k] = upLink_port_list ''' 得到了交换机的上行口 ''' # pprint(sw_upLink_port) # 将所有swS_ip_cam中涉及上行口的信息单元删除 swS_ip_cam_copy = swS_ip_cam.copy() del_list = [] for k, v in swS_ip_cam.items(): for m, n in v.items(): if n['port'] in sw_upLink_port[k]: # print(n['port']) # print(k, m) del_list.append((k, m)) # swS_ip_cam_copy[k].pop(m) # 遍历字典不能删除自己或别人操作，就是遍历的时候不能有这种动作 # pprint(swS_ip_cam[k]) # print(del_list) for i in del_list: swS_ip_cam_copy[i[0]].pop(i[1]) # pprint(swS_ip_cam_copy) ''' 这个得到了所有下行口的mac，也就是用户mac ''' swS_ip_downlink_cam_copy = swS_ip_cam_copy.copy() # pprint(swS_ip_downlink_cam_copy) client_ip_mac_sw_port_vlan_list = [] for client_ip, mac_vlan in client_ip_mac.items(): # print(v['mac'],mac) # 在比较arp里的mac和cam里的mac之前，先做mac的格式化 arp_mac = re.sub('[\\.\\-\\:]', '', mac_vlan[0]).lower() actual_vlan = mac_vlan[1] # print(cam_mac, arp_mac) client_ip_mac_sw_port_vlan = {} # 每次for循环都需要一个空的字典初始化一下。 for sw, info in swS_ip_downlink_cam_copy.items(): for seq, v in info.items(): cam_mac = re.sub('[\\.\\-\\:]', '', v['mac']).lower() # print(v) if cam_mac == arp_mac: cam_mac_format = cam_mac[0:4] + '.' + cam_mac[4:8] + '.' + cam_mac[8:12] # print(client_ip,mac,sw,v['port'],v['vlan']) client_ip_mac_sw_port_vlan[\"client\"] = client_ip client_ip_mac_sw_port_vlan[\"mac\"] = cam_mac_format client_ip_mac_sw_port_vlan[\"sw\"] = sw client_ip_mac_sw_port_vlan[\"port\"] = v['port'] # client_ip_mac_sw_port_vlan[\"vlan\"] = v['vlan'] # 存在一个mac 多个vlan 比如不同wifi等 client_ip_mac_sw_port_vlan[\"vlan\"] = actual_vlan client_ip_mac_sw_port_vlan_list.append(client_ip_mac_sw_port_vlan.copy()) # pprint(client_ip_mac_sw_port_vlan_list) pprint(len(client_ip_mac_sw_port_vlan_list)) todayis = datetime.now().strftime(\"%F\") # 将client_ip_mac_sw_port_vlan_seat_list写道文件中，后期写道库中，前端调用 with open(f'/root/yuan_gong/log/yuangong_ip_zuowei_{todayis}.json', 'w', encoding='utf-8') as z: json.dump(client_ip_mac_sw_port_vlan_list, z, ensure_ascii=False) if __name__ == '__main__': pass 综上所述： python3 /pycharm_project_418/switch/getData/getMAC/ssh_netmiko_get_mac.py python3 /pycharm_project_418/switch/getData/getArp/ssh_netmiko_get_arp.py python3 /pycharm_project_418/switch/pickInfo/ipMacPort/client_ip_cam.py 三个脚本跑一遍就会得到当前的用户ip、mac、sw、port、vlan的信息 如下图👇 此时就有了 上文开头处提到的 \" >> 此时就需要写一个脚本来实现基于mac、ip、交换机的端口的信息查询。\" 这么一个脚本 那么基于web页面收集上来的username、seat、ip就可以利用上面的结果进行查询了 当前材料：infocollection表有了、查询ip mac 端口信息的json文件也有了。 还需要再写两个脚本： 1、将infocollection表的信息通过上面的json文件进行查找，得到这么一个信息，并落库👇 2、然后将user_mac拆出来落入user_mac表；也将seat sw port落入seat_sw_port表 3、从infocollection得到user_mac和seat_sw_port两个固定信息表 的这么一个动作一定不能持续执行，一般5天完成批量众人信息的收集后就要停止该动作了。 4、最后就是基于user_mac，去查ip，去查端口，然后利用端口去索引seat_sw_port得到seat座位号。于是就有了最终的一个结果👇 上图生成的脚本就是每天都要运行一次的了，来保证ip变了，座位变了都能及时发现。 最终呈现的效果如下👇 最后，再具体的详细讲解，我会放到我的课程里去细聊。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:26 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/3-Dockerfile常见指令ENTRYPOINT用法.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/3-Dockerfile常见指令ENTRYPOINT用法.html","title":"第3节 Dockerfile常见指令ENTRYPOINT用法","keywords":"","body":"第3节 Dockerfile常见指令ENTRYPOINT用法 概述 我把cli就是手动敲的命令行叫做cli 我把CMD ENTRYPOINT就是说的Dockerfile里的配置 我把cli entrypoint就是手动命令行里的输入的东西 下文的沟通如上所示 CLI里的追加和覆盖 CMD是直接全部覆盖掉 CMD在Dockerfile里也是最有一个生效，前面都会被覆盖。 docker run 最后跟的参数也就是最后的那些个cli，会覆盖Dockerfile里的CMD 同样之前实验的CMD是一串CLI，也被直接覆盖了 但是ENTRYPOINT就不会了 CLI追加到Dockerfilie里的ENTRYPOINT docker run 最后的那些个cli就会作为Dockerfile里ENTRYPOINT的参数 没变化，再看其实是有变化的，只是docker ps 看不到太长了 上图可见docker run 的cli也就是cat 123 全部都当作ENTRYPOINT的参数了。 这一点inspect看还是不明朗的👇，只能自己知道有这回事，哦cmd的所有参数都是合并进去的。 查看合并后的CLI emmm，工具真多啊 CLI里替换entrypoint及追加cli docker run最后的那些个参数就会追加到ENTRYPOINT里，但是此时的Dockerfile里的ENTRYPOINT已经被docker run --entrypoint替换了。 就是好像cli里的选项--entyrpoint 这种写法没有达到Dockerfile的/bin/sh -c 这种效果 这样就勉强使用--entrypoing选项实现了Dockerfiler里的\"ENTRYPOINT cd;ls\"一样的效果。 Dockerfile里的CMD和ENTRYPOINT合并 这里就不再去docker run的时候写cmd和entrypoint了， 其实可以这么沟通：CMD ENTRYPOINT就代表了Dockerfile；而cmd和entyrpoint小写就代表了docker run的参数选项。 CMD推荐使用CMD [\"\",\"\",\"\"]的方式👇这样追加到ENTRYPOINT就不会有上图CMD方式里的默认/bin/sh -c 总结一下： 1、就是cli手动敲命令行的方式，基本都是所见即所得，就是不会给你自动补一个默认 /bin/sh -c 2、然后Dockerfile方式里的CMD如果是非列表格式的，就会自动补齐/bin/sh -c，所以这种方式追加到ENTYRPPINT也会把默认的/bin/sh -c补上去。 3、CMD其实就是作为ENTYRPPING运行的参数而存在的，或者说运行的选项。 exec 内部cli用help cli获取帮助 妈的敲错激起了，敲到dns上去了，好险好险...... 在shell里面敲sleep就是在当前SHELL1192下面输入的sleep，这是父子进程👇 而exec sleep 就会将当前的1192这个SHELL替换成sleep直接运行👇所以1192就从bash变成了sleep 体会下这个exec的作用 1、先了解变量在子进程中的一个local特性 一样的变量随程序跑完而消失 2、然后观察取消exec注释后的效果 进一步理解 上图也不是全对，主要就知道exec bash为什么将变量赋值保存了下来，原因很简单 1、继承了程序运行时(也即是子进程)里的环境，所以变量就有了， 2、然后程序运行本来时退出子进程的，但是exec bash就会用一个新的子进程来替代本来退出的程序的子进程，并且不会退出了。 3、将程序脚本的子进程得以变相的保留的下来，PID不变，但是换了个BASH。 环境变量如何在程序脚本执行完后得以保存 1、那么使用exec bash确实可以将程序脚本的环境(主要是变量)保存下来，那么进一步的使用场景是什么呢？ 2、第二种实现程序脚本运行完变量不会消失的方法就是source或者.执行文件，这样就是在当前SHELL执行，所以不是和bash file那样是开启子SHELL执行，不存在程序脚本执行完退出的动作，执行前后都是在同一个SHELL中，变量自然一直都在。 3、简单讲就是脚本跑完环境变量得以留存的需求：一个是开启子进程就是bash file这种运行方式或者./file(文件内容顶行#!/bin/bash)一样也是开启子SHELL，这种开启子进程的就通过ecex bash替换子进程将子进程得以不退出，环境得以保留；一个就是source或. file不开启子进程来运行，环境都没变，变量什么的自然都在。 # 总结：方法①开启子进程exec bash和方法②不开启子进程 上图注意逻辑 1、source和. file都是在当前SHELL里执行file的 2、exec bash确实执行了，也生效了，就是用新的BASH 在PID不变的情况下，替换了之前的BASH，所以exit就一层直接推出了 3、sleep 30由于被切换了bash，所以不会被执行。 梳理一下 1、exec bash 改成exec sleep或者其他的试试，应该不一定要exec bash才能继承环境，不是这么个意思，是只有用bash才能保住本来要退出来的子进程。而用exec sleep或起的，自然是可以继承环境，但是不能保住子进程不退出啊。 2、看看nginx容器官方的dockerfile的脚本里是否有有exec CMD的列表形式，里的所有参数变成了ENTYRPOINT命令的所有参数，又变成了exec \"$@\"的所有参数 3、ENTRYPOINT和CMD是docker run的时候生效的，所以-e是可以往里面传参的。 4、然后把下图理解一下 echo 'helo e5 ri 8 dai' > index.html --------------------------------- vim Dockerfile FROM nginx:1.26-alpine LABEL maintainer=\"oneyearice \" ENV DOC_ROOT='/data/website/' RUN makedir -p ${DOC_ROOT} COPY nginx.conf /apps/nginx/conf/ ADD index.html ${DOC_ROOT} ADD entrypoint.sh /bin/ EXPOSE 80/TCP 8080 #HEALTCHECK --start-period=3s CMD wget -O - -q http://${IP:=0.0.0.0}:{PORT:-80}/ ENTRYPOINT [\"/bin/entrypoint.sh\"] #CMD指令采用列表方式，其所有内容都将成为ENTYRPOINT的参数 CMD [\"/usr/sbin/nginx\",\"-g\",\"daemon off;\"] ---------------------------- cat entrypoint.sh #!/bin/sh #注意，alpine只有sh没有bash，此处要用sh cat > /etc/nginx/conf/conf.d/www.conf 👆关键是：CMD和ENTYRPOINT以及-e之间的组合 ENTRYPOINT是docker run的时候执行，然后ENTRYPOINT的脚本内容是： 1、生成nginx的配置文件。 2、结尾exec \"$@\"，这个$@就是脚本entrypoint.sh \"usr/sbin/nginx -g daemon off \" 这些所有位置参数了。 3、所以就是1、2合起来就是生成配置文件，并且运行nginx 4、再结合docker run -e的传参，就实现了动态的设定配置文件里的server_name和listen以及root的功能 5、确实比我上面的要高端一点，更加然别人肃然起敬，哈哈哈，至少我被这种用法唬住了~ 好，后面讲上述重新整理成实现截图。 执行一个脚本，然后运行程序的常规玩法 ENTRYPOINT执行一个脚本(一个环境初始化的脚本)，然后脚本最后一行写上exec \"$@\"，CMD写一行命令，这种套路就是ENTYPOINT的脚本先执行，然后再将执行权交给CMD。 再次实现动态传参的web定制效果 1、首先是原材料 通过Dockerfile可知build好了以后/data/website/里存在一个index.html页面。 然后entrypint.sh的脚本又是默认使用的/apps/nginx/html/下的index.html页面，由于ENV在之前设置$DOC_ROOT为/data/website/所以server块的root其实就是指向了/data/website/的。除非后面docker run -e DOC_ROOT=/apps/nginx/html 指回去，所以这里是一个神经病一样的配置了，需要优化的，优化的措施就是在Dockerfile里删掉DOC_ROOT相关： 实验暂时不改作为测试对比 ENV DOC_ROOT='/data/website/' mkdir -p ${DOC_ROOT} && 还有不要写$HOSTNAME这个只会是容器的ID，改成 server_name ${HOST:-\"www.ming.org\"}; 还有修改脚本执行路径为/，如果不ADD到/usr/sbin这些PATH路径下 2、然后build 3、然后run一下 发现没有UP，进一步排查发现是$@里是空值，理由如下 如果在脚本中添加一个行echo可知，脚本确实执行了，只不过参数没有拿到。 我们改变ENTRYPOINT的书写方式 发现此时$@确实拿到了参数，这样就可以让exec执行了，报错的问题先不着急处理，先梳理以上两张图的结论 1、CMD和ENTRYPOINT的结合没有问题，不管是ENTRYPOITN 用不用列表形式，其合并的逻辑是一样的 只不过非列表有一个默认/bin/sh 然后统统加上-c攒成列表的行为👆，而列表形式就比较干净👇 所以$@位置参数 我怀疑前一个$@里是否一点东西都没有，👇验证果然是的 2、但是$@的传递必须使用script.sh arg1 arg2 arg3的方式，而不支持bash script.sh arg1 arg2 arg3这种 但是$@的传参现在看下来只能用列表的方式才能规范化下得到想要的效果。这是在容器build的场景中，而在宿主的SHELL下script.sh arg1 arg2 arg3 和bash script.sh arg1 arg2 arg3 的$@倒是一样的 这一点容器没有宿主的SHELL灵活👆。 好下面继续处理之前的分号报错 原因就是👆CMD里daemon off;不要单引号 最终重来一遍： 1、原材料修改为 2、build 3、run 这里有个细节，就是你ngin -g daemon off; 上图是容器里这么执行确实时ok的👆，但是手动执行其实是会报错的👇 看到没，这也是容器里里代码逻辑的第二点细节， ①第一个就是我上面将的$@，容器build的时候必须使用列表格式，本质上也就是不认sh -c './script.sh arg1 arg2 arg3'，这种脚本里面读不出来$@，必须是script.sh arg1 arg2 arg3 ②第二个就是CMD里列表单元其实不用加引号，虽然手动的时候需要引号，也就是nginx -g 'daemon off;'手动不加引号不行，但是CMD里加了引号才不行👇下图就是一开始的配置结果报错反而 然后发现有出错了 因为这一次我用的是官方镜像，而官方镜像的配置文件压根不在/apps/nginx/conf这个下面，我操作的都是这个目录，压根就是错误的 不相信exec -it进去可见 所以再次重新修改buid的原材料 在此之前探明人家nginx里的目录是否存在，已经server块是否有子配置文件 有一个default，根据之前所学，default.conf首字母为d很容器就会抢先，这个注意下，后面测试 1、原材料 进入到官方的nginx，run起来看看index.html在上图的default.conf里明确制定了，所以我们脚本也要修改 去掉nginx.conf主配置文件，将www.conf生成到/etc/nginx/conf/conf.d/下 可预判优先级抢不过default.conf。实验继续 2、run 3、进去curl测试下 实测就是两个子配置文件优先级方面，系统还是都会去看一遍的，port > server_name 这些都是所有子配置文件合并起来看的，如果大家都一样，就是一直到server_name都一样才会去说看排序第一个子配置文件，也就是字母排序第一个的default.conf文件了。 以下就是调整www.conf名称抢先default.conf的测试过程👇 了解了这些细节后，下面测试完整走一遍 1、原材料 2、build 3、run 先不带-e参数run一次 curl IP就是走的default.conf，域名就是走的www.conf 进去调整www.conf重命名为a.conf，IP也会走www.conf了👇 然后再带-e run一次 一般不会写死IP，这里就是实验测试而已👆 然后上图的web03容器run起来测试如下👇 补一个dns解析如下 所以实验ok，到此结束。 确实可以实现server_name和port以及index.html文件的自定义，看效果 1、原始材料，build的时候修改index.html 2、run的时候修改域名和端口，测试如下 再来一个测试手法：-H修改主机头 上图👆的grep -Ev没有去掉空行，可以优化为 sh -c 'cat /etc/.......' 就行了。 无需本地写host 看一个例子脚本里的东西：高级表达式 注意：表达式只是表达式，并不是变量赋值 这图就是说，不存在变量赋值的情况下，你不要瞎搞👆 echo $HOST,s是查看HOST变量的值， echo ${HOST:-\"www.mong.org\"} 是查看HOST变量的值，如果HOST变量没有值，这个表达式的结果就是www.mong.org。不是说HOST变量的结果， 这两行从头到尾都没有说HOST的变量存在赋值的情况哦！ 同理看下面的例子，一个意思 上图echo加个提示，否则看不清 纠错DOC_ROOT在Dockerfile里的EVN是赋值了的，所以有的，但是HOST确实是没有赋值的。 规范化 1、CMD就是最后一个命令挂前台的 2、ENTRYPOINT就是初始化的环境配置的 虽然你可以将CMD的命令合并到ENTRYPOINT的脚本里(比如将nginx -g \"daemon off;\"放到entrypoint.sh的最后一行，并注释exec \"$@\"，但是不会这么做，不清楚，属于大家都这么用的规范问题。 什么 mysql、nginx都是这么玩的👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:26 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/4-Dockerfile常见指令用法.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/4-Dockerfile常见指令用法.html","title":"第4节 Dockerfile常见指令用法","keywords":"","body":"第4节 Dockerfile常见指令用法 ARG，变量 类似ENV，不过ENV是build阶段定义的变量，docker run阶段也同样可以使用 ARG只能在build阶段生效。 使用场景： 1、ENV就是build阶段定义的变量赋值，然后docker run阶段可以使用-e修改掉 2、ARG就是build阶段定义用的，docker run起来就没有这些变量了。 3、ARG可以写到FROM前面 ARG常用来定义版本号👇 ARG VERSION=3.20.0 FROM alpine-base:$VERSION LABEL maintainer=\"oneyearice \" ARG NGINX_VERSION=1.26.0 ADD nginx-$NGINX_VERSION.tar.gz /usr/local/src/ RUN cd /usr/local/src/nginx-$NGINX_VERSION && ./configure --prefix=/apps/nginx && make && make install && ln -s /apps/nginx/sbin/nginx /usr/bin && addgroup -g 2024 -S nginx && adduser -s /sbin/nologin -S -D -u 2024 -G nginx nginx && mkdir /apps/nginx/conf/conf.d COPY nginx.conf /apps/nginx/conf/nginx.conf ADD index.html /apps/nginx/html/index.html RUN chown -R nginx.nginx /apps/nginx/ #EXPOSE 80 443 #CMD nginx && tail -f /apps/nginx/logs/* CMD [\"nginx\",\"-g\",\"daemon off;\"] VOLUME，持久化 volume的目录可以自动创建的，无需事先创建 相当于同时：①实现数据持久化②也顺便创建了目录 build成功就说明VOLUME创建了目录了，因为entrypoint.sh里有一句echo $HOST > ${DOC_ROOT}index.html没报错，说明目录OK的。 目录exec进去可见 而且整个VOLUME创建的目录是持久化的，所以是独立于容器的目录的 这些LowerDir、MergerDir、UppereDir、WorkDir目录，容器删了，这些目录就没了 但是volume是独立的文件持久化的 测试 而且整个映射出来的宿主上的文件内容改了，容器里也同步的 容器里的所有文件都是在宿主机上存放的其实~ mysql的数据就很需要volume持久化一下 Dockerfile里的VOLUME遗憾 但是宿主机上的目录名称没有任何识别度，就是自动生成的这些volume在宿主上的目录是无法维护的，你都不知道谁是谁👇 容器如果删了，还可以docker inspect 查看谁是谁的谁，容器删了可就没法查了，只能编列cat里面的内容去一个个找了。 还有一个命令可以看volume的 总之Dockerfile里的VOLUME就是没法指定宿主的映射目录的，只能这样，也算是一个缺点。怎么解决，docker run -v可以做宿主和容器里的目录映射，docker compose的yml也可以实现。 也可以使用VOLUME挂载多个容器里的目录出来 EXPOSE仅仅是一个申明 申明的是-P要映射的端口是啥而已。至于是不是容器里监听的，也不一定，没有直接关系。 1、暴露端口的时候，可以用docker run -p 8080:80来 将容器里的80暴露到宿主的8080。 2、也可以用Dockerfile里的EXPOSE来说明容器里暴露的是80或443，但是宿主用哪个端口来对接就是在docker run -P来随机指定了，如果不像随机，那么还是用-p再次指定，此时Dockerfile里的EXPOSE就没意义了。 3、EXPOSE和VOLUME一个德行，都是仅仅申明容器里要暴露的端口和目录。至于暴露成宿主的哪个端口和路径，Dockerfile里没有解决，都是通过docker run -p和-v来解决，然而既然用了-p和-v，那么容器里的端口和目录都是直接写了，无需Dockerfile重复写了。 # 但是EXPOSE和VOLUME的区别是EXPOSE不管容器里是否监听某个端口，只是申明供-P暴露出来；而VOLME是容器里没有这个目录就给你自动创建出来(而且是mkdir -p 的方式来创建多层目录的)，然后暴露成宿主的目录。 4、然后EXPOSE就是配合docker -P，大P来使用的，告诉大P暴露容器里的什么端口为宿主的随机端口。 5、凡是EXPOSE写的端口不是容器里监听的端口的，都是坏人~，哪家好人干这种事啊，对不对，不写也是不对的，总不能让别人去猜你nginx或其他app监听的啥吧，不猜就去找脚本，或者你要是默认编译的基本就是80了。 以下截图就看EXPOSE就行了，其他前面的内容 443只是暴露出来，并不代表容器里有监听443，而实际情况是容器里只监听了80，443没有开，暴露是80和443都暴露的，所以👇 WORKDIR，切换工作目录 类似cd，切换文件夹的，确实切掉了， 但是如果在Dockerfile里 RUN cd xxx/xx/xx 也仅仅是在这一条RUN里切换了路径，下一行又回到容器里的/根了。实验测试下👇 1、原材料 2、build 2、run看效果👇 所以RUN cd和WORKDIR是不一样的，前置式只在RUN cd的哪一行临时切过去，后面又回到了默认的/路径，而WORKDIR就是后面都切过去了。 至于RUN一行里用&& 还是; 都行，一行都是临时切过去了。 ONBUILD， 父镜像build指定的CMD在当时不执行，在子镜像build的时候才执行。 举例：rm -rf /，就是说父镜像ONBUILD制定了rm -rf /，一旦子镜像继承了就把子镜像的根删掉。 # 你要继承我，我就把你删了。。。 这里只是一个没人这个干的例子，呵呵。 也许你不想让别人用的镜像作为父镜像，就可以这么做，ONBUILD RUN rm -rf /* 呵呵。 那么问题来了： WORKDIR /bin ONBUILD touch helo.log 这个touch在子镜像build的时候创建的，那么是在子镜像的/下还是下/bin下呢？是子镜像的/bin下的， 因为子镜像FROM xxx的时候已经继承了WORKDIR的默认工作目录已经是/bin了，然后ONBUILD又是子镜像才执行显然是在FROM以后的。 1、首先父镜像里是没有touch helo.log文件的👇 顺便看一个视频中的典型错误案例👇 图中错误就是，entrypoint.sh不会被执行，因为找不到路径， 理由很简单：你看下图是不是一样 解释给你听哦：①你要让光秃秃的xxx.sh，这种不带绝对/相对路径的脚本直接执行，那就要让xxx.sh放到$PATH变量里的路径里去；②结果你上面COPY是放到/根③你WORDIR /切到/根，人家也不会去/当前目录根里找这个xxx.sh的。哈哈，不要学到了K8S还在这里闹小学一年级的笑话~还说什么WORKDIR没切过去，这是WORKDIR的事嘛~，基础不牢地动山摇，人心不稳王座倾塌。 下面就是Dockerfile制作一个父镜像，然后再Dockerfile.son做一个子镜像，其中父镜像里的是ONBUILD touch，哦错了，是ONBUILD RUN touch才对👇 由于父镜像里已经WORKDIR /bin了，所以父镜像docker run起来进去就是/bin下的； 同样子镜像是继承的父镜像，所以子镜像的工作目录就是/bin的，所以再执行ONBUILD RUN touch helo.log就生成了文件了👇 判断下ONBUILD的 echo的动作是在WORKDIR前还是后 判读是在后，因为WORKDIR是父镜像build的时候就会执行，一旦执行，子镜像继承了就是工作目录默认就是/bin了，然后ONBUILD echo是CMD还是ENTRYPOINT啊，我的意思是在build阶段还是在docker run阶段生效啊？ 就算ONBUILD是build生效，那也是子镜像执行，而子镜像FROM xxx继承父镜像这个FROM就是父镜像的WORKDIR已经完成了，所以ONBUILD是肯定是在WORKDIR的目录里的了。 ​ 然后ONBUILD RUN或ONBUILD ENV、ONBUILD ARG、ONBUILD ENTRYPOINT 再看 buid一下，注意build的时候，镜像不要用之前的镜像名称，会有问题，待会单独研究。 验证结论的时候到了， 验证如下👇 看案例，就是父镜像没再次build导致子镜像还是用的之前的父镜像的Dockerfile👇，总之看CACHE比较明朗就： 然后为什么会有之前的影响，继续研究 还是还原故障 还原个屁，是自己搞错了，父镜像没有build覆盖原来的tag导致的 这里图中打叉的地方-t写错了，本来是覆盖nginx_env_web:1.5这个父镜像的，结果写成了_son了，所以原来的父镜像没变，也就是说ONBUILD echo '123321' > helo.log本来就在，所以子镜像继承了，人家CACHE既然看到了就说明利用了之前的分层了，就说明父镜像没有变了。 USER，指定后续指令的执行者 HEALTHCHECK,健康检查 注意👆上图IP:=哪里写错了，PORT也写错了，复习变量高级用法去，nnd。 starting就是正在检查中 过一会就发现检查失败了 失败的原因是HEALTHCHECK里的CMD里的变量高级用法写错了， 修改后OK👇 上图👆就是默认的时间是30s timeout，30s检查ok了就判定为healthy。如果30s检查不OK，就retries 3次也就是90s才会判定unhealthy。 然后研究下curl 和 wget的静默用法👇 FROM nginx # 添加健康检查脚本 COPY healthcheck.sh /usr/bin/healthcheck.sh RUN chmod +x /usr/bin/healthcheck.sh # 定义健康检查命令 HEALTHCHECK --interval=30s --timeout=3s \\ CMD /usr/bin/healthcheck.sh # 这里为啥不能直接写curl和wget，肯定可以的啊，脚本结果还导致少了一个人家健康检查无法执行的返回值2 if curl -I http://localhost:80/health >/dev/null 2>&1; then exit 0 else exit 1 fi #!/bin/sh if wget -q --spider http://localhost:80/health; then exit 0 else exit 1 fi 所以这样写就行了 #HEALTHCHECK --start-period=3s CMD wget -q --spider http://${IP:-0.0.0.0}:${PORT:-80}/ HEALTHCHECK --start-period=3s CMD curl -I http://${IP:-0.0.0.0}:${PORT:-80}/ &> /dev/null #这样做的好处，就是docker logs里就没有多余的curl wget信息了，不过这些系也是timeout到了才会出现，比如默认30s就是要到最后一秒才会出现curl或wget日志,不太能理解，因为--start-perod=3s默认就是3秒就执行了啊，我以为日志就容器起来3s就curl就会有日志了，结果docker logs查看发现要等timeout30秒到了才会有日志出来。 HEALTHCHECK 后面的CMD只是HEALTHCHECK里的一个固定参数，不要理解成Dockerfile 的CMD哦 如果健康检查失败还需要进一步去处理故障的，GPT给了一些思路，不过有待后面学到docker-compose的时候验证： 工作案例-抓某个软件的包 1、背景 公司打不开discord的客户端软件 软件还不是浏览器，无法通过F12直接看到那些URL打不开 2、需求 需要定位某个软件访问了那些域名，然后进一步写hosts，然后放行 3、classwire只能看到已经正常运行了的软件，对于打不开得，就是比如discord还在update，以及update好了还在starting的状态下是看不到 discord这个app的流量统计的。至少我的免费版看不到 4、落地方案，如下 参考两个链接 一个cmd里findstr如何写or 👇 https://blog.csdn.net/zhigang0529/article/details/86240577 一个是如何抓包抓软件👇 https://blog.csdn.net/weixin_51309915/article/details/122382555 方法1：有白名单限制的情况下进行 1、打开wireshark软件开始抓包 2、打开软件discord，让其update转圈圈，后面update好了starting一样的处理思路 这个已经没法复现了，我已经解决了，所以仅文字描述了 3、打开任务管理器 找到pid， 4、打开cmd 手动不断回车，配合discord update界面出现的时间，下图findstr 后面空格是或的关系，表示同时抓这些PID。 已经ESTABLISHED基本是OK的，不用管 找到SYN SENT 5、wireshark过滤该ip 提取第一个该ip出现的时间 然后取消过滤，再全部信息里，找到时间点的这条抓包，往上看到DNS解析对应的域名👇 这就知道了该软件目前需要访问的域名了，重复多次，找到需要访问的所有域名 然后①域名解析本地dns固定成一个IP②路由指向vpn③vpn节点放行该ip。这就是海外白名单管理的思路。 以上就是app如何抓包的思路，要抓到域名嘛 除此之外，还有一个方法 方法2：取消白名单限制去做统计 ★放开本地的海外白名单，针对自己的IP，然后打开GlassWire软件，此时由于海外全通，所以discord可以打开，包括update，start都行， 这样等软件正常运行了，GlassWire就可以正常显示该软件的域名访问了。 由于已经升级和能够打开了，所以还需要手动升级来让classwire看到流量 操作①需要退出软件②打开③手动升级 发现不全少了一个dl.discordapp.net，这个域名。可见上面的手动方法还是要掌握的。至少可以先用方法2，然后再用方法1查漏补缺。 说明 这种情况就是PID3118一开始不断和130.221.15.150去TCP CONN，但是dns解析写道104.18.52.172后，软件方面也延迟了一小会然后IP解析更正过来了以后，就可以了。 上图同样是通过wireshark过滤IP，找到时间，然后在全部信息里找到时间，看上面的A记录里的域名。 一开始是SYN还是和130.211.15.150建立，后面我解析固定后，discord这个APP的连接请求也就自动更正了 然后通过上面的实验发现 1、wireshark抓了很多很多很的包后，截图软件存在差距 2、微信截图依然顺序好用，但是PixPin截图异迟钝，慢的很咯，关闭wireshark后PinxPin截图反映速度恢复正常。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:27 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/5-Docker镜像制作优化和多阶段构建.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/5-Docker镜像制作优化和多阶段构建.html","title":"第5节 Docker镜像制作优化和多阶段构建","keywords":"","body":"第5节 Docker镜像制作优化和多阶段构建 镜像精简，便于迁移，因为后面都是K8S上跑容器，这些容器是可能在不同的宿主机之间迁移的。 写Dockerfile的时候应该尽可能合并层，就是合并行。 但是很多命令都看不全，可以加个选项查看,但是效果依然不行 优化1：变化的写在后面 Dockerfile里有变化的内容就是会频繁修改的，写到文件内容的后面 说明： 1、如果在Dockerfile内容的前面新增一个指令，也就是之前没有build过的分层指令，那么下面所有的分层都会重新build，不会利用CACHE的 重复build，也不是说重复build 然后这些CACHED就利用不起来了，感觉下面所有的分层就统统重做了，也不会说之前有过就能利用的。 这里面很难讲，但是也是逻辑清晰的，就是分几种情况 1、在Dockerfile的前面，新增一条RUN echo 123，那么下面什么ADD WORKDIR COPY这些就要重新执行，利用不到之前的CACHED了。 2、在Dockerfile的前面，删除RUN echo 123，那么下面的ADD WORKDIR COPY可以利用CACHED 3、在Dockerfile的前面，重新写上RUN echo 123，那么下面的ADD WORKID COPY可以利用CACHED 4、在Dockerfile的前面，将RUN echo 123改成RUN echo 122，那么下面的ADD WORKDIR COPY重新执行，利用不了CACHED。 所i有发生变化的内容，要写到Dockerfile的后面，否则就会导致build的时候很多之前的缓存无法利用。 优化2：COPY太多和遗漏 COPY * 和COPY .的区别 1、隐藏文件不管是*还是.都会复制进去 2、COPY * 的时候，文件夹，外层会剥离掉， # 无法理解的骚bug~ ​ 如果docker build目录里只有一个空dir1，那么就不会被COPY * / 复制进去； ​ 如果是dir1/f1，那么COPY * /就只会复制f1进去； ​ 如果dir1/f1,dir1/dir2/f2，那么就是COPY /就会复制f1和dir2/f2到容器里的/里，说白了就是进去的，而且是包含隐藏文件夹的。 针对COPY复制的文件太多了的问题 需要引入.dockerignore文件 .dockerignore忽略文件 .dockerignore里写的就是忽略的文件名，a.conf就是忽略，而!b.conf就是排除掉 不忽略的意思，也就是脱裤子放屁，其实就是强调一下咯。所以a.conf和b.conf就会被复制进去。 FROM补充 别名AS的作用，一般FROM写好就用一次就没了，就是build的时候，用别名无非是以后方便使用。 所以AS别名一定是有重复使用的场景，这个就是多阶段构建。 案例：多阶段构建 vim hello.go package main import \"fmt\" func main() { fmt.Println(\"hello,world\") } vim build.sh #!/bin/sh docker build -t go-hello:$1 . --------- vim Dockerfile FROM golang:1.xx-alpine COPY hello.go /opt WORKDIR /opt RUN go build hello.go CMD \"./hello\" -------------- bash build.sh v1.0 docker run --name hello go-hello:v1.0 现在宿主机上编译一遍看看 1、准备go语言的代码，呵 2、编译的工具有： ​ go是golang ​ c是gcc ​ java是javac 我是RockyLinux，就yum -y install golang就行了 编译后运行 然后制作image 找一个golang基础镜像，去hub.docker.com找到：golang:1.22.3 写Dockerfile来创建go的项目 1、原材料准备 👆注意：上图CMD没有用列表形式来写，实际上就是有一个默认的/bin/sh -c 在里面，而./hello此时是go语言不是shell，所以应该是执行不了的。需要改写为CMD [\"./hello\"]这样就没有/bin/sh的干扰了。不过后面测试发现/bin/sh -c 也行的... 2、build run 发现用的/bin/sh -c \"./hello\" 实际上是错误的cli，但是这种方式也可以，有点奇怪，为什么说它是错的，因为①理论上go就不能用/bin/sh去执行，②实际上下图所测就是不行 错误1 报错2 正确的 错误 错误 好像CMD里只能写一条CLI，多了就要用/bin/sh -c 'cli1;cli2;cl3'这种方式包装成一个来执行了。 ok了 ok了 ok了 多阶段构建正式开始-适合静态编译go 不适合java和python咯。 一个hello的go代码也就是1.9M，但是镜像就822MB大小，太夸张了。 所以如何进一步缩减镜像大小呢 FROM golang:1.22-alpine as builder COPY hello.go /opt/ WORKDIR /opt/ RUN go build hello.go FROM alpine:3.19.1 COPY --from=builder /opt/hello /opt/hello CMD [\"/opt/hello\"] 首先go是静态编译的，编译完了以后，就不依赖go编译环境了。就是RUN go build hello.go的go环境其实用完就不需要了。 所以就可以把基础镜像换一个更小的alpine或者busybox，加上之前编译的hello就可以运行了。 build一下 此时image就从851M缩小到9.27MB了~~ run一下ok 同样改成busybox来做二次构建 然后ok了 busybox的镜像会比alpine再少3MB左右，其实说白了就是go是静态编译的，哪个image小就用哪个就行了。 所以go的优势还是比较明显的 1、并发的优势，写出来就是并发的； 2、docker构建的优势，可以二次构建改成小的基础镜像，大大节省空间。 3、java、python、c都不行，这些都有大量的依赖库共享的动态的。 然后将上面的alpine或者busybox进一步优化，用scratch这个祖先镜像就是空镜像，因为go是静态编译，所有东西自带了 一个hello world的go编译后 用scrapt打包后也就是1.89MB，相当给力👇 补充docker images 查看的SIZE单位是MB，如何看到KB的精确值呢，用inspect 为什么go程序能够在scratch空镜像上跑，其原因有2：、 1、Go 是静态编译的：Go 编译器默认会将所有依赖项静态链接到可执行文件中，这意味着生成的二进制文件包含了运行所需的所有库。这样做的好处是可以确保可执行文件在任何环境中都能运行，而不需要依赖系统上预安装的库。 2、文件系统方面： 2.1、内核提供的文件系统支持： ​ 当容器启动时，它与宿主机的 Linux 内核共享同一个内核实例。Linux 内核本身提供了文件系统的支持，包括对文件操作、文件描述符管理等基本功能的支持。 ​ 容器的文件系统视图是通过 Linux 内核的 chroot 和 mount 等机制实现的，使得每个容器看起来像是有自己独立的文件系统。 2.2、容器镜像层： ​ 虽然 scratch 镜像是一个空镜像，但容器在运行时仍然有一个最基本的文件系统布局。这些文件系统布局由 Docker 容器运行时环境提供。 ​ 当你使用 COPY 指令将文件从构建阶段复制到 scratch 镜像时，这些文件被放置在容器的文件系统中，容器运行时将这些文件系统布局合并起来，使它们在容器内部可见。 2.3、构建阶段包含必要的文件： ​ 在你的 Dockerfile 中，你使用了 COPY --from=builder /opt/hello /opt/hello 将编译好的 Go 应用程序从构建阶段复制到最终的 scratch 镜像中。虽然 scratch 镜像本身是空的，但你复制进去的文件会成为容器文件系统的一部分。 ​ 因为 Go 应用程序是静态编译的，所以它不需要依赖额外的共享库或运行时环境，这使得它可以在 scratch 镜像中运行。 2.4、简而言之，scratch 镜像通过 Docker 提供的基础文件系统布局和内核支持，结合在构建阶段复制进去的必要文件，能够满足应用程序的文件系统需求。以下是简要的示意图： images如何查看当初FROM哪里的 docker inspect查看就行，不过要递归到最初的那个 docker history也行，不过同样从要找到最初的那个 scratch打出来的镜像ls没有如何查看文件 因为连ls这个命令都没有，如何查看呢 现在看不到了👆，以前可以看 然后一些案例可以补充到这里 1、基于Alpine的微服务Apollo配置中心 https://github.com/apolloconfig/apollo?tab=readme-ov-file git clone项目拉下来 编写Dockerfile FROM openjdk:8-jre-alpine3.9 RUN \\ echo \"http://mirrors.aliyun.com/alpine/v3.8/main\" > /etc/apk/repositories && \\ echo \"http://mirrors.aliyun.com/alpine/v3.8/community\" >> /etc/apk/repositories && \\ apk update upgrade && \\ apk add --no-cache procps curl bash tzdata && \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \\ echo \"Asia/Shanghai\" > /etc/timezone && \\ mkdir -p /apollo-config-server ADD apollo/ /apollo-config-server/ # apollo/.也行，apollo/*不行 ENV APOLLO_CONFIG_SERVICE_NAME=\"service-apollo-config-server.sre\" EXPOSE 8080 CMD [\"/apollo-config-server/scripts/build.sh\"] 过程排错：1 处理： 是宿主的句柄小于容器里的句柄了 这样就能跑下去了，时间比较长，明天继续 把宿主的ulimit的值改一下就继续run就行了，不过还得修改Dockerfile 安装openjdk8 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:27 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/6-Docker数据持久化和数据卷.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/6-Docker数据持久化和数据卷.html","title":"第6节 Docker数据持久化和数据卷","keywords":"","body":"第6节 Docker数据持久化和数据卷 之前VOLUME也可以实现数据持久化，就是容器删掉，数据还在宿主上放着，就是有点缺点，太乱了，存放没法自定义，或者要-v 命令行选项的方式来指定。 再来看下镜像和容器的分层数据存放 image的 容器的 首先容器run起来就是复制一份image 因为这个容器停掉了，所以数据释放掉了，running起来，才有merge才有复制image的那一份。 挂一个前台让他running 再看分层数据 这次就看到merged了👆 然后我将image和run起来的容器的分层目录 放一起看看 上图是容器里的meger复制了一份image里的diff，其实inspect容器可见LowerDir分层里直接写了image的diff分层， 容器生成的数据是放在diff下，比如日志、以及其他容器中生成的新的数据，这个diff就是可读可写层。 看下日志，应该就在UpperDir也就是diff里。不过我上面是tail -f /dev/null，没有日志输出，所以重新run一个有std的 然后去分层diff里看 但是显然是没有的，因为，没有落文件 落一个文件 现在diff里是空的 然后生成一个新的文件 然后UpperDir里就有了，于此同时merge作为合并层也就看得到了 这是容器起来后生成一个新的文件，如果是容器build的时候生成文件会出现在哪个目录里呢? 这里做了日志文件和生成了test文件，不过是build的时候就已经生成test了，看看 可见build阶段的文件是不会出现在UpperDir里的。而是出现在LowerDir里的diff里的 分层目录理解：结论来了 1、diff就是容器和镜像相比 diff出来的，就是多出来的 2、而UpperDir的diff就是容器的可写层 3、而LowerDIr的diff就是基于某个FROM基础镜像创建新的镜像的时候的底层。是可写的，因为不管是test还是access.log都是可以写的。 其实是不可写的，就是build的时候打进去的，后面可写都是UpperDir里的diff才是，LowerDirr的diff只是 区别于 FROM基础镜像的diff，同样也是只读层。 3、LowerDir里的每个路径之间用冒号分隔，代表一层层build的时候的封装。 4、merged就是联合文件整体的呈现。 下面开始学习持久化保存 1、session这些就是有状态的东西，没必要持久化。 2、mysql数据需要持久化，需要持久化。 持久化的的方法就是： 1、容器删除，/var/lib/doker/overlay2/xxxxxxxxx..xxx这个对应的文件夹就自动没了 2、对于重要的数据，单独存放到宿主的其他路径就行了。 具体的技术： 1、数据卷 2、数据集容器就是数据卷的变种，不常用 使用方法 docker run 命令的以下格式可以实现数据卷 -v, --volume=[host-src:]container-dest[:] ro 从容器内对此数据卷是只读，不写此项默认为可读可写 rw 从容器内对此数据卷可读可写，此为默认值 host-src 宿主机目录如果不存在，会自动创建 不管什么方法，肯定都不在/var/lib/docker/overlay2/下了，不会随容器删除而删除 方法1：手动指定宿主目录 #指定宿主机目录或文件格式： —v :[:ro] # 将宿主机目录挂载容器目录，两个目录都可以自动创建 方法2：卷ID # 匿名卷，只指定容器内路径，没有指定宿主机路径信息，宿主机自动生成/var/lib/docker/volumes//_data目录，并挂载至容器指定路径 -v # 示例： docker run --name nginx -v /etc/nginx nginx 方法3：卷名比卷ID识别度好，最通用的方式。 #命名卷将固定的存放在/var/lib/docker/volumes//_data -v : # 可以通过以下命令事先创建，如果没有事先创建卷名，docker run时也会自动创建。 # 里面外面都能自动创建文件夹，但是文件不行！ #示例： docker volume create vol1 # 也可以事先不创建，就是这句可以不写的。 docker run -d -p 80:80 --name -v vol1:/usr/share/nginx/html nginx docker rm 的-v选项可以删除容器的时候，同时删除相关联的匿名卷，只能是匿名卷 这种删除就会删掉①docker run -v 和Dockerfile里的VOLUME都是匿名，都会被删掉。 -v,--volumes Remove the volumes associated with the container 这是之前Dockerfile里的VOLUME也有类似的效果，好像就是方法2的效果一样。 这种就是随机字符串的目录名，没有可读性好的名字，所以一般就称之为匿名卷。没有名字 实验 把之前nginx的容器的Dockerfile来做挂载VOLUME实验 Dockerfile.son本次实验不要太关注，就是测试ONBUILD 指令的。 方法2 先跑一个不用方法3的，因为Dockerfile理由指定VOLUME所以也就是方法2的效果 先清一下none的镜像docker images -f \"Dangling=true\" 不run一下是没有VOLUME卷的是吧...应该是的 run起来看看 然后重点关注VOLUME 因为HOST变量没有值，所以文件也是空的👇 然后在宿主机上往里面写点东西玩玩哈哈 这不就有了吗，哈哈，其实应该去entrypoint.sh脚本里，给HOST赋值才对哦~~ 顺便也瞟一眼健康检查的log 方法1 ​ 先看下方法2的匿名卷(也就是VOLUME名称是随机一长串字符串)的效果 我想通过runlike看到VOLUME的值，看来build阶段的VOLUME，run阶段是看不到的，runlike就是看docker run的选项的。 ​ 只能通过inspect去看咯 可见这个VOLUME名称是不友好的，改之， docker run -d --name web2-run -P -v /dirtest/html:/dirtest/html -e DO C_ROOT=/dirtest/html/ -e HOST=ttttttt web1 👇图中-e DOC_ROOT 和-e HOST是传参，跳转页面转发路径，以及HOST这里赋值修复脚本里没给HOST赋值的情况。 上图👆curl的时候HOST要注意，已经被你改成tttttt了。 容器里面也是自动创建了这个目录 好了然后这个VOLUME也是自定义的就比较好维护了 方法3才是最棒的 docker run -d --name web3-run -P -v vol1:/dirtest/html -e DOC_ROOT=/dirtest/html/ -e HOST=www.test.tk web1 不要自己定义宿主的挂载目录，就用统一的/var/lib/docker/volumes/起个名字/_data/就行。 然后要注意就是Dockerfile里的VOLUME你也没删，docker run 的时候又-v vol1:/dirtest/html 所以就是创建了两个挂载卷了👆。 插播docker起不来排错思路 docker build ok，run的时候没有达到预期的排错 1、正向检查，Dockerfile是否ok 2、docker ps -a看看里面的信息，特别是CMD，不过看不全，没事用 由于又报错了 所以上面说的 还是改为截图吧 看的全 3、docker run去掉-d，不挂后台，run着看看 4、docker logs看看现象 5、docker exec 进去手动其服务或则CMD的命令，起不来无非是CMD那条问题，起来还有别的问题才是进一步的配置设置之类的，用ENTRYPOINT和CMD的组合来解释，通常是初始化脚本没初始化好等等。或者RUN里的的apk add漏安装包啦之类的。 下图说明如果挂载的宿主卷里有文件，然后Dockerfile里有COPY了同名这个文件进去，哪一个优先，宿主的优先会覆盖掉的，如果vol1里是空的，就不会覆盖同名的文件了👇 -v只能自动删除 匿名卷 ①docker run -v dir xxx 这种自动创建的匿名卷 ②Dockerfile里的VOLUME配置后run出来的匿名卷 查看卷 单独创建卷 方法1指定宿主机的目录就不是能在docker volume ls里看的到的了 看不到就不叫数据卷，方法1直接挂宿主机目录，这个目录就不是数据卷。 方法2，方法3，一个是匿名卷，一个是命名卷，都是有名字的数据卷。 只读，指的是容器里只读 容器里是只读 宿主里是可写的 此时容器里面就看到了 工作案例分享 linux如何拨v2ray 1、搭建桥接client，server利旧 1、安装docker https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 2、pull镜像 https://hub.docker.com/r/v2fly/v2fly-core/tags 3、run容器 https://github.com/v2fly/docker 修改配置文件为👇根据你server那边的进行配置就行。 不行，进去看， # 其实就是修改配置文件后没有重启容器 换下国内源 https://mirrors.tuna.tsinghua.edu.cn/help/alpine/ 安装curl测试，在容器里 然后正式实施的时候加上--restart always和安装runlike docker run -d --name v2ray --restart always -v vol1:/etc/v2ray -p 10086:1080 v2fly/v2fly-core run -c /etc/v2ray/config.json yum -y install python3-pip pip install runlike 2、搭建nginx反代，不需要搞错了 将sockt反指到本地监听端口 别搭了了，需求搞错了，目前正确的需求是将所有流入进来的流量，转发到本地的socks5:10086上去。 而不是再配置一个nginx去暴露一个端口 2、构建本地流量转发 1、使用ssh试试 不行，ssh也是要指定端口的 2、iptables 可行 3、redsocks 可行 明天继续 https://tttang.com/archive/1878/ https://www.cnblogs.com/zhenyuyaodidiao/p/5494569.html 而且要构建三层代理的转发，但是通过docker exec进去发现监听的是tcp/udp，也就是说只支持4层代理，就是socks代理。不是我需要的东西，该实验暂停~ 还是用softehter就行~ 然后如何设置全局代理，配置变量就行了，不管是本地还是给鄙人用 1、linux 2、windows 然后 浏览器可以代理，cmd不行，也就是说明了 socks不管是5还是4也无法代理cmd，全软件代理还得依靠APP本身指向socks。 cmd 里没看到proxy变量设置，所有没有走代理，手动配置一个，就像linux一样 cmd里还不用用socks变量，的用linux一样的socks5 概念梳理： 全协议：http的代理、socks4代理tcp、5就是tcp/udp都支持，这种需要APP自己走socks套接字才行的。默认浏览器就会走socks，但是cmd这些就需要手动配置代理变量为socks5，其他APPS同理。这是两个概念，全协议是隧道本身的属性，APPS走隧道时APPS自己的配置。 有一种就是无需APPS配置，直接所有软件 浏览器、cmd、等等 统统走隧道，就是全软件代理。这就要虚拟网卡将默认网关全部打到隧道里去： 全软件：最新的v2ray tun模式是由虚拟网卡的，电脑上所有软件的流量就虚拟网卡就是全软件代理了。包括cmd都代理了。 # 这种就是要虚拟网卡的。 全局：沟通要统一，我习惯全局就是这里，是全部流量都走代理，就是不分流的意思。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 10:42:16 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/7-Docker数据持久化和数据卷容器.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/7-Docker数据持久化和数据卷容器.html","title":"第7节 Docker数据持久化和数据卷容器","keywords":"","body":"第7节 Docker数据持久化和数据卷容器 用持久化做一下wordpress docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d --restart=always -v /data/mysql:/var/lib/mysql mariadb:11.3.2 docker run -d -p 8080:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php8.2-apache 也就是生成wp-config.php文件 👆上图localhost是错误的，前文就说过了，要写宿主机的IP。 总之，wordpress容器化就是 ①wordpress服务是一个容器，你这个配置界面是wordpress连接mysql的配置界面，其实是wordpress里面配置的localhost，所以是容器里的，而不是宿主的。要注意的。 ②mysql，又是另一个容器了 上面的数据配置提交后，实际上就是生成了wp_config.php文件 这个配置完后，mysq里的相关表信息就来了 然后登入进去后，新建文章，上传图片 就会在wordpress的目录下看到 一张图片会 根据 手机、平板、电脑，分辨率，来生成多张，将来好匹配不同屏幕。 然后发布文章 然后删除上面的两个容器，由于做了数据持久化，所以文章得以保存 删除容器docker rm -f xxxx， 如果你用docker -rm -v -f xxx，就要小心了，会不会-v一下子也就把持久化的数据也给删了，但是我们的这个实验不会，因为，-v只能删除匿名卷。 所以胆子不妨大一点~，追女孩子的时候脸皮不妨厚一点~，啥~ 跑题了~~~ 删除后，由于持久化的数据还在，所以docker run起来就还能看得见 然后重新run一下就恢复 检查页面已经恢复下 数据卷容器 就是用容器来实现数据持久化 再跑一个wordpress2 发现会自动将9090跳转到8080去 因为数据库没有发开 分开来一遍 这里又会涉及初始化mysql的配置 默认就是3306，也不知道我强行写成3406行不行 可以！wordpress初始化mysql可以带端口👆 你看这就一个宿主两个wordpress了 这是英文的还是👆 这是👇之前中文的 再来一个例子，两个容器共享数据的效果 用之前的自定义的image来弄 那么问题来了，过了这么久，我应该用哪个images去run呢，或者说我的这个image是怎么build出来的，用的哪个Dockerfile呢，对吧，简单，上次的研究就没有白费，👇 寻找熟悉的镜像的准备 ①inspect看看From的谁 ②hisotry看看build的过程 开始寻找 可见这几个镜像又276MB的还有48.3MB，看看为啥大小差这么多， 再看看 就这看到了为什么0.1版大在哪里👆然后用--no-truc展开看详细CLI 然后找找哪一个Dockerfile里又这个命令的👆 发现不再这个层级目录下 通过当前的Dockerfile的就会看到FROM alpine_3.19_self:v1.0 这就找到了👇 那么276MB的原因找到了，48.3MB为什么这么小呢 找到了 最后实验的时候，FROM的镜像改用了官方镜像了 也许可以这么验证，验证什么呢，就是 这个FROM，build出来了web001_entrypoint0.3 而web001_entrypoint:0.1是自定义的， run 进去看apk list 就知道了，比如如果是自定义的，那么apk list里是由iotop这个包的 好了，目前就找到这么个方法来定位自己大量的image不知道从哪个Dockerfile来的， 思路就是：找到Dockerfile里的特点，然后基于这个特点，比如这个Dockerfile里安装了某个与众不同的软件，就将对比的images分别run起来去看有没有，有就是FROM的这个，没有就是别的或者直接是官方的。 所以还是docker-compose好啊，哎，赶紧学到那里去看看有没有更好的解决方案~~ 下面继续实验，基于定位出来的这个镜像 然后这个镜像的Dockerfile和entrypoint看看 上图，但其实你会发现，TMD，这个其实web001_entrypoint:0.3当初build那会儿，entrypoint脚本里的 echo \"$HOST\" > ${DOC_ROOT}index.html 这一行是打开的，没有被注释掉的 实验思路： 两个容器共享一个持久化 有一个问题 命名卷和匿名卷，都是可以docker volume ls查到的，此时尽管docker ps -a也没了，目录也删掉了，但是docker volume ls可见，就会导致run的时候-v 指定命名卷就会报错 所以要用docker volume rm清理 结果不消息清掉了匿名卷，哈哈哈 重启一下就好了 下次记得删除volume不要用rm，要用docker volume rm 然后宿主机的路径，就是手动创建的目录，不算逻辑卷,，会自动创建，docker volume ls是看不到的 只要docker volume ls 看不到，就可以自动创建，并run的时候挂载过去 只要docker volume ls 看到 且 文件确实存在，就可以重复run 的时候挂载过去了 继续做2个容器挂载一个目录吧 第一个容器ok 第二个 也OK，而且是共用的一个命名卷mydata 数据卷容器的产生背景 问题来了 如果10个容器都挂同样的目录，比如👇 docker1 -v mydata1 -v mydata2 -v mydata3 docker2 -v mydata1 -v mydata2 -v mydata3 。。。 。。。 docker10 -v mydata1 -v mydata2 -v mydata3 就要写10遍 -v mydata1 -v mydata2 -v mydata3，配置太繁琐，于是就有了一下的解决方案数据卷容器 容器0正常使用-v -v -v去挂载，其他容器1 2 3 4 .. 参考容器0直接挂过去--就是说你容器0的持久化方案就是我们其他容器的持久化方案 这里所谓的容器0就是图中的Data container。 1、先创建一个服务器容器，这个服务器容器就正常-v -v -v 挂载好目录，不管是逻辑卷(①匿名卷②命名卷)，还是直接使用宿主的文件夹。 2、其他容器就好比client端，都是服务器容器的挂载方案进行复制挂载就行了。 上面两个图其实有一个理解误区，其实就是画图的不专业，是什么呢，就是箭头不是数据卷挂载的动作流，而是-v复制参考的动作流，你这个图搞的好像我删掉容器A或者前一张图的Data Container就会导致容器B C ；Container 1 2 3都断开连接了一样，其实不存在连接，不存在绕行，这个容器B C 绕行容器A的箭头只是复制了-v xx -v xxx -v xxx的配置而已，一旦复制过了，那么容器B C都是直接挂载访问到宿主机的。下面有截图实验证的。 实际配置很简单 docker run --volumes-from Mount volumes from the specified container(s) 实际配置开始-多个容器共享某部分数据的方案 1、配置数据卷容器，server啦，因为只是提供别的容器，来复制挂载目录的。所以就只需要写好-v就信了； docker run -d -v mydata:/data/website --name volume-server -e HOST='www.dalao.tk' web001_entrypoint:0.3 这里留了一个-e HOST=xxx来测试是否可以复制，和-v一样被其他容器复制。 而且容器都不要UP的，也不要用比较大的镜像去run，直接将上面的优化成 docker run -d -v mydata:/data/website --name volume-server -e HOST='www.dalao.tk' busybox 2、配置使用\"数据卷容器\"的容器，clients啦 docker run -d --volumes-from volume-server -p 81:80 --name web01 web001_entrypoint:0.3 docker run -d --volumes-from volume-server -p 82:80 --name web02 -e HOST=\"web02\" web001_entrypoint:0.3 docker run -d --volumes-from volume-server -p 83:80 --name web03 -e HOST=\"web03\" web001_entrypoint:0.3 看效果👇 但是要注意到上图的最后两行curl结果是一样的，别没有想当然的出现web02。理由很简单，就是当你上面敲了最后一行docker run的时候，此时就已经传参-e HOST=web03进去了，而entrypoint就是docker run的时候执行的。加上逻辑卷又是挂载同样的一个，所以最后一行的run就把外面逻辑卷里的Index.html的内容改成了web03。 所以此时下图这个的index.htm其实就是web03 然后server块里的东西依然还是web02的，所以才可以host:web01进行转发 以上就配置验证理解ok了 然后注意是复制的数据卷容器的-v，而不要傻傻理解为挂到某个容器里哦，所以★即使你把volume-server这个参考容器给删了，也不会影响其他容器，因为只是抄了它的配置而已。 任何一个有挂载西信息都可以作为别人的参考也就是数据卷容器--volumes-from xxx 补充：这些run的时候使用的镜像可以不同，只是--volumes-from一样使用一样的数据卷容器的-v挂载情况 上图注意： 1、一般就用命名卷就好啦，上图用的宿主机的目录挂载的 2、数据卷容器--就是用来被引用持久化容器，就不要run起来up的，也不要用大镜像busybox就不错；run完以后不删了就，当然删掉也没事，只要有一个容器引用了，后面就可以让别人引用它。 3、虽然宿主机的目录挂载方式docker volume ls看不到，所以一般就会说这个严格以上不叫逻辑卷，虽然不叫对吧，但是依然可以被--volumes-from来引用。 如果C1容器跑在A机器上，持久化自然也是做在A上，将来C1跑到B机器上运行，持久化的数据怎么同步到B呢，你说手动~~~，好的， 不过有更完善的解决方案-K8S，跨主机的方案。docker是单机。 多个容器共享某部分数据的方案的典型应用场景 nginx + php 以前是跑在一个宿主上的，现在容器化，如果跑在两个容器里(虽然一般是跑在一个容器)，所以可以这两个容器共享一个持久化目录。 nginx和php的程序 应该是共享一个目录，要在一起的，，我去复习之前的动静分离的文章 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:27 "},"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/8-Docker的默认网络和容器间通信.html":{"url":"1-容器技术和堡垒机JumpServer实战/3-Docker镜像制作及存储和网络管理/8-Docker的默认网络和容器间通信.html","title":"第8节 Docker的默认网络和容器间通信","keywords":"","body":"第8节 Docker的默认网络和容器间通信 网络简述 容器启动都会获取docker0一个网段的IP，默认都是172.17.0.0/16 同主机下不同容器互访，因为IP是动态获取的，存在变动的可能 不同主机下容器之间互访，首当其冲的就是IP段默认一样，出都出不来。听不懂啊，什么叫出不来，就是路由啊，同网段不会找GW，不走GW怎么出本地局域网呢。 启动一个容器，就会生成一个虚拟网卡，该网卡在宿主机表现为vethxxxx，在容器里表现为eth0#ifxxx; 一体两面 就这么理解就行了，官方称之为veth pair。 docker0相当于一个交换机，eth0--veth 都桥接上去。这样容器之间沟通，走docker0，容器出宿主的流量也走docker0不过还得过一层SNAT出去。 确认某个veth虚拟网卡所属容器 1、进入容器 看编号对应关系就行了，宿主上看9对应到容器的8，就是编号对应的。 也要注意这个veth-pair，一体两面的 MAC 是不一样的； 然后docker0和veth是桥接关系。 2、不进容器 但要注意：会导致 进入多层 bash，如果操作不当 所以这个方法 其实就是进入了命名空间了 其他工具看看docker0挂了几个容器或者几个veth 这个可以直观看到docker里挂的容器的NAME和容器里的IP和MAC，还不错👇 这个就是个排版看的舒服而已，ip a 看到的docker0自然是桥接到所有的vethxxx的。 同宿主容器之间的ping就是默认通的👇 解决容器IP不固定的问题 背景： 1、容器run起来了，IP不确定，run起来了IP是不是也会受dhcp的释放周期影响的咯。 2、如果你公司内网有172.17段，那么容器如果要和这个段互通，就不行了。 需求： 解决这个不固定的问题咯，然后就可以更好的自动化咯。以及更换容器的网段。 落实： 修改docker0的dhcp的地址段 方法1： vim /etc/docker/daemon.json { \"bip\": \"172.19.0.0/16\" # 这里写错啦，这写host和掩码而不是子网，改成0.1就行 } 改完以后，哪怕删掉重启电脑，这个ip就是你改后的样子了，不会恢复到出厂默认值的。 可能猜测是，考虑到以前已经有容器在了，不管是up还是exited的，哪怕你删除了，也给你留存上一次修改的记录。 如果你想恢复出厂IP，那么就手动写成172.17.0.1/16，重启docker，顶多在删掉配置...不过无所谓了。 方法2： vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --bip=172.19.100.0/24 方法2没有成功 桥接到自定义的网桥 默认是docker的内外网卡都是桥接到docker0这个虚拟网桥的，现在改掉。 1、首先创建网桥 yum -y install bridge-utils brctl addbr br0 ip a a 172.20.100.1/24 dev br0 brctl show 发现docker100还没有IP地址，然后docker0和docker100都是UP的。 2、修改服务启动文件 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -b docker 100 3、重载和重启 systemctl daemon-reload systemctl restart docker 4、查看确认 ps -ef |grep dockerd docker run --rm busybox hostname -i 再创建容器，就会桥接到docker100上了 在检查下容器里面是否可以ping通外面 可以的，通过抓docker100的包也可以看到 尝试抓容器的宿主虚拟接口的包 先定位这个容器id的内外IP是多少，当然我们通过上面抓包已经知道是172.20.100.2了。 所以这一次通过这个命令docker network inspect得知什么容器的里面的ip是什么后 再检查这个容器里的接口名称 如何知道exec 容器的接口名称 1、比如我要使用这个bd98e2b01c59容器来做抓包 就可以通过exec -it xxxx ip a 知道接口对，就知道抓外面的哪个接口的包了 2、然后还可以通过， docker network inspect 你要操作容器的id去确认待会抓包看到的内网IP是啥 3、然后开始tcpdump 那个接口 然后就看到包了 此时修了容器里的ip，也能够给通外面来，说明你修改了docker100为所有容器桥接的虚拟网桥，那么SNAT是自动给你做了的，无需配置的 可以通过iptables -vnL -t nat确认的 删除自定义的网桥 ip link down掉后，网卡信息还是少了很多东西的，上下两图docker1的信息都是全的。 此时再brctl delbr docker1就可以了 ip link set docker1 down brctl delbr docker1 使用nmcli创建网桥才能存得住 上面的btctl创建的网桥，重启就没了，这里改写为nmcli来实现 nmcli conn add type bridge ifname docker100 con-name docker100 nmcli conn modify docker100 ipv4.address 172.16.100.254/24 ipv4.method manual ipv4.gateway xxx ipv4.dns 192.168.10.2 由于之前创建过导致冲突 这个在/var/log/message里也看得到，需要删除/var/lib/docker/network里的db ​ 上图这个故障是：之前brctl创建了docker100，重启后docker100没了，由于docker100没了docker服务也就起不来了；此时我又用nmcli 创建了新的brige同样也叫docker100，我试图重启docker服务，结果就报了上图的错误，说明docker自己里面有一个docker100的名字的网桥了，一定是从之前的brctl的docker100对等过的，解决方法就是，rm -rf /var/lib/docker/netwokr/删掉这个网络的配置，其实里面有一个db文件的。 ​ 此时再重启docker就OK了。自此就完成了bridge的nmlcli的配置，以便重启不丢失。 同样改回去，删除 改回去， 再删掉bridge网卡 问题来了，此时再新建一个docker100，docker再改成-b docker100能否起来。预判起不来，理由如上，就是要删掉docker的network里的db。 其实理由就是，del可以删，但是此时docker服务不能重启，可以stop也可以不动它就让他active着，就是不能restart！，否则network数据在/var/lib/docker/network/files/local-kv.db里面产生；导致再次创建的docker100和里面的生成的docker100数据同名，但其实不是一个东西了。 解决这种冲突的方法就是： rm -rf /var/lib/docker/network/files/local-kv.db systemctl restart docker 但是有风险，就是docker的所有的网络信息就没了应该 删除虚拟网桥docker100的正确操作就是 systemctl stop docker nmcli conn del docker100 systemctl stop docker nmcli conn del docker100 nmcli conn add type bridge ifname docker100 con-name docker100 nmcli conn modify docker100 ipv4.address 172.16.100.254/24 ipv4.method manual nmcli conn up docker100 systemctl restart docker 这种操作网桥删掉再创建，docker的/var/lib/docker/network/下无残留，还会起得来的。 以下是一个完整的全过程演示截图 1、此时docker服务ok 2、直接del docker100 3、再补一个docker100回去 4、重启docker，是起的来的 故障点：docker100不在的情况下不能启动dockerf，否则docker会自动生成docker100的数据，导致你nmlic再次创建docker100的时候产生冲突。倒是起不来。 使用busybox来测试 通过help可见： 1、-f是要用的，用来保证容器里是前台，从而宿主上run -d 不会Exited 2、Home directory default . 表示 index.html要创建在运行httpd程序的工作目录就是当前文件夹 然后宿主curl看看，可见内容就是test index。 再用别的容器测试 OK 其实没有curl，用wget也行：wget模拟curl busybox的httpd -v就是带访问日志 但是问题是这个web的容器里的IP不是固定的，哇靠终于回到正题了，我哭哦~~~~ 解决容器IP不固定的问题 就是不用IP，用名字... 1、首先server运行的时候要带--name的，有名字就是固定的信息了 2、然后其他容器就--link 参数run就会自动写hosts吧，不过宿主怎么弄呢？ 实验开始 所谓server其实就是被访问的那一方了。 docker run -d --name web001 busybox httpd -v -f 显然还需要补一个端口暴露 再补一个index.html文件 这样就可以了 合成一个docker run试试，慢慢来嘛，就是玩，不要捉急实验本身，因为实验本身不代表你工作干活做事情的那个场景，你遇到的问题解决得越多，你就越发得蜜汁自省，哦是自省哦，就会更加能hold猪，加油~ docker run -d --name web001 -p 8080:80 busybox sh -c 'echo 12345 > index.html;httpd -v -f' link选项就是自动添加host了👇 添加了web001这个容器名 和 ip的解析。 从0开始计数的👆，所有就是丢了16个。 不知道为啥，我感觉这是坑。 初步定位了：就是使用默认的docker0不会卡这么久，而使用自建的docker100，我是用nmcli创建的这个网桥，就不行，ping就会卡30多个包才通这么久。。。 举例，wordpress的运行就可以采用link的方式了 docker run --name db001 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 docker run -d -p 80:80 --name wordpress --link db001 -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache 版本匹配注意下 👇图中MYSQL_PASSWORD写成了MSYQL_XXX，导致排查了半天。。。 换了个公司内部源，终于pull下来了 DB没有暴露端口，因为就是让wordpress访问的， 然后打开网页报错 因为之前实验有残留信息，删除之前的持久化路径，删除所有容器，重新run就好了 --link db001就是run 的时候就自动给你添加了 hosts解析，是db001这个主机名解析到db001这个容器name的IP。 这样就是实现了db容器里的IP如果变了，也不会影响业务了。而且mysql不对外暴露端口，你想攻击都攻击不了。 但是此时如果db001的容器里的IP地址手动改变，wordpress将无法访问。 但是如果db001的容器IP是DHCP重新获取而改变，则wordpress可以自动更新/etc/hosts文件，也就可以继续访问。所以一般不会有人手动👇修改，所以--link也就是可以解决容器里ip变动的问题的。 docker exec -it --privileged db001 sh ip a add 172.17.0.100/16 dev eth0 ip a del 172.17.0.2/16 dev eth0 此时wordpress无法连接db 而且只要容器1stop了，然后新开一个容器2就会抢了容器1的IP了 此时都不用重启wordpress，hosts文件就会自动更新，所以业务也一定是OK的（已测过了）。 容器别名 本质就是run的时候hosts里一个ip对应多个容器名称 实验过程，和别名使用场景不怎么相干了感觉，但是可以看到hosts的效果。 由于db001重新run，改名字，wordpress里的hosts文件还是写的db001，所以找不到了。 但是此时业务倒是正常的👇前提是db002要up的，虽然docker exec -it wordpress ls报错，但是业务出奇的正常 但是问题依然要处理，别名来解决 docker run --name db001 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 docker run -d -p 80:80 --name wordpress --link db001:\"db002 db003 db004\" -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache 使用场景： 1、程序里写死了是访问db004 2、容器run的时候就不一定非得要叫db004，可以叫db001， 3、然后run这个程序容器的时候可以使用 --lnk db001:db004，就可以将db004解析为db001的IP了。 大概可以这么用。 总结： 工作案例-linux的ping测 linux常ping脚本， 方法1： ping 10.100.8.29 | awk '{ print $0 \" \" strftime(\"%F %H:%M:%S\",systime()) }' >> test.ping 注意这种直接 >> 重定向到 文件，大概是60个ping包一起写入文件的。不是每个包写一个的，相当于默认的有一个I/O优化吧 方法2： ping 223.5.5.5 |& while read -r line;do echo \"$(date) $line\";done >> ping.log ping 223.5.5.5 |& while read -r line;do echo \"$(date) $line\" >> /tmp/123.ping;done 这种方式写入文件很及时，一个ping包就是一个。 时间格式优化下 ping 223.5.5.5 |& while read -r line;do echo \"$(date +%F_%H:%M:%S) $line\" >> /tmp/ping.test ;done 然后改成脚本，做成服务，可以实现异常停止后的服务自动起来，也包括重启机器自动起来。 做成服务 vim /etc/systemd/system/pingtest.service [Unit] Description=Ping Test Service After=network.target [Service] User=root Type=simple ExecStart=/path/to/pingtest.sh 8.8.8.8 Restart=always RestartSec=10 [Install] WantedBy=multi-user.target systemctl daemon-reload systemctl start pingtest.service systemctl enable pingtest.service systemctl status pingtest.service 测试kill 后自动起来 找到main pid kill -9会自动起来 kill 也会起来 stop纯人工停止自然不会起来咯，否则你怎么停服务~ 注意status里可以看到一些服务器启动日志的含时间。 系统message也可以看 以上就是简单的一个做法，或者不写服务用screen去做也不错， 不过正规还是要上监控的，比如用zabbix-agent去实现，这个前文也有。 案例2，如何做个远端节点的PING测监控 其实和斗数一样，也是看三方四正 举例，你要监控从上海到洛杉矶的云主机的ping测。就需要出5张图，也是一个三方四正 ①公司zabbix-----洛杉矶节点 ②proxy1代理----到洛杉矶节点 ③proxy2代理----到洛杉矶节点 ④公司zabbix----到proxy1节点 ⑤公司zabbix----到proxy2节点 看图 有了这个总结，以后做监控，基本上如果重要的节点，就可以一步到位全面监控住。 案例-ipsecvpn 云上的VPN的网关其实是PAT出来的，要IDC的SSG开启NAT-T来解封的。 https://cshihong.github.io/2019/04/17/IPSec%20VPN%E7%9A%84NAT%E7%A9%BF%E8%B6%8A-NAT-T-%E5%8E%9F%E7%90%86/ 文章中的这个传输模式同样不能转换端口的 还有一个问题就是rekey的问题， 理由：云上86400协商，云下28800协商，这是IKE阶段，也就是第一个阶段的周期，于是就会协商成28800也就是8小时，结果发现8小时就会断一次业务，排查发现必须是云下往云上ping一下或者发包来触发，否则要等15分钟左右才能重新拉起隧道。 因为，云上发起的rekey，云下ssg没有开启rekey功能，所以无法重协商SPA，所以就断了，解决方法有2： 1、云下做一个常ping放到screen后台运行就行了 2、最好是SSG上在ipsec阶段开启rekey，注意要和monitor一起，否则开不了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 10:43:02 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理.html","title":"第4篇 Docker网络管理和docker-compose编排和仓库管理","keywords":"","body":"第4篇 Docker网络管理和docker-compose编排和仓库管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:28 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/1-Docker四种网络模式.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/1-Docker四种网络模式.html","title":"第1节 Docker四种网络模式","keywords":"","body":"第1节 Docker四种网络模式 docker的5种网络模式 1、none：没有网络 2、bridge：桥接 3、container：容器 4、host：主机 5、network-name，这一种依赖于前4种 1、Bridge桥接模式 也是默认模式，类似Vmware的NAT模式 docker安装了默认给你优化了ip-forward=1，如果是k8s的containerd也许早期是没有的，可能就需要手动调整这些内核参数。 docker network ls docker inspect bridge # 可以查看网关 iptables -vnL -t nat # 配合docker port xxx 可见PREROUTING里的端口暴露 一些网段配置👇 vim /etc/docker/daemon.json { \"registry-mirrors\":[ \"https://docker-hub.iwgame.com\", \"https://ustc-edu-cn.mirror.aliyuncs.com\" ], \"hosts\":[\"tcp://0.0.0.0:2375\",\"fd://\"], # /lib/systemd/system/docker.service里需要删掉和这里的重复配置 \"bip\":\"192.168.100.1/24\", \"fixed-cidr\":\"192.168.100.128/30\", \"fixed-cidr-v6\":\"2001:db8::/64\", \"mtu\": 1500, \"default-gateway\":\"192.168.100.2\", \"default-gateway-v6\":\"2001:db8:abcd::89\", \"dns\":[\"1.1.1.1\",\"8.8.8.8\"] } 开始创建容器，顶多4个咯，因为/30嘛 第五个果然报错了 理解下两个GW 这个DefaultGatewayIPv4是默认网关，这里改成100.2，就不会走100.1了，要注意。 Host模式 类似于Vmware的桥接模式 容器里的网卡就是宿主机的网卡了，多个容器之间需要区分端口了。 也不存在端口暴露一说，因为直接就是宿主IP的端口了。 然后你想让容器性能非常好的时候，可以考虑使用host模式，因为没有NAT转发的步骤。 看看这个，完全就是宿主上所有网卡都看的到了 同样可以inspect看看，不过host这是宿主的网络，所以没啥可看的 None模式 none不是说一个网卡都没有，还是有一个lo口的 自己访问自己还是可以的 Container模式 实验开始 1、run一个正常的容器，也就是bridge 网络情况： 2、再run一个Container网络模式的，容器 不能说一摸一样，应该说就是同一个。能理解我这句话的意思吗O(∩_∩)O，一摸一样是两个东西 一样~，哈哈哈 然后，只是网络共用，其他文件都是隔离，也就是其他命名空间都是独立隔离的。 scp 才是基于ip的，docker cp是基于容器名的。 因为docker run --network container:c1 --name c2 这样c1和c2就是一个网卡，所以端口监听要错开，而且只能由被引用方c1来暴露。c2 c3这些引用方不能暴露端口的，run -P直接报错： 然后因为是共用的一个网卡，所以只要container:c1挂上后，c2也一样可以修改网卡的 c1修改网卡 此时c2容器里自然被改了 然后此时作用引用c1的c2创建可以影响c1，但是c2退出不会影响c1，c3引用c1同样之前c2创建的ip还在 如果c1这个被引用的源头没了（exit了，stop了）；注意即使start up了也没用了，c2 c3这些引用的也不会自动出现c1的接口的。 引用c1的其他容器的网卡也就没了 案例：wordpress 正常 wordpress和mysql，一般先启用mysql，后启用wordpress，如果反了，就要会wordpress网页打不开，等mysql后起来，才能打开。 所以是无所谓的，大不了等等就行了。 但是如果使用container模式的网络，就要先启用wordpress，我来试一下啊 docker run -d -p 80:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache docker run --name db001 --network container:wordpress -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 先运行wordpress： 然后就不知道填啥了都： 然后运行DB 因为运行的是--network contaner:wordpress，所以就是共用的wordpress容器的网卡，所以下图直接填写127.0.0.1就行了： 但是如果先启动db再启动wordpress就不行了 因为作为引用方，是不可以暴露端口的。 其实可以这样 这样wordpress和db在一个网卡上，虽然是两个容器。这样通信效率更高了。 如果遇到报错： 👆这是由之前实验数据导致的，它这个报错都是establishing，tcp连接了都，都是由db信息了。 以上就是docker网络的四种模式： closed就是none模式；就是单机不与外界沟通的模式。 bridege是默认模式；就是桥接到默认的比如docker0网桥上的，自然同一个做SNAT的。 joined就是container模式；就是jion别的容器的网卡。 opencontainer就是host模式，桥接道网卡的；host就是主机一个网段的意思，所以叫host。 有感： 用神之法：就是群星拱之，神冷静观之，不受群星直接影响，要过一个空白地（a space），这个空白地就是自由所在，也是成长所在。廉贞+武官入命的人一定要学会此法；否则容易被其强烈影响，该影响若被某个化忌的大运结合就很危险。或者某宫杀星落陷再结命主廉贞+武官，同样会很强烈。或者孤星坐命的人一样。 “Between stimulus and response there is a space. In that space is our power to choose our response. In our response lies our growth and our freedom.” — Often attributed to Steven Covey or Viktor E. Frankl 七杀临身的解法：就是无为而治，用习惯来实现成果，化为习惯也就是终身制的努力了。 关于无为：其实是润物细无声的意思，就是将行动化为细小的执行动作，正所谓日积跬步。 有句话叫：弱者道之用，就是这个道理：其意思就是要徐徐图之，不可用力过猛，弱者就是无为，就是慢慢来的意思，很多感情上、事业上、生活上的用力过猛的开始往往都没有能够达到最终的一个好的结局。无为不是真的不作为，而是表面上看起来没有特别大的动作，润物细无声，其实它一直都在，不曾放弃，只是以柔弱的方式存在，这样不会激发起反作用力。正所谓反者道之动，事物都是走向它的对立面的，你想要一个好的结果，只有弱者道之用才能不激发这个反向趋势。 地空用法：化煞为用，空灵之性，弱化群星对神的影响。要知道吉星也好杀星也罢，凶吉正如太极两仪，正如64卦我想高人应该选择的去用的。物尽其用也是这个道理吧。 神在哪：两眼之间其后，菩提树(大脑)下。 心神：心和神，正如心性和神，感性和理性的关系。心代表命盘和群星；神的强大就代表能否化盘为用，而不是被盘掌控。以神控盘，神清，神清则自在。以心牵神，神弱，神弱则堕入红尘欲望。但也要尊重心，自重就是尊重自己，不可被过度的理性变得僵硬固化，难....啊。 炼神之法：是不是如呼吸一样，长长久久，绵绵不绝，去感受其所在空间。 实操之法：当心里坠坠难安时，将心神聚集收敛，也就是将意念收集在两眼之间，菩提树下。也可以在心脏之处，但是菩提树下为佳。 大概是先看心，后看神的然后持续在神，能够体会热感。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:28 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/2-Docker的自定义网络和网络间通信.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/2-Docker的自定义网络和网络间通信.html","title":"第2节 Docker的自定义网络和网络间通信","keywords":"","body":"第2节 Docker的自定义网络和网络间通信 docker默认的网络环境，不管是什么模式都是自带的 哦，这个已经被我改掉了，还原一下 bridge就是对应docker0网卡所在的虚拟网桥 host就是对应eth0网卡所在的宿主机的真实网络 none就是不与外界通信的容器单机游戏了 这个docker network ls看到的三个网络(bridge、host、none)是默认自带一套的。如果不想用这套网络，可以自建。 自定义容器网络 不用原来的三个bridge、host、none，但是类型也就是DRIVER还是bridge、host、null [root@realserver2 ~]# docker network --help Usage: docker network COMMAND Manage networks Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. [root@realserver2 ~]# docker network create --help Usage: docker network create [OPTIONS] NETWORK Create a network Options: --attachable Enable manual container attachment --aux-address map Auxiliary IPv4 or IPv6 addresses used by Network driver (default map[]) --config-from string The network from which to copy the configuration --config-only Create a configuration only network -d, --driver string Driver to manage the Network (default \"bridge\") --gateway strings IPv4 or IPv6 Gateway for the master subnet --ingress Create swarm routing-mesh network --internal Restrict external access to the network --ip-range strings Allocate container ip from a sub-range --ipam-driver string IP Address Management Driver (default \"default\") --ipam-opt map Set IPAM driver specific options (default map[]) --ipv6 Enable IPv6 networking --label list Set metadata on a network -o, --opt map Set driver specific options (default map[]) --scope string Control the network's scope --subnet strings Subnet in CIDR format that represents a network segment [root@realserver2 ~]# docker network create -d --subnet --gateway # 注意mode不支持host和none，默认是bridge模式 -d 写了也白写，主要是有个其他swan不怎么用的技术模式在厘面。整体来说-d可以忽略。 docker network create -d bridge --subnet 172.27.0.0/16 --gateway 172.27.0.1 test-net 视频里老师用的是ubuntu的，可以table补齐看得到其他的network类型的 除了上面自定义的test-net类型，还有overlay、macvlan这些没见过的。 创建容器的时候使用新建的network，此时ip段就是172.27.0.0/16的了。 然后容器的网卡也是和物理网卡有一个eth-pair的就是一半在外面的， 里面是6：7，外面是7：6 同样这个eth-pair就不是桥接到docker0这个默认网桥了，而是自定义那个test-net 使用场景，10来个容器在独立的网段里，就可以用上面讲的自定义的虚拟网桥了。 然后容器里的IP确实是可以固定 自定义虚拟交换机直接ping容器名 还有一个：这种自定义网络可以直接ping容器名，存在自动解析的效果 使用默认的docker0就不行：什么不行，就是直接ping c1不会给你自动解析 只能使用--link来完善一下 所以在代码层面，解决容器IP不固定的问题 方法如下：3个方法都无需担心容器里的IP发生变化，都会自动更新，除非你人为进容器里修改IP。 1、使用自定义的虚拟交换机，ip可固定 2、是用自定义的虚拟交换机，无需IP固定，直接使用对方的容器名； 3、使用默认的docker0虚拟交换机，采用--link 来实现hosts解析 测试方法2的IP是否自动更新---要知道方法3--link c1的IP如果变了只要是dhcp的，机会自动更新的。 OK，没问题，就是自动更新的👇 还是用wordpress来实验，这次就使用自定义虚拟网桥来做吧 docker network create --subnet 172.27.0.0/16 --gateway 172.27.0.1 wordpress_net docker run -d --name db001 --net wordpress_net -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 docker run -d --name wordpress -p 80:80 --net wordpress_net -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache 看到这两个cli就要知道这个版本的适配要去wordpress的相关官方信息去找 https://github.com/WordPress/WordPress 实验记录 1、创建虚拟交换机 2、全部删掉，重新来一遍 由于是新建的虚拟交换机下的wordpress和db，所以数据库主机那项直接填写容器名db001就可以了，会自动解析的。 举例 如果容器里没法查看路由、网关信息，可以在外面通过Inspect容器去查看的 虚拟网桥之间的互通 1、默认就是不通的，因为有DOCKER-ISOLATION-STAGE-2这个隔离链 iptables的规则查看-S比较清晰 导出来再修改 iptables -S #只能看看 iptables-save > iptables.rule iptables-restore 添加ACCEPT 但这样就怕原来一些规则被抢了，这些规则如果仅仅是ACCEPT还好，或者是DROP你这里实验大不了就不DROP了，就怕是一些修改的动作被直接ACCEPT了。好像这里不涉及。 此时不同虚拟网桥下的容器就通了 但是这仅仅实验，不会这么修改docker自动生成的iptables 恢复iptables直接重启docker就行了 此时不通虚拟交换机下的容器就恢复到不通了，我们用另外一种方式来实现 docker network connect 虚拟交换机 容器 要让c1能够给和wordpress和db001互通，就把c1 连接到对方的虚拟交换机上。 本质上docker network connect 虚拟网络B 容器A 就是将容器A挂到虚拟网络B下，拿到了B网络的一个IP（当然会加一个接口）。所以此时容器A就可以ping通网络B下的容器了，因为在一个网段了。 当然hostname -i有时候也看不全 其实是hosts解析丢了 删掉重来看看 然后梳理一下bridge和docker0的对应，以及wordpress_net和ip a 上面的对应关系怎么查看找的 首先 ip a上关注docker0和br-xxxx这种格式就是代表虚拟网桥了 然后 这就找到了 同时inspect 网桥里还可以看到哪个容器挂载网桥里 然后看看正常情况hostsname -i 可以看到ip的，因为/etc/hosts里有解析就能看 然后故障就复现了👇 原来network connect和disconnect会破坏容器里的/etc/hosts文件里的解析 wordrpess的docker里太精简了，给他安装一下工具 https://mirrors.tuna.tsinghua.edu.cn/help/debian/ sed也行，这里直接cat > xxx 可见： hostname -i只是看看解析的地址，并不一定是真实的ip地址👆 hostname -I是准的 总之，你使用docker network connect / disconnect 要小心/etc/hosts的内容变化的，同时要小心不要踩到hostname -i的坑。 慢慢来吧👆 工作案例-openvpn容器化 1、准备材料 checkpsw.sh #!/bin/sh PASSFILE=\"/etc/ppp/chap-secrets\" LOG_FILE=\"/etc/openvpn/logs/openvpn-password.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \\\"${PASSFILE}\\\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $3;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\\\"${username}\\\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 chmod +x checkpsw.sh connect #!/bin/sh day=`date +%F` if [ -f /data/logs/openvpn/$day ] then echo \"`date '+%F %H:%M:%S'` User $common_name IP $trusted_ip is logged $1\" >>/data/logs/openvpn/$day else mkdir -p /data/logs/openvpn/ touch /data/logs/openvpn/$day echo \"`date '+%F %H:%M:%S'` User $common_name IP $trusted_ip is logged $1\" >>/data/logs/openvpn/$day fi chmod +x connect entrypoint.sh #!/bin/sh P_PATH=/etc/openvpn/easy-rsa W_PATH=/etc/openvpn sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk add openvpn easy-rsa mkdir -p $P_PATH cp -a /usr/share/easy-rsa/* $P_PATH cd /etc/openvpn/easy-rsa/ echo yes |./easyrsa init-pki cat > /etc/openvpn/easy-rsa/pki/vars > server.conf user-static-ip/user1@test.com chmod +x connect mkdir -p /etc/ppp cat >> /etc/ppp/chap-secrets > ${HOST_IP}.ovpn EOF sed -ir \"//r ${P_PATH}/pki/ca.crt\" ${HOST_IP}.ovpn sed -ir \"//r ${P_PATH}/pki/issued/client.crt\" ${HOST_IP}.ovpn sed -ir \"//r ${P_PATH}/pki/private/client.key\" ${HOST_IP}.ovpn sed -ir \"//r ${P_PATH}/ta.key\" ${HOST_IP}.ovpn mkdir -p /dev/net mknod /dev/net/tun c 10 200 chmod 666 /dev/net/tun exec \"$@\" chmod +x entrypoint.sh Dockerfile FROM alpine:3.20.0 LABEL maintainer=\"oneyearice \" ADD connect checkpsw.sh /etc/openvpn/ ADD /entrypoint.sh /entrypoint.sh ENTRYPOINT [\"/entrypoint.sh\"] CMD [\"/usr/sbin/openvpn\",\"--config\",\"/etc/openvpn/server.conf\"] 2、制作镜像 docker build -t openvpn:240606 . 3、启动容器 docker run的时候加上--cap-add NET_ADMIN docker run -d --name openvpnser --cap-add NET_ADMIN -p 1194:1194/udp openvpn:240606 然后可以通过logs查看日志 2048长度的时间较长 ./easyrsa gen-dh 耗时较长，会卡在Generating DH parameters, 2048 bit long safe prime 提示处，因为底层是：openssl dhparam -out /etc/openvpn/easy-rsa/pki/b03e75eb/temp.1.1 2048 能拨上去了，但是网络不通 这里有一个简单点的案例参考 https://blog.51cto.com/fengwan/1896431 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:28 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/3-Docker实现跨宿主机通信和Docker-compose介绍.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/3-Docker实现跨宿主机通信和Docker-compose介绍.html","title":"第3节 Docker实现跨宿主机通信和Docker-compose介绍","keywords":"","body":"第3节 Docker实现跨宿主机通信和Docker-compose介绍 跨宿主机通信 利用桥接实现跨宿主机的容器间通信 上图有个点：就是br0和eth0默认是没有桥接在一起的，是需要手动connect的。 上图的veth是画在br0上的，下图的veth是画在宿主机上的和eth0同层级的。两者画法存在冲突其实就是必然有一个是错误的理解。我TM都不用懂什么docker就能判断这里一定是错误的一个。通过查看网络文章以及参考GPT得知，下图画法是错误的，应该将veth画在docker0上才对。 恢复默认的网卡吧，什么docker100、wordpress_net网桥 都删掉吧 方法1：手动桥接二层打通 将eth0桥接到docker0 也是属于手动的方法，而且会带来eth0与外界通信中断。 1、制作两个测试的容器C1 和 C3 ，要求两个容器的IP不一样，要求两个容器分别在两个宿主机上。所以C1在192.168.126.132上，C3在192.168.126.133上，C2也在133上先于C3启动抢走172.17.0.2这个IP，专业昂C1就是172.17.0.2，C3就是172.17.0.3，ip区分开来了就。 2、把物理网卡桥接到br0也就是docker0 然后 132宿主上 brctl addif docker0 eth0 133宿主上 brctl addif docker0 eth0 此时C1和C3就通了 但是ssh却断了，初步原因就是因为物理网卡桥接到docker0上了。 估计eth0变成了docker0网络里的ip，docker0是172.17.0.0/16的子网，eth0是192.168.126.132的IP。目前没有找到解决的方法，继续吧。 但是这不是常规手段，仅仅是个测试。 方法2：手动打通3层 将两端的ip子网区分开来，走三层互通 这种方法也不太好，需要修改docker0的网段，需要配置路由--2个宿主就要互指路由，10个宿主你岂不是要full mesh就是90条路由了。 1、修改图中左侧172.17.0.0为172.27.0.0段先 记得重启docker服务 2、互指路由 这就通了 也不用管NAT的事情和iptables的规则，就直接通了。不会说被iptables给挡了，也不存在做什么DNAT。因为docker0的IP本身就是铺在宿主机物理网卡的那一层的。 然后iptables隔离链也是隔离的docker0访问非docker0这个方向的流量，而非docker0访问docker0的是不隔离的，简单来讲就是docker0这些虚拟交换机出去的拒绝，进来的放行的。 看图：docker0 --> !docker0 也即是从docker0里出去的才会命中，才会走下面的DROP，然后下面的DROP反而是进入docker0的拒绝来着。就是说连起来看就是docker0出去回来的包被DROP了。 所以本段的实验是外界直接进入docker0的流量未曾匹配到这个策略 ( 哪个策略呢，就是调用DOCKER-ISOLATION-STAGE-2的父策略--就是上面的红框的策略 ) 所以不会被DROP。 方法3：使用专业的第三方软件解决跨主机通信 主要是K8S里的组件：flannel、calico 也有脱离K8S单独使用的软件。 Docker Compose 本质上就是docker做了二次封装 比如这么一个场景：多个容器先后启动，每个容器还有很多启动参数，你说有runlike知道当时run了那些选项，但是runlike并不是十分好用，里面很多是默认无需care的选项也一并呈现了，然后多个容器的先后启动也无从得知。所以需要一个文本记录这些启动措施。 这些措施涉及：启动顺序、run的参数、这些参数就多了还涉及 网络、存储等。 compse就是解决方案，类似ansbile ansbile也分：①如果临时执行 就是一条条命令就完了；②如果稍微复杂的任务就不推荐cli的方式了，而是用playbook来写，完了用ansible-playbook调用就行了。 也好比iptables：①iptables 命令一条条加也是临时的；②也可以写到iptables配置文件里，然后统一重启服务就行，或者iptables-save > iptables.rules ; vim iptables.rules; iptables-restore docker也是如此：①docker 命令就是之前的docker run这些命令；②docker compose就是配置文件，类似剧本的功能。 为了实现在单机上的综合的复杂项目，于是产生了docker compose 类似Dockerfile一样，有一个默认文件名，compose的默认文件名就是docker-compose.yml或docker-compose.yaml，只要在当前工作目录下就不用指定。 下面学习如何些docker-compose.yml，本质上就是将docker run的命令写到compose里。 docker-compose不是独立的工具，底层是依赖docker的。 docker-compose就是docker命令的脚本化。 docker-compose是单机的解决方案。单机上跑多个容器需要有编排--限执行谁后执行谁。 https://docs.docker.com/compose/ https://docs.docker.com/compose/compose-file/07-volumes/ docker-compose应用场景：①单机咯②大规模还是K8S。主要还是比如办公网的一些服务：vpn、confluence jira、等就可以用独立的宿主+docker+docker-compose来时间，这样简单方便啊，比如公司的gitlab也可以这样玩，没必要说一定要K8S啊。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:28 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/4-Docker-compose单机编排工具安装和实战案例.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/4-Docker-compose单机编排工具安装和实战案例.html","title":"第4节 Docker-compose单机编排工具安装和实战案例","keywords":"","body":"第4节 Docker-compose单机编排工具安装和实战案例 docker-compose也是docker官方制作的 关键字编排，就是根据业务来管理，就是docker ps 看到的就是所有容器的陈列，不清楚所属的项目，但是docker compose就是业务层次清晰。也可以之启动某个业务的容器。 安装docker-compose 方法1： 方法2：直接yum ubuntu查看版本 yum安装的就是版本低 方法3：直接github下载 其实就是官网上走一波install步骤就行了啊 wget https://github.com/docker/compose/releases/download/v2.27.1/docker-compose-linux-x86_64 mv docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 👇可见确实是可执行文件 也没有任何依赖 因为完全就是go写的，静态编译的。 通过help可见docker-compose也和docker有些像的 但是这种👆副本的效果没啥意义，因为docker-compose是单机游戏，你再多的副本都是在一个机器上的，HA并不给力。 SERVERS也是要写在yml文件里的，这里是CLI格式咯，就好像ansbile的cli和ansible的playbook，就好像iptables的cli和iptalbes的配置文件。 写在yml里，然后docker-compose up就完了。 yml是按业务分文件夹存放的，根Dockerfile一个逻辑。 在这个yaml文件下去执行docker-compose up就会自动读取该yaml文件，就和Dockerfile的操作一样的。 也可叫docker-compose.yml，后缀yaml，yml都行。 docker-compose语法 和版本还有关系，需要注意；如果发现up不起来，比如之前的，比如别人的yml，你up不起来，可能就是版本问题，格式变了。 其实有工具可以将docker run的cli自动转成docker-compose的yaml的。 https://www.composerize.com/ 该网站也有cli的方式 https://linux.cn/article-14970-1.html composerize的cli 👆cli的方式，两个docker run 好像不会给你自动合并出来，可能要手动两次 自己合并，不如web上方便。 composerize的web ui 将之前的wordpress的两个run 改写成 一个 compose 不过这个网站只能转换docker run的cli。网络、创建存储，就不支持了。所以上图的wordpress_net这个虚拟交换机是没有的。 试试 mariadb写在前面就先执行的意思，虽然这里无所谓 name: services: mariadb: ports: - 3306:3306 environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=123456 container_name: mysql restart: always volumes: - /data/mysql:/var/lib/mysql image: mariadb:11.3.2 wordpress: ports: - 8080:80 container_name: wordpress volumes: - /data/wordpress:/var/www/html restart: always image: wordpress:php8.2-apache 然后正常设置wordpress就行了 不过可惜上面的compose是前台的 退出容器也就是推出了 不过start一下也就ok了 想后台，-d就行 不过没有定义网络，就会自动生成一个网络 这个自动生成的虚拟网络命名就是用 yaml文件里的name_default来起名字的。 正好👆就是一个业务一个网络，妥妥的~~但是不同业务就是不同项目之间就默认隔离了。 docker ps ，docker-compose ps就需要在yaml下才能看到 images在compose里看到的就是compose这条线索的images 总之docker images看的是全的，docker-compose images看的就是它自己的 而且还得在yaml文件的所在目录下，不同的yaml的路径看到的结果自然是不同的。 看本项目容器里的各自的进程 docker-compose up # 创建+启动 docker-compose down # 停止+删除 扩容和缩容，0就是删除 不过扩容有前提的， ①yaml文件里的container_name不要写 ②端口冲突也就起不来了 复杂一点的yml version: '3' services: db: image: mariadb:11.3.2 container_name: db restart: unless-stopped environment: - MYSQL_DATABASE=wordpress - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=wordpress - MYSQL_PASSWORD=123456 volumes: - dbdata:/var/lib/mysql networks: - wordpress-network wordpress: depends_on: - db image: wordpress:php8.2-apache container_name: wordpress restart: unless-stopped ports: - \"80:80\" environment: - WORDPRESS_DB_HOST=db:3306 - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=123456 - WORDPRESS_DB_NAME=wordpress volumes: - wordpress:/var/www/html networks: - wordpress-network volumes: wordpress: dbdata: networks: wordpress-network: driver: bridge ipam: config: - subnet: 172.30.0.0/16 由于yaml里wordpress里已经配置了DB的，打开网页，设置站点标题和管理员就进去了。 再来一个yml name: spug-001 services: db: image: mariadb:11.3.2 container_name: spug-db restart: always command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci volumes: - /data/spug/mysql:/var/lib/mysql environment: - MYSQL_DATABASE=spug - MYSQL_USER=spug - MYSQL_PASSWORD=spug.cc - MYSQL_ROOT_PASSWORD=spug.cc spug: image: openspug/spug-service #image: registry.aliyuncs.com/openspug/spug # 这是国内镜像 container_name: spug privileged: true restart: always volumes: - /data/spug/service:/data/spug - /data/spug/repos:/data/repos ports: - \"80:80\" environment: - SPUG_DOCKER_VERSION=v3.2.1 - MYSQL_DATABASE=spug - MYSQL_USER=spug - MYSQL_PASSWORD=spug.cc - MYSQL_HOST=db - MYSQL_PORT=3306 depends_on: - db docker-compose config -q # 语法检查 然后就docker-compose up -d，结果报错了 这里面有CF的CDN啊 emmm，改路由吧，从海外线路出去，或者image换成国内的：registry.aliyuncs.com/openspug/spug 好了👆 初始化登入密码 工作案例 简单加载小脚本 #!/bin/sh char=('\\' '|' '/' '—') while true;do for i in \"${char[@]}\";do echo -ne \"\\r$i\" sleep 0.2 done;done # 根据需要将while改成loading的实际进度执行时间就行了，不过要记得最后清除掉符号。 To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video 优化：使用斜杠转圈圈来表示加载的效果 #!/bin/sh char=('\\' '|' '/' '—') end=$((SECONDS+10)) echo \"转$end秒\" while [ $SECONDS -lt $end ] do for i in \"${char[@]}\" do echo -ne \"\\r$i\" sleep 0.2 done done echo -ne \"\\r\" # 清除最后的加载字符 优化2：使用贪吃蛇小点点来做加载的显示。 [root@realserver2 loading2]# cat loading_gpt.sh #!/bin/sh #char=('\\' '|' '/' '—') char=('⠋' '⠙' '⠹' '⠸' '⠼' '⠴' '⠦' '⠧' '⠇' '⠏') end=$((SECONDS+10)) #echo \"转$end秒\" while [ $SECONDS -lt $end ] do for i in \"${char[@]}\" do echo -ne \"\\r$i\" sleep 0.2 done done echo -ne \"\\r\" # 清除最后的加载字符 printf \"\\033[32m✔\\033[0m\\n\" [root@realserver2 loading2]# [root@realserver2 loading2]# [root@realserver2 loading2]# bash loading_gpt.sh ✔ 优化3：单任务转圈圈，汇总任务也准圈圈 先看几个shell基础 运算符里RANDOM可不加变量符号 kill和wait的是start_task 这个函数的pid，就是行首的加载贪吃蛇效果。 最终要这种效果👇 [root@realserver2 loading3]# cat loading.sh #!/bin/bash chars=('⠋' '⠙' '⠹' '⠸' '⠼' '⠴' '⠦' '⠧' '⠇' '⠏') tasks=(\"任务1\" \"任务2\" \"任务3\") # 开始任务 function start_task { local task_index=$1 local char_index=0 echo -ne \"\\n\\r\" while true; do echo -ne \"${chars[$char_index]} ${tasks[$task_index]} \\r\" char_index=$(( (char_index + 1) % ${#chars[@]} )) sleep 0.1 done } # 完成任务 function finish_task { local task_index=$1 echo -ne \"√ ${tasks[$task_index]}\" #echo -ne \"\\n\\r\" } # 定义更新进度汇总行的函数 update_progress() { echo -ne \"任务进度: \" for status in \"${task_status[@]}\"; do #echo -n \"$status \" #printf \"\\033[32m$status\\033[0m\" echo -ne \"\\033[32m$status\\033[0m\" done #echo -ne \"\\n\\r\" # 保持在第一行 } update_progress # 主程序 function main { for ((i=0; i /dev/null 2>&1 wait $task_pid finish_task $i # 顶行汇总 task_status[$((i))]=\"✔\" echo -ne \"\\033[$(( i + 1 ))A\\r\" # 移动光标到第一行第一列 update_progress echo -ne \"\\033[$(( i + 1 ))B\\r\" done echo } # 运行主程序 main 优化为带框框的 [root@realserver2 loading3]# cat loading002.sh #!/bin/bash chars=('⠋' '⠙' '⠹' '⠸' '⠼' '⠴' '⠦' '⠧' '⠇' '⠏') tasks=(\"任务1\" \"任务2\" \"任务3\") # 开始任务 function start_task { local task_index=$1 local char_index=0 echo -ne \"\\n\\r\" while true; do echo -ne \"${chars[$char_index]} ${tasks[$task_index]} \\r\" char_index=$(( (char_index + 1) % ${#chars[@]} )) sleep 0.1 done } # 完成任务 function finish_task { local task_index=$1 echo -ne \"√ ${tasks[$task_index]}\" #echo -ne \"\\n\\r\" } task_status=() # 定义更新进度汇总行的函数 proress_intial() { echo -ne \"任务进度: [\" for ((i=0; i /dev/null 2>&1 wait $task_pid finish_task $i # 顶行汇总 task_status[$((i))]=\"✔\" echo -ne \"\\033[$(( i + 1 ))A\\r\" # 移动光标到第一行第一列 update_progress echo -ne \"\\033[$(( i + 1 ))B\\r\" done echo } # 运行主程序 main 优化：任务进度汇总为，柱状增长形态 不会了.... 优化：并发状态 .... docker-compose up -d 这条cli如果敲击多遍之间，某个容器配置改变了，那么就会重建这个容器以及依赖于这个容器给i的容器。 对就是敲击怎么滴~ 如果修改docker-compose.yml，然后运行docker-compos up -d，只会影响修改的那个容器，也就是说只是重新创建修改以及受到修改影响的容器。举例 1、Harbor的容器都依赖log 所以只要修改了log，所有容器都会重建 修改log的监听端口 为0.0.0.0 因为所有都依赖log 所以所有都重建 所有都依赖log的证据 2、修改jobservice容器的target路径 改为变量测试，也就是仅仅重启了受影响的容器 同样修改变量的值，一样会重启该容器 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:28 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/5-Docker私有仓库Harbor的部署和上传下载镜像.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/5-Docker私有仓库Harbor的部署和上传下载镜像.html","title":"第5节 Docker私有仓库Harbor的部署和上传下载镜像","keywords":"","body":"第5节 Docker私有仓库Harbor的部署和上传下载镜像 简述 官方的hub.docker.com的上传 比如自己的一个image推上去 docker tag openvpn:v001 oneyearice/openvpn:v001 # 打标签就是hub.docker.com的账号/镜像+tag 1、注册账号 2、打标签，打的是仓库的ID和镜像信息，待会登入哪个仓库就用哪个仓库的ID。 3、登入指定仓库，不指定就是官方hub.docker.com 密码本地是存起来了，而且是文明的👇，就是base64格式的而已，可以转 果然echo xxx |base64 -d就转回来了 4、push 报错了，好像是两个账号冲突了，就是，oneyearice/这个路径，的用oneyearice这个账号来登入，结果上面用的是oneyearice@gmail.com登入的，这个账号的主目录是oneyeariceXXX是今天刚刚注册的，deactive这个账号就行了。 其实可以不必用hub.docker.com的账号密码，可以用专门的access token Harbor Vmware公司做的 https://goharbor.io/ https://goharbor.io/docs/2.11.0/ https://github.com/vmware/ https://github.com/vmware/harbor 组成： harbor是十几个容器组成的： 1、nginx：nginx做反代，这是一个容器；给后端各种服务提供反代 2、AdminServer：对应启动组件harbor-admin server，是系统的配置管理中心附带检查存储用量，ui和jobserver启动时需要加载adminserver配置。 3、UI：以UI为前端的core service：涉及UI、API、Auth，而Auth 4、API： 5、Auth：对接AD/LDAP 6、AD/LDAP 7、LogCollector 8、DB 9、ReplicationService 10、Basic Registry 官方提供了这些容器组件启动的配置文件，也就是docker-compolse的yml文件。 安装Harbor 1、安装docker 2、安装docker-comose 然后docker-compose又是依赖docker服务的，所以harbor的启动就需要docker+docker-compose 3、下载harbor离线包 4、解压 harbor内部所有功能组件都是用容器提供的，而容器的镜像都在harbor.v2.11.0.tar.gz这个包里了，这就是所有images的打包文件。相当于docker save tar tvf预览一下，可见都是分层镜像文件👇 然后理论上就是docker load导出images，但是人家提供了脚本给你了👇，脚本里面就有docker load 命令 这个还不是docker-compose.yml文件哦，后缀名也要改成yml👇，这个文件是初始化文件，将来跑完install.sh后，会生成docker-comose.yml文件的。 5、修改yml文件 修改部分内容👇 ①harbor的访问域名或者IP ②ssl证书可以关掉，实验用无所谓 ③默认的admin账号密码 其他不用修改 metric是Prometheus监控用👇 6、运行install.sh进行安装 早期需要安装python环境，现在python都是自带了，版本也不低。 加载镜像👇 其后就会创建docker-compose.yml文件，然后就会使用docker-compose up -d啦👇 可见proxy就是nginx啦，然后访问一下就OK了 关机下班~~ 开机发现harbor的这个几个容器就没有起来几个 原因可以看logs，但是看不出个所以然来 但查看docker-compose.yml里可见很多都是有依赖关系的，猜测原因如下 1、开机启动了docker，systemctl enable docker 2、docker起来了以后，这些容器就立马restart always了 但是有依赖的 看👆图，可以得出两点结论 ①十有八九是因为存在依赖启动关系，才导致一窝蜂的restart always没有启动得起来 ★ 看来光有restart always不够，还需要daemon.json里的live-strore true来辅助！ ②笔者具有深厚的sed技能理解能力，此乃通才的潜质~~~，解释下，这里的sed 里的分号的逻辑其实是存在两种动态的逻辑的：1、就是或者 2、就是并且，哈哈哈不相信？一个sed里的多个语句用分号隔开，笔者也就是本大爷竟然扯犊子说既有或者 也有 并且的逻辑，哈哈哈，不信你看 早在一年前就梳理出来了 再次验证说明 1、如上上图的sed取出docker-compose.yml里的关键词 container_name、restart、以及depends_on的子模块内容。 这个就是典型的或者 2、然后上图之前的sed专栏里，同样使用 来说明了此时 sed 里 多个;分号间隔的 或者 关系；是不等价于多个 管道符 递归的。 1和2就是 或者逻辑 存在于 挑选内容的时候 3、在删除内容的时候，分号代表的逻辑就是并且 4、所以sed 里的 多个动作用分号隔离，多个动作之间的关系 是并且，此时等价于多个grep；并且的逻辑存在于取东西 是或者，此时不等价于多个grep；或者的逻辑存在于去东西。 5、方法论：当你用sed 取东西的时候，一个sed里的分号之间的动作 就是 或者 当你用sed 去 东西的时候，一个sed里的分号之间的动作就 并且递进 怎么理解到生活中来呢，就是上图生活化的例子，这里再次梳理 一个A4纸，好比你要用sed处理的对象；你有两种动作， 第一种动作 从A4纸上 取两个小形状，此时就是 或者的关系 取出第一个小圆片(^_^) 作为结果是不可能被grep出一个三角形的，就好比你取出字母A，然后A里grepB出来是不可能的。 第二种动作 将A4纸 去掉 两个小圆片，此时就是 并且的关系 去掉一个小圆片的结果是背景的A4纸，此时自然可以交给grep再次去掉另一个小三角的。这就是并且递进的关系。 那就起个名字吧，就叫做蒙面侠效应~ 上面的GIF最终像不像眼罩，那不就是蒙面侠嘛~ 所以哥们下次和人吹牛逼，就会说sed有一个蒙面侠效应~，然后观察对方表情，酌情进一步解释，当然对方在努力听明白后会恍然大悟 原来如此，然后就忘记了蒙面侠这个起因词汇，当然脑子快的会问一句蒙面侠是什么鬼~~~ 好了继续Harbor 打开harbor管理页面 公开是指 下载的时候是否需要login。 上传怎么着都需要login的。 项目归属成员 然后用user1推一个镜像上来 域名是当初这里配置的 docker tag SOURCE_IMAGE[:TAG] harbor.oneyearice.org/project001/REPOSITORY[:TAG] docker push harbor.oneyearice.org/project001/REPOSITORY[:TAG] 直接推是推不上去的，不仅仅是因为没有login，而且默认是走的443👇 所以要login，还要解决默认走443的问题 1、解决443问题 写域名写IP都行 这个refused就是80端口实际上没了 重新up一下就对了 此时就是没有login的事了 就好了 此时网页上就看到了👆 发现镜像点进去一些操作都是灰的的 唉~~笨呐，勾选一下就好了 可以拉的 用复制的CLI就是一串编码 一样好用的 效果一样的👇，其实就是版本tag往往也就是哈希值来做一样的效果。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:28 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/6-Docker私有仓库Harbor高可用和双向复制.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/6-Docker私有仓库Harbor高可用和双向复制.html","title":"第6节 Docker私有仓库Harbor高可用和双向复制","keywords":"","body":"第6节 Docker私有仓库Harbor高可用和双向复制 Harbor的自动启动 前一篇发现 reboot宿主后，Harbor的9个吧 容器没有全部起得来，当时处理方式就是 1、docker的daemon.json里要有live-restore:true的配置 2、reboot后由于Harbor的docker-compose.yml里的除了log容器以外，其他都是有depends_on的，所以启动次序没有 这就是我要的效果了👆 到这可以判断下 1、从上往下启动来着 2、harhor-log 首先启动 3、registry，其次启动，但是log启动如果速度慢了，registry也许就启动不了了，因为查看docker-comose.yml里没有延迟设置 4、reboot看看哪些起不来 这次就一个jobservice没起来 再reboot 这次就一个log起来了，你看不稳定的 就是log启动慢了，依赖它的其他服务启动的时候log没起来呢，就gg了。 然后restart docker一下就基本都好了，这是因为有daemon.json里的live-restore和compose里的restart always结合的效果吧。 Harbor安装后启动问题 1、部分启动OK，不稳定 ​ 如上所述 2、数据存放默认定义在了/data下 ​ 不过openvpn的image只找到找个名称的文件，但是image文件大小对不上，可能还在其他地方 可以自定义harbor的数据持久化目录 然后运行./prepare脚本 就自定了harbor的数据卷的地方，但是需要重新docker-compose down / up了 而且新的数据持久化目录里是空的， 注意down up才行，restart不行 然后就发现之前上传的镜像没了 =============寻找harbor上传的image到底放在哪了👇====================== 原来的openvpn还在旧的路径里，当然图中的openpvn只是名称不是image image在registry里不过名字是hash值了，而且不太好找，比如我上传一个500MB的镜像，他会压缩到141MB👇 然后就是找找个文件到底在什么地方 关注layer层文件 通过搜索，du -sh *观察文件大小，发现141MB也被拆分成分层镜像了 但实际文件确不在这些路径里，在这里👇，上传的镜像整体是在regsitry大文件夹里面 找到一个 所以结论来了：你上传一个500MB的image，harbor首先给你拆分 分层，然后给你压缩分别存放。页面上看到的就是141MB的压缩后大小。传统观念是我上传一个镜像，我可以从后台也就是底层linux去找到这个镜像然后导出来，对不起这种方式harbor不支持了，因为image被拆分了，只能通过cli去pull下来了。 =============寻找harbor上传的image到底放在哪了👆====================== 改回去上传的镜像就回来了 可以看出来，down是从yml文件里 ，从下往上down的，up就是从上往下up的。 言归正传开启自启动咋解决 方法1，写个service文件 [root@harbor ~]#vim /lib/systemd/system/harbor.service [unit] Description=Harbor After=docker.service systemd-networkd.service systemd-resolved.service Requires=docker.service Documentation=http://github.com/vmware/harbor [Service] Type=simple Restart=on-failure RestartSec=5 ExecStart=/usr//bin/docker-compose -f /apps/harbor/docker-compose.yml up ExecStop=/usr/bin/docker-compose -f /apps/harbor/docker-compose.yml down [Install] WantedBy=multi-user.target [root@harbor ~]#systemctl daemon-reload [root@harbor ~]#systemctl enable harbor 如果将上面的up down改成start和stop反而不灵了--就是起不来。 reboot试试 方法2，写rc.loal咯 vim /etc/rc.local #! /bin/bash cd /apps/harbor /usr/bin/docker-compose up 保存退出👇修改执行权限 chmod +x /etc/rc.local Harbor的HA Harbor最关键还是镜像别丢了就行。 1、不太完美的共享存储 前面顶一个LB，比如nginx，lvs、httpproxy 后面是几个Harobr做实例， 但是数据不放在Harbor上了，单独放到后面，供几个Harbor共享存储。 数据用NAS，NFS来做；然后NFS要做好备份，比如rsync的实时备份...不知道会不会降低IO降低业务效率啊。 总之这个方案不太优秀~ 就是这个data_volume目录是远程挂载的 2、基于镜像复制 就是存储时单独的，但是互相复制的；类似mysql 双主A/A模型。 前端还是LB，比如nginx， 后面是几个Harbor做实例，数据也是跟着Harbor走的。不用共享数据存储。 第一种方案就是共享存储NFS NAS的搭建了，重点时第二种方案的学习 Harbor之间的数据存储的复制，其粒度更精细--就是多个项目，指定哪几个做HA，是可以这样的。 有些测试的可能就没必要做HA。 实验开始 搞第二台Harbor 当然监听端口2台都得改改，第一台页不用改，到时候LB那边顶前面就行了 1、docker 的配置同步下 记得重启服务 systemctl enable docker systemctl start docker 2、安装docker-compose wget https://github.com/docker/compose/releases/download/v2.27.1/docker-compose-linux-x86_64 mv docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 3、然后就开整harbor #下载 wget https://github.com/goharbor/harbor/releases/download/v2.11.0/harbor-offline-installer-v2.11.0.tgz #解压 tar xf harbor-offline-installer-v2.11.0.tgz #修改 cd harbor mv harbor.yml.tmpl harbor.yml vim harbor.yml 注释下👇： #https: # # https port for harbor, default is 443 # port: 443 # # The path of cert and key files for nginx # certificate: /your/certificate/path # private_key: /your/private/key/path # # enable strong ssl ciphers (default: false) # # strong_ssl_ciphers: false ... 修改 hostname: harbor.oneyearice.org # 据说是HA的时候两个Harbor必须用IP，我先不用 data_volume: /data/harbor ... ... #初始化 ./install.sh # 此时docker-compose ps就可以看到harbor已经起来了，web打开看看是ok的就行 admin/Harbor12345 默认账号密码 #写service实现开启和异常退出时自启动 [root@server ~]# cat /lib/systemd/system/harbor.service [unit] Description=Harbor After=docker.service systemd-networkd.service systemd-resolved.service Requires=docker.service Documentation=http://github.com/vmware/harbor [Service] Type=simple Restart=on-failure RestartSec=5 ExecStart=/usr/local/bin/docker-compose -f /apps/harbor/docker-compose.yml up ExecStop=/usr/local/bin/docker-compose -f /apps/harbor/docker-compose.yml down [Install] WantedBy=multi-user.target #加载 systemctl daemon-reload systemctl enable harbor docker-compose down # 先停掉之前的docker-compose手动up的服务，再统一使用s systemctl restart harbor # 再统一使用service来启动和停止，否则service还管不了之前手动启动的服务。 #这样就就ok了 4、配置HA 注意，可能需要将2台Harbor的初始化yml里的hostname改成IP地址，当然我觉得没必要，先不改。 如果改就需要./prepare后再systemctl restart harbor的。 现在有两台Harbor 一台192.168.126.132 ；一台192.168.126.133 1、先新建需要同步的项目：两边都要创建，最好同名 注意第二台上的项目名称少写了8，但是不要经，待会132直接可以把K888s连项目带镜像一起推过来👆 2、其次就要做两遍单向复制，实现双向 关键词：仓库管理--什么项目进行同步 、 复制管理--怎么个同步法 可见👆不仅仅支持harbor的同步，还支持从其他地方比如AWS，比如gitlab之间的同步。 也支持docker官方的registry同步--这个是指registry这个私有方案吧，还是说hub.docker.com啊，应该不是说hub.docker.com 其实我倒没希望有一个类似缓存代理的功能，就是从这个harbor下载的公共镜像都缓存，下次其他人下载也能够利用缓存。nexus倒是用的比较多。 目标名就是同步的对方Harbor的项目名 肯定写IP了，否则域名一样就出问题，不过我上面harbor.yml初始化的配置里 都用的一个harbor.oneyearice.org域名我觉得不碍事，所以没改，继续看看呗~~ 访问ID和密码，严格来讲，是对方仓库项目里的成员ID和密码来着，不过admin是都有了。 因为没有启动SSL，所以去掉勾选 报错了，回头去修改hostname为IP吧 两台除了IP不同，操作都来一遍👆 你看，192.168.126.133一改为IP，👇132去连接就好了，我估计这个hostname十有八九写到nginx的server块了，你写域名的结果就是你要用域名去访问。 查一下nginx的配置呗 没找到..... 纠错: 算了 两边都改成IP吧，就是说HA的时候两台harbor.yml里都要用IP 现在两遍就都OK了，👇图的项目名称不必事先存在，不过最好不要瞎折腾，这里仅仅顺便讲清楚而已。 好像SSL不去掉也行。。。 还是去掉吧，万一后面复制的时候验证SSL就麻烦了 3、继续配置怎么同步 也就是复制管理 新版是有pull和push的，老版的只有push 还是用push，推，用推可以做到实时同步，因为pull拉应该不能实时的去知道什么时候去拉。就是本地镜像一变就push过去，但是要实时拉，就得知道对端镜像的实时变化了不方便可能。理论上是可以让对端将镜像变化的信息发过来的，但是实际应用就不好说，所以还是朱永push吧。 如图👆可见pull拉是没有触发事件的，没法做到实时 简单配一下 此时已经实现了192.168.126.132--->192.168.126.133的单向复制。 测试下 ①再开一个机器192.168.126.134，并修改docker配置文件，加一下没有ssl的忽略配置 这里临时👆配置了两个Harbor的ip地址，后面加了nginx后就删掉2个ip，改成1个nginx的代理ip就行了。 ②两个Harbor都登入一下👇 PS：密码是明文存放的 推一个busybox上去 此时133那台harbor也就自动复制过去了 注意133上的K888S是事先不存在的。上文页提到了，而且log也看得到👇，就是仓库管理那里写啥就推了啥估计。 此时由于133没有push到132的 复制规则 ，所以 这个busybox:v002只会存在于133上，但是由于哈希值一样的，只是改了个tag，所以harbor给你合并了 再推一个alpine上到133去 然后将133-->132的反向复制给做了 然后发现此时132上没有立马出现alpine和busybox:v002的两个镜像。 应该是触发动作没有，所以133上push一个再 随便推一个存在都行 此时132就拿到133所有的多出来的镜像了，不对，还少了一个busybox:v002👇 再推一个nginx到133看看 好nginx已经复制到132了 再看看busybox:v002 还是没有 所以复制的事件触发规则是👇 push哪个image就复制哪个image的，不会说一个imageA触发所有的差异化的images 所以再推一次需要同步的那个image到133 此时133就会触发同步到133了 删除，在132上删除镜像 此时133上镜像的名称文件夹还在，但是里面的东西确实没了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:28 "},"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/7-Docker私有仓库的安全加密HTTPS实现.html":{"url":"1-容器技术和堡垒机JumpServer实战/4-Docker网络管理和docker-compose编排和仓库管理/7-Docker私有仓库的安全加密HTTPS实现.html","title":"第7节 Docker私有仓库的安全加密HTTPS实现","keywords":"","body":"第7节 Docker私有仓库的安全加密HTTPS实现 书接上文，再开一个135的IP机器作为 nginx反代 1、当前拓扑 132和133两个harbor的双向复制上一篇已经搞定，基本简单的实现了就。 下面134就是client访问135这个nginx。 2、docker跑一个nginx services: nginx: image: nginx:1.27.0 container_name: nginx_server ports: - \"80:80\" - \"443:443\" volumes: - ./nginx/conf.d:/etc/nginx/conf.d - ./nginx/html:/usr/share/nginx/html - ./nginx/logs:/var/log/nginx restart: always # 说明： # - 使用最新的 nginx 镜像。 # - 映射容器的 80 和 443 端口到主机。 # - 映射本地的 nginx 配置文件目录到容器的配置文件目录。 # - 映射本地的 html 文件目录到容器的 html 文件目录。 # - 映射本地的日志文件目录到容器的日志文件目录。 # - 设置容器重启策略为 always。 up一下 编写配置文件 同样做成服务 vim /lib/systemd/system/nginxd.service [unit] Description=nginxd After=docker.service systemd-networkd.service systemd-resolved.service Requires=docker.service [Service] Type=simple Restart=on-failure RestartSec=5 ExecStart=/usr/local/bin/docker-compose -f /apps/nginx/docker-compose.yml up ExecStop=/usr/local/bin/docker-compose -f /apps/nginx/docker-compose.yml down [Install] WantedBy=multi-user.target 此时就实现了反代了 但是此时登入有问题的，因为轮询调度，ID密码输入，再点就调度到另一台Harbor上去了。 验证就用同样的密码 分别登入两台Harbor就行了，不用测了没问题的，不放心可以测的。 就是nginx调度轮询，没有session导致的。估计连三次握手都握不起来，更遑论验证密码呢！ 所以改成session调度 1、会话绑定，也就是nginx的session调度：①基于cookie的(harbor的cookie是多少也不知道)②基于源IP的(PAT环境就不适合了) 2、session复制，tomcat里有这个功能可以做，harbor没有做不了。 3、redis，理论上可以，但是harbor不知道怎么配置redis，让harbor去redis读session。 下面基于session 1、cookie好像是sid👇 但是好像不能确定 所以还是用👇 2、源IP来做会话保持吧 ip_hash说是源IP，其实是源IP的/24，不是/32的 所以得用hash👇 应该是做成服务了，所以出现问题了， ①docker-compose管理的就用docker-compose来管理，不要用docker； ②做成服务的，就要用服务来start，restart，stop，不要用docker来弄。 systemctl restart nginxd 就好了 上传镜像要需要注意https 因为现在是nginx代理的，所以insecure-registries里要写nginx的地址或域名 再次登入OK 由于没做dns，直接改tag为IP👇，但是报错了 这是因为当初登入的是IP地址，不是域名👆，👇 这样，报错就和IP PUSH的时候一致了 报错写明：Entity Too Large 再次push ok HTTPS实现 HTTPS的SSL证书可以做在nginx上 也可以做在单机版的Harbor上 新版Harbor本身的SSL证书比老版的要复杂些 https://goharbor.io/docs/2.11.0/install-config/configure-https/ 以下是对上面链接内容的简要说明👇 1、CA的私钥 2、CA用私钥自签名 3、CA给他用户也就是server颁发证书 ​ 3.1 server的私钥 ​ 3.2 server的申请文件 ​ 3.3 CA给server颁发证书 cat > v3.ext 比老版多了一个文件👆，然哥们我一看，这不就是让下面PC浏览器显示安全的关键操作嘛，就是subectAltname就是我之前折腾的那东西啊👇 ​ 其实不是什么 比老板 多了了，而是时代变了(屎大便了)，浏览器要人为安全就得这么干，不仅仅是Harbor的新老版本之分，而是所有的都变了。 然后还有一个nginx的ssl生成的便捷脚本也是一样 两处地方都是用到了一个知识点👇 然后这里Harbor的ssl解决方案里自然跑不掉这个新的政策subject Alternative name的意义。 啊~~~~ 继续学习 继续说明https://goharbor.io/docs/2.11.0/install-config/configure-https/这里的内容 👇这个其实好像是无用功，理论上是转换个，其实就是内容都一样，就是改了个后缀名称 其实官方这么些，也是对的，①改后缀②用这种方式改后缀也是大而全的cli ①因为docker deamon会认为.crt是CA的，所以要改成.cert作为client证书，client就是CA的用户，而CA的用户也就是 PC--SERVER ，的server服务器证书了。上图client就指的是server 链接里同样有不完善的地方 这里点进去也不是很明朗，就是yml文件的ssl配置。 以下是实验 # 创建证书相关数据的目录 mkdir -p /data/harbor/certs cd /data/harbor/certs # 生成CA的私钥 openssl genrsa -out ca.key 4096 # 生成CA的自签名证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=ca.ming.org\" \\ -key ca.key \\ -out ca.crt # 生成harbor主机的私钥 openssl genrsa -out harbor.ming.org.key 4096 # 生成harbor主机的证书申请 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=harbor.ming.org\" \\ -key harbor.ming.org.key \\ -out harbor.ming.org.csr # 创建x509 v3扩展文件(时代变了新增内容) cat > v3.ext 然后 mkdir -p /data/harbor/certs/ cp harbor.ming.org.crt harbor.ming.org.key /data/harbor/certs/ vim /apps/harbor/harbor.yml ...... hostname: harbor.ming.org # 注意：此行必须是网站的域名，而且harbor主机的/etc/hosts可以不解析此域名，不能是IP地址，否则登入时会报如下错苏 Error response from daemon: Get \"https://harbor.ming.org/v2/\": Get \"https://192.168.126.132/service/token?account=admin&client_id=docker&offline_token=true&service=harbor-registry\": x509: cannot validate certificate for 192.168.126.132 because it doesn't contain any IP SANs # 而且 之前Harbor HA那会也对hostname有写法要求，好像写IP就得用IP去做HA。在👇这一篇里有提到 ，不过当时没有测试域名写个解析得效果可惜了：https://oneyearice.github.io/1-%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E5%92%8C%E5%A0%A1%E5%9E%92%E6%9C%BAJumpServer%E5%AE%9E%E6%88%98/4-Docker%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%92%8Cdocker-compose%E7%BC%96%E6%8E%92%E5%92%8C%E4%BB%93%E5%BA%93%E7%AE%A1%E7%90%86/6-Docker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93Harbor%E9%AB%98%E5%8F%AF%E7%94%A8%E5%92%8C%E5%8F%8C%E5%90%91%E5%A4%8D%E5%88%B6.html # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /data/harbor/certs/harbor.ming.org.crt private_key: /data/harbor/certs/harbor.ming.org.key ...... # 使上面的配置生效 cd /apps/harbor/ ./prepare docker-compose down -v docker-compose up -d 以下使过程截图 生成证书文件 1、生成ca的key，用这个key生成ca的自签名 # key就是私钥 看看时间到是否10年 2、生成用户的key，用这个key 生成csr证书申请文件 3、生成openssl v3 扩展文件 4、利用ca.crt和ca.key和用户的csr文件生成用户的证书文件，并生成serial字符串 这里上图少了一个换行符，导致CLI卡住不动 查看生成的用户证书，确认时限10年 将上面的证书文件配到Harbor上 就使用，用户key和crt文件。 然后重新启动harbor的dockers，也就是 docker-compose down -v docker-compose up -d 不过之前使写了service的 所以直接restart就行 慢慢就重新创建了👆 一定要做好数据卷的持久保存。 1分钟大概 访问就看到鸟 然后导入证书，试试域名访问，已经IP访问的效果 然后要测试是否可以上传下载镜像，因为开了ssl 然后还要测试HA，因为之前是用的IP还互指HA的，当时用域名还不行咧。 好一个个来 1、导入证书 双击导入 IP访问不行，换域名试试 发现 少了通配符，去试试，重新颁发crt，不麻烦，因为Harbor调用路径没发生变化，文件名称不会改变，# 但是要./prepare的因为crt文件名称没变，但是内容变了，容器不是链接而是内容的复制进去到容器里的。 重启harbor也就是docker-compose down 然后docker-compose up -d 发现PC浏览器还是老的证书 于是怀疑要./prepare一下， 果然 然后导出来，再导进去 不行 可惜了，先试试FQDN 继续改 删除老的证书 导出导入 FQDN写错了 改 重启harbor试试 导出导入，谁让你这么玩的，你可真牛逼，受信任的根证书颁发机构，这个是要导入CA的证书，FUCK 终于可以了 回到过去，通配符的设置 FUCK 然后修正DNS 所以结论来了 只要 证书主题背景的备用名称 里的通配符包含哪个就行了呢？ 👇这个无所谓，你和域名不一样无所谓， 所以结论 就是证书主体背景的备用名称 包含你访问的域名就行了，其他无所谓 然后就是要PC导入ca.crt CA的证书到受信任的根证书颁发机构 √ 最后修正为规范的统一的配置 # 创建证书相关数据的目录 mkdir -p /data/harbor/certs cd /data/harbor/certs # 生成CA的私钥 openssl genrsa -out ca.key 4096 # 生成CA的自签名证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=ca.ming.org\" \\ -key ca.key \\ -out ca.crt # 生成harbor主机的私钥 openssl genrsa -out harbor.ming.org.key 4096 # 生成harbor主机的证书申请 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=harbor.ming.org\" \\ -key harbor.ming.org.key \\ -out harbor.ming.org.csr # 创建x509 v3扩展文件(时代变了新增内容) cat > v3.ext 然后要测试是否可以上传下载镜像，因为开了ssl 然后还要测试HA，因为之前是用的IP还互指HA的，当时用域名还不行咧。 2、检查HA 由于132这台改成域名+ssl，果然 同样要 安装ca.crt吧 yum install -y ca-certificates # 一般就有 # 1√ 根证书到/etc/pki/ca-trust/source/anchors目录中远程就scp啦 cp ca.crt /etc/pki/ca-trust/source/anchors/ # 2√ 执行以下命令来更新 CA 证书存储，以便系统识别新的 CA 证书。 update-ca-trust # 如果根证书是以*.pem结尾，需要转换成crt，然后再执行上述步骤。命令如下： openssl x509 -in ca.pem -inform PEM -out ca.crt ============ # 你可以使用 openssl 命令来验证 CA 证书是否已经成功安装。运行以下命令并检查输出是否包含你刚才添加的 CA 证书。 # 这个也可以不做，直接用curl -L 域名 测试效果就行了，排查故障倒是分布检查需要的 openssl verify -CAfile /etc/pki/tls/certs/ca-bundle.crt your_cert.pem #确保系统信任新的 CA 证书：在安装和更新 CA 证书后，系统应该信任由这个 CA 证书签署的所有证书。可以使用以下命令测试： # 其实没必要，直接curl -L 域名就行了，就能测试了，这个👇是没有做上面步骤的时候测试的吧，不管了。 curl --cacert /etc/pki/tls/certs/ca-bundle.crt https://your-secured-site.com 测试 算了改回去，将132的harbor.yml里hostname改成IP地址，重新./prepare 再restart harbor 不对啊， 1、如果要做HA，那么就是nginx顶前面，就nginx自己做HTTPS就行了，HA之间就用IP。没毛病。 2、如果不做HA，那么就是单台，那么也就无需这里测试什么连接性了，对吧。 3、其实可以做到132 133 134 两台harbor自带ssl，一个nginx也带ssl的，就是hostname用IP就行了，具体如下👇测试OK： 试试看吧，改回IP ​ 然后去看132，harhor.yml里的hostname改为IP后的HTTPS是否OK吧。。 没问题啊 谁说一定要域名的 然后要测试是否可以上传下载镜像，因为开了ssl 3、测试上传下载image 目前上传ok，但也要知道没有走https上传 删除insecure-registries里的相关站点👆，让其走ssl。 这个被拒绝了，要登入一下看看 1、登入是要登入的，就是/root/.docker/config.json里有这个域名的信息 2、重启了docker，harbor服务估计也有问题，80没了 3、导入ca解决证书不信任问题，但是只是停留在curl层面就像PC打开浏览器的效果 4、即使curl层面判断安全了，但是docker push login 还是不认。 5、所以OS系统层面有系统层面的信任途径，docker有docker层面的信任途径 OS就是上面3里面操作的 docker层面的来搞一下👇 # 转换harbor的crt证书为cert后缀，docker识别crt文件为CA证书，cert为客户端证书，这里的客户端应该说的是CA颁发给客户端也就是server服务器的证书，而非docker的客户端。而且全文也只有ca证书和server的证书。 # 不过其实这里是仅仅只是改了后缀而已，不存在转换一说 openssl x509 -inform PEM -in harbor.ming.org.crt -out harbor.ming.org.cert # 所以仅仅等价于👇 cp -a harbor.ming.org.crt harbor.ming.org.cert # 不相信可以对比两文件 diff harbor.ming.org.crt harbor.ming.org.cert md5sum harbor.ming.org.crt harbor.ming.org.cert # 默认的配置文件路径里创建 # 如果是docker就👇 mkdir -pv /etc/docker/certs.d/harbor.ming.org/ # 如果四containerd就👇 mkdir -pv /etc/containerd/certs.d/harobr.ming.org/ # 在docker客户端使用上面的证书文件 # 注意，官方介绍还需要同时复制harbor.ming.org.key和ca.crt，实际不需要 cp harbor.ming.org.cert 或 harbor.ming.org.crt harbor.ming.org.key ca.crt /etc/docker/certs.d/harbor.ming.org/ # 注意，实际只需要复制一个文件即可 # ①实际操作1 cp harbor.ming.org.crt /etc/docker/cert.d/harbor.ming.org/ # 无需重启服务，docker客户端即可上传下载镜像 # 新版如果无法登入，就重启docker服务 # ②实际操作2 就行了 systemctl restart docker 确实转的没意义👆 就是要重启docker的，但是docker重启后，harbor估计就不正常了 harbor是做成服务的，状态倒是OK，但是监听端口80没了 不得已重启harbor吧 然后就看到一开始提到的报错了 [root@realserver2 harbor]# docker login -u admin -p Harbor12345 harbor.ming.org WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error response from daemon: Get \"https://harbor.ming.org/v2/\": Get \"https://192.168.126.132/service/token?account=admin&client_id=docker&offline_token=true&service=harbor-registry\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.126.132 because it doesn't contain any IP SANs 前文解决方法就是hostname写域名，但是这方法被我否掉了，因为HA没法做了。 当然你说做了HA，SSL就配置在nginx就行了，单机不做SSL。也行~ 单机就hostname改为域名，然后不做HA，SSL也就OK了。 但是我就要既做HA--hostname就得IP，然后就要ngnix和后面得nodes都做ssl呢，你说神经病。我呸 就是要hostname写IP，正面解决这个报错，这个报错的本意还是证书里的信息选项不全。 cannot validate certificate for 192.168.126.132 because it doesn't contain any IP SANs 因为这个服务器证书里没有IP SANs信息，什么叫IP SANs Subject Alternative Name (SAN) 这个熟悉吧，所以加一行就行了 [ alt_names ] IP.1 = 192.168.126.132 DNS.1 = *.ming.org DNS.2 = ming.org 将v3.ext优化为 authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=*.ming.org DNS.2=ming.org 重新颁发server的证书，服务器的证书啊，一些文章称之为客户端证书，其实是指CA的客户端，CA为其客户颁发的证书。哦，这是一个意思。 然后重新配置docker信任的ssl证书 再看最终效果 好了 搞定👆 搞定啥了？搞定了 1、HA 2、nginx的ssl证书不要讲了 3、HA环境下harbor.yml 的 hostname 必须写IP的情况下，每个单台harbor的ssl证书也做好了 4、操作逻辑如下 1、curl 的 ssl 信任路径和加载 2、docker的ssl信任路径和重启docker的必要性 3、重启docker对harbor的服务起来没影响，但是对可怜的监听端口有影响，需要重启harbor 4、harbor.yml这个初始化配置yml文件里hostname以后就记住必须写ip了 5、报错如果是refuse拒绝，一般可能就是监听端口没了 6、新版ssl要让浏览器也就是curl认可，需要SAN(subjectAltName)里有DNS1的域名， 7、新版ssl要让docker push的时候不报错IP SAN，就要让SAN(subjectAltName)里有IP1信息，就是配置为ssl服务器的IP就行了 差不多了，其他就是删改你乱七八糟但是又一个过程不拉的讨人厌的繁琐的操作记录了 tomcat那篇弄完再来👇 构建tomcat镜像运行jpress 1、ubuntu基础镜像: ubuntu22.04 2、jdk镜像: jdk-ubuntu22.04 3、tomcat镜像: tomcat-jdk-ubuntu22.04 4、最后一个jpress镜像也就是业务镜像: jpress-tomcat-jdk-ubuntu22.04 ​ 1 2 3 就是都属于基础镜像，4就是业务镜像会存在改动，所以变动就做4步就行了。 5、然后利用多阶段构建新开一个ubuntu然后二次构建，这样整体镜像就会小很多。如果是go不是java就直接用busybox来构建甚至用scratch空基础镜像来弄。 这步感觉要考虑插入上面哪一步里面，最好是第四步了。 # 就是能够脱离原环境到一个新的基础环境里运行得情况下就可以进行多阶段构建。还需要打几个常会用得检查工具 如 ping curl 7、上传到harbor上 8、从另外一台主机下载 docker run jpress 上传的jdk源码包，可以删掉，或者直接用ADD解压缩进去，但是还是有过程文件残留还是会占用空间，不一定能释放得掉，即使删掉一些，镜像可能也不会缩减。 案例 openssh升级高版本 以下未验证👇，思路应该使对的 用telnet上去编译安装openssl openssl升级 直接换rocky9 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:29 "},"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer.html":{"url":"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer.html","title":"第5篇 Docker资源限制和堡垒机JumpServer","keywords":"","body":"第5篇 Docker资源限制和堡垒机JumpServer jump001 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 13:05:28 "},"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/1-Docker基于Cgroup实现内存和CPU资源限制.html":{"url":"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/1-Docker基于Cgroup实现内存和CPU资源限制.html","title":"第1节 Docker基于Cgroup实现内存和CPU资源限制","keywords":"","body":"第1节 Docker基于Cgroup实现内存和CPU资源限制 传统限制手段PAM PAM里的limit模块 可以控制某一个用户的资源使用 1、控制CPU运行时间 # cpu 2、控制使用的进程个数 # nproc 3、打开文件个数 # nofile 4、物理内存 # rss docker种的限制方式 java服务很可能会占用较大的资源 tomcat里梳理举例不做限制oom的情况 涉及堆内存 早期docker版本有一个swap开启的告警，现在没了 WARNING: No swap limit support # 没有启用swap 限制功能会出现次提示报警，这是早期的docker才会 不过即使是老版本这个告警也没啥，不处理也没关系。仅仅是一个warning而已。 看一下oom_socre_adj的内容 虽然可以手动改，但是通过修改为-1000来实现进程保护--防止被杀掉，这种操作也是比较少见的。而且这个文件的存放路径是PID的，举例nginx的PID你每次启动的PID也是不同的，也就是说不是一个路径存放了的。实际操作也不是靠谱的。而且这只是防止这个PID被杀掉，并不是说限制资源的使用的角度出发的。 oom_adj就是oom_score_adj的老版本，并且自动兼容的一个文件，可以不用管 而oom_socre是系统给你算出来的值，不可调 压力测试Stress-ng https://wiki.ubuntu.com/Kernel/Reference/stress-ng 安装也有docker版 https://hub.docker.com/r/polinux/stress-ng 可以用来判断docker是否限制住了某个容器应用的内存；或者看其是否会OOM。 yum 安装下 stress-ng -h查看 stress-ng --cpu 8 --iomix 4 --vm 2 --vm-bytes 128M --fork 4 --timeout 10s # 开启进程，占8个cpu，占用4个io，开2个vm就是256*2=512MB内存，但是vm-bytes=128，所以还是256MB内存的占用，--fork就是开启子进程4个，timeout就是压力测试时间是10s。 测试下 CPU 估计就是每个CPU做大量运算，比如加解密之类的计算 结果就是1颗CPU的占用 就是占用了4个CPU 内存 内存就是看RSS物理内存就行，VIRT是进程跟OS申请的内存--虚拟内存，不是实际占用的RSS内存。 确实是256MB，但是CPU也是跟着占用了1颗了，差不多占用0.7的使用率 开3个vm就是256*3=768MB的内存占用，不过同样会消耗3个CPU 开3个vm就是256*3=768MB的内存占用，不过同样会消耗3个CPU 下面开始测试容器 测试是否能够做到docker限制资源的使用 思路：就是docker run 的时候 ①限制资源；②同时用stree-ng撑爆资源，③观察结果如果没有撑爆说明就限制住了。 docker pull polinux/stress-ng # 一般关注的是CPU和内存的资源占用，以及磁盘IO 确实是512MB内存占用👆 也可以通过docker stats xxx来看，不过就是动态的 目前由于docker run 的时候没有限制内存的使用，所以上图👆LIMIT里显示的就是物理内存的最大值3.6GiB。 docker 限制资源 Docker可以强制执行硬性内存限制，即只允许容器使用给定的内存大小。 Docker也可以执行非硬性内存限制，即容器可以使用尽可能多的内存，除非内核检测主机上的内存不够用了。 https://docs.docker.com/config/containers/resource_constraints/ -m 和 --oom-kill-disable 一起使用的场景就是：这个容器很重要不能被oom杀掉，所以自身限制内存使用的同时，也不要让别人杀掉自己。但是不能单独使用--oom-kill-disable这样会造成自己无节制的使用内存，然后把系统宿主的进程杀掉从而导致业务同样出问题。 swap就别研究了，k8s好像也不推荐使用swap的，swap这种就别用了。 ​ 限制内存 docker rum --name c1 -m 100m polinux/stress-ng -m 2 # 限制内存100MB，但是容器里是开到了512MB 以前版本可能存在这么一个文件 这个值就是限制的内存的大小 限制CPU --cpu period喝--cpu-quota都不用了。 测试1-分配CPU个数 docker run --rm --name c2 --cpus 1.5 polinux/stress-ng -c 2 # 压测开到2个cpu，但是限制--cpus指定为1.5 这样限制0.5个CPU，即使压测给到2个CPU，也不会涨上去。 而且这个0.5个CPU的占用不是说就盯着一个CPU干的，而是均摊到所有CPU上的 测试2-容器之间按比例分担CPU docker run --rm --name c2 -c 1024 polinux/stress-ng -c 4 docker run --rm --name c3 -c 1024 polinux/stress-ng -c 4 # c2 c3两个容器都压测开到4个CPU，然后run的时候给到1024的值也就是1024/2048的占比使用 然后输入docker stats看所有的容器的占用情况 再开个容器同样也是1024的占比 此时差不多300%的占比就变成均分了 再开一个2048占比 此时2048就占了一般，其他两个1024就各占剩下一半的一半 比例是一目了然的，但是压测-c 4 明明是4个CPU，但是实际是3个CPU的使用情况--从第一张图上可见不超过300%其实。 测试3-CPU绑定 查看CPU绑定关系 ps axo pid,cmd,psr 开了三个压测，发现CPU没有固定在某一颗上，是会飘的 为什么明明开了3个压测容器，为什么这么多个，因为是父子进程 父进程确实就是3个，没错👆 docker run -d --name c1 --cpuset-cpus 3 -c 1024 polinux/stress-ng -c 4 # 绑定到第四块CPU上 下图选中的父子进程是多出来的，也就是c1压测容器的进程👇 这些就固定绑定到3号CPU了 一般限制也就是上面几个资源 内存、cpu的各种参数 docker run --cpuset-cpus 3 -c 1024 --cpus 1.5 -m 100m # cpu绑定、cpu分配比例、cpu分配个数，内存 超分超卖 cpu可以 然后CPU是可以多个容器绑一颗的，属于可压缩资源 内存不行 第一种oom-宿主卡爆 内存超分就会产生oom咯 docker stats 观察如果oom就会 此时宿主ssh操作就会卡顿 第二种oom-宿主没事 人工计算好了再分配，此时宿主不会卡，但是容器里受限，一样会有oom 这种分配没问题，但是也要知道stress-ng -m 10就是256*10*4=10G的内存压下去了，但是你有限制了 200m*4=800m的内存才，所以stress-ng这个容器里会认为oom了 所以在console控制台里同样会看到oom-kill的 但是容器日志里是看不到的 还是人为限定就可以了 这样就stress-ng容器压测也不会有oom了 工作案例 1、自有机房入伏前一个月 最好准备好空调故障应急措施 ①是否在保 ②在保内的维修、投诉电话、上门点在哪、维修师傅电话、厂家调货流程周期--以为配件调货AUX是师傅直接联系厂家，总台都看不到的 ③第三方上门30块起步，KFR-120W这一个压缩机主控板就要1000多，一天给你修好但是费用要1600大洋。 ④基于维护等待配件的漫长时间里，机房温度过高如何处理 ​ 小机房，先上5 6 个电风扇(这招其实真不错，对着机柜吹就能降10度)，但是电风扇直吹的区域，周围会被吹出来的热气导致气温上去，大概上去个3度。(￣▽￣)\" ​ 然后L3的空调可以关掉了，开也行，就是开30分钟就必须关掉否则就开始吹热风了，温度比室内环境还高！ ​ 买些冰块放机房里吧 ​ 打开机房门，用空调导风管，将大厅里的空调风导进机房里。 2、抱脸模型下载 1、首先你的能够从浏览器直接下载 2、然后复制连接到迅雷，这样就可以利用迅雷从最佳的源下载了，速度噶快 3、对比哈希值，防止被种码 3、谷歌企业邮箱 workspace里的第二档收费才支持群发的功能，同样支持群发单显 1、首先你的有一个域名，推荐godaddy 2、然后就大致按如下走一遍就行了，结合下一步操作操作 https://gnfob.com/google-email/ 3、邮件的跟踪，比如收件人的已读于否，是要插件来支持的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:29 "},"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/2-JumpServer介绍和部署.html":{"url":"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/2-JumpServer介绍和部署.html","title":"第2节 JumpServer介绍和部署","keywords":"","body":"第2节 JumpServer介绍和部署 1、概述 看看企业版，然后就弄社区版吧~~O(∩_∩)O 2、实验-直接用docker部署 选择有很多，分开来各个组件也有独立的容器化，这里选择单机直接安装，就用jms-all这个容器 安装方法1：一键安装 https://docs.jumpserver.org/zh/v4/quick_start/ 官方一键安装脚本也是容器安装的 其中2222端口是CLI窗口界面👇，登入需要先页面使用默认密码登入下也就是改一下密码，才能从cli登入。 一键安装也可以维护的 启动停止升级cli在这里👇 安装方法2：手动分步安装 意义就是，学会分步安装，听不懂？即使jumpserver可以不用这种分步安装，其他的服务可能也会用的，其他服务即使用不到，分步安装过程中的问题解决也会出现在其他使用场景里，这是必然的。所以学东西 眼睛放亮一点，格局打开~ 基于容器部署 1、docker环境安装 2、mysql 3、redis 4、jumpserver docker安装 参考前文，这里写ubuntu意思下 apt update && apt -y install docker.io systemctl restart docker mysql的安装 1、版本现在可能不讲究了，之前是高版本的mysql，jumpserver不适配 ​ jumpserver-v2.28.7之前版本不支持mysql8.0需要选择mysql5.7。 2、配置文件要改 ​ 默认字符集要改，但是镜像是官方做的，就需要自定义配置文件了。具体在容器里的配置文件里添加，characer-set-server=utf8 # 好歹用utf8mb4啊 # 所以要做持久化 mkdir -p /etc/mysql/mysql.conf.d/ mkdir -p /etc/mysql/conf.d/ # 生成服务器配置文件，指定字符集 tee /etc/mysql/mysql.conf.d/mysqld.conf 安装redis 启动 docker run -d -p 6379:6379 --name redis --restart always redis:7.2.5 连接 yum -y install redis redis-cli -h a.b.c.d 进入redis交互界面 a.b.c.d>info 回车后显示👇 # server redis_version:x.x.x redis_git_sha1:000000 redis_git_dirty:0 部署jumpserver 需要先生成key和token 迁移和更新升级就要检测SECRET_KEY是否与之前设置一致，不能随机生成，否则数据库所有加密的字段均无法解密。BOOTSTRAP_TOKEN也一样 https://docs.jumpserver.org/zh/v3/installation/migration/?h=secret_key # key.sh文件内容 #!/bin/bash if [ ! \"$SECRET_KEY\" ];then SECRET_KEY=`cat /dev/urandom |tr -dc A-Za-z0-9 | head -c 50` echo \"SECRET_KEY=$SECRET_KEY\" >> ~/.bashrc; echo SECRET_KEY=$SECRET_KEY; else echo SECRET_KEY=$SECRET_KEY; fi if [ ! \"$BOOTSTRAP_TOKEN\" ];then BOOTSTRAP_TOKEN=`cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 16`; echo \"BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN\" >> ~/.bashrc; echo BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN; else echo BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN; fi 然后运行 bash key.sh 最后检测 tail -n2 .bashrc #获取两个变量的值 SECRET_KEY=DiShVgRRBMEqM7kJ41ArYdvfVQ216amp3FUVxiHdxJNxTObkNU BOOTSTRAP_TOKEN=QZ54bEJphGpRhP1R docker run --name jms_all -d --restart always \\ -v /opt/jumpserver/core/data:/opt/jumpserver/data \\ -v /opt/jumpserver/koko/data:/opt/koko/data \\ -v /opt/jumpserver/lion/data:/opt/lion/data \\ -p 80:80 \\ -p 2222:2222 \\ -e SECRET_KEY=DiShVgRRBMEqM7kJ41ArYdvfVQ216amp3FUVxiHdxJNxTObkNU \\ -e BOOTSTRAP_TOKEN=QZ54bEJphGpRhP1R \\ -e LOG_LEVEL=ERROR \\ -e DB_HOST=192.168.126.133 \\ -e DB_PORT=3306 \\ -e DB_USER=jumpserver \\ -e DB_PASSWORD=123456 \\ -e DB_NAME=jumpserver \\ -e REDIS_HOST=192.168.126.133 \\ -e REDIS_PORT=6379 \\ -e REDIS_PASSWORD='' \\ --privileged=true \\ jumpserver/jms_all:v3.10.12 登入就行了，用户名密码 都是admin，这个和一键安装不一样，一键安装的密码是ChangeMe 尝试复现一键安装的故障： 1、首先停止本地的harbor systemctl stop harbor 2、然后复制官方的一键安装 安装一如既往正常的，然后reboot看看是否会出现故障👆 此时不出意外就会出现ssh登不上的情况，然后就可以找一下具体故障点了。 不过可惜没有复现，猜测是harbor和jumpserver容器网络冲突了，导致eth0物理口的IP没有起来，当时是手动ifconfig down eth0 ifconfig up eth0后才能够ssh上去。 明天复现看看吧，也可能是192.168.126.133那台docker run的时候run成了132了？不对啊，早就ctrl c了 工作案例 1、关于comfui，往往需要高配显卡 但是现在随着时间的推进，很多云上也有此类机器，而且SD比如会成为服务直接对外提供，无需人工搭建，下面就是成本问题。 2、自己折腾的必要性，在于自己动手丰衣足食。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:29 "},"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/3-JumpServer用户管理和添加服务器资产.html":{"url":"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/3-JumpServer用户管理和添加服务器资产.html","title":"第3节 JumpServer用户管理和添加服务器资产","keywords":"","body":"第3节 JumpServer用户管理和添加服务器资产 呃呃呃，我的目的就是减少沙箱这种收费软件的LIC的使用，而期待Jumpserver来做安全审计的远程操作。 然后邮箱是要的，因为一般涉及告警发送，钉钉后面再看，api也要的。 架构上，肯定不会说先拨vpn再连jumpserver，直接公网加端口怼出去，反正有审计，关键还要做风险操作的及时告警，这个学完后自己折腾下。 邮箱设置 其实也没啥好说的，就是常规配置，倒是这里有一个extmail的常规操作 我的extmail邮箱满了，web点击要么都是无标题，要么就直接页面报错了，所以需要清空一下邮件 1、删除收件箱、Junk(垃圾邮件)、Trash(垃圾箱)的邮件 2、删除用户Maildir下的extmail-curcache.db文件 然后刷新web就好了 然后jumpserver的测试一下啊邮件就发过来了👆 这里如果配置SSL 465发送邮箱会报错 你还在为^H而困恼嘛 stty erase ^H 关于容器里(其实其他地方也可能遇到)较多遇到Backspace往前删除，删不掉的清空，这个时候简单操作一下就行了 其实backspace就是^H，所以最原初的你敲backspace，屏幕上显示的就是^H，所以需要优化下👆 其实正确的方法是安装bash 因为你就算解决backspace，你的上下左右键还是没办法弄，而切换到bash就都有了，如果没有bash就安装一下 然后继续解决邮件465SSL发送报错DH长度的问题 查文档 1、这是python 的处理方式 https://blog.csdn.net/weixin_47383889/article/details/125019751 2、这是openssl，linux里的处理方式 https://www.cnblogs.com/testzcy/p/17425364.html 3、这个好像才是正解 https://stackoverflow.com/questions/61626206/what-could-cause-dh-key-too-small-error 4、都不对，jumpserver的邮件发送好像是django里的模块实现，所以要从这里入手 进入容器的时候用的是sh，所以还得切bash，没有就安装 然后安装vim sed -i 's|http://deb.debian.org/debian|https://mirrors.tuna.tsinghua.edu.cn/debian|g' /etc/apt/sources.list.d/debian.sources sed -i 's|http://deb.debian.org/debian-security|https://mirrors.tuna.tsinghua.edu.cn/debian-security|g' /etc/apt/sources.list.d/debian.sources apt update apt install vim 4、都不对，jumpserver的邮件发送好像是django里的模块实现，所以要从这里入手 https://kb.fit2cloud.com/?p=77 再安装postfix yum -y install postfix systemctl enable postfix systemctl start postfix 再设置s-nail set smtp=smtps://smtp.gmail.com:465 # 这行删掉就是25的smtp了 set smtp-auth=login set smtp-auth-user=your-email@gmail.com set smtp-auth-password=your-password set from=your-email@gmail.com set ssl-verify=ignore s-nail好像要配置一个真实的邮箱用户和密码了，但是老早的centos的postfix+mailx哪里就不需要，直接匿名发送了就。 上面就测试了jumpserver所在宿主是可以465发送邮件的，问题还是在jumpserver容器里的DH问题 jms_all容器里测试 使用mailutils提供mail命令好比s-nail，ssmtp提供postfix的邮件client端但是比postfix轻量级。 apt install ssmtp apt install mailutils ---------------- root@f8fcc4d9b3a8:/opt# cat /etc/ssmtp/ssmtp.conf |grep -Ev '^(#|$)' root=postmaster mailhub=mail.iwgame.com:25 hostname=f8fcc4d9b3a8 AuthUser=shenyiming@iwgame.com AuthPass=Cisc0@123 UseTLS=NO UseSTARTTLS=NO FromLineOverride=YES --------------- root@f8fcc4d9b3a8:/opt# cat /etc/ssmtp/revaliases root:shenyiming@iwgame.com:mail.iwgame.com ubuntu:shenyiming@iwgame.com:mail.iwgame.com OK的，所以定是jumpserver使用邮件发送模块里的DH的问题，可能就是django里发送的 不搞了，一键安装方式v4.0.0的版本也一样不行 docker安装的版本是 一键脚本安装的版本是 不搞了，一键安装方式v4.0.0的版本也一样不行 操作 当前用户是admin登入的， 该角色身兼三职①管理员，操作jumpserver，②审计员，安全log查看的人，③使用jumpserver提供的跳转功能干活的，走工作台了就。 创建账号 管理员走控制台 1、分组，开发，测试、运维等 2、建用户 新版本反而没有钉钉认证了，老版本有的 这个密码选项图中的也能用，不过发给用户那边的链接其实就是127.0.0.1的地址，因为是容器里的嘛，也没有优化好，所以不推荐用这种方式。 然后测试了重置密码(会有邮件的交互)也都是ok的，就是同样要注意127.0.0.1改为真实的宿主IP的问题，要是能找到容器里的这个127.0.0.1的设置就好了。 不过还是不推荐邮件参与进来，直接给一个默认密码，然后让用户首次登入修改就很好。 注意点编辑进去的系统角色显示有问题，记住你选啥，就是啥，外面确认是对的就行 先看看审计员的界面 也没啥好看的，就是一堆日志，不过也有工作台就是没有配置任何机器给他用。主要还是看日志。 此时再开一个浏览使用别的账号登入一下，啥，为啥要别的浏览器，因为有cookie啊，chrome用的admin，chrom无痕用的审计员账号，所以再开一个firefox登入一个普通用户。 此时审计员可以看到记录，不管是在线记录，还是登入记录 连管理员的操作日志都能看到 审计专员还是挺牛逼的，哈哈哈 普通人员只有工作台 下面就开始添加资产 这里有一个底层做事情的逻辑 1、用户---通过跳板机---链接业务机器 ​ 用户通过账号进入跳板机，而跳板机分配该用户可以链接哪些机器 ​ 所以跳板机就需要事先连接好后端的很多机器 2、ansible就是如此：ansible要管理机器ABC，就要配置一个主机清单文件，然会为了方便就要做key验证。 3、jumpserver亦是如此：把后端需要远程访问的机器配置到jumpserver资产里，后面分配给不同的用户。 ​ 这些jumpserver关联的后端资产可以有(一般就是linux和windows)：服务器(linux\\windows\\Unix)、数据库、网络设备、应用、K8S。 ​ 涉及到另外的账号概念：早期jumpserver称之为，现在统一叫系统用户里的特权和普通👇 jumpserver里的三种用户 前面梳理了 管理员、审计员、用户，现在 关于账号的理解，这里有一个宇宙法则：谁的，什么系统的账号，就TMD从这个角度去梳理就一目了然。就问你屌不屌~一念之间，天地翻转，地天泰啦~ 1、登录用户 ​ ① 管理员 ②审计员 ③用户 ​ 说白了就是jumpserver自身的账号。 2、系统用户 里的 特权用户 ，早期jumpsever版本叫 管理用户 ​ 对后端服务器具有管理权限的账号root或administrator以及sudo ALL权限的用户，用于管理后端服务器 ​ 说白了就是后端的root账号 3、系统用户 里的 普通用户，早期jumpserver版本叫 系统用户 ​ 给登录用户使用ssh连接后端服务时对应的系统用户，一般是后端服务器的普通的系统用户账号 ​ 说白了就是后端的普通账号 jumpserver内置了ansible，拿后端的root账号是可以批量去后端机器上创建账号的 然后用户通过jumpsever的账号 登录 jumpserver后，就可以使用批量的各个账号去登入 后端机器了。 xiaoming 图中的xiaoming哦，看图👆，通过xiaoming账号登入jumpserver，jumpserver通过不同机器的各自的root账号+ansible批量创建下发后端机器的所需用户ID，就是哪些linux机器的用户，windows的也有ansible？回头看看 用户管理：就是jumpserver自己的登入账号 账号管理：就是后端服务器的账号，root和master以及其他的 新版本，root还是普通就一个选项就搞定了👇 往前的版本如下👇就是系统用户里的 普通 和 特权 之分， 开始创建账号 1、直接创建账号是要有前置条件的，需要关联资产也就是这个账号登入的后端机器，所以要去新建资产 2、可以先新建账号模板，将来创建账号的时候调用就行了 3、新建资产-也就是后端的机器 实验开始 先准备两台机器作为跳板机的资产 就133 和 134吧， 1、先创建账号模板，不同后端机器使用不同的管理员账号，一般来讲偷懒就是dev环境就用一个哈哈。 图中的自动推送，肯定是在这个图中是不能选的啦，因为推送本质上jumpserver应该就是用的ansible，所以要先有root权限才能推的，或者做了key登入理论上分析这样的没错的。 顺便看看自动推送 然后看 这里并没有，所以相关性有待继续学习 2、然后新建账号推送 图中的无需再次输入密码有待验证，因为少了一个老版本里的托管密码按钮。 老版本的逻辑也是类似，就是一个密码的自动生成，自动托管，什么意思，就是上图红色字表达的，你不用二次输入密码，不管是不是图中的按钮具体意思，总之这里的开发意图都是如此。因为我也是没有学完，这里直接下结论也是为了更好的梳理，要敢于下结论，不管他的具体作用是什么，一定是为了什么。要有这个态度和眼光，就好比机房精密空调，不管怎么个精密，一定是机房的需求--送风一定要到位，到什么位置，到机柜底部送来冷风，对吧。从最终结果同样也是最初需求出发，准没错。咦~怎么有点像 我不管过程，我就要结果，我日TMD。。。 再来一个账号推送，这个是测试账号 1和2 两步动作，相当于 普通账号有了，特权账号也有了，下面就开始关联后端资产了。 所以前文的操作，就是 新建账号模板-且都是特权；新建账号推送-且都是普通。思路倒是OK的，也是从需求出发的，特权的用来连接后端机器，普通账号就是推送过去，挺好的，就不知道界面里的 账号模板里的 普通账号 自动推送怎么玩的，估计也是类似的逻辑吧。 3、新建资产 两台机器 192.168.126.133和134，分别作为开发环境 和 测试环境的机器 我再搞一个windows吧，恩，放到实验最后工作案例吧，也是我的初衷 然后就搞两个文件夹出来 好 能推送啥哦，不就是后端机器的普通账号咯。总感觉该产品的这里逻辑设计还不够简单。 选择立即推送后的界面无明显变化 联想到要推送的就是普通账号，于是去 自动化-账号推送里看看，应该是后面要补充下资产来明确往什么机器推什么帐号了。 下一篇继续~~休息下 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:29 "},"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/4-JumpServer实现管理服务器资产.html":{"url":"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/4-JumpServer实现管理服务器资产.html","title":"第4节 JumpServer实现管理服务器资产","keywords":"","body":"第4节 JumpServer实现管理服务器资产 书接上回，为什么其他章节不怎么出现 书接上回，因为，此片和前一篇实乃一篇也~~ 强调初心 1、让xiaoming这个开发，通过xiaoming账号登入 jumpserver 2、然后再通过dev这个账号，去登入后端机器进行操作 当前133 和 134 后端机器上只有root，没有普通账号呢还 是需要jumpsever讲这两个账号分别下发到不同的机器上的 账号下发下去后，就涉及 “资产授权”了。就是好比网络ACL一样。 重点看这张图，我认为jumpserver的管理层次理解的线索就是这张图了👇 账号信息： 后端机器的账号 1、所有账号：事先在这些后端机器上创建好，比如windows的就只能如此，所以上图针对windows就要选择\"所有账号\"； 2、指定账号：后端机器上没有事先创建，这是事先在jumpserver里创建的普通账号模板，然后推下去的。而其是和网络访问权限一起配置的，这就很搓的一个开发思路，作者肯定是纯开发，没有深入干过网络工程师，没有这种网络上的清晰逻辑。我来开发，肯定拆开来弄。 而且 \"账号管理\"-\"自动化\"-\"账号推送\"，现在回过头来看，应该就是一个独立的模块，用来单独下发普通账号的。 其实如果我开发，只做一个功能，只有一个用户账号入口，就是 ①配置后端节点root账号：这是人家后端机器自带的，写进jumpserver里来； ②新建普通用：不管后端节点里有没有，这是你jumpserver层面来看的，都是新建；然后不要开放任何自动创建的按钮给用户，就是建好以后就会自动下发的，如果后端有这个用户，忽略报错，如果没有就真的新建。但是密码就不要说了，所以如果后端节点有，你要提示冲突，让人工介入（①要么改后端密码②要么jumpserver这里换用户名） ③SSO：只选择用 各人登入jumpserver的用户，来登入后端机器。 3、虚拟账号：唉，别管了，就是跳板机而已，花里胡哨的。 所以他这个产品，推送后端机器账号出现在 ①独立的推送功能 ②账号模板里的自带推送 点击上图的同步更新账号信息后👇，其实我也不知道他在同步个啥，root密码给人家改掉？可以的。 这个更多是模板修改后同步到调用模板的账号里去。关键是上图的那个\"自动推送\" 明天再点开看看。感觉是所有调用该模板的账号的关联后端资产上账号的同步了。应该是的。这些东西我应该不会去一个个实际测试了，我是有多闲啊~~~ ③开通跳板机上用户权限的时候的推送 ④ 推你马勒戈壁的推送 所以你是一个球，任何球面都能进入到球心对吧，你就是个球。 继续操作 选择开发权限里的推送吧，所以需要在用户模板里去创建普通用户，不过那里好像关联资产，哦，如果关联资产了倒是可以在用户模板里直接自动推送了。 表达不准确，账号模板只是用来创建账号， ①账号模板里的\"同步更新账号信息\"，就是会讲所有用该模板创建的账号，所关联的后端节点的密码更新推送了。见上图第二张图也就是②账号模板里的自带推送段落里的图。 ②账号里的\"立即推送 \"到了后端机器。 ③账号模板里不点击\"同步更新账号信息\"，也可以让开通权限的时候选择账号模板，那里也可以自动推送，见上图咯就是上面的最近的一张图。 下图👇是上面的②账号里的 立即推送。 这样应该就是说，👆账号界面自带的推送了。 1、创建账号模板的普通用户 在此之前，已经完成了 ①jumpserver的用户组和用户的创建 ①特权用户模板 ②资产的创建 ③注意：没有进入到\"账号列表\"去创建账号，这里的账号都是自动生成的。反正本次实验是的。 继续在创建一个test，这样两个普通账号模板就好了 2、资产授权 选择 文件夹 或者 单个资产 ，然后 创建，这就是对文件夹或者单个机器 授权 然后编辑：针对什么组成员，访问 什么 资产组 ，使用什么账号连接后端资产，账号是模板里的还会涉及自动下发。 点击确认，立刻就会把这个 dev 👆 也就是 调用的账号模板里的账号，当然我这里是普通账号，下发到上图的资产节点里，所谓节点就是文件夹，就是dev账号下发到所有文件夹里的机器上。于是👇之前id dev还看不到，现就就有该帐号了 同样把test用户针对test资产的授权给做了 3、测试不同用户的操作 在此之前总结下前面的操作步骤 ①jumpserver的用户组和用户的创建 ②特权用户模板 ③资产的创建 ④注意：没有进入到\"账号列表\"去创建账号，这里的账号都是自动生成的。反正本次实验是的。 当然后期维护，也是可以从任何地方新建的，这个就是你对jump理解到位了，就很灵活了。是好事，就是有个新手适应期了，呵呵，我不太爱这种表达方式，不过大家都讲，比较好沟通。 授权的就是dev和test组，里面成员情况如下 使用王麻子这个dev成员登入jumpserver发 发现操作权限有问题 点击连接进去发现只有SFTP 授权没问题， 找到了，是资产那里没有给出来，就是说资产本身就没有定义ssh这些东西，只定义了sftp，所以轮不到授权说好话。 直接改成ssh就行 再次登入wanglin账号dev组的 点击连接 进去就是看到SSH 这就OK了 同样看看test组的，也就是134机器的 也OK 4、审计功能 这些操作，审计员，或者管理员，反正就是有审计权限的账号，就可以进入审计台 就能看到操作录屏了，这就是你要的了👆 上面是录屏，下面是cli的记录 操作录屏下载后， 下载后需要jumpserver提供的播放器 exe下载安装 我就不装了，TMD还报错风险提示 5、发现风险剔除动作 1、先模拟一个正在操作的状态 王麻子这个dev进来正在操作 优化下 好看点 中午休息下~~~ 此时在审计平台里就可以看到在线会话， 此时审计这边可以点击终断，操作者那边就断开了 但是在连又能连上了 这他喵的终断有个什么意义。 后期需要API介入直接断开和取消其用户的控制权限 6、事先过滤掉风险CLI 1、创建风险命令组 去验证下alias一个mv=rm能否跳过这里的禁用措施 哪些是风险CLI呢 1、常规众所周知类：rm、reboot 2、跳板动作类：ssh、telnet、mstsc远程rdp、vnc？ ssh这种也是高危哦，因为可以再次跳到其他机器上 rm reboot shutdown ssh telnet alias 未完未补充.... 2、调用上面的CLI组，进行命令过滤 生效了没呢，在线用户需要踢下线，让其重新登入才能保证生效 此时用户的ssh连接被终端，重新连接ssh就行了，这里其实就是用户与后端机器的连接断一下就行了。 通过alias来变动也不行，直接交互式的alias mv='rm'是不行的，但是vi进去是可以的👇 诺，所以alias也是高危CLI 但是你说去禁止掉alias，其实一样，下图就是jumpserver禁止掉了alias，但是人家vi进去，然后运行. .bashrc，这样就同样实现了alias，但是又没有运行alias，所以还是可以删除文件。 所以只是说帮助使用者协助他不让其删除，这是站在使用者也不想删东西的角度的，如果他就想删，vi总不能不给吧，所以录屏时候追责就行了。 ​ 此外，除了bashrc里的alias，还有vi进去写脚本，同样限制不住。 然后再看看cli方式的跳板机操作 jumpserver的cli管理台是2222端口来着 使用jumpserver的登入用户王麻子登入，就用cmd来吧 输入Enter后再输入1就连接到那个序号的host了 打问号？显示帮助信息 下一篇，就梳理管理DB之类，最后案例再弄管理windows，别最后，这里直接弄windows吧，正好工作上用得到 工作案例-jumpserver-跳windows 1、首先需求 数据不能落家里去 访问内网就用RDP windows的密码在用户自己手里，也就是堡垒机登一次，后端windows再登一次 2、开搞 1、创建账号 先创建组 再创建用户，并归属上面的组 3、配置后端资产 在资产列表里，DEFUALT里右键新建节点 内网RDP 其实就是个文件夹 仔细👆看图，有个细分的windows-RDP，大概率就直接用它了。 因为资产添加的时候没有指定jumpserver上的账号，所以\"账号列表\"里\"资产树\"里看不到的 只有带上账号信息的资产，才会出现在账号列表里👆 4、配置资产授权 针对整个内网RDP文件夹 进行 授权 其实我觉得这两个都可以，反正都是内网机器上的账号 测试 首次登入修改密码 再次登入后 试下 一直灰的啊👆 注意看图 所以回过头去修改吧，上面的猜测是错的 修改为👇 此时界面发生了变化 但是一直转圈圈啊.... 去容器里面telnet 3389试试 选择web整个模块进去👆 然后修改apt为国内源 root@jms_web:/opt# sed -i 's|http://deb.debian.org/debian|https://mirrors.tuna.tsinghua.edu.cn/debian|g' /etc/apt/sources.list root@jms_web:/opt# sed -i 's|http://deb.debian.org/debian-security|https://mirrors.tuna.tsinghua.edu.cn/debian-security|g' /etc/apt/sources.list root@jms_web:/opt# 通的啊 为啥半天连不上呢 妈的好像是账号输错了，不是这个原因，账号错误应该报错账号的，结果账号改对了也不行。 是平台选错了，全部改成windows是可以的，否则192.168.25.70选择windows-rdp和windows-tls都不行，只能是windows，然后全部改成windows也是可以的。 好了，三台不同windows都好了👇 其他零碎的: 这个是window版本不同导致RDP过去的报错，资产里面 平台改成windows或者windows-TLS就行了👇 插播GPTCF验证的地址 内网放行GPT，不断跳转CF验证需要放行这个URL，这个URL可以说是闪现了，所以一般抓不到 针对这种，完整的方案，好像之前也有👇可以参考下 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:29 "},"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/5-JumpServer管理数据库资产和批量导入导出.html":{"url":"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/5-JumpServer管理数据库资产和批量导入导出.html","title":"第5节 JumpServer管理数据库资产和批量导入导出","keywords":"","body":"第5节 JumpServer管理数据库资产和批量导入导出 1、先准备一个mysql作为后端资产 1、ubuntu sed apt update apt -y install mysql-server vim /etc/mysql/mysql.conf.d/mysqld.cnf #bind-address =127.0.0.1 #mysqlx-bind-address = 127.0.0.1 # 也可以用sed 直接注释掉127.0.0.1的行 sed -i '/127.0.0.1/s/^/#/' /etc/mysql.conf.d/mysqld.cnf systemctl restart mysql # 检查端口监听是不是不再是127了 ss -tlnup # 看到*:3306 就可以了 mysql mysql>create database wordpress; mysql>create user wordpress@'192.168.%.%' identified by '123456'; mysql>grant all on wordpress.* to wordpress@'192.168.%.%'; 2、红帽系统 sed yum -y install mysql-server systemctl enable --now mariadb mysql mysql>create database wordpress; mysql>create user wordpress@'192.168.%.%' identified by '123456'; mysql>grant all on wordpress.* to wordpress@'192.168.%.%'; mysql>create table wordpress.t1(id int); mysql>exit 2、jumpserver上配置DB 然后授权，可以利旧的 不过要改一下里面的 授权账号 上图👆二选一，要么所有账号，要么精确到wordpress账号 然后用 王麻子测试 不好意思mysql的IP地址写错了。。。 改掉就好了 这里可见一个命名规范👇 去改掉 因为这个连接jumpserver是一进来就是wordpress这个数据库里的了👆。 反正就是mariadb的提示符不太友好，CLI界面也就是2222端口连接jumpserver也是一样 然后已经可以操作了，但是连接性还是显示错误，显然界面刷新或者显示判定有问题，其实是OK的。 大概知道了，可能要👆手动点一下三个点里面的 测试 一下就可以了。 看看mysql提示符一样，不咋地👇，不过连接性都是能过够正确显示了。 资产的批量添加 手动新建一台windows后端资产 然后导出来当作模板 再ctrl+c ctrl+v 写多个机器 然后上传就行了 就是节点也就是文件夹的id注意下 同步的按钮的作用 看下 1、模板的信息修改 2、然后调用模板的所有地方不会自动同步的，需要手动的 3、修改和同步操作备忘 此时 资产列表 和 账号列表 里的 用户都不会自动更新的 手动进去模板里进行同步👇 此时 资产列表 和 账号列表 里就更新了 推送账号了解下 我推1-自动推送账号一般就是普通账号-看下 使用场景：用在单独下发账号的时候 用root去ansible推的 这里就是要给独立的推送账号的模块，不过前提是 root 这个特权账号要先配到到推送的机器上👇 我推2-还可以这里推 使用场景：首推这种使用方式，就是从源头就定义了要推下去的。 创建模板的时候就定义号自动推，就是谁用我这个模板，谁就自动推， 谁是谁，谁就是资产咯 确认后，等10s钟 就退下去了，这是模板里自动退了，谁用了这个模板，谁就自动得到了 我推3-就是上一篇讲的 资产授权里 也可以推 使用场景：这个就是从碗底吃饭的人爱用的方式了，我上一篇也是这么用的。 这个上一篇写过了，这里就简单看看上图就行了 我推4-账号列表里推 使用场景：也是修改的时候推的，就是不是一个从无到有的过程，而是不走模板，直接创建账号的时候，关联资产了，确认了，然后在外面的界面里也可以推 10s不到，也推下来了 网域列表 就是你的资产在互联网上，很多个地方，你不希望搭建jumpserver 1 2 3 那么多台，于是可以直接利用网域网关的逻辑去实现一个jumpserver搞定多地的资产跳登。 https://docs.jumpserver.org/zh/v4/guide/admin/asset/domain_list/?h=%E7%BD%91%E5%9F%9F 好像也没说啥，自己走一遍看看玩 一 、弄网域 1、创建网域 2、创建网关列表 点击进到上面创建的网域里，创建网关列表 找了一个我的QQ云上的机器的公网IP 此时netArea001这个网域里就有个一个网关， 1、当然这里可以创建多个GW，所以才叫网关列表，好比腾讯云这个网域里自然有多个VPC，一个VPC一个GW接入； 2、然后将需要跳板连接的机器也就是资产 去 \"资产列表\" 里创建好， 3、最后关联到这些资产到对应的网关里就行了 好实操开始 二、补资产 1、创建文件夹归类 2、创建资产 这里变通下，就用网关本身的内网IP充当后端资产就行咯👇 上图👆的账号就新增一个root测试。 三、回到网域里 1、关联资产 👆上图连接性错误，也好理解，是QQ云上的内网IP，资产那边怎么可能是OK呢。 测试下， 也不知道是通还是不通。。。因为如果走GW，就会通的。。。 四、资产授权 五、测试-OK~ 看图👇是说gw不可用 报错鸟，我的是2208，GW的SSH那里改下👇 顺便测一下GW 再次测试，看图👇是说gw dial 去连接资产失败 哦，因为GW的内网IP，实验代替了后端资产，所以一样要改SSH为2208，👇改掉 搞定~~ 掌声在哪里~~ 不过连接性 还是显示不对，不管了 跳板机的登入都好了，显示问题，或者这里的联通测试 没有走GW去测 不管了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:30 "},"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/6-Haproxy介绍和多种安装方法.html":{"url":"1-容器技术和堡垒机JumpServer实战/5-Docker资源限制和堡垒机JumpServer/6-Haproxy介绍和多种安装方法.html","title":"第6节 Haproxy介绍和多种安装方法","keywords":"","body":"第6节 Haproxy介绍和多种安装方法 概述 lvs 四层代理，转发性能强 nginx 4层和7层，主要还是7层，且转发性能一般；nginx应该说是专业的web服务器。 HAproxy都行，都是高性能，还有WEB界面。 HAproxy的HA还得靠keepalive那么HAproxy的HA是啥呢，是后端调度的冗余--是后端节点的HA调度。 HAProxy本身的HA还是用keepalive来做的。LVS的HA同样也是keepalive，nginx的ha也还是keepalive。 图中HAProxy可以用LVS代替，就是用来做4层转发；图中KA就是keepalive缩写。 右边的nginx 套nginx/tomcat，前面的nginx就是做7层转发，后面的nginx就是web了。 这图就是参考一下 HAProxy简介 LVS： 4层转发效率最高，因为c---lvs----ser, lvs在中间是不产生两个段tcp的。 HAProxy：4层转发效率虽然比nginx高，但是比lvs低，因为c---HAproxy----ser，HAProxy在中间是拆包后重新与server建立tcp了，前后是两段连接。 也就是说HAPorxy转发的流量，不管是4层还是7层，都是改变了数据包的源IP地址的。所以和nginx一样还要做真实IP的透传。 四层 LVS: Linux Virtual Server Nginx: 1.9版本之后 通过 stream模块实现 HAProxy：High Availability Proxy mode tcp 不支持udp? 七层 HAProxy 通过 mode http 指令 Nginx 直接写在 http模块 应用场景 四层：Redis mysql rabbitmq memcached等 七层：nginx tomcat apache php 图片、动静分离、api等 其实就是一个GUI带来的好处。 官网 1、免费社区版 https://www.haproxy.org/ 用TLS，stable别用，号称稳定也别用。 2、收费企业版 https://www.haproxy.com 安装HAProxy 红帽系列 yum直接就可以安装，但是版本较低 容器也可以安装，但是这是做边缘网络转发的，不推荐使用容器，容器本身就是有性能损耗的。 新版本的安装 debian/ubuntu有直接的安装包，其他的要么是容器，要么就是自己编译了 编译安装 1、前置软件Lua https://www.lua.org/start.html lua -v yum install -y gcc readline-devel curl -L -R -O https://www.lua.org/ftp/lua-5.4.7.tar.gz tar zxf lua-5.4.7.tar.gz -C /usr/local/src cd lua-5.4.7 make all test make install # 官网上竟然没有这一条... # -R就是时间设置为remote-time就是往上文件的时间，而不是当前本地的时间 [root@realserver2 ~]# curl --help all |grep -E '^ .*-R' -R, --remote-time Set the remote file's time on the local output 下面就是有R没有R的区别 这就安装好了 2、编译安装HAProxy Rockylinux: # HAProxy 1.8及1.9版本编译参数： make ARCH=x86_64 TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 PREFIX=/usr/local/haproxy # HAProxy 2.0以上版本编译参数： yum -y install gcc openssl-devel pcre-devel systemd-devel zlib-devel curl -LRO http://git.haproxy.org/?p=haproxy-3.0.git tar xvf haproxy-3.x.x.tar.gz -C /usr/local/src cd /usr/local/src/haproxy-3.x.x/ # 查看安装方法 ll Makefile cat READEME CAT INSTALL # 参考INSTALL文件进行编译安装 make clean # 注意如果将来需要被Prometheus监控的话就要加上USE_PROMEX=1 make -j 4 ARCH=X86_64 TARGET=linux-glibc USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_PROMEX=1 USE_LUA=1 LUA_INC=/usr/local/src/lua-5.4.7/src/ LUA_LIB=/usr/local/src/lua-5.4.7/src/ make install PREFIX=/apps/haproxy ln -s /apps/haproxy/sbin/haproxy /usr/sbin/ tree /apps/haproxy/ # 源码包安装，还需要弄一个services文件，以及起送services的cfg配置文件 vim /usr/lib/systemd/system/haproxy.service [Unit] Description=HAProxy Load Balancer After=syslog.target network.target # ExecStarPre是与启动，就是启动前的检查，如果不行就不会启动了 # USR2是自定义的信号，haproxy拿来reload了应该 # 这里service文件里加了haproxy.pid文件所以haproxy.cfg配置文件里就不要再指定haproxy.pid了 [Service] ExecStartPre=/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -c -q ExecStart=/usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /var/lib/haproxy/haproxy.pid ExecReload=/bin/kill -USR2 $MAINPID LimitNOFILE=100000 [Install] WantedBy=multi-user.target ------------------------- # 编写haproxy的配置文件 mkdir /etc/haproxy vim /etc/haproxy/haproxy.cfg global maxconn 100000 chroot /apps/haproxy stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin #uid 99 #gid 99 user haproxy group haproxy daemon #nbproc 4 #cpu-map 1 0 #cpu-map 2 1 #cpu-map 3 2 #cpu-map 4 3 #pidfile /var/lib/haproxy/haproxy.pid 不用配置，配置反而提示冲突 log 127.0.0.1 local2 info defaults option http-keep-alive option forwardfor maxconn 100000 mode http timeout connect 300000ms timeout client 300000ms timeout server 300000ms listen stats mode http bind 0.0.0.0:9999 stats enable log global stats uri /haproxy-status stats auth haadmin:123456 #listen web_port # bind 0.0.0.0:80 # mode http # log global # server web1 127.0.0.1:8080 check inter 3000 fall 2 rise 5 # 后端服务器的检查 ------------------------------------------------ # 准备socket文件目录 # mkdir /var/lib/haproxy # 下面useradd的时候带上-m就行了，而且用户和用户组还精确到了haproxy，这里就不要敲了 # 创建用户和用户组 useradd -r -s /sbin/nologin haproxy -d /var/haproxy -m # -d /var/haproxy本来是多此一举，但是-m加上去就很完美了，啊，你说不用-r ，也不用-m，那不行，那样就会自动创建邮箱了。这也算一个固定用法了吧。 # 检查配置文件语法 haproxy -c -f /etc/haproxy/haproxy.cfg # 加载service文件 systemctl daemon-reload # 启动 systemctl enable --now haproxy 过程记录 报错处理，make的报错，一般就是要安装xxx-dev 发现并不是dev而是zlib-devel 最后安装oK，👇但是没有配置文件的。 就一个二进制文件，所以写个软连接就行。如果文件夹下很多个二进制，则将这个文件夹的目录加入PATH变量。 PATH变量 然后配置文件在源码包里的example目录下有案例的 纠错 -r -d就是多此一举？是的，不管是家目录还是mail都不会创建，所以没必要-d👇 kill USR2是啥 有点小问题好像 注释掉pid文件，已经后端健康检查的那段就行 然后打开haproxy的管理界面 URL哪来的？cfg里配置的👇 进入网页后： ubuntu: # 安装基础命令及编译依赖环境 apt update && apt -y install gcc make libssl-dev libpcre3 libpcre3-dev zlib1g-dev libreadline-dev libsystemd-dev # 安装Lua方法1：包安装Lua apt update && apt -y install liblua5.x-dev # 安装Lua方法2：编译安装Lua cd /usr/local/src wget https://www.lua.org/ftp/lua-5.4.7.tar.gz tar zxf lua-5.4.7.tar.gz -C /usr/local/src cd lua-5.4.7 make all test make install # 官网上竟然没有这一条... # 或安装系统自带的Lua #👇 https://haproxy.debian.net/#distribution=Ubuntu&release=jammy&version=3.0 apt update # apt-get install --no-install-recommends software-properties-common # add-apt-repository ppa:vbernat/haproxy-3.0 # apt-get install haproxy=3.0.\\* # HAProxy 1.8及1.9版本编译参数： make ARCH=x86_64 TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 PREFIX=/usr/local/haproxy # HAProxy 2.0以上版本编译参数： yum -y intall gcc openssl-devel pcre-devel systemd-devel tar xvf haproxy-3.x.x.tar.gz -C /usr/local/src cd /usr/local/src/haproxy-3.x.x/ # 查看安装方法 ll Makefile cat READEME CAT INSTALL # 参考INSTALL文件进行编译安装 make clean make -j 4 ARCH=X86_64 TARGET=linux-glibc USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_PROMEX=1 USE_LUA=1 LUA_INC=/usr/local/src/lua-5.4.7/src/ LUA_LIB=/usr/local/src/lua-5.4.7/src/ make install PREFIX=/apps/haproxy ln -s /apps/haproxy/sbin/haproxy /usr/sbin/ tree /apps/haproxy/ #!/bin/bash #只支持离线下载源码安装，不支持在线下载安装 HAPROXY_VERSION=3.0.3 HAPROXY_INSTALL_DIR=/apps/haproxy STATS_AUTH_USER=admin STATS_AUTH_PASSWORD=123456 HAPROXY_FILE=haproxy-${HAPROXY_VERSION}.tar.gz LUA_VERSION=5.4.7 LUA_FILE=lua-${LUA_VERSION}.tar.gz SRC_DIR=/usr/local/src CWD=`pwd` CPUS=`lscpu |awk '/^CPU\\(s\\)/{print $2}'` LOCAL_IP=$(hostname -I|awk '{print $1}') VIP=192.168.126.100 MASTER1=192.168.126.132 MASTER2=192.168.126.133 MASTER3=192.168.126.134 . /etc/os-release color () { RES_COL=60 MOVE_TO_COL=\"echo -en \\\\033[${RES_COL}G\" SETCOLOR_SUCCESS=\"echo -en \\\\033[1;32m\" SETCOLOR_FAILURE=\"echo -en \\\\033[1;31m\" SETCOLOR_WARNING=\"echo -en \\\\033[1;33m\" SETCOLOR_NORMAL=\"echo -en \\E[0m\" echo -n \"$1\" && $MOVE_TO_COL echo -n \"[\" if [ $2 = \"success\" -o $2 = \"0\" ] ;then ${SETCOLOR_SUCCESS} echo -n $\" OK \" elif [ $2 = \"failure\" -o $2 = \"1\" ] ;then ${SETCOLOR_FAILURE} echo -n $\"FAILED\" else ${SETCOLOR_WARNING} echo -n $\"WARNING\" fi ${SETCOLOR_NORMAL} echo -n \"]\" echo } check_lua_file (){ if [ ! -e ${LUA_FILE} ];then color \"缺少${LUA_FILE}文件!\" 1 curl -L -R -O https://www.lua.org/ftp/lua-${LUA_VERSION}.tar.gz color \"${LUA_FILE}文件已下载\" 0 else color \"相关文件已准备!\" 0 fi } check_haproxy_file (){ if [ ! -e ${HAPROXY_FILE} ];then color \"缺少${HAPROXY_FILE}文件!\" 1 exit else color \"相关文件已准备!\" 0 fi } install_packs () { if [ $ID = \"centos\" -o $ID = \"rocky\" ];then yum -y install gcc make gcc-c++ glibc glibc-devel pcre pcre-devel openssl openssl-devel systemd-devel ncurses-devel libevent-devel readline-devel elif [ $ID = \"ubuntu\" ];then apt update apt -y install gcc make openssl libssl-dev libpcre3 libpcre3-dev zlib1g-dev libreadline-dev libsystemd-dev liblua5.3-dev lua-devel else color \"不支持此操作系统!\" 1 fi [ $? -eq 0 ] || { color '安装软件包失败,退出!' 1; exit; } } install_lua () { cd ${CWD} check_lua_file tar xf ${LUA_FILE} -C ${SRC_DIR} LUA_DIR=${LUA_FILE%.tar*} cd ${SRC_DIR}/${LUA_DIR} make all test make install } install_haproxy(){ check_haproxy_file cd ${CWD} tar xf ${HAPROXY_FILE} -C ${SRC_DIR} HAPROXY_DIR=${HAPROXY_FILE%.tar*} cd ${SRC_DIR}/${HAPROXY_DIR} if [[ ${VERSION_ID} =~ ^(7|8) ]] ;then install_lua cd ${SRC_DIR}/${HAPROXY_DIR} make -j ${CPUS} ARCH=x86_64 TARGET=linux-glibc USE_PROMEX=1 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 USE_LUA=1 LUA_INC=${SRC_DIR}/${LUA_DIR}/src/ LUA_LIB=${SRC_DIR}/${LUA_DIR}/src/ PREFIX=${HAPROXY_INSTALL_DIR} else install_lua cd ${SRC_DIR}/${HAPROXY_DIR} color \"HAPROXY编译安装开始...\" 0 make -j ${CPUS} ARCH=x86_64 TARGET=linux-glibc USE_PROMEX=1 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 USE_LUA=1 fi # mkdir -p ${HAPROXY_INSTALL_DIR} make install PREFIX=${HAPROXY_INSTALL_DIR} [ $? -eq 0 ] && color \"HAPROXY编译安装成功\" 0 || { color \"HAPROXY编译安装失败,退出!\" 1;exit; } [ -L /usr/sbin/haproxy ] || ln -s ${HAPROXY_INSTALL_DIR}/sbin/haproxy /usr/sbin/ [ -d /etc/haproxy ] || mkdir /etc/haproxy #[ -d /var/lib/haproxy/ ] || mkdir -p /var/lib/haproxy/ cat > /etc/haproxy/haproxy.cfg /lib/systemd/system/haproxy.service /dev/null && color 'HAPROXY安装完成!' 0 || { color 'HAPROXY 启动失败,退出!' 1; exit; } echo \"-------------------------------------------------------------------\" echo -e \"请访问链接: \\E[32;1mhttp://${LOCAL_IP}:9999/haproxy-status\\E[0m\" echo -e \"用户和密码: \\E[32;1m${STATS_AUTH_USER}/${STATS_AUTH_PASSWORD}\\E[0m\" } install_packs install_haproxy start_haproxy 以上脚本已调整，并检查ok 工作案例-CMD脚本-CDN地址观察 检查CDN的IP是否稳定下来了，就是不再出现有问题的那个IP 1、站点A，ping测丢包严重，发现43打头的哪个ip地址 2、发现是cdn地址， 3、dig xx.xx.xx +short 4、ipconfig /flushdns 刷新 5、再次dig xx.xx.xx +short 后发现没有43开头的那个ip地址了 6、使用cmd循环测试下，确实没有43.开头的那个ping丢包验证的IP了。 for /L %i in (1,1,10) do @dig hd.zxxxxxx.com +short |findstr \"43\" && timeout /t 1 >nul 工作案例-jumpserver使用注意 我是直接把jumpserver挂外网的，直接访问是jumpserver做了安全限制，报错如下 处理方法如下 再重启就行了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:30 "},"39-Tomcat/39-Tomcat.html":{"url":"39-Tomcat/39-Tomcat.html","title":"39-Tomcat","keywords":"","body":"39-Tomcat 39--001 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 12:59:11 "},"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/Nginx的二次开发版和Tomcat基础使用.html":{"url":"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/Nginx的二次开发版和Tomcat基础使用.html","title":"Nginx的二次开发版和Tomcat基础使用","keywords":"","body":"Nginx的二次开发版和Tomcat基础使用 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/2.Web相关技术和JAVA语言介绍.html":{"url":"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/2.Web相关技术和JAVA语言介绍.html","title":"2.Web相关技术和JAVA语言介绍.md","keywords":"","body":"2.Web相关技术和JAVA语言介绍.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/3.Java的工作机制和开发模式及JDK.html":{"url":"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/3.Java的工作机制和开发模式及JDK.html","title":"3.Java的工作机制和开发模式及JDK.md","keywords":"","body":"3.Java的工作机制和开发模式及JDK.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/4.Java和Tomcat包和二进制方式安装.html":{"url":"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/4.Java和Tomcat包和二进制方式安装.html","title":"4.Java和Tomcat包和二进制方式安装.md","keywords":"","body":"4.Java和Tomcat包和二进制方式安装.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/5.Tomcat的应用发布和目录结构.html":{"url":"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/5.Tomcat的应用发布和目录结构.html","title":"5.Tomcat的应用发布和目录结构.md","keywords":"","body":"5.Tomcat的应用发布和目录结构.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/6.Tomcat的配置文件结构和应用发布.html":{"url":"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/6.Tomcat的配置文件结构和应用发布.html","title":"6.Tomcat的配置文件结构和应用发布.md","keywords":"","body":"6.Tomcat的配置文件结构和应用发布.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/7.Tomcat部署Jpress和Halo博客系统应用.html":{"url":"39-Tomcat/Nginx的二次开发版和Tomcat基础使用/7.Tomcat部署Jpress和Halo博客系统应用.html","title":"7.Tomcat部署Jpress和Halo博客系统应用.md","keywords":"","body":"7.Tomcat部署Jpress和Halo博客系统应用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/Tomcat集群会话管理和JVM性能优化.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/Tomcat集群会话管理和JVM性能优化.html","title":"Tomcat集群会话管理和JVM性能优化","keywords":"","body":"Tomcat集群会话管理和JVM性能优化 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/1.Tomcat状态页管理.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/1.Tomcat状态页管理.html","title":"1.Tomcat状态页管理.md","keywords":"","body":"1.Tomcat状态页管理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/2.Tomcat安全加固和利用Nginx实现Tomcat的反向代理负载均衡功能.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/2.Tomcat安全加固和利用Nginx实现Tomcat的反向代理负载均衡功能.html","title":"2.Tomcat安全加固和利用Nginx实现Tomcat的反向代理负载均衡功能.md","keywords":"","body":"2.Tomcat安全加固和利用Nginx实现Tomcat的反向代理负载均衡功能.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/3.Nginx反向代理实现Tomcat实现会话复制集群.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/3.Nginx反向代理实现Tomcat实现会话复制集群.html","title":"3.Nginx反向代理实现Tomcat实现会话复制集群.md","keywords":"","body":"3.Nginx反向代理实现Tomcat实现会话复制集群.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/4.Memcached介绍和安装使用.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/4.Memcached介绍和安装使用.html","title":"4.Memcached介绍和安装使用.md","keywords":"","body":"4.Memcached介绍和安装使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/5.Memcached使用和编译安装及MSM集群工作原理说明.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/5.Memcached使用和编译安装及MSM集群工作原理说明.html","title":"5.Memcached使用和编译安装及MSM集群工作原理说明.md","keywords":"","body":"5.Memcached使用和编译安装及MSM集群工作原理说明.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/6.Tomcat的会话服务器实现MSM和redisson实战案例.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/6.Tomcat的会话服务器实现MSM和redisson实战案例.html","title":"6.Tomcat的会话服务器实现MSM和redisson实战案例.md","keywords":"","body":"6.Tomcat的会话服务器实现MSM和redisson实战案例.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/Tomcat集群会话管理和JVM性能优化/7.JVM内存结构和垃圾回收算法.html":{"url":"39-Tomcat/Tomcat集群会话管理和JVM性能优化/7.JVM内存结构和垃圾回收算法.html","title":"7.JVM内存结构和垃圾回收算法.md","keywords":"","body":"7.JVM内存结构和垃圾回收算法.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/JAVA码编译和Nexus私有仓库.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/JAVA码编译和Nexus私有仓库.html","title":"JAVA码编译和Nexus私有仓库","keywords":"","body":"JAVA码编译和Nexus私有仓库 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/1Java程序OOM现象原因分析.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/1Java程序OOM现象原因分析.html","title":"1Java程序OOM现象原因分析.md","keywords":"","body":"1Java程序OOM现象原因分析.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/2.Java程序heap内存使用启动参数.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/2.Java程序heap内存使用启动参数.html","title":"2.Java程序heap内存使用启动参数.md","keywords":"","body":"2.Java程序heap内存使用启动参数.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/3.JVM内存优化参数和Tomcat性能优化.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/3.JVM内存优化参数和Tomcat性能优化.html","title":"3.JVM内存优化参数和Tomcat性能优化.md","keywords":"","body":"3.JVM内存优化参数和Tomcat性能优化.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/4.Java源码编译过程和maven工具安装优化.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/4.Java源码编译过程和maven工具安装优化.html","title":"4.Java源码编译过程和maven工具安装优化.md","keywords":"","body":"4.Java源码编译过程和maven工具安装优化.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/5.Java源码编译实战案例.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/5.Java源码编译实战案例.html","title":"5.Java源码编译实战案例.md","keywords":"","body":"5.Java源码编译实战案例.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/6.Nexus私有仓库安装和maven私有仓库实现.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/6.Nexus私有仓库安装和maven私有仓库实现.html","title":"6.Nexus私有仓库安装和maven私有仓库实现.md","keywords":"","body":"6.Nexus私有仓库安装和maven私有仓库实现.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"39-Tomcat/JAVA码编译和Nexus私有仓库/7.Nexus实现YUM和APT私有仓库.html":{"url":"39-Tomcat/JAVA码编译和Nexus私有仓库/7.Nexus实现YUM和APT私有仓库.html","title":"7.Nexus实现YUM和APT私有仓库.md","keywords":"","body":"7.Nexus实现YUM和APT私有仓库.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:58 "},"40-Zabbix/40-Zabbix.html":{"url":"40-Zabbix/40-Zabbix.html","title":"40-Zabbix","keywords":"","body":"40-Zabbix Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix架构部署/Zabbix架构部署.html":{"url":"40-Zabbix/Zabbix架构部署/Zabbix架构部署.html","title":"Zabbix架构部署","keywords":"","body":"Zabbix架构部署 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix架构部署/8.常见的监控系统和Zabbix架构版本.html":{"url":"40-Zabbix/Zabbix架构部署/8.常见的监控系统和Zabbix架构版本.html","title":"8.常见的监控系统和Zabbix架构版本.md","keywords":"","body":"8.常见的监控系统和Zabbix架构版本.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix架构部署/9.Zabbix基于包安装案例和一键安装脚本.html":{"url":"40-Zabbix/Zabbix架构部署/9.Zabbix基于包安装案例和一键安装脚本.html","title":"9.Zabbix基于包安装案例和一键安装脚本.md","keywords":"","body":"9.Zabbix基于包安装案例和一键安装脚本.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/Zabbix实现系统和应用监控及自定义监控模板.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/Zabbix实现系统和应用监控及自定义监控模板.html","title":"Zabbix实现系统和应用监控及自定义监控模板","keywords":"","body":"Zabbix实现系统和应用监控及自定义监控模板 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/1.Zabbix的Agent版本说明和部署.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/1.Zabbix的Agent版本说明和部署.html","title":"1.Zabbix的Agent版本说明和部署.md","keywords":"","body":"1.Zabbix的Agent版本说明和部署.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/2.Zabbix实现Linux和windows主机的监控.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/2.Zabbix实现Linux和windows主机的监控.html","title":"2.Zabbix实现Linux和windows主机的监控.md","keywords":"","body":"2.Zabbix实现Linux和windows主机的监控.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/3.Zabbix性能优化从内置MySQL迁移至独立MySQL服务器.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/3.Zabbix性能优化从内置MySQL迁移至独立MySQL服务器.html","title":"3.Zabbix性能优化从内置MySQL迁移至独立MySQL服务器.md","keywords":"","body":"3.Zabbix性能优化从内置MySQL迁移至独立MySQL服务器.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/4.Zabbix基于内置模板监控Nginx和PHP服务的状态.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/4.Zabbix基于内置模板监控Nginx和PHP服务的状态.html","title":"4.Zabbix基于内置模板监控Nginx和PHP服务的状态.md","keywords":"","body":"4.Zabbix基于内置模板监控Nginx和PHP服务的状态.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/5.Zabbix自定义监控项和模板实现TCP十一种月限状态机的监控.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/5.Zabbix自定义监控项和模板实现TCP十一种月限状态机的监控.html","title":"5.Zabbix自定义监控项和模板实现TCP十一种月限状态机的监控.md","keywords":"","body":"5.Zabbix自定义监控项和模板实现TCP十一种月限状态机的监控.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/6.Zabbix自定义模板的导出导入和值映射.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/6.Zabbix自定义模板的导出导入和值映射.html","title":"6.Zabbix自定义模板的导出导入和值映射.md","keywords":"","body":"6.Zabbix自定义模板的导出导入和值映射.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/7.Zabbix实现触发器和滞后.html":{"url":"40-Zabbix/Zabbix实现系统和应用监控及自定义监控模板/7.Zabbix实现触发器和滞后.html","title":"7.Zabbix实现触发器和滞后.md","keywords":"","body":"7.Zabbix实现触发器和滞后.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/Zabbix告警功能主动模式与分布式监控.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/Zabbix告警功能主动模式与分布式监控.html","title":"Zabbix告警功能主动模式与分布式监控","keywords":"","body":"Zabbix告警功能主动模式与分布式监控 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/1.Zabbix自定义图形仪表盘展示和用户权限管理.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/1.Zabbix自定义图形仪表盘展示和用户权限管理.html","title":"1.Zabbix自定义图形仪表盘展示和用户权限管理.md","keywords":"","body":"1.Zabbix自定义图形仪表盘展示和用户权限管理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/2.Zabbix实现邮件告警的两种方案.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/2.Zabbix实现邮件告警的两种方案.html","title":"2.Zabbix实现邮件告警的两种方案.md","keywords":"","body":"2.Zabbix实现邮件告警的两种方案.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/3.Zabbix实现微信告警和告警分级及故障自愈.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/3.Zabbix实现微信告警和告警分级及故障自愈.html","title":"3.Zabbix实现微信告警和告警分级及故障自愈.md","keywords":"","body":"3.Zabbix实现微信告警和告警分级及故障自愈.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/4.Zabbix的主动模式和被动模式.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/4.Zabbix的主动模式和被动模式.html","title":"4.Zabbix的主动模式和被动模式.md","keywords":"","body":"4.Zabbix的主动模式和被动模式.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/5.Zabbix实现JAVA应用的监控和SNMP工作原理.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/5.Zabbix实现JAVA应用的监控和SNMP工作原理.html","title":"5.Zabbix实现JAVA应用的监控和SNMP工作原理.md","keywords":"","body":"5.Zabbix实现JAVA应用的监控和SNMP工作原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/6.Zabbix通过SNMP协议监控网络设备和分布式Proxy代理的工作原理.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/6.Zabbix通过SNMP协议监控网络设备和分布式Proxy代理的工作原理.html","title":"6.Zabbix通过SNMP协议监控网络设备和分布式Proxy代理的工作原理.md","keywords":"","body":"6.Zabbix通过SNMP协议监控网络设备和分布式Proxy代理的工作原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbix告警功能主动模式与分布式监控/7.Zabbix分布式主动模式和被动模式实战案例.html":{"url":"40-Zabbix/Zabbix告警功能主动模式与分布式监控/7.Zabbix分布式主动模式和被动模式实战案例.html","title":"7.Zabbix分布式主动模式和被动模式实战案例.md","keywords":"","body":"7.Zabbix分布式主动模式和被动模式实战案例.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbixt自动化运维和高可用/Zabbixt自动化运维和高可用.html":{"url":"40-Zabbix/Zabbixt自动化运维和高可用/Zabbixt自动化运维和高可用.html","title":"Zabbixt自动化运维和高可用","keywords":"","body":"Zabbixt自动化运维和高可用 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbixt自动化运维和高可用/1.Zabbix网络自动发现和自动注册.html":{"url":"40-Zabbix/Zabbixt自动化运维和高可用/1.Zabbix网络自动发现和自动注册.html","title":"1.Zabbix网络自动发现和自动注册.md","keywords":"","body":"1.Zabbix网络自动发现和自动注册.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbixt自动化运维和高可用/2.Zabbix自动化运维API调用和性能优化.html":{"url":"40-Zabbix/Zabbixt自动化运维和高可用/2.Zabbix自动化运维API调用和性能优化.html","title":"2.Zabbix自动化运维API调用和性能优化.md","keywords":"","body":"2.Zabbix自动化运维API调用和性能优化.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbixt自动化运维和高可用/3.Zabbix6.0的官方高可用实战案例.html":{"url":"40-Zabbix/Zabbixt自动化运维和高可用/3.Zabbix6.0的官方高可用实战案例.html","title":"3.Zabbix6.0的官方高可用实战案例.md","keywords":"","body":"3.Zabbix6.0的官方高可用实战案例.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"40-Zabbix/Zabbixt自动化运维和高可用/4.Zabbix利用Grafana进行图形展示.html":{"url":"40-Zabbix/Zabbixt自动化运维和高可用/4.Zabbix利用Grafana进行图形展示.html","title":"4.Zabbix利用Grafana进行图形展示.md","keywords":"","body":"4.Zabbix利用Grafana进行图形展示.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/41-KVM虚拟化.html":{"url":"41-KVM虚拟化/41-KVM虚拟化.html","title":"41-KVM虚拟化","keywords":"","body":"41-KVM虚拟化 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/01.虚拟化发展历史和技术分类.html":{"url":"41-KVM虚拟化/01.虚拟化发展历史和技术分类.html","title":"01.虚拟化发展历史和技术分类.md","keywords":"","body":"01.虚拟化发展历史和技术分类.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/02.KVM架构和安装.html":{"url":"41-KVM虚拟化/02.KVM架构和安装.html","title":"02.KVM架构和安装.md","keywords":"","body":"02.KVM架构和安装.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/03.KVM使用virt-manager创建虚拟机.html":{"url":"41-KVM虚拟化/03.KVM使用virt-manager创建虚拟机.html","title":"03.KVM使用virt-manager创建虚拟机.md","keywords":"","body":"03.KVM使用virt-manager创建虚拟机.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/04.KVM实现自动化创建虚拟机.html":{"url":"41-KVM虚拟化/04.KVM实现自动化创建虚拟机.html","title":"04.KVM实现自动化创建虚拟机.md","keywords":"","body":"04.KVM实现自动化创建虚拟机.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/05.KVM虚拟机管理VirtIO驱动安装.html":{"url":"41-KVM虚拟化/05.KVM虚拟机管理VirtIO驱动安装.html","title":"05.KVM虚拟机管理VirtIO驱动安装.md","keywords":"","body":"05.KVM虚拟机管理VirtIO驱动安装.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/06.KVM创建Windows254的模板和libvirt相关工具及虚拟机迁移.html":{"url":"41-KVM虚拟化/06.KVM创建Windows254的模板和libvirt相关工具及虚拟机迁移.html","title":"06.KVM创建Windows254的模板和libvirt相关工具及虚拟机迁移.md","keywords":"","body":"06.KVM创建Windows254的模板和libvirt相关工具及虚拟机迁移.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/07.KVM相关管理工具和磁盘格式说明.html":{"url":"41-KVM虚拟化/07.KVM相关管理工具和磁盘格式说明.html","title":"07.KVM相关管理工具和磁盘格式说明.md","keywords":"","body":"07.KVM相关管理工具和磁盘格式说明.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/08.KVM存储管理和默认网络模式.html":{"url":"41-KVM虚拟化/08.KVM存储管理和默认网络模式.html","title":"08.KVM存储管理和默认网络模式.md","keywords":"","body":"08.KVM存储管理和默认网络模式.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"41-KVM虚拟化/09.KVM的网络模型和管理.html":{"url":"41-KVM虚拟化/09.KVM的网络模型和管理.html","title":"09.KVM的网络模型和管理.md","keywords":"","body":"09.KVM的网络模型和管理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"42-Docker容器技术和堡垒机JumpServer实战/42-Docker容器技术和堡垒机JumpServer实战.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/42-Docker容器技术和堡垒机JumpServer实战.html","title":"42-Docker容器技术和堡垒机JumpServer实战","keywords":"","body":"42-Docker容器技术和堡垒机JumpServer实战 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/Docker安装和镜像管理.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/Docker安装和镜像管理.html","title":"Docker安装和镜像管理","keywords":"","body":"Docker安装和镜像管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/0.容器技术和Docker特性.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/0.容器技术和Docker特性.html","title":"0.容器技术和Docker特性.md","keywords":"","body":"第0节 容器技术和Docker特性 1、背景 类比集装箱 隔离成本低 docker 只是 一款优秀的 容器技术 ，容器不单单指docker。 docker是结合linux内核开发出来的，不是完全独立开发出来的。将linux内核的两个核心技术加以组合，并封装了额外的功能软件。 docker启动快 build构建----ship传输---run运行 之前docker那本书上有一段敲过实验，可以看到本地build的命令其实可以在远端build然后拉下来运行的。如果不指定就是本地build本地运行，中间是省掉了一个本地到本地的ship过程。 不过这里的ship更多讲的是拿整个build好的镜像到处运行的意思。VM里有迁移的动作，不过也没有docker的ship来的更加频繁。 Any APP ， Anywhere，比如windows构建的镜像，到linux一样运行使用。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:14:58 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/1.容器和虚拟机特性比较.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/1.容器和虚拟机特性比较.html","title":"1.容器和虚拟机特性比较.md","keywords":"","body":"第1节 容器和虚拟机特性比较 虚拟化技术的对比 简单点，就是主机虚拟化有HYPERVISOR \\ GUSET OS 2层OS层面的虚拟化层， 而容器只有一层容器层。少了一个完整的OS，节省了很多资源。 useSpace 1 2 3用的是HOST OS，而HOST OS就是内核的一些系统文件，共享的系统OS咯。 container层就可以理解为主流的docker 这里我想到一个点，“联合文件系统”，才是关键吧。画图毕竟是画个图，container层其实就涉及联合文件系统才能做到不同的容器之间共用基础文件系统层。当然container可能不仅仅是FS层咯。接着学，这里简单思考下，让上帝笑一笑，接着学，接着奏乐，接着舞。 关于user space 1 2 3 共用HOST OS的情况下，如果space1里的修改了内核参数，是否会影响space 2 3的内核参数。继续舞~~，存在space1黑掉了底层HOST OST的某些关键文件导致space2故障的可能性。 ​ 再一个，官方都说是anywhere运行，所以你linux的docker，在windows上一样运行的，如果HOST OS用的是宿主的，那么linux和windows OS肯定不一样的，那么是如何工作的呢？涉及底层工作逻辑，稍后继续研究... 所以有不少就是在虚拟机里跑容器，就是图二里跑图三👇，虽然这么套着用损耗更大(hypervisor+guestOS+container)哈哈，但是归在灵活啊，所以很多都确实在这么用的。 Bins/Libs：依赖库也是个运行环境，ldd /bin/ls这些也是库，一些依赖如果不全就有问题。 再举例：go语言编译的都是静态库，不是共享库，所以go开发的软件拷贝到其他机器上，比如从ubuntu考到centos就能直接运行的。 size，大小，容器肯定小很多，都少了一个相对庞大的OS层的 startup，启动方面，如果是go这种容器就很快，但是如果是java容器照样慢，OS的启动都可以忽略不计了。 intergration ，继承性方面，比如迁移 容器就很快 容器：好比集装箱 docker，n.搬运工人, 码头工人，搬运管理集装箱的，所以docker就是管理容器的技术，一揽子解决方案的技术。 k8s：来自希腊语 舵手，不是剁手。开船的人好像比码头搬运工有高级了一点。码头工人是在一个码头里工作，舵手是满世界跑，今天一码头，明天一码头。所以k8s听名字就知道更牛。 定位其实 docker和k8s之间是不冲突的，但是K8s好像在抛弃docker，哈哈~所以还是有冲突的，比如不是一家公司的利益冲突，哈哈~。 kvm和docker都是单机游戏：一台宿主机上管理其上创建的所有虚拟机或容器。都是只限于一台机器上的管理。 而k8s是所有宿主上的全管了。 kvm 单机vm管理工具 --- 配合--- openstack 多主机VM管理平台 docker 单机容器管理工具 --配合-- k8s 多主机容器管理平台 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:15:25 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/2.Docker组成和容器运行规范.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/2.Docker组成和容器运行规范.html","title":"2.Docker组成和容器运行规范.md","keywords":"","body":"第2节 Docker组成和容器运行规范 docker组成 1、Docker主机host，就是宿主，宿主可以是物理机也可以是虚拟机。你是node节点哦 2、docker是C/S结构的，所以还需要一个Docker 服务端 server：就是一个守护进程。 docker属于容器技术类里的一种，docker自己的工作机制就是C/S，而其他容器的工作机制可能就没有什么客户端服务端的情况。 3、docker是C/S架构，就需要client使用cli去调用docker API，docker api就是cli咯，这个就好比mysql-client去操作mysql-server一样的。就是Docker-client通过cli连接docker-server，连接上去再通过docker-server管理容器资源。我们操作docker命令cli的时候其实就是这么一个通过docker-server去连接执行的过程。 4、docker除了上面的c/s和API外还引入了模板思想。这就是Docker 镜像 images 模板：镜像，表现形式就是 文件 ； 静态文件 不运行知识占用一定的磁盘空间而已。 容器：镜像副本，实例，表现为 进程 多实例。 运行了就会还会占用内存空间了。占不占磁盘空间呢，占！因为容器运行起来，基于同一个镜像创建的多个容器的话，那么这些容器各有各的log，可能配置文件也不同，所以磁盘空间也是占用的。 5、docker容器container就是从镜像复制出来的一份运行起来而已。镜像复制很多份(你创建容器的时候就会复制的)，每一份就是一个容器。 6、docker镜像放哪呢，放在仓库里Docker 仓库 registry，方便重复使用。类似yum仓库。harbor是私有内部仓库。 容器的生命周期很短，生产中平均下来也就是1 2天，这个说法对吗？肯定不对啊，我docker跑gitlab，你觉的会是1 2天，1 2年还差不多。 容器的IP不想虚拟机是固定的，所以需要一个服务注册，自动发现的机制，可以实现如何访问容器。不然容器的IP老变，我怎么找你呢 # NFS那里也有注册和发现的机制。 因为你要追求容器的灵活性动态性，所以一整套的方案就要考虑清楚。 ​ 在企业里一般先搭建一个registry，里面事先把需要生成中用到的各种镜像全都放里面，当然这是一个持续的过程，事先不一定的哈哈。 ​ 这些镜像比如有nginx官方做的，ubuntu官方、mysql官方做的各个自家的镜像。都传到互联网公共仓库上了。但官方镜像不一定满足你的需求，可能还需要自己定制。 ​ 读图，client运行命令，就会连接docker守护进程，docker守护进程收到这个cli后帮你去执行这个操作，一般docker守护进程也client运行的机器都是一台了。 ​ docker pull就是从仓库拉取镜像，这里也会涉及本地docker私有仓库以及做代理的事项； ​ docker run就是在pull拉下来镜像了基础上，将其run起来，表现为容器。自然可以基于同一个镜像run出来多个容器。镜像就是静态文件，而容器就是进程了。 ​ docker build就是自己只做镜像，自然就会在本地磁盘上表现为文件咯，然后再执行docker push将自定义的镜像推到仓库里去，便于共用。 ​ 以前我们启服务都是sytemctl start xx了，现在都是docker run咯。不需要再去写什么systemctl的service文件了。 容器怎么隔离的-namespace namespace是内核里的一个特性，自然也是靠内核层实现的。 有很多namespace，不同的namespace隔离不同的资源，比如网络的namespace，PID的namespace等。 MNT Namespace(mount) 不同容器的根/其实是隔离的 IPC Namespace(inter-process communication) 共享内存，消息队列都是隔离的 UTSNamespace(Unix Timesharing System) 主机名和域名的隔离 PID Namespace(process identification) 进程编号隔离，大家都是1号进程，但是互不干扰就是隔离。整个进程树的隔离 Net Namespace(network) 典型代表就是ip地址可以一样的，正因为隔离了才能一样哦 User Namespace(user) 用户账号的隔离，就是这个容器上的root和root组和那个容器上的root和root组不搭嘎。 以上就是利用了内核里的namespace隔离机制实现了类似虚拟机的隔离，只不过虚拟机的隔离是OS级别的，而容器时内核级别的。所以比如一台物理机作为宿主，就直接用linux内核namespace实现了隔离，而不是再在其上安装虚拟机了。 namespace时linux的内核，windows里有吗？有也不一样啊，容器又是如何实现的linux的容器在windows也能跑的呢？ windows上跑容器，其实时先安装了一个linux内核的，所以才能通用。 这些namespace也不是一下子一起出现的，有些时后期开发的，比如linux内核3.8才支持user namespace，不过现在内核都还挺高的。 ​ 容器依赖的用户空间--UserNamespace必须是linux内核3.8才支持的，所以3.8以下的内核就无法跑容器，centos6是2.6的内核不支持是容器的，但是可以升级内核来跑容器。 ​ 用ubuntu的好处就是内核升级快，一些容器的新特性就用起来，其实不仅仅是容器，很多基于高版本内核的特性都是如此。 既然内核的namespace是容器之前就存在的，那么其管理工具也是自然有的 ns enter是进入到namespace里👇 如图👇只是654的PID进程，里面可以看到是划分了多个namespace的。 同样pid为1的进程 普遍发现多了以下namespace，比如time 、time_for_children、pid_for_children、cgroup这些上面都没提到，说明是一些的内核namespace了。 这些新增的namespace可能是较新的技术的依赖，不一定是必须的可能。 下图👇-n是进入了654 PID进程里的network namespace里了。 上图还不是演示的很好，应该用docker来演示，就能看到进入到每个容器里的ip地址都是不一样的。 除了隔离还要限制资源的使用-Cgroup 一个宿主上的若干个容器资源的使用肯定要分配好的，否则容易出现争抢的情况。所以这就需要资源的分配和限制。 Control group就是内核中的另一个重要技术，联系，上面讲了内核里的一个技术namepsace 上面的截图里其实已经看看到了cgroup了，就是干这事的。 通过过滤内核文件就可以知道确实是集成了CGROUP的。 总之一句话：docker运行之所以能够做到 \"隔离\" + \"限制\" 就是依靠内核的namespace和cgroup两技术来实现运行空间的隔离和资源使用的分配和限制。 docker里的namespace和k8s的namespace不是一回事 还有一个 docker的namespace其实就是linux内核的namespace，内核什么版本里有什么名称空间，可能高版本的内核会多几个，但关键的就是上文所讲的那几个。 而k8s的namespace是用户空间里的东西，是用户人为创建自定义的，今天创建明天删除都是可以操作的。 容器管理工具-目前就是docker为主 docker提供了完整的管理工具集 一般就是先制作镜像(把业务的各种服务做成镜像)，然后这些镜像复制出来到该落在的地方，然后跑起来自然就变成了容器。 ​ 镜像里存放的东西就是👇App B和Bins/Libs 依赖的这些环境包括：配置文件、库等。 👇下图理解注意下，严谨点不要理解错了：容器是利用了OS本身的内核，所以下图不是容器的分层，容器里是没有内核的，容器是用的宿主机的内核。kernel层画在这里也是表示一个完整的操作整体示意一下的。 容器是有标准规范的，容器以及生成容器的镜像 ，是表现为一些文件，而这些文件是有格式的，是由国际规范的，简称由国际范的~ 只要遵守 镜像 的国际标准，容器的国际标准，理论上用什么容器工具都而已，不仅仅是docker 阿里巴巴的pouch https://github.com/AliyunContainerService/pouch 红帽的Podman 官网地址：https://podman.io/ 项目地址：https://github.com/containers/podman docker 命令和 podman 命令 的操作是无缝迁移的，就是一样的。不过podman是没有C/S结构的。 但是我发现github里podman的版本更新明显要勤快的多的多， ​ 将来会遇到的问题，容器里的网络排错，连ping都没有，该如何排错呢，以此为例的意思大概就是；容器本质上会极其精简的(除非你自己构建的时候把这些检测工具打进去)，在这个精简的容器里很多常规VM里的操作都是无法进行的，于是排错检查的很多常规手段就失效了。不过ping命令总归要打进去吧，还有ss，ps 都可以打进去的。 规范的事情 docker毕竟是一家公司，如果该公司GG，那么容器管理工具是否就缺失了很重要的一个工具了。 所以OCI指定了规范runtime spec和image format spec只要满足规范，就能够保证容器的可用性(移植性、相互可操作性)。 也许吧 用户都是和高级运行时打交道，然后通过高级运行时去调用低级运行时里面的环境，最终来运行容器。 用户一般就关心高级的，底层的不关心。 目前来说低级运行时，主流的就是runc，docker和其他的都是用的runc(运行时) runc其实是个应用程序，通过runc代码程序来运行容器并管理起来。 runc也是docker公司开发的，后来开源了，大家都在用。 containerd也是docker公司开发的，后来跑到社区里了， K8S从1.24开始抛弃docker，其实是不绕走docker引擎调用containerd了，而是直接调用containerd。 docker 命令去连接docker引擎然后调用containerd，再经过垫片的解耦，调用runc，最终运行容器。 shim的作用好比垫片，解耦了containerd和runc，将来这两个开发是独立的项目，各开发各的，只要能对接shim垫片就行了(就是都遵守shim垫片的api接口)，好比螺丝螺栓之间的垫片。 上面一段文字其实就很不错，就是一些开发思路，不过也会增加工作量。应该是随着项目增大才需要这种垫片的存在。 ​ 安装docker的时候，上图的一套都装上了。 docker info查看可见默认运行时是runc，实际的runtimes是io.containerd.runc.v2 据说有些公司就是作，docker不用非要用其他的工具👇，不过K8S里不是有个什么工具嘛，不用docker这个cli不是正常的嘛 什么意思呢，就是说只安装containerd后面的这些，前面的管理工具也就是cli这方面用其他的。 镜像仓库 1、官方的需要本地作缓存代理，也就是nexus之类的统一管理私有仓库，或者verdaccio这种NPM代理缓存库也OK 2、国内的镜像站点，比如阿里的仓库 3、Harbor：vmware提供的自带web界面，自带认证功能的镜像私有仓库，很多公司在用。 4、image registry：docker官方提供的私有仓库部署工具，无web界面，目前用的少。 容器编排工具 docker是单机的 举例，wordpress要跑在容器上，一般就是拆成多个容器： 1、nginx/apache ： 容器1 2、php ： 容器2 3、mysql ： 容器3 三个容器，启动，之间还有一定的依赖关系，apache依赖于php，php又要连数据库，所以启动的时候，1、先启动mysql，2、在启动php、3、再启动nginx/apache。 如果docker cli来实现，就是从上往下先后敲入👇 docker run mysql docker run php docker run nginx 于是就存在单机编排工具docker compose这是官方实现单机的容器的编排工具。该工具类似ansible ansible的玩法是：简单的就ansible命令临时执行下，一些复杂任务就要写playbook了(ansible-playbook playbook.yml)剧本里都写好了谁先执行，谁后执行了。 docker cli 好比 ansible cli docker compose 好比 ansible-playbook xxx.yml 多个容器用docker-compose跑起来就简单些，而且单个容器用compose跑可能也是不错的选择，因为又类似配置文件的一个存在，不是依靠临时的cli去搞。compose可以理解成脚本化的docker cli。 不过单机上的编排也是不够的，要跨主机进行管理就需要跨主机的容器编排工具 docker swarm 这是官方的，也淘汰了， Mesos+Marathon 也GG了 现在就是Kubernets google领导开发的，内部项目为Borg，且其同时支持docker和CoreOS，当前已成为容器编排工具事实上的标准。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:15:38 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/3.Docker在线和离线安装多种方法实现.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/3.Docker在线和离线安装多种方法实现.html","title":"3.Docker在线和离线安装多种方法实现.md","keywords":"","body":"第3节 Docker在线和离线安装多种方法实现 安装 https://docs.docker.com/desktop/install/linux-install/ 所需平台： 所需资源： 说是4G内存，其实2G也能安装。工作中肯定不是2G就能OK的。 生产中的服务器，内存也有高达1T的，就是为了跑好多容器的。 安装方法，我们一般不按照destop这种GUI版，正常就安装Engine https://docs.docker.com/engine/install/centos/ sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Install Docker Engine Install Docker Engine, containerd, and Docker Compose: Latest Specific version To install the latest version, run: $ sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin $ sudo yum install docker-ce # 上面一行其实只要敲这一行，安装docker-ce就行了，后面的都会走依赖包自动安装好的。 -ce所谓的ce就是社区版，还有一个-ee就是企业版。 If prompted to accept the GPG key, verify that the fingerprint matches 060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35, and if so, accept it. This command installs Docker, but it doesn't start Docker. It also creates a docker group, however, it doesn't add any users to the group by default. Start Docker. $ sudo systemctl start docker Verify that the Docker Engine installation is successful by running the hello-world image. $ sudo docker run hello-world This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits. You have now successfully installed and started Docker Engine. 使用国内源的安装方法 https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 操作备忘，我之前就有docker，这里需要删掉 ubuntu方面 这种指定版本安装存在的小问题就是，docker-ce你指定了是24.0.0，但是其他依赖包没有手动指定，就会下载最新的了。 docker-ce是install的时候指定的版本，其他未指定的就下载最新的了。 ​ 所以要安装docker 某个老版本的时候，需要严禁一点，就是手动指定各个包的版本。其实也就是图上dpkg -l |grep docker所列出来的各个包 ​ 最新的版本的安装自然就是统一的，就保证仓库的rpm源是最新的直接yum就行了。 这样就保证了ce和ce-cli的版本一致，当然最好还是全部一致。 我就不重新装了，我没啥要求，yum install就行了，要注意docker的各个包版本要一致，不要踩坑。 这是我的版本 容器里安装docker-client，并不是套娃，而是client的命令 server就用宿主机的 client就调用远端的server执行docker cli 场景： 容器里面将来可以执行docker cli连接远端的docker server也就是docker engine来执行 docker操作。 比如👇C1容器里执行docker cli把C2容器给停了，当然C1要通过docker cli 命令连接到宿主上的docker服务进程来执行命令的。 ​ 其实就是本来docker-cli都是在宿主上敲的命令，现在可以在运行的容器里面执行，换而言之，C1一旦安装了docker-cli就可以变相的等价于宿主机上敲docker-cli以此达到控制宿主上所有容器的效果。 ​ docker的命令真正的执行都是docker-server来执行的，docker-cli在哪里敲都一样，你在宿主上敲和在容器里敲都一样的效果。 ​ client和server可以不在一台机器上，网络可达就行了，之前看书的时候也试过cli里可以指定连接的engine也就是server的。 就跟mysql client可以管理本机的mysql db，也可以管理远端的db一样，cli里指定-h a.b.c.d就行了。 那么问题来了该如何在容器里安装docker-cli命令呢 1、通过镜像提前封装进去，可以的，可能就是不太灵活 2、使用官方通用的安装脚本来弄 两条命令就ok了👆 docker的二进制安装-适用于无法上网的机器 然后二进制安装的一个优点👇就是可以明确看到之前的这几个模块， 再写个service文件并做好开机启动 一般就是docker cli就和本地的engine连接就行了，所上图的socket一般也不会修改为让别人机器远程dockercli连过来的ip+port了。除非特殊需求~ 再一个docker是以root用户运行的，因为很多系统底层的资源的权限需要root调用。 二进制安装脚本可能需求按需修改的地方 1、版本和国内镜像源 2、下载的URL，由于是国内的所以路径要修改的 其他无需改动就是下载后，解压，写service 然后运行就可以了 离线安装的时候-也就是内网无互联网访问的情况下安装，就是需要把安装包下载下来，因为docker是go写的，所以都是静态库，比较好安装的，依赖问题少。 脚本在视频目录层的课件目录下有的，要用去那里那就行了👇，以上是补充👆 其实也没啥，就是下载现成的离线包，解压，二进制执行PATH路径都可以直接移动到/user/local/bin下就行，以及service这些常规操作。 别人跟你要要docker 3.0 ，你别当真哦，呵呵~ 也就是说这些文件，拷贝到任何机器上，都可以正常运行的。所以这些复制到容器里面即使那里是极其精简版的linux也照样可以运行。 pstree -p可能也可以看到进程之间的调用，不过不一定全，因为rocky和ubuntu也不太一样，然后就算ubuntu的pstree可以看到👇，也不全，可能是还需要进一步启动容器后才能看全吧。 监听在socket文件上，也就是docker cli通过这个socket来和dockerd服务也好引擎也罢来通信的。 下面就开始学习docker命令了，通过命令学习技术的逻辑和方案的落地，而不是去机械的命令，我反正从不记命令，这也是我写这些文章的底层原因，因为我学了就忘了，连小时候考完试就忘，从来都是如此。但是话说回来，要用的时候忘记了可不行啊，所以还得在这里写笔记以备不时之需，到时候也是需要鼓足勇气耐着性子复习一遍的，反复多次，捡起来的速度就会很快，比如一些投影仪一些智能照明的设备如何在云上操作，他们其实就是只能穿1层NAT，两层就需要在内部的最近的一层NAT里做好DNAT了，至于为什么能够穿一层，那就是之前我还没上传的ssh隧道里的知识了，是我同事遇到了安装师傅的协作要求放行内部AC的DNAT端口，其实就是AC上作DNAT，我就可以立马反映过来其实不是外部发起的流量，而是内部智能电灯发起的ssh隧道，SSH -R 9527:destser:5000 -Nf CloudSer 正因为兄弟我不止一次复习过那片文章，也是我自己记录的，所以才能够快速反应过来，但是cli我是不记得，逻辑我是喜欢的，但是生活往往要放弃逻辑，这又是另外的话题了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:15:55 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/4.docker命令基本用法说明.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/4.docker命令基本用法说明.html","title":"4.docker命令基本用法说明.md","keywords":"","body":"第4节 docker命令基本用法说明 cli分类 swarm不行了，被k8s替代了，不学。 管理类👇 container 管理容器 image 管理镜像 network 管理网络 volume 管理存储 进一步还可以看docker image 大类下的子命令👇 但是老司机不爱这种分类的cli，更爱直接点比如 docker image ls 就= docker images docker container ls = docker ps docker info 还可以看到docker compose的版本👇 以及 server 里的容器： running运行的几个，paused暂停的几个，stopped停止的几个 docker 跑一个hello world， 1、要知道docker 跑容器，容器靠镜像，镜像哪里来呢 2、本地找找 3、仓库找找 上图就的Hello from Docker！字眼 就说明执行成功了，容器就是运行完就释放掉了，这里又不是当作守护进程来跑的，所以docker ps里看不到了就。 docker pull hello-world 只下载 不run docker run hello-world = docker pull hello-world + docker start hello-world 注意知识容器进程退出了，只是程序退出了，这个容器还在磁盘上。 进程没了，磁盘空间照样占着的会， start 起来后运行完了也会退出的 很重要的一个目录-image下载存放目录 这个目录是默认存放镜像和容器相关文件的，这个目录没了，所有容器 和 所有镜像就都没了。 速度线索尝试总结，我感觉要单拎出来①★：这个目录必须是一个高速且大的磁盘，又高又大，因为所有的images和containers都默认扔在这里，跑起来，硬盘慢也会拖累容器的运行速度。生产中可以找个目录单独挂一个高速的SSD，没预算就算了哦，一起ssd也是ok的，哈哈，缝缝补补有三年也是ok的 彻底删除容器 1、卸载软件：apt purge docker-ce # ubuntu的 centos的“：yum remove docker-ce ? 还是yum history list 找到id后yum history undo xx 卸的还全呢，对吧。 2、删除软件产生的文件：rm -rf /var/lib/docker 学到这里，我又要逼逼两句了，删除A，就是要两步走①删除A②删除A的影响，有首歌叫 你身上的香水味~~，就是人走了，还得通风。HA也是这个道理，不管是线路的HA还是节点的HA，当HA的成员挂了一个，作为HA的底层逻辑同样需要①去掉故障成员②去掉故障成员当前的影响(比如全网ARP，会话缓存、MAC缓存等)③如果故障恢复一样要观察一下是不是真的恢复了稳定了才能将故障点重新加回HA并继续提供服务。 docker info也是可以看到整个默认存放的关键目录的 可以换成成别的目录，比如换到一块牛逼plus的单独硬盘上去，或者将整个目录挂过去都是一样的。 搞一个run后不退出的容器 镜像是基于不同的操作系统做出来的，所以docker run也好pull也罢都是要注意的， 然后还可以到tag里去看细节 👇看着就是不同系统的镜像，docker pull nginx就是下载linux的且是latest的版本。 alpine是一个精简的linux os。是基于某个os制作出来的镜像。区别默认的应该就是deban和ubuntu的os做出来的。这句话怎么了理解哦： 就是docker的分层模型 container层就是docker engine了实现了容器的隔离，通过namespace和cgroup两大组件做到了隔离和资源的分配以及限制。 那么要理清楚一个点：就是 这些版本代表的意思是啥，对分层理解就会加深一点， 根据GPT多说，可以认为她讲的是对的，我们下载的镜像它肯定不是dockerengine ，docker engine是docker server进程，而images名称里的os就是说我整个image利用了底层宿主os的内核(大家都是linux内核啦)，然后我image里还自带一些基于整个内核实现精简os，整个os是半成品os是没有内核的os，GPT称之为用户空间--Alpine linux的用户空间，所以这部分+宿主的内核就构成了完整的os了。 之前学习sendfile这个0复制技术逻辑的时候，还头头是道，其实殊不知当时的整张图就是在OS里聊的，对吧，用户空间，内核空间，这些就是OS啊，唉，蠢笨如我~~~ user Space 1 2 3 就是你下载的image 并且运行起来的 容器，这些容器通过 container也就是docker 引擎 (namespace + Cgroup) 来隔离且限制地使用了HOST OS的内核，但是OS本身还有用户空间，这部分userSpace1 2 3 的容器是没有使用的，他们各自用的自家的image里封装好的自带了os一部分的用户空间。 好了反复拉扯应该靠谱了 容器运行起来就是一个小OS，自己image里的用户空间 + 底层宿主的OS里的内核空间 构成了一个完整的OS，那么是有自己的IP地址的。查看方法docker inspect # 其实还可以inspect 镜像 网络 存储 等所有资源的详细信息，好比一个万能命令。 inspect里是有容器的IP的，ip a是看不到的哦👇 再run一个nginx，ip就是0.3，按需发放的。 · 再把容器启动起来，大小就恢复了 图中N是image的nginx的n简称，N1 N2就是两个容器，各自启动就会各自从image复制一份镜像过来。停止N1后从镜像复制过来的那部分就删了，但是N1的数据还在。 不要用swap，算作内存优化吧 以前的docker info 就是老版本会有一个提示就是swap的限制limit 解决方法就是，关闭swap，内存不够加内存，别TMD用磁盘作内存，到时候速度跟不上麻烦的很！ docker 和 k8s都 要关掉swap！ swapoff -a # 临时禁swap 在/etc/fstab里删掉挂载swap的那行，保存后，并swapoff -a一下。 还可以停止swap的服务 mask就是停掉了，怎么不是stop和 disable呢。 mask掉，start都起不来的， docker0也是网桥 确实下是否是网桥，安装查看工具 如果开一个容器，容器也会生成专门的网卡，容器的网卡就会和这个网桥docker0关联在一起 而且这些容器专门的网卡ip a都能看到，只是IP还需要docker inpsect去查看 这点和kvm一样👆：关联网卡，以及ip地址，docker是172.17 kvm那里是192.168.122；都可以改。后面专门单篇讲网络。 同样和kvm一样的，iptables 规则也会自动生成 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:16:10 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/5.Docker配置优化.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/5.Docker配置优化.html","title":"5.Docker配置优化.md","keywords":"","body":"第5节 Docker配置优化 docker 工作是基于镜像，而镜像存储的驱动，通过docker info可见是overlay2。 这些存储引擎不管是那种，都是联合文件系统 比如ext4 xfs的文件系统，磁盘挂载到目录，是不可以重复挂载的，也就是一个目录你两个硬盘挂载的时候，前面一个就被顶掉了。 而docker的联合文件系统时可以把多个设备(所谓设备就是硬盘、分区、lvm这些)挂载到一个目录下，其效果就是这个目录下既可以看到设备1上的数据，也能看到设备2上的数据。 ​ 为什么可以采用联合文件系统，就是因为docker的image是多层镜像。 image是一层层构建出来的 容器生成，是复制一份镜像，并在其上又添加了一层writable可读可写层，这层就用来存放容器自己的数据--比如LOG。 docker stop xxx ，就是把复制过来的image层删了，其上的容器子深深的writable层还在的。 容器启动的时候其实是可以修改存储驱动的 不过，想来一般是不会去改的。确认是overlay2就行了。万一不是就换docker版本~ 查看支持的文件系统👇注意aufs是aufs，autofs是autofs，不要瞎搞 docker优化 下载源，其实应该私有库作缓存来弄比如nexus 可以使用阿里的个人镜像加速地址 其实最好是用私有缓存库比如nexushttps://cloud.tencent.com/developer/article/1764866 socket通信-docker-client和server之间的通信 默认走的本地socket文件，无法支持远端的调用。 配置方法👇下图绿框框；但是用的不太多。一般就是本地连接。 该配置文件类似cli里的-H选项，既有socket文件也有tcp，可能就是写两个-H咯 还可以改service文件 先通过修改daemon.json去试下 但是报错了👆 换个思路，去service文件里改改 然后把daemon.json里的hosts那行删了，修改为 再reload 和 restart 此时tcp和socket文件都好了 此时就可以远端client 调用 server 了，也就是docker cli 连接 docker server引起后 调用命令了 然后远程跑一个nginx看看，此时容器都是退出的 去远端windows的cmd里执行远程cli，run一个nginx 然后去server看下docker ps 这就实现了远程执行docker cli 同时curl 瞧瞧，因为是容器里的ip没有暴露出来呢，所以ipsect看到ip后宿主机上curl下👇 要注意docker engine也就是服务端在哪，容器就运行在哪里，客户都的cli只是一个连接调用工具，很好理解，好比mysql 远程执行命令一样。 将来在K8S里的跨主机的远程docker调用可能就是用的这个。目前docker学习阶段还用不到。 看来是查不到就往下一个地址查找。 现在都是systemd了👇 不用改了，以前是这么改的👇也是daemon.json里 还有这个容器和镜像存放的路径-迁移就是复制粘贴该路径 默认是这里👆，这么改👇改到单独的高性能ssd上去，前文讲过了，目录不该直接挂到新磁盘上去，或者该目录这个目录挂过去也行👇 在旧版本的 Docker 中，可以使用 \"graph\" 指定 Docker 存储容器和镜像的路径。但是在较新的 Docker 版本中，该选项已被废弃，取而代之的是 data-root。因此，您可以尝试将 \"graph\":\"/data/docker\" 替换为 \"data-root\":\"/data/docker\"。 修改后的配置文件内容如下所示： jsonCopy code{ \"registry-mirrors\":[\"https://si7y70hh.mirror.aliyuncs.com/\"], \"data-root\":\"/data/docker\" } 修改完成后保存文件，并尝试重新启动 Docker 服务。您可以执行以下命令： bashCopy codesystemctl daemon-reload systemctl restart docker 还可以在service文件里配置 systemctl daemon-reload下，再重启一下systemctl restart docker 此时，会自动创建文件夹，但是docker images里是空的了，docker ps -a也是空的，之前退出的那些容器都没了。 原来的容器和镜像的文件都没过去 max-concurrent-dowload|upload，同时拉去和上传的限制 这个操作会涉及max-concurrent-dowload参数的限制，但不是完全涉及，如果images在本地就不用担心，知识限制了和docker 仓库之间的下载/上传的并发数。 日志的问题 1、容器的日志都是默认放在硬盘上的 2、日志的大小要限制下 3、日志文件也是落在容器和镜像存放的路径/var/lib/docker下的 通过docker info可见，👇当然我的路径改过的 log就在docker root下的container文件夹里的每个容器文件夹里。 限制配置 日志的最大值和滚动覆盖👆 docker日志越来越大把硬盘撑满了，如何解决 1、上图👆限制+滚动日志 2、docker也是在容器和镜像的目录下的也就是/var/lib/docker/，将这个目录挂到单独的磁盘就行了。 3、周期清除无用的镜像和容器👇 curl伪装浏览器，也就是伪装访问方式-A可以-H也行 实验 1、先跑一个nginx 2、然后curl访问这个容器的ip 这就使用curl 伪装了一个用户访问方式--user-agent 3、找到有该user-agent关键词的日志 总之就在这些文件里 很贱的grep -r就行了，不要用-R -R是会继续搜索软连接的，比如上图的第一个No such file or directory就是一个ln -s👇这种搜索一般没必要 将来日志会越来越大，而且不仅仅日志，images和容器也会越来越多，之前的处理方式有一种(是在runner上的操作可能会简单的）， 清除无用的镜像容器慎用 live-restore 如果docker服务重启了，容器会不会重启？不会，docker重启，容器就停掉了👇 反之删除配置文件里的live-restore，重启docker后，所有容器就停了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:16:22 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/6.Docker镜像搜索和下载.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/6.Docker镜像搜索和下载.html","title":"6.Docker镜像搜索和下载.md","keywords":"","body":"第6节 Docker镜像搜索和下载 镜像管理 比如从网上下载了基础镜像，然后在上面封装了一些网络检测工具， 然后继续封装jdk，tomcat，最后跑容器就是又添加了一层可写层，而可写容器层往下都是镜像层是只读的。 未来打镜像就是采用这种逻辑不断地打，最终看到的镜像其实是多个镜像累积起来的结果。有点像mysql的增量备份，每次增量备份一点点，最终累加起来的才是完整的数据。 分层构建，将来每一部分 都可以独立抽取出来，用来构建其他镜像，也就是说镜像层是可以复用的，正以为分层所以复用起来很丝滑。 根镜像、父镜像、子镜像、子子镜像，大概有人会这么称呼 这个每一层的将来复用，不知道是不是联合文件系统 自己去复用啊，我感觉应该是docker自身能够复用才行啊。而不是用户手动去复用，用户没办法说去复用哪一层吧，没这个操作的空间啊。 通过docker image history nginx查看人家是怎么构建每一层的 镜像有了，创建好了，拉好了，落在本地其实都是表现为一个个文件夹的 最大的文件夹就是overlay2，里面的东西其实就是很多层，overlay就是层的意思，是所有images的层级铺开来的，才能共用，复用。这就是所谓的联合文件系统里的overlay2这种。 删除容器后 删除容器后，发现这些分层的东西就少了很多👇 docker rm删除容器和镜像👆 进一步再删除镜像，后发现overlay2里就空了 也就是说容器和镜像删光了，overlay2文件夹里就空了，现在pull一个nginx看看 overlay2文件夹里的就是分层的数据了👆。一个文件夹就是一层但我没仔细数哦，哈哈，大概如此吧 镜像去哪找 1、hub.docker.com 要用官方的镜像 2、cli，命令行搜索的方式不全，没有上面去网站搜索的方式好，就是去hub.docker.com还有其他的比如谷歌的站点。 自己做的镜像也可以上传上去，只是别人一般是不敢用的。 还有比如k8s的一些镜像，k8s就是谷歌开发的，对于谷歌来讲，docker公司肯定没有他们家的大，所以谷歌的镜像也就没有放到docker的仓库里，而是谷歌自己的仓库里的。 比如下图的这些很多就是谷歌的，但是docker上也有，就是别人上传的方便下载的 3、代理的配置方法 其实就是科学上网咯 上图就是配置你的代理机器，走代理的意思，可惜是错误的👆实测这样配置服务都起不来，其实你完全可以做在路由器上，就不用管这里的怎么配置。 不过这里的配置方式也要知道，方法越多，应用起来就越灵活。 设置配置文件里的配置，还有一个脚本是修改系统层面的代理，同样也会被docker读进去的👇 以上是部分咯，修改代理的方法，三选一就行了👇参考https://blog.csdn.net/peng2hui1314/article/details/124267333 总结如下： 1、网络里的路由器层面直接做路由，略 2、主机层面，的系统代理 3、docker服务层面的代理，就是上面的脚本，其实也就是服务文件里的三行内容，其他都是配套语句呵呵👇 4、容器里面， 5、可惜daemon.json这个文件的配置我没有找到上图图示的配置成功的案例，也许是老版本的配置，类似hosts了吧，不管了。 不过容器里的配置倒是和第3点-容器里的配置一样的，哈哈。 镜像制作 比如java程序需要一个镜像，要做java镜像， 然后java又依靠JDK，又要做JDK层对吧 然后JDK又依赖linux，又要做linux层， 一般来讲，是系统镜像就从官方拉取(系统镜像前面的文章也解释过了，其实是没有内核的用户空间的部分)，而业务镜像才是自己制作。 这就回答了下图👇的问题，用户空间里是有os的，只有os的一部分，然后再结合下层宿主的os的内核 就能构成完整的os了。以此来提供服务的。 然后docker制作镜像的时候，底层不是要linux嘛，一般也不会选择ubuntu，centos、rocky、redhat都不会，因为都太大了，压缩后还要25MB 都太大了，所以会选择一些积极精简的linxu，比如aphine或busybox👇 alpine就是基于busybox开发的 生产中用的比较多的alpine 1、包安装工具 2、alpine的仓库配置 3、用法也不一样 ubuntu的仓库文件 alpine是不同的 ustc是中科大的 alpine也是有完全镜像和极简镜像的，好比centos的all-in-one和mini一样👇 指定版本下载，要去网站看看版本的格式 然后再复制，粘贴就行了 然后网站上看到的镜像是压缩后的，pull下来会变大就是解压了应该👆 cli 用法 只显示镜像id：docker images -q # 操作镜像靠ID是不好识别 批量删除就很方便 xargs -i 就行了；或者直接docker rmi `docker images -q` cli用法，显示镜像和tag： docker image ls --format # 操作镜像靠名称和tag，也可以识别 删除也要加上tag的，否则就是删除就是删除latest这个tag 容器跑着的时候删除image，不会真的删掉 被占用的镜像如果使用docker rmi -f 'IMAGE ID' 这种方式 delete 肯定和写 image name tag不会delete，而且 连untag动作都不会执行。 业务也ok 此时即使停掉容器，删掉容器，那个镜像也不会自动删掉，就成为了一个无名的镜像，也叫dangling镜像 只查看这种dangling镜像的方法，docker images -f 就是filter过滤出来哪些特征的images 删除这种就这样docker rmi -f $(docker images -f dangling=true -q) # 就行啦 或者docker images -f dangling=true -q |xargs docker rmi -f # 也行 不过要停掉占用该镜像的容器，否则还是会删不掉👆 系统中有一个清理的命令docker system 这样不仅仅是没有用的images，其他的停掉的容器和没用的网络，缓存都清了 有些容器停了，但是不是说就可以删的，你就不能用这个命令了！所以这个prune还是慎用！ inpsect是通用型命令 上图的问题就是，images里看到的container应该就是当初这个镜像是通过容器commit出来的，所以会带上容器字眼👇 然后详情如下，容器、镜像、网络 ，都这么看👇 [root@nginxproxy ~]# docker inspect 05455a08881e [ { \"Id\": \"sha256:05455a08881ea9cf0e752bc48e61bbd71a34c029bb13df01e40e3e70e0d007bd\", // 镜像的唯一标识符。两个镜像的内容一样(也就是RepoDisgests一样)，但是构建时间不同或者名称tag不同，id也是不同的。 \"RepoTags\": [ \"alpine:latest\" ], // 镜像的标签，这里表示这是 alpine 镜像的最新版本。 \"RepoDigests\": [ \"alpine@sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\" ], // 镜像的摘要信息，用于验证镜像的完整性。 \"Parent\": \"\", // 父镜像的ID，为空表示此镜像没有父镜像。 \"Comment\": \"\", // 镜像的注释。 \"Created\": \"2024-01-27T00:30:48.743965523Z\", // 镜像的创建时间。 \"Container\": \"4189cbc534955765760c227f328ec1cdd52e8550681c2bf9f8f990b27b644f9c\", // 用于创建此镜像的容器的ID。看来很多都是从容器直接commit提交出来的镜像，而不是docker build出来的咯，可能是~ \"ContainerConfig\": { // 创建该镜像的容器的配置信息。 \"Hostname\": \"4189cbc53495\", // 容器的主机名。 \"Domainname\": \"\", // 容器的域名。 \"User\": \"\", // 容器内命令运行的用户。 \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, // 这些设置控制是否将stdin、stdout、stderr附加到容器。 \"Tty\": false, // 是否为容器分配一个tty设备。 \"OpenStdin\": false, \"StdinOnce\": false, // 这些设置控制容器的stdin。 \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], // 容器的环境变量。 \"Cmd\": [ \"/bin/sh\", \"-c\", \"#(nop) \", \"CMD [\\\"/bin/sh\\\"]\" ], // 容器的默认命令和参数。 \"Image\": \"sha256:9a5ce069f40cfe0f2270eafbff0a0f2fa08f1add73571af9f78209e96bb8a5e9\", // 创建容器时使用的镜像ID。 \"Volumes\": null, // 容器使用的卷。 \"WorkingDir\": \"\", // 容器的工作目录。 \"Entrypoint\": null, // 容器的入口点。 \"OnBuild\": null, // Dockerfile中的ONBUILD触发器指令。 \"Labels\": {} // 容器的标签。 }, \"DockerVersion\": \"20.10.23\", // 创建镜像时使用的Docker版本。 \"Author\": \"\", // 镜像的作者。 \"Config\": { // 镜像配置，类似于ContainerConfig，但用于运行时。 \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/sh\" ], // 容器启动时执行的命令。 \"Image\": \"sha256:9a5ce069f40cfe0f2270eafbff0a0f2fa08f1add73571af9f78209e96bb8a5e9\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": null }, \"Architecture\": \"amd64\", // 镜像的架构。 \"Os\": \"linux\", // 镜像的操作系统。 \"Size\": 7377074, // 镜像的大小（字节）。 \"VirtualSize\": 7377074, // 镜像的虚拟大小。 \"GraphDriver\": { // 镜像的存储驱动信息。 \"Data\": { \"MergedDir\": \"/var/lib/docker/overlay2/bb16c3d711597eff0baab5640f474ef4c8c1c40df0351e0a21343e1362676504/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/bb16c3d711597eff0baab5640f474ef4c8c1c40df0351e0a21343e1362676504/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/bb16c3d711597eff0baab5640f474ef4c8c1c40df0351e0a21343e1362676504/work\" }, \"Name\": \"overlay2\" }, \"RootFS\": { // 镜像的根文件系统信息。 \"Type\": \"layers\", \"Layers\": [ \"sha256:d4fc045c9e3a848011de66f34b81f052d4f2c15a17bb196d637e526349601820\" ] }, \"Metadata\": { // 镜像的元数据。 \"LastTagTime\": \"0001-01-01T00:00:00Z\" } } ] 补充个细节，运行中的容器，删除其使用的镜像，此时rmi -f imageID是不会执行的，rmi -f imageName也只能untag也就是去掉名字 而已，此时image就是none，成为dangling镜像， 于此同时运行的容器image字段就不再是nginx了而是镜像的ID了👇 podman是红帽的docker容器管理工具 习惯很重要-pull的时候指明名称和tag pull的时候要带上版本的，不带就是latest，你今天的latest，和你下一次或者几个月后的latest基本上就不是一个版本镜像了！一定要清楚的哦~ ​ 写上版本，运维清晰，不会出错。 然后比如需求是，用最新的alpine，那你是不是要docker pull alpine，也是不可以的，网站去看下 但是pull之前还得打开浏览器就不delicious，于是找了cli的方式👇 利用xargs 排个版就看的一目了然了 肉眼可见3.19.1 是...最新的。。。 这会又发现不一样的了，20240329 结果比3.19.1还新 那么这个20240329在hub.docker.com上为何看不到，搞不好这压根不是官方的镜像，哈哈。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:16:36 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/7.Docker镜像导出导入.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker安装和镜像管理/7.Docker镜像导出导入.html","title":"7.Docker镜像导出导入.md","keywords":"","body":"第7节 Docker镜像导出导入 镜像导出和导入-比如互联网下载后导入开发内网去-导出要用name:tag docker save 默认是标准输出也就是屏幕打印了，所以需要-o write to a file ,instead of STDOUT 👆上图是屏幕拒绝了这个输出，其实还是有STDOUT动作，也就是说可以在上图的cli后使用重定向的。 因为save出来是tar包格式，所以要加上.tar tar包只是打包的，没有压缩的这里，所以还需要压缩一下，减少磁盘空间占用 优化命令，就是导出来就是压缩的效果 1、首先使用重定向 2、然后 怎么直接打出来的，大小还少了11KB了👆，呵呵，应该数据没丢哦，哈哈哈；回头导入的时候看一下就知道了。 3、复制到其他docker机器上load load的时候👆-i或 的原因是当初save导出镜像的时候用的是ID而不是名称+TAG，所以源头就错了 即使你想通过tag去补也是不行的，因为你压根就不知道这个ID是什么镜像inspect也看不到了 除非你run起来进去看看是个啥容器，再看看image的哈希值，然后去官网比对哈希值，然后打上对应的版本哈哈 搞错了，再来，导出的时候不带tag就是所有tag达成一个包，带tag就是那个tag的导出咯👇 把一个机器的所有镜像导出导入到别的机器 补充，你也可以指定下载哪个，而不是通过自动判断你是amd64的， 就是没有tag了，自己打咯用上面的docker tag 来打 所有镜像导出 1、也算是一条命令搞定吧 因为压缩要时间的，所以你自己选择具体cli👆 综上所述，推荐的cli是 sort卡住不动的CPU👇 一个sort 100% cpu 啊？不正常。正常，稍微分析下就知道答案了： ​ 卡的原因也好理解--就是docker save xxx 其实就是images的导出内容了，是内容啊，所以很大，一直在sort排序，所以sort处理大量的数据了就，什么你表示怀疑，哦，那你用docker save xxx |cat 看看就知道啦 你说sort忙的过来不，这还是一部分，下面没完没了的乱码 二进制 格式👇 纠正截图👇 删除所有镜像 docker rm -f `docker images -aq` ，再导入就全部回来了👇 这样也行： 还有虽然是tar包，但是解开后是看不到images的，而是一层一层文件，还得是load才行 然后看看tar包里的这些文件吧 一个解包多出这么多，继续 发现👆layer.tar解开里面就是一个精简的 linux的 根目录 / 对比发现，少了boot这个么一个重要的文件，这很好理解，bootfs可不是在容器里，而在kernel里是宿主的内核，容器里只有rootfs层以及以上--这些是用户空间的东西。 然后继续看 这个精简镜像里的bin下的二进制都是busybox这个二进制的别名。 所以这些cat 、 df 、grep、mv、ls各种linux的命令都是busybox这个二进制模拟出来的，果真很busy啊~ 尝试使用该牛逼的二进制，结果发现用不了👇 难道有库依赖，还真有👇 然后busybox容器里的分层文件里，果然有这依赖库的👇 只要把这个依赖复制到宿主的/lib下应该就可以了，因为要在宿主用，宿主找的这种so文件还是会从/lib下去找吧 将依赖复制到宿主的/lib/或/lib64/下，记得退出一下，否则还是识别不到 可见busybox --list 得到具体的指令 我用xargs 做了横向排列，看到了ping 于是busybox用ping 优秀👆 very good 👆上图可见busy的大小是极简的，不仅仅cli少，而且cli里的某个命令的功能也是少很多。但是也够用了👇 补充：打tag的 下载的时候也要指定使用哪里的镜像，否则就是从deamon.json配置的仓库下载，什么没有json文件，那就是默认的官网了。 上图👆是tag打了要上传到服务器的，所以要这么打，到时候，push的时候也就是推送到对应的服务器和目录的。 如果是本地用，就只写个镜像名和tag就行了 总结 1、docker安装，还是要先把仓库配置好的，具体操作很简单，就是 https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 2、离线安装，见 离线安装也分官方脚本和非官方的，哈哈 https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 3、小结下 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:07:02 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/Docker容器管理和镜像制作.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/Docker容器管理和镜像制作.html","title":"Docker容器管理和镜像制作","keywords":"","body":"Docker容器管理和镜像制作 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/1.Docker运行容器的常见用法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/1.Docker运行容器的常见用法.html","title":"1.Docker运行容器的常见用法.md","keywords":"","body":"第1节 Docker运行容器的常见用法 docker怎么管理容器的 概述 互联网上的镜像，不需要登入，直接下载，一般公司私有的镜像才需要login后去下载 COMMAND ​ 可以修改为自己希望执行的命令，以及加上ARG也就是跟上参数，但是加的的cli必须是人家容器里有的命令否则无法执行的。比如上图hello-world容器里只有/hello这个命令，其他的红色框框👆里的cil都是没有的， 所有都是报错的。 # 我就想到其实容器里是否可以封装一个busybox呢，哈哈基本很多常用cli就都有了。我觉得还不错~ ​ 镜像就是没有boot(内核)的根文件系统+app程序文件(mysql、nginx、等各自的程序包括各自依赖的库)。 下个busybox看看多大是否可以后面集成到我工作中的images里 1、首先hub.docker.com搜索latest 2、然后找哈希值一样的版本号， 有同学问，那你为什么不直接下latest，你猜我为啥 5MB不到，可以接受吧~ alpine举例，说明commond的是前台cli，否则就是运行后即退出 ​ 容器运行起来不退出，要求容器运行时对应的COMMAND的是前台执行，而👆/bin/sh是一个后台执行的cli，所以运行后就提出了。类似docker run nginx执行了就是前台占用。 ​ 所谓前台执行就是执行后就霸占了前台，而传统systemctl start nginx这种启动方式在容器里就不能使用了，因为后台执行，容器里执行就退出了。 怎么停不掉啊👆，关闭窗口还是up的，因为没有-it交互，你的ctrl c 发不进去👆 加个-d 放后台，但是不会退出，因为用的cli 是前台就行。 --name选项，指定容器NAME 比如指定容器的NAME方便后面，比如容器之间的互访 随机的名字肯定可读性不好👆 -it交互模式进入容器里，看看文件，看看状态等操作 -i是交互，还不够，还需要一个tty接口，类似交换机的vty -i -t 还不够，还需要交互的sh否则👇 -it 配合什么shell，就是尝试尝试就行了 👆这就进来了 不仅仅有bash也有/bin/sh 只是用了bash交互，run的command还是原来的那个脚本 进来后， 通过👆boot里是空的就知道了，容器用的是宿主的boot(内核)，所以这里只是空的。 这个nginx是基于Debian做出来的，Debian系列的典型代表就是ubuntu了，apt就是yum了 安装ps ps就有了👇 容器里用sed修改为清华源，安装更快，调试更方便 容器安装东西大多数都是调试用的。 sed -i 's|http://deb.debian.org/debian|https://mirrors.tuna.tsinghua.edu.cn/debian|g' /etc/apt/sources.list.d/debian.sources sed -i 's|http://deb.debian.org/debian-security|https://mirrors.tuna.tsinghua.edu.cn/debian-security|g' /etc/apt/sources.list.d/debian.sources 容器exit就真的将容器退出了，即使容器本来时run起来就前台执行的那种，你用-it bash进去再exit，容器就不再时running了👇 如果不想exit退出的时候把本来应该前台执行的容器退出也可以用ctrl p q 组合键来安全退出，不过此时nginx就不再时前台了，而是后台up了，类似于docker run -d nginx了 上图👆不是nginx这种run起来就up的，一样也会被ctl p q变成后台up👇 这下倒学到了一个让hello-world后台UP的方法，赶紧试试👇，不行人家压根就没有shell，所以无法进去。而有shell的倒是可以用这种方式后台UP着。 run -it用的少，大多数都是exec -it来针对运行中的容器进行交互排错 比如run了一个nginx 发现某个问题，需要进去排错，于是docker exec -it进去 hostname只在容器内部生效，不具备容器互通的功效，只是在容器内部可以拿来本地通信。 通过这个hostname和本机的程序通信 容器运行起来后，退出自动删除，临时测试屁股擦的比较干净 只要容器run完退出的，那么就给你删除 ​ 所以，要做守护式容器，就得至少有一个进程是前台永久运行的👆 很多添加前台永久运行的手段： docker run alpine:3.19.1 tail -f /dev/null 👆上图就是前台运行不会退出，防止被ctrl c退出，于是加一个-d，是先保证一个前台cli再放到后台👇 然后前文也说了，这里也复制过来一并看下👇 怎么停不掉啊👆，因为没有-it交互，你的ctrl c 发不进去👆 放屁-it也不行 关闭窗口也没用 只能docker rm -f了 👇 那么问题来了， 1、docker run alpine tail -f /dev/null 无法ctrl c 中断 2、docker run nginx 为啥可以ctrl c 中断 难道nginx的COMMAND里的也就是xxx.sh里是用了-it的？ 不行1：用sh -c 包装也不行，就是不接受ctrl c，不知道为啥 docker run -d nginx -d的后台和nginx的前台 1、-d的后台是docker run 这个命令后台执行 2、nginx容器前台执行，是nginx容器的COMMAND是容器里的cli是在容器里前台 所以要区分开来 一般都是-d 在后台run，服务类的容器肯定要容器里的cmd是前天的。 nginx为例，容器里的cmd是前台执行的，日志(比如访问日志)就是屏幕STDOUT的，docker run -d run在后太后，日志就用cli去看 后台运行-d 的容器log查看也简单docker logs idxxx就行👇 docker logs -f xxx 就很nice 还有传统艺能watch -n xxx 下图是wathc -n 5 docker logs nginx001的效果👇，就是-n 5秒刷一次，也是实时的。 容器的自启动，不是说原来exited，变成了up，而是说up还是up 1、之前学过一个，这是docker 服务重启，容器原本UP的还是UP。 stop 过30s start 都是ok的，就是说容器随docker服务，原来启动的，还是启动的 当然原来是停止的，就不会自启动了。 2、容器服务重启的时候，重启的时候容器的状态是什么 + policy 是什么 ==> 重启引擎后 容器的状态会是什么 no ： 退出了不重新UP， 这是默认值，# 重启的时候容器是退出的（不管是异常还是正常），那么重启服务后，容器还是退出，这就是no。 on-failure[:max-retries] : 异常退出就重新UP，尝试N次。 non-zero exit status，这是$? ≠0 的意思，就是异常退出，状态码不等于0。 # 重启前容器是异常退出的，重启服务后，容器就会尝试起来。 always ： 无论退出码是什么--也就是不管是正常退出还是异常退出--也就是不管认为退出($?=0)还是异常退出($?≠0)都随宿主机器起来而起来； # 重启服务(啥重启机器，重启机器也是对应到重启docker 引擎，搞搞清楚，讲的什么东西，当然整体还是不错，这里讲的很，不好说什么了)的时候容器不管是怎么退出的，是你重启导致退出还是本来就是认为退出的，重启后都给你起来。 与no相对。 unless-stopped ： 重启机器(服务)的时候容器如果是人为退出的，那么启动了就不会给你UP容器，如果容器是异常退出的，比如重启的时候容器还是UP( 那么重启的时候容器就会是异常退出 )，重启后容器就给你UP起来 然后重启机器，此时web003就不会起来了，因为docker run 的是后默认是---restart no的，就是说重启等动作反正导致你退出的，docker引擎起来你容器也不会up的。 通过上面的截图可知web01是--restart always 方式启动的容器 启动后就是👇 所以重启宿主要用--restart always 这种或者至少--restart unless-stoped，而重启docker 服务，原来是up的希望它继续在重启服务后继续up就可以用live-restore配置选项，但该选项也做不到原来退出重启服务后给你起来的，而--restart always就可以👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:18:50 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/2.Docker容器查看常见用法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/2.Docker容器查看常见用法.html","title":"2.Docker容器查看常见用法.md","keywords":"","body":"第2节 Docker容器查看常见用法 容器里的root docker 运行的时候，在容器里面，通常是以root运行的。 容器本身的文件还是分层落在宿主机上的overlay2目录里的👇，至于为什么是2个m53.txt这个后面再说，涉及分层结构的梳理。 该文件的所有者也是root 就是容器里的m53.txt所有者是root，容器外面宿主机上看到的m53.txt所有者也是root。 是一个root嘛？容器里的root是用的宿主机上的root嘛？并不是，容器里的root是一个假root就是一个普通用户。证明👇。 而宿主机是可以的👇，所以证明了容器里的root并不是传统意义的root超级用户。 df看不到挂载覆盖的效果，lsblk可以 顺便知道下dm0 dm1这些 所以dm-0 dm-1 这种就是lvm了，逻辑块。 privileged--容器里使用宿主机的root，除非特地这么要求，否则别加该选项。 索然/dev/mapper下面没有lvm的逻辑块，但是要知道这个目录下面其实就是软连接 真正的rl-root其实是连接到dm-0去的 这一点，在宿主机上可以看到👇 所以容器里直接mount /dev/dm-0就行了 然后 此时再到宿主上直接就看到了 这样容器里的某些操作就危险了，比如 此时宿主上的echo就没了 但是发现echo还能用， 忽然反应过来，type 一下看到了builtin，说明什么，说明echo是buildin在/bin/bash里的，是内部命令。 好，mv echo看不到效果，mv /bin/bash吧哈哈 直接GG👇 所以除非生产中有明确要求容器里可以操作宿主机的权限，否则别加该--privilieged选项。 那么如何知道run的时候加了那些选项，比如是否加了危险的provilieged 看全部的run的时候加的选项 不过还不是太明显，不是tmd一目了然，使用一个第三方工具runlike来一目了然。 -p就是--pretty 更清晰点排版 查看容器信息 前面一直在用ps images这些， 这里再补充下-f选项的过滤 https://docs.docker.com/reference/cli/docker/container/ls/ 下图链接有问题，纠正为👆 然后支持--filter选项的又有很多dockers cli ： https://docs.docker.com/config/filter/ -f等价于--filter，就是短选项和长选项的意思，好比-h ＝ --help 然后--format就是不想其他的，它只有短选项。 -f删除所有退出的容器 docker ps -f 'status=exited' -q # 退出容器的编号，删除👇起来方便 docker rm $(docker ps -f 'status=exited' -q) -s查看容器空间占用 容器分层，image共用，所以括号里的是image大小，容器自身的可写层，一般是一些log，占用501B --format格式化，能够提供全面的信息，提供赛选或者排序 按时间排序，显示名称、id、时间 按时间反序👇，找到最近的三个 再跟一个|head -3就行。 关于sort的补充，上图其实是3和4合成一个字符串进行排序的，正儿八经可能不是这个思路，而是先安3列排序，然后再按4列排序，同一天里再按时间排序。 以下是sort的补充，很关键👇 ​ -k3其实不仅仅按第3列进行排序，还会继续对比到尾行，就是3列然后看每行的。这种不清不楚的，没意义，不如下面的明确用法来的靠谱。 以后啊，按哪一列排序就用-k3,3 这就是按第三列排序，-k3,3n就是按第三列且当作数字排序。 所以固定用法👇，先按1列数字，再按2列数字，再按3列字符串排序 所以👇才是正确的cli，虽然三种结果一致 参考文档https://fancyerii.github.io/2019/06/15/sort/ sort -t. -k5,5n # 排序ip地址 docker top CONTAINER 在外面查看容器里的进程 有时候进去反倒没有top、ps这些命令，在外面反而支持 容器里面的进程ID进程数都是从1开始的，而外面看到的却是3277这种真正落在宿主上的进程ID了。因为容器里跑的进程真正还是跑在宿主机上的。 属于containerd-shim 什么是是shim，前面文章里有讲👇 docker stats 查看容器的CPU MEM等资源占用 docker stats就是看所有容器的资源使用情况 找个命令可以看看net的i/o，block的i/o，以及通过观察到的cpu内存使用情况 结合 后面讲的docker的资源限制 来限制使用上限。👆上图可见现在是没有资源限制的，通过LIMIT那一列可知。 LIMIT没限制，free里total多少，上限就是多少。 如果不限制，将来很可能一个容器就把你资源耗光了。说到限制，肯定就会想到QoS，所以容器的限制能否做成类似带宽的QoS，什么意思，就是所有容器有一个基本的紧巴巴资源额度，上不封顶，但是如果某个容器A除了自己紧巴巴的资源以外也占用了额外的资源，此时如果整体资源富裕没问题，如果有别的容器也run起来了人家连紧巴巴的日子都过不了，所以容器A要立刻让出占用的资源的。这就是QoS里的带宽保障机制。队列机制~ 比如es是java开发的，java启动优化参数👇 比如这里 对于 堆的限制： -Xms64m -Xmx12m就显示了初始值和最大值，如果不加资源就越来越大，想用多少就用多少， docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms63m -Xmx128m\" elasticsearch:7.6.2 注意上图docker stats xxxx是全屏显示的，退出后往上翻滚才会出现多行的效果。 然后再看看不显示内存使用的效果，-Xms -Xmx就是堆内存的限制。内存空间里的堆栈空间。可见虽然LIMIT还是1.708G也就容器限没有限制，但是MEM USAGE使用大小从1.114G降低到了上图的357M，明显是限制住了 # docker 服务没有对 容器限制，但是容器内部自己做了限制。我对你没有要求，你自己对自己要求高 就是这个意思。 docker inspect的一些用法。单拎一个信息出来 因为都是json还比较好处理，比如查看容器IP地址 查看IP地址，好像是 题外话-网络故障处理-could not resolve host ​ “dns是真复杂” -- 这话是以下所有内容写完后，有感而发写在这里的。这还只是client端的dns，之前还学习过bind9。唉要不是底层，谁愿意研究这些东西，耗时间，用到还少，人家奉承你一句大佬，讽刺你也是一句大佬。 啊哈哈，认命~好好干，多读书~ 1、最近总是收到故障 开发隔三岔五说 ： could not resolve host ，比如gitrunner里的pipline走不下去，报错看就是dns解析问题。 2、观察、分析 发现ping也是会卡5s，且很稳定 解决方法就是写host就ok了，但是不能总是些host啊，于是抓包 开两个窗口 同时抓包 发现如图5s后二次dns请求，这不是所有linux都这么倒霉。 # 这里其实分析有问题的👇 进一步研究，发现有个参数optonst timeout 默认就是5s，就是超时5s后重试，可是实际情况是第一次有拿不到IP的情况的，然后5s二次查询是对的，但是很多时候第一次就拿到IP了，还是会二次查询的。 ​ 不知道为啥一定是2次，所以优化就是改为1s timeout就能加快了，就是2次查询间隔缩短为1s，然后通过single-request-reopen将1s也缩减到20ms的间隔。也算解决问题了 解决方法： 错误示范1： 继续优化配置👇，下图注释也有问题，不是肉眼已经感知不到2次dns，而是就是一次，只不过一次里面存在了A和AAAA请求。 同时抓包可见 关于single-request-reopen的分析，正确的结论在文尾，这里仅作过程记录，看看就行。 不加这个参数的dns行为👇还是这个图，注意看两次（一个A和AAAA算一次）dns的源端口都是39125 加了这个参数👇，两次dns的报文的源端口是变化的。 也不行 可以了👇 这是对这个参数的解释 ★进一步优化，最终的解决方法来了👇 于此同时抓包，下图👇，也有不准确的描述，不叫\"时间大大缩减\"，而是就一次查询里发送了A和AAAA。 此时遗留一个问题，就是第二次的dns请求就是拿不到IP地址的，但也是也不影响结果的。 以上梳理存在问题，就是AAAA没有看到，也没有理清楚V6和AAAA的关系。 还有一个问题，上面没有解释清楚为什么是2次dns，为什么很多linux就发送一次dns👇 于此同时抓包 这里是最终结果，以上过程里很多是不正确的，看看就好 首先看这张图，这是ping卡住5s的抓包 为什么存在两次dns查询，因为第一次失败了；其实第一次查询里既有V4也有V6（V6的AAAA也走的v4的socket，因为没有v6的GW），而AAAA没有拿到响应，所以人家判定了超时timeout是对的，这才触发了2次查询，所以关闭IPV6也许也是一个办法，或者防火墙放开IPV6（这个机器是在内网的，请求dns server是要经过防火墙到达外网的）。 # 不是防火墙放开IPv6，而是A和AAAA走的一个socket，FW认为是一个session的重复reply可能就直接drop掉了。但是为什么第二次就不drop了？？这是个遗留问题。 ---------一次ping的完整的抓包，涉及节点的抓包--------- 于此同时自己本地抓包 于此同时dnsserver上抓包，注意dnsserver上时间要快5s，date time 验证过了，上图的13:50:15时间对应下图12:50:21👇 ★我猜测哦，可能第一组A和AAAA的一个socket的时间离得太近了只有0.000010s=10μs，而第二组的A和AAAA虽然共用也给socket但时间相隔有个497737-477340=0.20397s=20397μs 也就是微妙 所以就是dns明明回了A和AAAA，但是由于A和AAAA共用的一个socket，注意都是33867，可能就是这里出了问题，防火墙那边可能基于同样的socket也就是五元组就丢弃了。导致client没有收到，但是为什么第二次又可以了呢！也许有时候5s超时第二次都不行的情况也是粗在的。 ​ 所以处理方法：区分A和AAAA的socket，这样就避免了FW的误判丢包--这点只是猜测。 ----↑-----一次ping的，涉及节点的抓包----↑----- 然后我又抓了两次 当然上图的dns 配置就是 所以可以得出上面的猜测很可能是对的，就是第一次两个dns查询（A和AAAA)间隔时间太短了10毫秒，防火墙干掉了第AAAA的那个回包，为什么是回包而不是去包，因为dnsserver上抓包能稳定地抓到2个请求，并回了两个reply，而client第一次没有看到AAAA地replay就是fw干掉了啊，还能是什么原因呢；系统判定查询里存在失败，所以再次timeout 5s后发起查询，这次系统肯定做了offset这种东西，就是A和AAAA发起的间隔拉开了，一定代码里有这个参数！在0.2s级别所以FW没有干掉，能够跟上这个节奏，我判定还是我的SSG干掉的，不能自己发的自己判断不出来的。 ★★ 我处理的方法是再/etc/resolv.conf文件里添加一行options single-request-reopen 此时抓包就变成了，只发送一次dns查询了，注意一次里既有v4也有v6，不过v6和v4的不再共用一个socket了，也就是不再使用相同的源端口。 结论再次总结： ①因为fw只放行了内网机器到外网dns的ipv4的dns流量，或者压根FW的接口就没有V6地址； # 这种说法也是不对的，虽然是AAAA，但是走的还是v4的socket；就是AAAA是v6的解析查询，但是走的还是v4的socket，就是ipv6的dns查询此时此刻用的是ipv4地址去问的。所以A和AAAA的查询其实在外界看来就是一个socket，所以可能就是fw丢弃了一个reply，一个socket里的两个报文里的太近了，去的时候确实没有被FW干掉，dnsserver也收到了并回了，但是回来两个报文经过FW的时候被认为重复了，干掉了第二个，FW肯定是针对回包有判定机制一个socket的判定机制，去包没有肯定。 ②所以在默认dns查询的时候v4和v6由于使用的一个socket； ③系统基于v6没有响应判定为请求失败，因为fw丢弃了一个socket； ④于是系统进行第二次查询，而两次查询间隔默认是5s； ⑤这就是为什么ping卡住5s的原因； ⑥处理方法，将v4和v6的socket区分开来后，区分后FW自然不会再丢弃，系统不再认为第一次请求失败，于是不再发送第二次请求，也就是没有默认5s的timeout了。于是就解决了故障了。 DNS解析超时排查/etc/resolv.conf里的single-request-reopen参数说明 将👇 options rotate timeout:1 attempts:3 single-request-reopen 添加到/etc/resolv.conf 中 #释义： 循环查询 超时时间1s 重试次数3 只收到一个IPV4应答或者只收到一个IPV6应答，重新开一个socket查询 妈的还是别人讲的好,但是也有细节不对，上面我的一些分析也OK的。 https://www.cnblogs.com/zhangmingda/p/9725746.html 以下是复制出来的内容，复制出来反而图片出来了，好奇怪，哈哈哈 说明： 在RHLE6/CENTOS6的环境里，需要在/etc/resolv.conf添加以下参数options single-request-reopen。具体原因请看下面。 ​ 其实不仅仅是6，7，rocky 9.3 一直都是如此默认就是一个socket。 具体： 一. 在RHEL5/CentOS5/Ubuntu 10.04等linux下，dns的解析请求过程如下 1 主机从一个随机的源端口，请求 DNS的AAAA 记录， 2 主机接受dns服务器返回AAAA记录， 3 主机从一个另一个随机的源端口，请求 DNS的A 记录， 4 主机dns 服务器返回A记录， 二. 如果是RHEL6/CentOS6，交互过程有所不同，如图： 1 主机从一个随机的源端口，请求 DNS的A 记录， 2 主机从同一个源端口，请求 DNS的AAAA 记录， 3 主机接受dns服务器返回A记录， 4 主机接受 dns服务器返回AAAA记录， 三. 上面3,4并没有严格的先后顺序，实际的顺序受网络环境，服务器环境的影响 理论上讲centos6的这种工作机制，效率更高，端口复用度更高，能节省更多的资源。 但是这里也同样存在着一个问题。比如在存在防火墙等机制的网络环境中，同样源目的ip,同样源目的port，同样的第4层协议的连接会被防火墙看成是同一个会话，因此会存在返回包被丢弃现象。如下图。 此时的整个dns解析过程如下： 1 主机从一个随机的源端口，请求 DNS的A 记录， 2 主机从同一个源端口，请求 DNS的AAAA 记录， 3 主机先收到dns返回的AAAA记录， 4 防火墙认为本次交互通信已经完成，关闭连接， # 这个说法很nice，和上面我的猜测吻合 5 于是剩下的dns服务器返回的A记录响应包被防火墙丢弃 6 等待5秒超时之后，主机因为收不到A记录的响应，重新通过新的端口发起A记录查询请求，此后的机制等同于centos5） # 新的端口，我抓包可见并不是新端口7 主机收到dns的A记录响应； 8 主机从另一个新的源端口发起AAAA # 并没有，还是A和AAAA连个查询，源端口还是原来的，只不过间隔时间变长了，从10毫秒变成了0.2s量级。 9 主机收到dns的AAAA记录响应； 我们看到在这个解析的序列里面，dns解析有5秒的延迟发生。所以当用linux系统安装大量远程包的时候宏观上看延迟就非常大了（linux是不缓存dns解析记录的）。 总结： 那么到底options single-request-reopen这个参数的作用是什么的，man 5 resolv.conf的结果如下 #man 5 resolv.conf single-request-reopen (since glibc 2.9) The resolver uses the same socket for the A and AAAA requests. Some hardware mistakenly only sends back one reply. When that happens the client sytem will sit and wait for the second reply. Turning this option on changes this behavior so that if two requests from the same port are not handledcorrectly it will close the socket and open a new one before sending the second request. 一句话总结 就是ipv4和ipv6的请求和在一个五元组里，回包在经过fw的时候被干掉了第二个；所以使用参数将其分开为两个socket。 vim进去的时候需要enter才能进，wr的时候不让wr 进去的时候以为是有.swap缓存，结果不是，wr!强制也不让保存， 结果df一看，磁盘满了 磁盘空间满了，rmi都不灵了，用docker system prune -a 这个暴力的命令吧，不加-a稍微温和一点就是仅仅 -a, --all Remove all unused images not just dangling ones 好了删了550MB后占用率降到99%，此时继续修改docker的daemon.json 然后再补充一下docker里的dns注意事项 1、首先，/etc/resolv.conf是个很牛逼的文件，为什么牛逼上面说了，但是很多人没细究过 第一、其中的dns只能生效前3个 2、其次，要让resolv.conf里的3个dns能够转起来，必须配置rotate选项，否则系统就盯着第一个干，不会用第二个， 以下是截图👇 换种轻松的测试方法 而配置rotate后下；注意别用dig nlsookup来测试，测不到的，用ping，curl这种类似APP的工具来测试，dig和nslookup 只会使用第一个dns。 尝试测试3个nameserver轮询的效果 想到126.223是不是没有53端口监听，所以压根发不出去，所以换一个name server 用ping测一样 而且通的情况下，也会轮询的， 这样好像不太好吧，优先用内网dns，然后再用第二个，不行在用第三个，才是最优吧。 如果是第一个dnsserver解析不到的时候跳下一个dnsserver，这种需求是不对的， 举例：你要将第一个dnsserver配置本地的dns，你就要将dnsserver的上级指向公网，然后第二个配置公网dns； 所以不管是linux还是windows的主备dns，都是当主dns不可达后，才会使用备dns。 3、再次，docker run起来的dns也还是容器里的/etc/resolv.conf里的nameserver，同样也是3个顶多，容器里的resolv.conf哪里来的，①docker配置文件/etc/docker/damon.json 优于的方式结合了 ②宿主的resolv.conf 以下是截图👇 ★容器的dns的各种配置，就看daemon.json是否指定，没有就用宿主的，①nameserver②option选项都是如此。 下图是生产环境的，我不管他们的容器专人负责，但是我外面宿主给你容器也给优化，一样也实现了，而且他们还不知道被优化已经，这就叫阴德--帮了别人，别人还不知道。学易经最重要的就是正，正观念，很多名词 世人不清楚什么意思，比如什么是阴德，这就是阴德；比如什么仁义，什么是小人，都讲不清楚的，而易经里列举了很多例子告诉你怎么定义这些名字，这就叫树立，站得住了你就。 话说回来，docker 里的dns，其实你就用3个顶多了，然后考虑到rotate是通也是会轮询的，所以优先使用内网dns是对的情况下，还是去掉rotate，然后10.2上做好上级查找就行了，关键是single-request-reopen要写上，否则会出问题，这点在前文也交代了。 ​ 不过内网有些是海外的站点，这个可能会涉及必须请求8.8.8.8的情况，理论上你就把有故障海外的域名在10.2上通过dnsmasq指到8.8.8.8上也行比如这样 server=/docker.com/8.8.8.8 # 理论上这么写ok，但是现实是骨感的，因为TMD的docker build的时候，他瞄的开发谁知道他用什么URL。 参考资料 https://blog.arstercz.com/linux-dns-issue-note/ https://blog.arstercz.com/linux-%e7%b3%bb%e7%bb%9f%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86%e5%90%8d%e7%a7%b0%e8%a7%a3%e6%9e%90/ https://blog.csdn.net/qq_56676115/article/details/119389610 人家也是建议用rotate，看到没，网上很多人用这个的，这是折中的方案了， ​ 而且不是说单个nameser故障，我三个server 一个内网，一个8.8.8.8 一个223.5.5.5 ，你说怎么故障，要么第一个所有人都不能用，后两个也不太可能， ​ dns server本身故障不是考虑点，解析不出来A记录拿不到，拿不对才是着眼点，于是rotate还能保证一下。成功率肉眼可见的必须是8.8.8.8解析的那些海外的(少数海外域名)就是1/3，所以开发会有疑问为什么时好时坏，哈哈哈，如果想提高到100%，就得开发告诉我你TMDdocker打镜像的时候都用了TMD什么网站，我给你一条条写道第一个内网dns里，然后去掉rotate对吧。谁签头做这事呢？谁会听你的，懒得折腾。再说了，国内镜像源这个才是最优吧。不止一次说了用国内的，然后内网仓库也搭建了N多了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-16 10:36:52 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/3.Docker容器管理和端口映射.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/3.Docker容器管理和端口映射.html","title":"3.Docker容器管理和端口映射.md","keywords":"","body":"第3节 Docker容器管理和端口映射 docker 删除后的一些情况 docker 删除特定的容器两种方法 docker rm `docker ps -aq` # 这个其实不加-f也可以，大不了UP的报错，就是不太安全感觉 docker rm `docker ps -f status=exited -q` # 这种也ok docker container prune # 官方封装好的cli docker container prune -f # 同上，只不过是不用回答y/n 改名运行中的一样改 测一下 docker rm `docker ps -aq` 退出状态是夹在两组UP之间的， 没问题啊，还是那句话docker rm $(docker ps -aq) 就是不能删除UP的，自然就会把退出的删掉 container = image + data，container停止image层就删了，只留下data，简单来讲是这样，细究就是image不是复制而是共用过来的，meged就是image层的东西 容器停止时不会造成数据丢失的，那么dockers stop后释放的空间是什么，肯定不是image对吧，image也不是说复制一份，可能是隔离的namespace这些隔离程序随着容器起来一个就要做一些隔离代码？ # 说法不对，stop后 落差空间大小在4220KB，答案见下文 m53.tx在容器外面也是看得到的👇 diff就是新增加的文件。 下图就是容器stop后释放的空间👇merged看着像image复制过来一份，但其实空间只有4228KB远远不是IMAGE的4.26MB 日志也是落在外面的，也不会删的 其实容器里的数据都是在/var/lib/docker/overlay2目录下的，所以也不会随着容器stop就没了。 有空看看这个，可以回过头来看， https://zhuanlan.zhihu.com/p/374924046 链接里的截图👇 看分层overlay2里确实变小了啊， 但是docker ps -s看到的变大了，可能原因如下👇 容器数据是否持久？ 注意容器的数据会随着容器删除而删除的，上面是说容器的数据不会随着容器的stop而消失；说白就是overlay2目录里的东西，docker rm掉就没了，stop还在的。如果mysql 容器，肯定不能这样，就要做持久化。 上图的目录只要退出来，这个目录就没了 测的玩 所有images和容器都删掉，此时overlay2里是空的 然后pull一个busybox看看 diff里的东西4220KB字节👆 run起来后，多了两个分层目录 然后空间大小，run起来的空间占用主要在merged目录，看起来就是等于image里的diff目录大小；所以这个容器里的merged目录就是从image里的diff拿来的，所以就是复制了一份image的全量了基本上 container = image + data ，容器起来就是复制了一份image这话没毛病，可能多个容器基于一个image存在共用，这个后面再测试，至少第一个容器看来就是多了merged4220KB，然后等于image的diff4220KB，也基本等于docker images 看到的image大小4.26MB=4260KB。 diff目录是和谁的diff不同，应该是容器起来后和image之间的不同。 然后再stop掉看看空间占用，stop掉merged目录就没了。 UnionFS到底是怎么个共用来着，给个效果看看啊，我自己看的也没看出来👇 container = image + data，就是复制一份image的。 所谓UnionFS联合文件系统，这种复用的情况应该主要是image之间的联合复用，容器方面如图所见就是run一个就复制一份image了，在这个点上是不存在复用的。 容器的暂停vs停止 kill 容器发送信号 docker kill ID # 停止容器 docker kill -s 1 ID # 加载配置文件 如何验证确实加载了配置文件呢--通过top查看比如nginx的work进程，因为加载配置文件master进程不会重启，但是worker进程会重启 类似nginx -s reload用法，nginx -s reload也等价于kill -1 $(pgrep nginx |head -1) 一般第一个就是nginx的master，kill -1就是等价于reload配置文件。httpd一样，这里的docker -s 1也是一样。 docker attach exec进去再退出来，并不会导致容器退出 此时再复制一个窗口，一样可以exec -it进去，但是两个exec 进去的shell是独立的， 现在通过attach可以实现screen -x 的 两个人共享shell的效果 上图由于alpine是①没有退出-it进去的②且默认cli是/bin/sh，所以attach就成功挂上去了，然后窗口时一个tty，共享的试图。 四个窗口也是一样的，也就是复制4个窗口出来然后👇 attach用法 1、偷窥别人当前正在干什么，劝你耗子为汁 2、从安全考虑，怎么禁止别人进来，别run -it就行了，而是用start后，exec -it进去。 进入容器通过exec -it sh 一般是要跟一个cli的，exec 不一定都需要进去，直接执行一条cli就出来也行👇 bash一般可能没有，sh一般是有的 exec能够执行的cli都是容器里的有的，有什么用什么 这样就可以进入容器，进一步排查问题，没run是进不去的。所以docker run [-d] 能够up的都是容器里是前台cli挂着的，然后容器没有前台cli的都是run完就exited--也就是run不起来的，也就是没法exec -it sh进去的。 # 简而言之，exec能进去的都是容器里有前台运行进程的。 容器的端口问题 别人容器时一个nginx，80端口也是容器里的ip的80. 随便瞧瞧run的参数 容器里的IP出来都是走的SNAT，不是说172.17.0.2转成172.17.0.1的SNAT哦，而是容器里出来走的是物理网卡的IP的SNAT。 这个cli比较适合容器里查看本地IP👆 这个172.17.0.2，本地宿主机上是可以访问的，因为路由器是本地指向docker0的。 暴露端口 docker run -P --name web01 -d nginx # 将容器里所有的端口都暴露到宿主的随机端口 docker run -p 80 --name web03 -d nginx # 仅将容器的80映射到宿主的随机端口 docker run -p 80:80 --name web02 -d nginx # 仅将容器里的80映射到宿主的80 docker run -p 192.168.126.130:80:80 -d nginx # 宿主IP:宿主PORT:容器PORT docker run -p 192.168.126.130::80 -d nginx # 宿主IP:随机端口:容器端口 docker run -p 192.168.126.130:8080:80/udp --name web03 -d nginx # udp docker run -p 8080:80/tcp -p 8443:443/tcp -p 53:53/udp -d --name nginx #★多端口 小p指定，大P随机 测试就OK了 然后看日志 docker port web01查看端口暴露 固定宿主的映射端口，前提是外面的端口没有被占用，里面的端口你得知道 里面的端口inpsect可见 外面ss -tlnup 确认没被占用就行了 测试OK docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d --restart=always mariadb:11.3.2 # -e 是传递变量进容器，前提也是容器里支持的变量可以传一些值才行。 docker run -d -p 8080:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php8.2.apache 启动db 此时另一台机器就可以连了 账号也有了： 启动wordpress的前端容器： 所以此时页面走一波 关于数据库主机不能填默认的localhost，因为这是在容器里面，localhost指的是容器里面的本地，外面宿主也好，其他容器也好针对localhost看到的又是各自的本地。 可以写容器的ip，但是不好，因为该ip会变；所以要写宿主的IP和暴露出来的端口。 如果一不小心配成了容器里的ip 下图是配置了172.17.0.2后 返回重配未192.168.126.130发现不行，需要进入wordpress容器里删除对应的配置文件 这文件里的IP改改就行了，干脆直接删除这个文件，这个时候再重新去页面配置就可以了 这个细节：图中wordpress默认也是有一个灰色的，但是其实只是提示而已 这就OK了 容器如何修改监听暴露端口 直接进去改了 然后重启容器，尝试发送SIGHUP信号试图重载配置，但是未成功👇 restart -s 1也不行 进去刚才的配置文件发现端口没改过来，看来要先stop 容器再改了 再次尝试docker stop xxx，然后vim修改未90909，然后docker start xxx还是不行，需要停止docker引擎 尝试上图操作先停止docker，发现配置文件可以改过来了，但是由于容器时restart always了，还是不行，需要重启容器，下面cli可以实现不删除容器，修改暴露端口的需求 systemctl stop docker systemctl stop docker.socket # 这个不需要 docker stop xxx # 这个不配：后面需要重启容器：配置的话：后面需要启动容器，哈哈哈~ vim /var/lib/docker/containers/xxx..xxxaad092/hostconfig.json # 修改暴露端口 systemctl start docker # 启动服务后，还得手动重启容器，否则容器之前没有停可能就保持了之前的状态信息然后 docker restart 9472a68122b9 # 发现必须restart，容器跟随docker服务起来好像不行有点奇怪。不奇怪，因为之前没有停，直接被服务重启了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:19:15 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/4.Docker容器管理常见用法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/4.Docker容器管理常见用法.html","title":"4.Docker容器管理常见用法.md","keywords":"","body":"第4节 Docker容器管理常见用法 docker查看日志 概述 docker logs 本质是查看容器在终端打印出来的信息，只要你的容器有STDOUT就会被docker logs在外面抓到，不过一般来讲都是容器里的日志才会在屏幕打印，所以docker logs也就起了个名字叫logs否则正宗点应该叫docker stdout，哈哈。 run一个nginx看看上面说的，通过docker top 可见nginx是daemon off也就是容器里是前台运行的，这样才能保证容器run起来不会停止，然后再docker run -d 将 docker run xxx 放入后台执行--否则容器up后会占用宿主的前台。 简单讲一句话：容器的run起来就是要：里面是前台，外面是后台 解释：里面是前台才能run起来不stop这是容器的固有要求；外面是后台，容器up后才不会占用宿主的前台。 docker logs其实是看的容器里的屏幕打印(stdout和stderr) 测试如下👇 此时就可以通过docker logs看到容器里的STDOUT 证明就是STDOUT而不仅仅是LOGS👇 我本以为正因为nginx将两个日志软连接到了stdout和stderr也就是屏幕打印，才会被外面宿主的docker logs抓到，结果当我直接产生屏幕输出的时候docker logs 并没有抓到。 而且cat也就卡住了--在容器里👇，我知道了，因为你exec -it进去不是之前run的时候的那个tty了，所以exec 进去的stdout在docker logs web01的那个tty是看不到的，echo不进去和cat不出来则是因为①看似echo >> access.log和cat access.log实则echo >> /dev/stdout和cat /dev/stdout 自然是：前置不是一个tty外面docker logs自然看不到，后缀cat /dev/stdout卡住其实就是宿主上单独cat一个道理，什么道理，哟还会抬杠啊，就是cat=cat /dev/stdout是等待STDIN将STDIN的内容输入到STDTOU上 改为run -it方式进去，然后用attach也就是一个tty去测试，理当实现 ： 容器里的屏幕的东西都能在docker logs 看到了。 此时logs就能拿到stdout了，这就上文红色字体所说的一个tty才行。 换一种证明方式👇 正因为tailf -f 是屏幕打印的内容，所以docker logs 就把这个屏幕打印的内容抓出来了， /etc/hosts----tail--->STDOUT----docer logs---->宿主看到了 而不是说/etc/hosts变成了log，哦，不要瞎理解。 ​ 前一个例子，STDOUT由于是不同的tty，所以没法同步信息--一边屏幕的STDOUT---一遍docker logs看到，因为不是一个tty。 ​ 现在一个例子tail -f /etc/hosts，也就是上图你exec -it进去往/etc/hosts里echo点东西，这样即便不是同一个tty：一边exec 进去往这个文件里写东西，一边另一个running的容器里由于前台tail -f /etc/hosts了，只要别的tty往该文件写东西，那么容器的tail -f是会看到的，于是容器本身的屏幕也会有同步的打印。 # 落文件了所以不同tty都是同步的该文件而已。 ​ 而前一个例子不同tty中间可能有文件作为中转桥梁，他们是各自的tty在各自的STDOUT或者STDERR。 docker run --name test01 busybox /bin/sh -c 'i=1;while true;do echo $i;let i++;sleep 1;done' logs一样可见， 还是那句话logs看的容器的tty正式当前你run的那个容器的tty 你若此时exec -it 进去该容器，就是另一个tty了，此时STDOUT由于是不同的tty不会在docker logs里看到，因为docker logs看的是就是那个容器里的tty终端。 然后sh -c 要用单引号👇以下是区别，其实就是你要将里面所有的都原封不动的传递到docker run 里的sh -c里去，所以自然不能解析的，所以必须是单引号，统统是字符串扔进去。 补充：为什么一直是100？因为i=100，双引号展开后为\"i=1;while true;do echo 100;let i++;sleep1;done\" 也就是说容器里实际执行的是echo 100，哈哈哈，你说为啥一直100呢。 docker ps docker看到的容器的日志，在宿主机上也有文件落地的 查看容器日志落在宿主上的文件位置： /var/lib/docker/containers/0axxxx...xxxb471ca/0axxx...xxx471ca-json.log 然后logs越来越多，会导致磁盘空间占用越来越多，常用方法 关于替换掉容器里的原本cli ​ 比如原来容器的cli是一个脚本来启动容器的，现在该脚本被你替换为tail -f xxx了，自然也就不会启动nginx服务了，所以这种情况就要小心了。 虽然外面看 但是里面你把脚本替了，里面端口都没开👇 替代cli的使用场景，一般是用来测试查看的。 容器里的DNS以及hosts文件 类比宿主的hosts本地解析情况 容器里多了一个容器IP，且解析到容器ID。当然127.0.0.1还是老样子 还有个仅仅是验证hosts文件，可以docker run --rm 来做，演示完退出容器后直接删除容器 docker run的时候就新增host解析 host文件添加进去就好了，不过外面也可以 再次理解下-it -d 和cli👇 docker容器里的dns 这点我前文就总结过了，结合工作中的故障案例进行和归纳总结 这里补充其他的点：比如ubuntu的情况、比如除了daemon.json 还有docker run --dns=来指定。 ubuntu的容器的dns 和rock-linux一样也是从docker引擎的daemon.json优先然后再结合/etc/resolv.conf里来的，但是ubuntu的宿主机的/etc/resolv.conf文件里看到的其实不是真实的宿主使用的dns，而是要通过 上图的/etc/resovle.conf是容器里的dns的使用文件，该文件自然也是从宿主机的docker引擎的daemon.json 结合 宿主的dns 来生产的。 ​ 而宿主机的dns👇下图显示了ubuntu的特殊的查看方法（ubuntu的/etc/resolv.conf里看不到真实的dns配置的） 同时还有其他的cli查看👇 dns和options的优先级，从上到下： ①docker run指定的参数优先 ②daemon.json指定的参数其次 ③宿主机的/etc/resolv.conf指定的参数最后 ④指定的参数优先的意思就是，就两个参数其实，一个dns 一个options，比如：run 和 json里都没有指定options，而宿主机的resolv.cnf里制定了，那么就是宿主的配置优先了。 注意上图的一个点：就是10.1.1.1是不会生效的，因为/etc/resolv.conf文件里的namserver只认前三个。测试过了的在前面的章节里--就是本段文字的标题下面的第一张图那边做了测试截图的。 docker run --rm --name web01 --dns=1.1.1.1 --dns=2.2.2.2 --dns=3.3.3.3 --dns-option=timeout:23 --dns-option=attempt:3 --dns-option=rotate --dns-option=\"single-request-reopen\" busybox cat /etc/resolv.conf 这样差不多了吧，单个容器生效，也不影响其他容器，也不用dameon.json，也不用宿主的/etc/resolv.conf，就挺专业，哈哈。 search要删掉 还有一个search xx.xxx一般不怎么用，错了，不是不怎么用，而是一定要删掉，防止乱给你自动添加捣乱。 没啥用，谁指望search来补齐啊；而且可能带来意料之外的故障。 search怎么删，不是去/etc/resolv.conf里删，这只是个动态生成的文件，源头在网卡配置文件、或者nmcli去配置修改。 ubuntu里 centos一样类似的网卡，或者用的是NetworkManager的直接用nmcli去修改就行了 图中ipv4.dns和IP4.DNS的区别，一个是前面配置的，一个时候后面最终生成的，你可以理解成配置文件和最终状态，唉，这些太细了，不管了，总之实际操作的时候带点脑子就行了。不理会这句话也行。 容器和宿主机文件怎么交换 docker cp -a 好比cp -a，保留属性 操作下，跑一个容器up着 然后将宿主机的一个文件，复制进容器里 docke cp -a /etc/issue test01:/tmp/ 拷进去： 拷出来 容器的环境变量 这东西懂的都知道很关键的： 1、cmd里的set，看的是环境变量，可能看的都不全对吧，毕竟仅仅是当前用户的吧；存在软件退出了，网络设置里的代理里也清空了，但是set里发现PROXY变量的存在，这种情况也可能导致网络问题，故障定位就很难，出现过一次，具体细节我忘了，结论就是set里也要观察是否配置了代理，容易忽略这个点。 2、env也就是linux看环境变量的cli。 3、环境变量里的PATH变量，crontable里的写清楚PATH变量. crontable正儿八经还要写 这些都是环境变量 每个容器自己都有环境变量，环境变量的用途和镜像密切相关，有些是与众不同的的环境变量。 再看看mysql的变量 mariadb的变量 还可以传递变量用-e 注意上图mysql如果PASSWORD变量没给进去，是起不来的， 如果用了--restart=always，就会看到容器状态是一直重启中 还可以写到一个文件里统一安置 不过这些例子的环境变量没有具体意义，mysql的环境变量一般这么设置 docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d --restart=always mariadb:11.3.2 去掉env观察 可得👆是password没给。其实是you need to specify one of the following as an environment variable，是三个变量给一个就行了。 然后evn取代了docker-entrypoint.sh的脚本，也报错，可见password变量是在cmd之前就需要的。 ​ 然后设置为随机密码或者空密码：具体-e的传递参数的格式，这是当初创建镜像的作者指定的，要去hub.docker.com去查看mysql镜像的传递参数格式，而不是去mysql官方网站去查看。 然后问题来了随机密码MYSQL_RANDOM_ROOT_PASSWORD到底是多少呢，看logs就行啦~👇 尝试登入mysql，由于没有做端口暴露，所以我用容器的ip去连👇 好，那么问题来了：如何安全的设置mysql的密码， 1、你用随机密码，logs里可见，不安全；所以需要手动进去修改，则算是一种解决方案； 2、√ 你用手动指定-e MYSQL_ROOT_PASSWORD=123456 来指定，结果history里可见，不安全；所以我们要vim mysql.env进去定义变量，chown 600 mysql.evn，然后source mysql.env，然后再-e MYSQL_ROOT_PASSWORD=$VAR就行了。 这样就安全了， 然后再试试用文件传参的方式--env-file👇 上图提示No space left on device，磁盘又够了，删吧 后面会讲怎么制作镜像，让用户使用镜像的人可以传递变量，也就是制作镜像的时候要考虑的事情了。 还有别忘了 清理容器 清理前先看看：可以i用 docker system docker system df 停掉几个up的容器后，现在RECLAIMABLE里看到可以回收的空间了 解释如下 docker system events docker system info = docker info docker system prune 四个-all分别是 所有停止的容器 所有没有使用的网络，网络后面讲 所有没有标签的镜像 所有dangling的build cache dangling就是没有标签的镜像，产生的原因如下👇，就是容器正在用着一个image，你删image的时候还得是用的TAG，则删除了TAG，也就是没有TAG，也叫dangling。注意哦，容器用在是删不掉image的哦，这一点通过ID去删就明显看到了，通过TAG去删也只是去掉了TAG而已。 容器没有UP的，Exited的，image是可以删掉的 然后容器run起来是复制了image一份，否则容器停止后将image删除，为什么容器还能继续UP呢，所以正是因为容器run这个动作就是复制一份image的。 还一个补充一下：docker的PS1 需求，我想修改docker里的提示符 诺，是不是so easy，再找找busybox的PS1变量落在那个文件里就行了 如果实在找不到文件，就用-e的方式传进去也行啊。 搞定~ 但其实，不推荐修改，因为这样你 不注意以为还在宿主机上，不知道已经进容器了，就尴尬了。 案例，使用docker自动部署一个开源运维软件 https://github.com/openspug/spug https://www.spug.cc/ docker run -d --restart=always --name=spug -p 80:80 registry.aliyuncs.com/openspug/spug # 通过镜像名指定的从阿里云下载，走的是https默认。 此时页面可以打开了 还需要初始化一个登入密码 docker exec spug init_spug admin 123456 # 初始化并创建登入账号密码 init_spug是人家容器里自带的脚本，这里也就是当作cli还执行的 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:19:29 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/5.Docker制作镜像方法说明和手动制作镜像.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/5.Docker制作镜像方法说明和手动制作镜像.html","title":"5.Docker制作镜像方法说明和手动制作镜像.md","keywords":"","body":"第5节 Docker制作镜像方法说明和手动制作镜像 比如开发了一套系统，如何迁移到容器里，如何制作成容器镜像，这就是本章的目标。 手动制作镜像 不推荐 概述 ​ 好比虚拟机的模板制作：手动安装虚拟机、定制(关闭selinux、iptables安全加固、优化内核等基本初始化配置、常用软件包)、关机，以此虚拟机为模板，后续克隆即可。 ​ 然后手动制作容器镜像的话，比如，找一个alpine镜像，启动后，进入容器，进行定制化，安装软件、账号创建、仓库优化等。基于这个定制的容器来生成镜像。 ​ 存在的问题，就是每次镜像需求发送细微变化，你都要进到容器里再次修改后生成。纯手动，不推荐使用，说它纯手动是因为基于交互式方式进去操作这种，脚本化不方便，其实真要说起来，什么操作不能自动化，手动制作镜像的方式也可以改成脚本，只不过有docker build的方式才不会这么弄罢了。 pull镜像 下载ubuntu镜像，啥，tag版本号怎么得知的，hub.docker.com里找lastest然后对比MD5找到版本号。或者skopeo list-tags docker://ubuntu看看也行。 run进去定制 其实镜像的CMD就是/bin/bash，所以run后面的bash可敲可不敲： 至于当前的shell是啥要大写去查echo $SHELL 好了，下面使用我们熟悉的bash来定制， 时间校准下，ss命令、curl、wget、vim 、vi也没有 ubuntu的国内镜像可以修改为👇 或者用清华的https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ 我这边就不改了，就用默认的 apt update下 算了还是用国内的吧 再次apt update 还是有问题 看看这个 https://mirrors.ustc.edu.cn/help/ubuntu.html sed -i 's@//.*archive.ubuntu.com@//mirrors.ustc.edu.cn@g' /etc/apt/sources.list.d/ubuntu.sources 还是老样子 其实是时间不对，容器里的时间是从宿主来的，宿主安装chrony后，再创建容器就行了 # 这话其实不一定对，因为后面还是安装不了软件，我最终的方法是使用默认的不挂在时区文件，反而又成功了看下面段落的再来一遍的独立测试过程 不过时区如果也要改的话就这样 容器时区和时间怎么同步宿主 时区就行，时间自动的 这个其实还是用的宿主的时区，和时间，所以GMT+8要保证宿主机上的设置好先。 定制化安装一些常用工具命令行 apt -y install wget curl net-tools procps psmisc iputils-ping iproute2 vim tzdata tcpdump telnet traceroute tree iotop unzip zip nfs-common lrzsz && apt clean 我让GPT分析了原因， 通过阅读GPT的回答，可知可能正式我用 docker run -it --name test01 -v /etc/localtime:/etc/localtime:ro ubuntu:24.04 只读挂载容器里的/etc/localtime文件，导致tzdata无法修改这个文件。 反正是tzdata设置不了导致的👇以下是再来一遍的 apt安装软件失败和成功的对比-寻找故障点 1、寻找故障点 挂载时区文件 然后发现安装软件到这些都OK， apt -y install wget curl net-tools procps psmisc iputils-ping iproute2 vim tzdata tcpdump telnet traceroute tree iotop unzip zip nfs-common lrzsz && apt clean 以上粗体的软件，安装都依赖tzdata，所以都会涉及localtime文件的覆盖，而localtime当初创建容器的时候就是被宿主机给占用了的，即使没有ro的方式也是被占用的，也无法被覆盖。 2、应对措施 1、时间不对可能导致apt update出错；但没必要修改时区。 2、如果进一步修改时区，就可能导致apt install 的包依赖到tzdata就无法mv覆盖 这真是个大聪明的机制。 3.1、解决方法也有：使用docker cp 将时区文件复制进去，而不是-v挂载进去，这样tzdata安装的时候就可以mv 覆盖这个时区文件了。 3.2 、其实解决方法也可以：不要修改时区，UTC也好GMT也罢，都是准确的时间，无非是可读性不好吧。 以上故障总结 1、apt update报错及解决 答案就是时间不对 尝试复现上面的报错 ①先明确容器的时间哪来的：系统时间的UTC时间，是不看系统时区的，只看UTC对应的时间。 ②再复现报错，date -s 改掉系统时间，然后utc时间自然跟着变，就能看到报错 进一步分析，时间判断机制，人家apt update的时候又文件里写着时间的，对比你的时间不对就报错。 https://linux.cn/article-12666-1.html 根据这个大兄弟提供的思路，我找到了docker里的判断基准文件 通过修改宿主机的时间，无需修改时区，然后直接run容器，就可以apt update成功 chronyc makestep # 手动同步网络时间校准下 2、挂载时区文件导致依赖tzdata包无法安装及解决 首先不挂在时区文件，就不会存在这个问题👇 不映射时区文件tzdata安装ok👇 映射时区文件，导致安装tzdata依赖包出错 处理方法 使用docker cp 将时区文件复制进去，而不是-v挂载进去，这样tzdata安装的时候就可以mv 覆盖这个时区文件了。 👇下图是cp -L将软连接的源文件内容复制进去了，仅cp -a这种是没用的，因为软连接进不去，因为容器里没有软连接指向的源文件。 所以要用-L复制： 此时再次安装 apt -y install wget curl net-tools procps psmisc iputils-ping iproute2 vim tzdata tcpdump telnet traceroute tree iotop unzip zip nfs-common lrzsz && apt clean 就很丝滑~也会涉及tzdata的安装，它会修改一些东西，比如 这些都是tzdata设置的，也不知道他基于什么给我设置成上海时区，也许是从宿主拿的，测试一下 并不是人家tzdata默认时区就是上海，哈哈 将外面宿主的时区改成掉 然后再将容器里的时区改掉👆，最后安装tzdata发现时区还是上海，而且最终信息人家也告诉你了就是默认的上海时区👇 所以关于容器的时间，其实就简单了 1、宿主时间要对就行。 2、容器进去apt updata后，直接安装tzdata时区时间就都OK了，此为无脑操作，也挺香。 容器里一般是没有selinux，防火墙也是关闭的 因为容器里没有getenforce、sestatus这两个确认selinux没开的命令 所以就用文件ls -Z 的方式来看，即使已经关闭了selinux，但是这个文件当初是受到selinux关照的，也会有selinux的信息的👇，同时前面就会有一个点 . 而容器里是没有的 然后将定制好的ubuntu制作成镜像 通过docker commit将容器生成镜像，不要和docker save搞混了，docker save是image的导出以及docker load对镜像的导入。而docker commit是容器生成镜像。 这样镜像就从容器制作出来了👆。 然后就可以基于自己制作的镜像来创建容器了 一般镜像制作 按图的思路进行打各种服务的镜像，以后的应用都不是应用了，全是镜像了。而app镜像全是基于系统环境镜像的，也就是比如APP2镜像之前还得有Tomcat8镜像和JDK8u镜像和系统基础镜像以及原始镜像。 所以镜像自然就会非常多了， 由于都是基于一个base镜像打出来的，所以底层环境，工具、账号开局的都是一样的，账号ID也是一样的，这就解决了NFS如果ID不一致的问题(回头补一个NFS的章节梳理下这个问题) 如果非要按下图👇红色箭头来一步到位的方式来打镜像 也不是不行，只是APP2和APP3都是直接从原始镜像制作出来的，这样环境可能是不统一的，就算一个人做出来的，环境也是基本不一致的，维护不好。 镜像制作出来，不要本地存放，要上传到仓库服务器harbor统一安置，其他机器都从harbor拉去。 用commit这种方式制作出来的镜像，也就是从容器进去手动安装配置的结果，这种方式过几个月你都压根记不起来当初你在容器里做了哪些配置，然后commit出来的镜像里带了哪些功能，维护很不方便。这种方式也无法自动化，因为没有一个文档性的东西把历史操作存起来，都是临时性的操作，不好规范化，脚本化，不好运维。 好处就是commit命令会了，基本就会制作镜像了，哈哈。 生产是不用这种方式制作镜像的。 自动(批量执行)制作镜像 需要一个脚本文件( 好比ansible剧本 )，里面定义了如何来创建定制镜像的详细过程。 该文件自然有自己的格式，语法。也就是dockerfile里的常用10来个cli的学习。以及docker build命令依据dockerfile文件来创建镜像。 docker build -t xxx:v1.0 . # -t 后面跟的是镜像名词和版本tag， .就是相对路径，默认就找Dockerfile这个文件。 docker build -t xxx:v1.0 -f /xx/xx/xx/file001 # -f 手动指定文件，就不使用默认的Dockerfile文件了。 而这个默认dockerfile其实是有固定大小写名称的👇Dockeffile 所以每个镜像都用docker build + Dockerfile来创建的， 这么多镜像，是不是都叫Dockerfile不就冲突了嘛，所以需要用文件夹区分开来。 关于Dockerfile们的存放路径，可以按上图，跟着镜像存放，也可以单独放到独立的文件夹下分门别类 mkdir -p /data/dockerfile/{base/{ubuntu,centos,alpine,busybox},web/{jdk,nginx,tomcat}} 将来这些分类下面就都会有一个Dockerfile 比如你要做nginx镜像，就得先去alpine或者centos制作操作系统镜像。 wifi故障案例 故障现象：开大会的时候，FTP传文件速度只有100KB/s； 故障复盘： 1、笔记本在夹层楼道第二个台阶处连接到1F夹层会议室里的AP后，此时信号处于中接近弱的状态，此时FTP传输文件就是50KB/S， 2、然后将笔记本移动到夹层会议室里，此时WIFI信号开始变强，但是FTP的速度并不会上涨 3、然后删除ftp的任务，重新传输，此时速度才会肉眼可见很快的达到2MB/s的样子。 用户行为导致的故障可能分析1： 1、用户的笔记拿到休息区的柜子上的时候，wifi信号还吊死在别的AP上，导致信号弱。 2、用户打开FTP后进行传输的时候，wifi信号是弱的，此时ftp速度很慢。 3、等到wifi连接稳定(切换到最近的AP)信号变强，但是FTP速度只会随着信号变弱而变慢，并不会随着信号变强而变强。 4、此时需要删除FTP任务，重新进行文件传输，让FTP重新协商。 用户行为导致的故障可能分析2（该故障方式已复盘）： 1、用户在二楼的时候就已经开始用wifi传输文件了，然后笔记本移动到1楼休息区的时候经过楼道wifi变弱了导致FTP速度变慢； 2、等到用户wifi稳定后(切换到最近的AP)信号变强，但是FTP速度只会随着信号变弱而变慢，并不会随着信号变强而变强。 3、此时需要删除FTP任务，重新进行文件传输，让FTP重新协商。 应对方法： 用户到哪里，就断开WIFI重连一下(该操作确保终端就近连接AP)，然后再进行FTP或者其他文件的传输 （ ”然后“ 的道理就是：确保wifi稳定后再传输数据，防止信号不稳定的时候，那时软件传输数据的时候是认为网络通道是狭窄的，等信号强了有的软件可不一定能够快速探测到通道OK了从而给你加速传输）。 国内paypal电信拨号线路突然打不开 1、故障现象 用户所在线路未电信拨号线路，之前可以登入国内的paypal，今天打开很慢一直转圈圈好久才出页面，登入就登不进去 2、分析 首先了解：虽然和本次故障不相干 https://zhuanlan.zhihu.com/p/663322201 其实F12看到www.paypalobjects.com很多URL都是走的的这个域名，但是该域名通过电信线路就打开很慢，加载慢，但肯定不是DNS问题，发现该域名IP是美国IP：内部解析和外部解析一致，也就是说内部并没有做任何dns的设置。 结合联通海外线路一般是比电信要好的这么一个普遍共识，所以切到联通测试发现打开速度杠杠的。 3、处理方法 切换到联通线路就行 至于电信拨号打开慢，登入不上，又不是公司网络问题，理由很简单：ip一直有监控60多天地址没变过，用户之前ok，现在不行，经测试发现确实不行，说明就是ISP自己的问题。此类故障统一切换线路就行了。怎么切？切用户啊，因为paypal涉及很多地址，不可能全部一条条打到联通线路去，只能且用户，并告知用户他的出口线路已切的情况。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:19:42 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/6.Docker自动制作镜像常见指令说明.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/6.Docker自动制作镜像常见指令说明.html","title":"6.Docker自动制作镜像常见指令说明.md","keywords":"","body":"第6节 Docker自动制作镜像常见指令说明 Dockerfile的编写 官方文档： https://docs.docker.com/reference/dockerfile/ 一行一个指令 # 表示是注释 首先指定基础镜像:FROM cd /data/dockerfile/base/apline touch Dockerfile 1、基于alpine来 明确下较新的版本 FROM alpine:3.19.1 # 从该镜像继承 alpine:3.19.1 又是从拿来的呢？这个就要去官网查看了 可见alpine的父镜像是scratch来着。而且ubuntu、centos、busybox都是from的她。然后jdk，tomcat这些APP又是基于OS的。这个scratch就是所有镜像的祖先，这就是一个空镜像 。 https://hub.docker.com/_/scratch 再看回alpine的Dockerfile 参考人家的alpine制作，其实就是from一个scratch原始空镜像，再ADD一个迷你rootfs就行了这个rootfs其实就是OS的用户空间，而OS的内核空间就是用宿主机的。 ​ 你要自己手动创建一个类似alpine的镜像，就可以定制以下这个rootfs文件 也就是说确实还有进一步缩减的空间。 去看看busybox的Dockerfile，里面rootfs更小。 其实，没必要折腾，直接官方的基础镜像拿来用就行了 既然没必要折腾那么就折腾下吧 # 非常规操作 目标：重新自己做一个最小OS，参考busybox咯 然后下载下来 然后看看是否要修改 还是要加压，因为本来我想直接用tar -r -f 追加工具进去，但是-r不支持压缩格式，单单一个tar包不带gz是可以的 进一步缩减cli文件，再加点比如curl命令 删除就怕这些bin文件之间有依赖关系就不好弄了，先不管，删除没见过的所有bin文件 搞不懂了👇，每删一个就会出现新的文件来占用空间 删掉差不多了，再弄个curl进去试试 打包 定制的busybox.tar.gz反而变大了，呵呵 预览以下包里内容当作检查 注释要单独一行，不能跟再CLI后面 这样就好了 这就是LABEL打进去的标签 然后run就报错啦 因为我把sh删掉了，哈哈哈 重做，只要sh等3个bin文件 打包后，build run一下 可惜curl还是没有把依赖库弄好，搞笑的是ls也被我删掉了，搞得没法ls，然后find也没有，curl到底在哪也不知道。 好了，总之思路就是curl和依赖都要打包，然后过程如上。这就是定制os的思路。一般os还是不自己折腾的。 再来一遍 上面错了，cp -a是复制的软连接， 再来一遍吧 很好报别的依赖库了，所以你知道了OS定制还是挺麻烦的，不像基于别人的os，直接安装工具就好了，比如commit方式最简单， 不过上面的情况，我也可以用docker cp 的方式来处理，不过要处理的就多了去了👇 尝试把这些ldd依赖弄进去 但是继续测试发现，这些库弄进来也把sh的依赖搞出问题了 那么就这个文件不弄进去 对比了原来sh依赖的这个库和curl依赖的这个库发现不一样 应该是curl的依赖把这个libc.so.6覆盖了，导致sh启动不了 用官方的tar包里的源文件，将这个库还原回去，可行~下文方法1就是上面折腾的总结 方法1：从tar包入手的镜像定制，ldd是关键 这种方法就是从scratch开了，呵呵，比较叼，一般不这么用，属于非正规方法。 mkdir -p /data/dockerfile/os/busybox-my-base cd /data/dockerfile/os/busybox-my-base vim Dockerfile FROM scratch ADD busybox.tar.gz / # 这句其实就是解包，解开并解压缩到容器里的/根路径下 LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ maintainer=\"oneyearice \" \\ version=1.0 \\ other=\"value3\" CMD [\"sh\"] 以下是完整的一个os自定义过程 需求，将busybox里的cli只保留sh find ls 并打入curl 找到官方Dockerfile 就在Dockerfile文件的同级目录下找到这个tar包 结果是个软连接，找到正主， 复制Raw进行下载 down下来 解压 并将原压缩包移入上层目录，待会会用里面的lib库做修正 进入bin删除sh ls find以外的所有文件 复制curl以及其依赖的库 此时lib里的东西有一个sh依赖的libc.so.6也被curl的依赖覆盖了，curl的依赖是从宿主复制过去的。所以要还原，否则容器里的sh用不了，连容器都启动不了 覆盖回去 打包 编写Dockerfile build后再run就搞定了 贪狼，不计后果，勇往直前，好吧，就是好累。 以上就是完成了一个busybox的OS定制，打入curl和去掉N多cli的操作。不过一般不会这么做的，这里只是我的一个操作排错的记录而已，存思路验证过程。也算完成了 这里记录下images大小，回头用正规方式将curl封装进busybox，再看看那时的镜像大小 # 宿主机上编译安装curl wget https://curl.se/download/curl-8.7.1.tar.xz && \\ tar -xJf curl-8.7.1.tar.xz && \\ cd curl-8.7.1 && \\ ./configure --prefix=/opt/curl-static --enable-static --disable-shared --with-ssl && \\ make && \\ make install ./configure --prefix=/usr/local/curl --disable-shared --enable-static --without-libidn --without-ssl --without-librtmp --without-gnutls --without-nss --without-libssh2 --without-zlib --without-winidn --disable-rtsp --disable-ldap --disable-ldaps --disable-ipv6 # 以上还不是完全静态编译，复制到容器里还是缺库 我觉得还是应该找一个bin二进制的独立文件，或ldd出来放好，build的时候COPY进去。 方法2：别人的镜像增加一个软件，ldd也是关键 上面的编译老是依赖共享库，算了，我直接ldd弄一个出来得了 cd /data/dockerfile/os/busybox-curl/curl-static # 复制出curl的所有依赖库 ldd `which curl` |awk -F '=>' '{print $2}' |awk -F '(' '{print $1}' |grep -E '/' |xargs -i bash -c 'cp -L {} ./' mkdir lib mv * lib # 复制curl的bin文件 mkdir bin cp -L `which curl` lib 得到👇 然后编写Dockerfile # 使用 BusyBox 基础镜像 FROM busybox # 复制编译好的 curl 目录到镜像中 COPY /curl-static /curl-static # 将curl的bin文件和库合并(不覆盖的方式)进入镜像的bin和lib里 RUN cp -n /curl-static/bin/curl /bin/ RUn cp -n /curl-static/lib/* /lib/ # 设置默认命令 CMD [\"sh\"] 搞定 方法3：下载一个独立的bin文件COPY进去 这种更简单，就是要找到人家一个bin文件就是独立的curl这种，all-in-one吧 https://curl.se/download.html 找到linux系统下的binary 搞定 LABEL指定镜像元数据 镜像的说明：标签、版本、作者等 #一行格式 LABEL multi.label1=\"valuel\" multi.label2=\"value2\" maintainer=\"oneyearice other=\"value3\" #多行格式 LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ maintainer=\"oneyearice \" \\ other=\"value3\" 作者的指令早期是 MAINTAINER # 现在改掉了 LABEL maintainer=\"oneyearice \" RUN命令 alpine里安装常用工具 这些默认没有，现在需要安装一点，这个比上面busybox里安装curl要简单多咯。 先替换为国内镜像源 点击问好得到镜像源替换信息 复制 sed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories 好在一般linux里没有vim但是有sed命令的，alpine容器镜像自然是sed命令的。 上面是清华源的使用，我下面用中科大的 https://mirrors.ustc.edu.cn/help/alpine.html sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories 其实可以先run一个alpine进去手动安装看看，这样比较清楚具体过程是否ok 比如tcping其实说到底就是tcptraceroute写出来的脚本，你自己也可以写啊👇写好了： FROM alpine:3.19.1 LABEL maintainer=\"oneyearice \" RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories # 跟新源后再安装 # 一个RUN产生一层镜像，6个就6层镜像，层越多，效率越低，减少层的方式 RUN apk update RUN apk add curl RUN apk add vim RUN apk add ss RUN apk add telnet RUN apk add tcptraceroute # 将上面的6个RUN合成一个RUN，就不会打6层镜像了 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk add curl vim tcptraceroute iproute2 apk update 类似 ubuntu的apt update 但是完全不是yum update哦，别搞错啦，哈哈。 apk update是yum makecache更新镜像源 apk upgrade是yum update，更新软件 apk确实好比apt，而apt的老版本就是apt-get search一下telnet包在哪？ 也可以用busybox里的telnet busybox瑞士军刀啊，这个叼，很多命令都不用找，直接busybox，不过alpine就是利用的busybox 当然不是FROM busybox而是定制的tar包 telnet 如果不用busybox就用search到的，不过要去掉后面的版本好之类的 FROM alpine:3.19.1 LABEL maintainer=\"oneyearice \" RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories # 跟新源后再安装 # 一个RUN产生一层镜像，6个就6层镜像，层越多，效率越低，减少层的方式 RUN apk update RUN apk add curl RUN apk add vim RUN apk add ss RUN apk add busybox RUN apk add tcptraceroute # 将上面的6个RUN合成一个RUN，就不会打6层镜像了 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk --no-cache add curl vim busybox tcptraceroute iproute2 # 但是如果一层，将来镜像复用还能复用得到吗？如果将RUN的替换镜像源、各个工具的安装都单行来写，将来是不是其他镜像也能更好的复用呢，否则上面虽然整合成一条RUN，但是一个分层，别人如何复用呢？ # 常规使用上来考虑还是写成一个RUN比较好，不同基础镜像的工具也不太可能复用，而且复用里面到底怎么复用的也不是很清楚比如UinonFS(overlay2)。 所以你写在Dockerile里的RUN apk add xx xxx xxx xx 如果前面没有这个包，就直接报错了。 没办法只能先run一个进去安装成功了，再写道Dockerfile里了。 apk没有类似ubuntu里的apt clean这个命令，所以要用apk --no-cache来安装软件，这样安装后安装包就会自动删除。简称不缓存安装信息。减少镜像的空间。 可以参考别人的安装工具集 一个自用的alpine:Dockerfile 经过上面的不断校对，下面就是一个相对完善的Dockerfile啦 FROM alpine:3.19.1 LABEL maintainer=\"oneyearice \" # 跟新源后再安装 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk --no-cache add tzdata && ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && echo \"Asia/Shanghai\" > /etc/timezone && apk update && apk --no-cache add iotop gcc libgcc libc-dev libcurl libc-utils pcre-dev zlib-dev libnfs make pcre pcre2 zip unzip net-tools pstree wget libevent libevent-dev iproute2 vim curl tcptraceroute busybox-extras tcpdump 赶快去试试吧 build 灰常顺利 run起来看看 一些依赖python的可能由于py没装好就报错，其他还是ok的👆这个节后再折腾，无所谓了~ 抓包也是ok的 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:20:01 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/7.Docker自动制作镜像常见指令说明.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker容器管理和镜像制作/7.Docker自动制作镜像常见指令说明.html","title":"7.Docker自动制作镜像常见指令说明.md","keywords":"","body":"第7节 Docker自动制作镜像常见指令说明 ENV 设置环境变量 注意是shell的环境变量，而不是普通变量 之前 run一个mysql容器设置变量的方法，就是基于EVN实现的👇 当时那个mysql容器里的变量就是通过-e来传递的。 #变量赋值格式1 ENV # 此格式只能对一个key赋值，之后的所有内容钧会被视作其 #变量赋值格式2，此格式可以支持多个key赋值，定义多个变量建议使用该格式，减少镜像层 ENV = =\\ = ... #如果中包含空格，可以反斜线\\进行转义，也可以通过对加引号进行标识；另外，反斜线也可以用于续行。 续行，可还行~ 老师词汇不错~ #只使用一次变量 RUN = #引用变量 RUN $key #变量支持高级赋值格式 ${key:-word} ${key:+word} 去看看官方mysql的ENV配置 发现就2行，也没看到密码的变量，发现有个脚本 找啊找啊，找到了👇 就是说人家还是通过脚本整体去搞定变量这件事了。 下面还是用ENV的方式来定义吧 run起来看下变量是否进去了 ok👆 注意观察CACHED 再次build可见 现在的RUN sed 那一行也是之前做过的，所以这里就是CACHED，利用的缓存了，这就是分层了。 这是build的时候定义的环境变量，如何在后面再去修改呢，-e咯 多个-e就是改多个 ENV的跨阶段有效性 再看个阶段状态，从Dockerfile build成image ENV是有效的，然后ENV还在image RUN 成容器的时候也有效，也就是说ENV环境变量设置跨了两个阶段，其实肯定的，第一个阶段是定义，第二个阶段是应用嘛，但是作为类比，其他的指令就不是了比如RUN 比如CMD ARG等👇 举例RUN指令只在build的时候才有效 RUN只在build阶段有效，RUN起来是不会生效的，证明如下 这小段👆内容呢，其实作为我来讲，压根不会有这个困惑，①build build， build完了Dockerfile里的东西就算用过了，不会再持续生效，这才是常理 ②build的东西持续到run生效，其实ENV只是设置了环境变量，从而再run起来变量当然生效，你不能说run起来ENV这个动作还在，对吧，压根没有的困惑，被解释成了一片所谓的知识，这就是这段文字的我的个人理解。 但是这个图确实有意义，它明确指出了哪些命令在哪个阶段起作用的，除了ENV我认为不应该也出现在run阶段，因为build完了的变量是持续的，而不是ENV这个动作是二次生效的。以下有证明改图的右边的EVN是不存在的，常识性的东西。 变量高级用法 Dockerfile里一样这么用 讨厌的咧~ 变量的作用范围 然后注意两个RUN指令之间没有继承关系，我说的是变量不会继承 👆图中红箭头并不会集成变量，所以结果test里还是ming # 因为第二个RUN应该是独立的，name依旧为空，所以author=ming👇 改成一条RUN就行啦 由于是一个RUN指令里的name，所以能够继承，所以author高级赋值就依据name里的值为123了👇 两个RUN之间不能继承，但是RUN可以继承上面的ENV啊，ENV是环境变量了，肯定OK的，RUN好比函数里的变量属于local本地局部变量。 上图👆奇怪没有看到ENV赋值的语句在build中呈现。 总之ENV和RUN的研究就差不多了，当然-e去覆盖只是覆盖的一摸一样的那个变量，不能说你build的时候z=x+y，然后-e 赋值x，z就变了，这是不可能的，又不是python的列表字典赋值--指针的意思。所以要注意别被人带偏了。上图的-e都是无所谓的参数只是放在这里看看。 然后变量高级赋值的+-补充理解 这里有一个高级赋值的案例，是用在监听socket上的写法 COPY复制指令 把脚本复制到镜像里 copy的时候可以顺带改属性 给执行权限 看下效果 这就在容器里的根目录下生成了test.log文件里面的内容如上👆 COPY的是源文件，必须是 ①相对路径 + 且 ②是Dockerfile的同级或下级目录 ADD复制和解包指令 比COPY多一个直接解包的功能 1、先准备一个打包文件 所以mv xxx ~-/ 就是将当前文件xxx移动到上一个工作目录中 2、然后用ADD指令复制并解压到镜像里 看看alpine官方的Dockerfile也是用的ADD CMD容器启动命令 CMD就是指定一个命令作为容器启动的默认命令。 一般服务性的容器就是CMD是挂前台，这样容器运行后就不会退出 这个前面梳理过了👇 启动服务的时候，将服务最终的前台服务进程写在CMD后面 CMD和RUN的不同 CMD和RUN都是执行命令，但是阶段不同，RUN是build的时候用过就用过了，CMD是容器run起来的时候才会生效的。 也就是说CMDbuild的时候是不生效的，而docker run的时候才起作用，这个确实要注意的。 制作基于alpine的自定义nginx镜像 # 准备相关文件 # 先定义文件目录结构，分门别类base、web分开来 mkdir -p /data/dockerfile/{base/{ubuntu,centos,alpine,busybox},web/{jdk,nginx,tomcat}} mkdir /data/dockerfile/web/nginx/1.26.0-alpine/ cd /data/dockerfile/web/nginx/1.26.0-alpine/ wget https://nginx.org/download/nginx-1.26.0.tar.gz echo Test Page based nginx-alpine > index.html cp ../1.26.0-centos7/nginx.conf . cat nginx.conf user nginx worker_processes 1; daemon off; ... location / { root /data/nginx/html; .... # 编写Dockerfile vim Dockerfile cat Dockerfile FROM alpine-my-base:3.19.1 LABEL maintainer=\"oneyearice \" ENV NGINX_VERSION=1.26.0 ADD nginx-$NGINX_VERSION.tar.gz /usr/local/src/ RUN cd /usr/local/src/nginx-$NGINX_VERSION && ./configure --prefix=/apps/nginx && make && make install && ln -s /apps/nginx/sbin/nginx /usr/bin && addgroup -g 2024 -S nginx && adduser -s /sbin/nologin -S -D -u 2024 -G nginx nginx COPY nginx.conf /apps/nginx/conf/nginx.conf ADD index.html /data/nginx/html/index.html RUN chown -R nginx.nginx /data/nginx/ /apps/nginx/ EXPOSE 80 443 # 这里由于修改了配置文件，配置文件里有daemon off前台运行了，所以直接nginx一敲就行了 CMD [\"nginx\"] # 如果利用默认配置文件就需要手动前台运行 #CMD [\"nginx\",\"-g\",\"daemon off;\"] # 构建镜像 vim build.sh cat build.sh #!/bin/bash #----------------------------------------------------- docker build -t nginx-alpine:1.16.1 . 下面是实验过程，比上面的配置文件少了一个自定义配置文件 还少了 端口暴露，要注意下 build一下 很顺利👆 run一下看看 ok跑起来了👆 测试OK 这样就制作出了一个镜像，同时也是按照 OS基础镜像-----安装常用命令得基础镜像------应用镜像得思路来弄得。 可惜的是上面的镜像没有把log做出来，人家官方的log直接就是能👇看得到下图是官方的log 看看人家官方的日志，关于日志前面有讲过的 分析下为什么我的alpine定制的nginx没有logs 所以其实是少了这个动作应该， 试试，在web01的容器里直接修改看看 好像不行没日志出来👇 去看看人家的好像这么tail也看不到 于是再看看自己的docker logs 有了！！，哈哈，原来上面的做法是对的， 只不过tail 看不到stdout罢了，所以将上面的操作改写成Dockerfile就行了 自定义的nginx-alpine做好日志 1、自己折腾方法1--就不丑 重新build run 就ok啦 而且官方就是这么做的👇 2、人家的做法2-也挺好 你tail access.log 那error.log呢，这不是错误日志没有STDERR了，没有屏幕输出，怎么docker logs查看呢。 所以视频里老师就tail /apps/nginx/logs/* 牛逼哈哈哈也行~ buid后run，再logs看看👇 这里再次复习一问题 docker run运行一个容器，端口映射为8080，一段时间之后忘记当初运行容器的命令，请问如何修改映射的端口为80。 1、首先题目要看到，你说停掉重新run，run的选项你知道嘛？你说就那几个，对，万一人家用了非常规的选项呢。 2、所以处理方法有三， ①就是前文讲过的去修改分层文件里的配 ②或者还可以，在你已经安装了runlike第三方工具的前提下，可以直接看出当初run的所有默认和非默认的选项， ③或者还可以，inpsec自己去一个个看也可以找到当初run的配置选项。 汇总 👆上图的kill好像也不怎么用，然后--restart always肯定要用 不过要注意一个细节就是👇 还有一些问题得能够讲清楚，还得继续学习 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:20:17 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/Docker镜像制作及存储和网络管理.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/Docker镜像制作及存储和网络管理.html","title":"Docker镜像制作及存储和网络管理","keywords":"","body":"Docker镜像制作及存储和网络管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/1.Dockerfile常见指令用法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/1.Dockerfile常见指令用法.html","title":"1.Dockerfile常见指令用法.md","keywords":"","body":"第1节 Dockerfile常见指令用法 接上节，浅复一下 FROM：所有镜像都有父镜像，scratch镜像 LABEL：版本信息，功能说明，作者 ENV：环境变量，build后run起来可用，后续run得时候-e可覆盖 RUN：build时候执行得shell命令，比如编译安装，ln -s，配置文件修改，脚本操作 COPY：宿主的文件复制到镜像里，文件得放在build动作的当前工作目录，且使用相对路径（一般就是Dockerfile所在目录，除非Dockerfile 是 -f 放到其他地方了，此时COPY的源文件还得必须是build这个动作的当前目录）。 ADD：比COPY多了一个自动解压缩，然后复制的时候，目标目录可能不像COPY那样需要加上/，来严谨对待，但是我们还是要规范的。 CMD：容器run来后执行的命令，一般用来挂前台保证容器run起来后不会停止的。CMD的要求就是执行程序得是前台的不能是后台的，比如nginx命令通常就是后台的，需要明确指定为后台nginx -g daemon off这种。 同时 alpine 、 ubuntu 、centos 这些系统都是/bin/sh的都是会run起来就Exited了，所以创建web服务、java服务、微服务，这些服务的时候就需要CMD是一个前台的程序。 CMD的缺陷： 配置文件的处理 现在要在Dockerfile里定义好配置文件， 1、先把容器跑起来，然后从里面复制一份conf出来 2、优化下配置文件 涉及的参数如下： worker_processes auto; ... events { worker_connections 10240; } ... ... include /apps/nginx/conf/conf.d/*.conf; vim build.sh #!/bin/sh doker build -t nginx-alpine-self:$1 . chmod +x build.sh ./build/sh 1.26.0-v1.0 run起来然后查看下配置文件是否修改👇 已修改，然后看下conf.d文件夹是否自动创建 并没有创建👇 所以Dockerfile里要创建该目录了 这也是分层多的和分层少的 自己要注意的地方，比如这里由于RUN太长了，前面还有编译的动作，所以要考虑拆成两个RUN就会是的前面的RUN层级可以被复用。 再run起来，看看conf.d是否ok 这样就有了独立的配置文件存放目录conf.d 3、配置server块 3.1 创建子目录编写server块 这样配置文件就有了👆 3.2 还得有一个网页html文件 3.3 编写Dockerfile 肯定是基于上面制作得nginx-alpine-selfv1.3来进一步封装啦，要利用之前得nginx的配置文件，特别是conf.d这个目录得存在。 FROM nginx-alpine-self:v1.3 LABEL maintainer=\"oneyearice \" RUN mkdir -p /data/website # 存放也没html文件的 COPY index.html /data/website/ # html复制进去 COPY www.ming.org.conf /apps/nginx/conf/conf.d/ # 子配置文件，也就是server块文件扔进去 # 由于继承的镜像里有CMD挂前台了，所以这里无需再配置CMD了 这样这些原材料就准备好了 build一下 搞定👆 然后run起来，run的时候要用-p而不是大P，理由如下： 1、容器里的nginx默认就是监听80，但我们现在讨论的是 端口暴露这个动作； 2、-p 小p就是明确将容器里的80暴露成宿主的某个端口； 3、-P 大P就是将Dockerfile里定义的 指定暴露的容器里的端口，暴露成宿主机的随机端口。 然后就好了👇 复习下端口暴露的情况查看除了docker ps 还有👇 哎，不对，html页面内容不对，里面有两个网站的，一个是nginx.conf主配文件里的，一个是conf.d下的子配置文件，明显这个helo helo ----xxx被nginx.conf里的location配置给截胡了。 上图主配置文件的server块里的root再location /下，然后使用的是相对路径，这个相对路径就是当初编译的时候的路径，怎么查看呢👇 所以root html就是在/apps/niginx/html下的index.html文件了 汇总一下 所以被截胡了 所以换个curl方式就可以优先应用子配置文件了👇 然后进一步把IP地址的curl也让子配置文件优先👇 nginx的转发优先级 基本实验需求已实现，但是关于nginx的server块优先级，涉及子配置文件，主配文件，location，还有server块里的ip+port > servername ,servername里的又细化为 以上是模糊的梳理，还需进一步明确优先级。 1、include写在主配置文件的server前面，基本上就是include那里的子配置文件优先看了 ​ # 然后因为80是默认的，写不写都是，所以基本上轮不到主配置文件的server块了 ​ # 然后多个include还是看谁在前 2、然后include所指的子配置文件夹里(一般就是conf.d)就看多个文件之间，以及单个文件内的server块之间的优先了 ​ 2.1 按字母顺序查找 ​ 2.2 单个文件里就按 ​ ①ip+port 最优 ​ ②才是这张图👇 3、然后再实现一个curl ip 走一个，curl 域名走一个需求，方法①代理②配置文件里写map？ Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:17:06 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/2.Dockerfile常见指令用法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/2.Dockerfile常见指令用法.html","title":"2.Dockerfile常见指令用法.md","keywords":"","body":"第2节 Dockerfile常见指令用法 动态化处理 但是安图配置后👆还是不行，换一个方法 ①环境变量里预先定义 ②使用envsubst来替换 然后再编写到Dockerfile里就行了 1、删除所有镜像和容器，重新构建 docker rm -f `docker ps -qa` docker system prune -a 2、构建base os：构建alpine自定义镜像 3、构建app 镜像 4、构建app 带传递环境变量的镜像 搞定 换个PORT端口传递进去 ok，nginx的端口和HOST自定义就搞定了👆。 注意事项： 1、使用-e传递参数进容器，不是在build阶段，而是在run阶段，所以想要用这个变量，Dockerfile里要是用CMD指令而不是RUN。 2、然后测试的时候不要 -it sh进去看效果，因为会替代CMD导致Dockerile的CMD不执行了。 3、因为父镜像里有CMD，你子镜像也要用CMD，用屁股想也是要最后一个CMD生效的，不可能多个CMD生效的，否则就不存在-it sh替代一说了，所以要写全，比如nginx -g 'daemon off;' 4、nginx 配置文件里可以写变量，但是nginx自身是不认环境变量的，即使主配文件的main块里写env也不会生效-实验结果就是不生效，虽然官方有这个指令说明。 5、使用第三方插件的gettext包里的envsubst命令来替换就行了。 6、上面的Dockerile里的apk 安装需要改成RUN apk add --no-cache gettext ，实现安装后删除安装包的效果。 进一步思考 上图理解： 首先一般来讲nginx.conf里是不配置server块的，所有子配置文件都放在一起，然后系会展开合并在一起去判断，先按ip和端口、再按server_name，如果两个一样，再按子文件的命名顺序，比如a.conf的www.mimng.org优于b.conf的www.ming.org。 1、nginx的转发优先级：ip+port优先，如果一样，就看server_name，server_name里再细分 2、curl的时候使用ip+port，就会从上往下查找配置文件 3、理解1和2参考👇 好了实现就行了，继续看视频吧。 Dockerfile镜像制作和使用流程 工作案例分享-ip和座位号track 1、背景 员工IP动态获取，存在变动性， 员工座位也是经常换，存在变动的， 如何知道该员工当前准确的IP是什么，座位在哪里？ 2、需求 需要实现ip以及座位的自动校准 3、思路 用户的电脑的MAC是固定的，基于该MAC每天动态的获取最新IP就行了 交换机接口和座位号的对应关系是固定的，基于该MAC在哪个接口下就知道了该员工的座位号了 4、落地 首先需要收集所有员工姓名和MAC的清单：如何实现很多人的信息一键收集呢？ 其次有一个基于MAC快速获取IP的接口：如何编写基于mac信息获取ip及座位号的接口？ 所以： ​ 你需要有一个web前端，让用户填写姓名、座位号，并且适当的做一下限制防止有人乱写。 通过http的字段获取报文里的IP信息，根据IP找到此人的MAC、找到交换机的端口。 ​ 然后将user和mac入库形成user_mac表，将seat、sw port 入库形成seat_sw_port表。 ​ 然后每天或者每15分钟都行，根据mac找到当前的IP，根据mac找到所在的sw port，然后遍历seat_sw_port库，找到所在的seat，这样就得到了一个完整的记录user、ip、mac、seat、sw、port ​ 当然这一条记录也要加上aging老化时间，这样当该员工离职后，就会有一个自动删除记录的机制。 5、具体的实现步骤 信息的收集 web前端的细节本文不涉及，我也不太会，请别人写的，这里说明下具体注意点 1、页面长这样 2、为了防止别人乱填，做以下限制 首先当用户填写姓名和座位号的时候，就会在服务器那头收集三个信息分别是：姓名、座位号、IP地址这三个，并入库infocollection 限制如下： ①座位号格式必须是你希望的格式，是你们公司规范化后的格式，比如'[a-bA-B][0-9]{2}'这么一个规范。 ②如果姓名和IP已存在，则覆盖原来的记录，给用户纠错的空间 ③如果姓名和座位号已存在，则弹出窗口 \"该座位号已绑定，请联系管理员\" 3、以上就收集到了很多员工的三个信息 name \\ seat \\ ip 下面就是针对 这三个信息的处理 信息的处理 然后就根据当前的ip找到两个信息 ①mac地址及其②所在的交换机的端口 >> 此时就需要写一个脚本来实现基于mac、ip、交换机的端口的信息查询。 这里提供了我的三个脚本，按序执行 python3 /pycharm_project_418/switch/getData/getMAC/ssh_netmiko_get_mac.py python3 /pycharm_project_418/switch/getData/getArp/ssh_netmiko_get_arp.py python3 /pycharm_project_418/switch/pickInfo/ipMacPort/client_ip_cam.py 这里可能还涉及早期写的execl表格处理，不用管 下面是对三个脚本的解释说明 ssh_netmiko_get_mac.py说明 ​ 该脚本是用来获取各个厂商的交换机的mac地址表 #!/usr/bin/python3.6 # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") from netmiko import ConnectHandler import time import os import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime def ssh_singlecmd_netmiko(sw, cmd): # net_connect = ConnectHandler(device_type='cisco_ios',host='IP地址',username='用户名',password='密码') net_connect = ConnectHandler(**sw) # current_view = net_connect.find_prompt() net_connect.enable() # 进入CISCO或H3C的特权模式，针对H3C可能不灵，但目前是好的。 # 执行命令，返回结果为字符串，赋值给output result = net_connect.send_command(cmd) # if \"confirm\" in output: # output += net_connect.send_command_timing( # \"\\n\", strip_prompt=False, strip_command=False # ) net_connect.disconnect() fileName = f'{sw[\"ip\"]}' os.chdir(\"/switch/mac_port\") save = open(fileName, 'w') save.write(result) def get_mac(): ###################### # 将yaml文件转成字典 # #################### os.chdir(\"/pycharm_project_418/switch\") currentpwd = os.getcwd() print(currentpwd) with open('swList.yml') as f: swInfoList = yaml.load(f, Loader=yaml.Loader) # 将yaml文件转成数组，里面是字典，每个字典是一个sw # 去掉核心交换机172.16.13.254,不要在这里去了，第一步就是大而全，其中处理，后面在过滤吧。 # pool = ThreadPool() pool = ProcessPool() ip_port_error_list = [] # 最终结果 # 格式化为netmiko需要的几个字段 for swInfo in swInfoList: # pprint(swInfo) swInfo.pop('community') swInfo.pop('location') pprint(swInfo['ip']) # pprint(swInfo) if 'cisco' in swInfo['device_type']: print(swInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(swInfo, 'show mac address', )) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'hp_comware' in swInfo['device_type']: print(swInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(swInfo, 'disp mac-address',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'ruijie_os' in swInfo['device_type']: print(swInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(swInfo, 'show mac',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) pool.close() pool.join() if __name__ == '__main__': get_mac() ############################### ## 外网核心交换机和外网楼层交换机 # ############################### - ip: '192.168.11.1' device_type : 'hp_comware' username: 'xxxx' password: 'xxxxx' secret: '' community: 'snmp@xxx' location: '-1F机房' - ip: '172.16.12.1' device_type: 'cisco_ios' username: 'xxx' password: 'xxx' secret: 'xxx' community: 'office@xxx' location: '-1F机房' - ip: '172.16.13.1' device_type: 'ruijie_os' username: 'xxx' password: 'xx' secret: 'xxxx' community: '' location: '-1F机房' ssh_netmiko_get_arp.py说明 用来收集内外网GW上的信息 #!/usr/bin/python3.6 # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") from netmiko import ConnectHandler import time import os import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime def ssh_singlecmd_netmiko(sw, cmd): # net_connect = ConnectHandler(device_type='cisco_ios',host='IP地址',username='用户名',password='密码') net_connect = ConnectHandler(**sw) # current_view = net_connect.find_prompt() # print(current_view) # 显示当前所在试图一般为用户模式 net_connect.enable() # 进入CISCO或H3C的特权模式，针对H3C可能不灵，但目前是好的。 # current_view = net_connect.find_prompt() # print(current_view) # 显示当前所在试图一般为用户模式 # 执行命令，返回结果为字符串，赋值给output result = net_connect.send_command(cmd) # if \"confirm\" in output: # output += net_connect.send_command_timing( # \"\\n\", strip_prompt=False, strip_command=False # ) net_connect.disconnect() # print(result) timeNow = datetime.now().strftime(\"%Y-%m-%d\") # 不要时间了，直接IP命名 fileName = f'{sw[\"ip\"]}' os.chdir(\"/switch/arp\") save = open(fileName, 'w') save.write(result) def get_arp(): ###################### # 将yaml文件转成字典 # #################### os.chdir(\"/pycharm_project_418/switch\") currentpwd = os.getcwd() print(currentpwd) # with open('gw.yml') as f: gwList = yaml.load(f, Loader=yaml.Loader) # 将yaml文件转成数组，里面是字典，每个字典是一个sw # pprint(gwList) # pool = ThreadPool() pool = ProcessPool() ip_port_error_list = [] # 最终结果 # 格式化为netmiko需要的几个字段 for gwInfo in gwList: # pprint(swInfo) gwInfo.pop('community') gwInfo.pop('location') pprint(gwInfo['ip']) # pprint(gwInfo) if 'juniper' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'get arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'hp_comware' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'disp arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'cisco_ios' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'show arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) elif 'ruijie_os' in gwInfo['device_type']: print(gwInfo['device_type']) try: x = pool.apply_async(ssh_singlecmd_netmiko, args=(gwInfo, 'show arp',)) # 这里的pass仅仅是防止出错导致程序无法运行，在函数里面的try才是有实际意义的异常处理 except BaseException as e: print(e) pool.close() pool.join() if __name__ == '__main__': # 做成函数是为了将来在页面做成按钮，实现动态实时获取，现在每天写到文件，后面再弄DB之类的。 get_arp() ############ ## 外网GW # ######### - ip: '192.168.11.1' device_type : 'hp_comware' username: 'xxxx' password: 'xxxx' secret: '' community: 'snmp@xxxx' location: '-1F机房' - ip: '172.16.12.1' device_type: 'cisco_ios' username: 'xxxx' password: 'xxxx' secret: 'xxxx' community: 'xxx@xxxx' location: '-1F机房' ############ ## 内网gw # ######### - ip: '2.0.1.2' device_type: 'juniper' username: 'xxxx' password: 'xxxx' secret: '' community: '' location: '-1F机房' - ip: '172.16.13.1' device_type: 'ruijie_os' username: 'xxxx' password: 'xxxx' secret: 'xxxx' community: '' location: '-1F机房' client_ip_cam.py说明 先说明一个前置脚本swIp_mac_port.py #!/usr/bin/python3.6 # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") from netmiko import ConnectHandler import time import os import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime import re import os def sw_ip_mac_port(): # 从pyConsole的/swith/提取arp和mac文件，该文件是每天一份18点15出 os.chdir('/switch/mac_port') # 去掉外网核心、内网核心、楼栋互联的100和101不统计这些非接入PC的交换机 with os.popen('ls |grep -Ev \"192.168.|172.16.13.254|172.16.12.100|172.16.12.101\"', 'r') as p: # with os.popen('ls |grep -Ev \"172.16.13.254\"', 'r') as p: info = p.read().strip() # print(info) # 格式化cam表，去掉无用的，做成字典 swS_ip_cam = {} thisSw_vlan_mac_port = {} for i in info.split('\\n'): # 一个代表一个sw if i: # 因为上面做了p.read().strip()，所以这里算是多余的吧。一般不会出现i是None了。 sw_ip = i.split('_')[0] # print(sw_ip) # with os.popen(f'cat {i} |grep -i DYNAMIC', 'r') as a: # 端口开启了mac-security后client的mac就是static了，此处需要优化 with os.popen(f'cat {i} |grep -Ei \"dynamic|static|Learned\" |grep -Evi \"cpu\"', 'r') as a: # cam = a.read().replace('DYNAMIC', '') cam = re.sub(r'(?i)dynamic', '', a.read()).strip() # (?i)这是正则的写法忽略大小写的意思 cam = re.sub(r'(?i)static', '', cam).strip() cam = re.sub(r'(?i)learned', '', cam).strip() cam = re.sub(r'(?i)aging', '', cam).strip() # cam表格去掉无用的 # print(cam) thisSw_vlan_mac_port = {} unit_info = { } count = 1 for m in cam.split(\"\\n\"): z = m.split() # print(z) try: if len(z[2:]) > 1: port = z[2] + z[3] else: port = z[2] if sw_ip == '192.168.11.1': unit_info = { count: { 'vlan': z[1], 'mac': z[0], 'port': port, # 这个可不能顶层赋值了啊，我的好习惯变成坏事了。或者顶层附一个else的值，再取消else，无语。 } } else: unit_info = { count: { 'vlan': z[0], 'mac': z[1], 'port': port, # 这个可不能顶层赋值了啊，我的好习惯变成坏事了。或者顶层附一个else的值，再取消else，无语。 } } thisSw_vlan_mac_port.update(unit_info.copy()) count += 1 except BaseException as e: print(sw_ip, e, '十有八九是cam表没取到') # pprint(thisSw_vlan_mac_port) thisSw_vlan_mac_port = { sw_ip: thisSw_vlan_mac_port.copy() } # pprint(thisSw_vlan_mac_port) swS_ip_cam.update(thisSw_vlan_mac_port.copy()) # print('===========================') # pprint(swS_ip_cam) return swS_ip_cam # 到此就得到了每台sw的cam表，但是注意没有去掉上行口信息，下面就开始去掉上行口信息。 # 首先得获得每台得上行口,思路来了，每个字典里去掉上行信息，这个需要配合arp表，即去掉网关IP的mac对应的端口，这个端口出来后，所有从这个端口学到的mac都删掉，就得到干净的CAM表。 # 所有真就不应该在mac初始环境去处理，不好，原则是尽量少的登入SW。 # 下面转到clientIP_mac_port模块 if __name__ == '__main__': z = sw_ip_mac_port() pprint(z) 输入的样子如下👇 下面就是最后一个脚本client_ip_cam.py # -*- coding=utf-8 -*- import sys sys.path.append(\"/pycharm_project_418/\") import time import os import json import yaml from pprint import pprint from multiprocessing.pool import ThreadPool from multiprocessing import Pool as ProcessPool from datetime import datetime import re from switch.pickInfo.ipMacPort.swIp_mac_port import sw_ip_mac_port from openpyxl import load_workbook os.chdir('/switch/arp') with os.popen('ls', 'r') as p: file_name = p.read() client_mac_ip = {} for i in file_name.split(\"\\n\"): if i: gw_info = i.split(\"_\")[0] grepFilter = 'grep -E \"[0-9]{1,3}\\.\" ' with os.popen(f'cat {i} | {grepFilter}', 'r') as r: context = r.read() # print(context) for m in context.strip().split(\"\\n\"): # print(m) if m: client_ip = m.strip().split()[0] client_mac = m.strip().split()[1] client_vlan_actual = m.strip().split()[2] client_aging = m.strip().split()[4] # 这里面要赛选出一个mac多个ip的条目，并取出aging大的那一条 if 'eth' in client_vlan_actual: client_vlan_actual = client_ip.split('.')[-2] client_aging = m.strip().split()[5] if client_mac in client_mac_ip: if int(client_aging) > int(client_mac_ip[client_mac][-1]): # 如果aging最大，也就是最新就覆盖之前的mac ip vlan # print(client_mac, client_ip,client_vlan_actual,client_aging) # print(client_mac, client_mac_ip[client_mac]) client_mac_ip[client_mac] = [client_ip, client_vlan_actual, client_aging].copy() else: client_mac_ip[client_mac] = [client_ip, client_vlan_actual, client_aging].copy() # 这里补上arp里的vlan才是对的 # pprint(client_mac_ip) ''' 生成client_ip_mac[client_ip] = [client_mac, client_vlan_actual].copy() # 这里补上arp里的vlan才是对的 ''' client_ip_mac = {} for k, v in client_mac_ip.items(): client_ip_mac[v[0]] = [k, v[1]] # pprint(client_ip_mac) # exit() ''' 得到用户ip和mac信息 ''' # pprint(client_ip_mac) # 第一步找到client们的网关的MAC dev_net_sw_mgmt_gw = '172.16.13.253' # 内网交换机的管理段网关 official_net_sw_mgmt_gw = '172.16.12.254' # 外网交换机的管理段网关 os.chdir('/switch/arp') # 找了两个接入sw用来查看GW的MAC access_sw_for_find_dev_net_sw_mgmt_gw = '172.16.13.1' access_sw_for_find_official_net_sw_mgmt_gw = '172.16.12.1' with os.popen(f'cat {access_sw_for_find_dev_net_sw_mgmt_gw} |grep {dev_net_sw_mgmt_gw}', 'r') as p: z = p.read() dev_net_sw_mgmt_gw_mac = re.findall('([0-9a-f]{4}\\.[0-9a-f]{4}\\.[0-9a-f]{4})', z)[0] # print(z) # print(dev_net_sw_mgmt_gw_mac) with os.popen(f'cat {access_sw_for_find_official_net_sw_mgmt_gw} |grep {official_net_sw_mgmt_gw}', 'r') as p: z = p.read() official_net_sw_mgmt_gw_mac = re.findall('([0-9a-f]{4}\\.[0-9a-f]{4}\\.[0-9a-f]{4})', z)[0] # print(z) # print(official_net_sw_mgmt_gw_mac) # 第二步根据网关MAC，找到各个sw的cam表的什么呢？找到该MAC对应的端口即为上行口，去掉所有该口学到的MAC，就得到了用户MAC。 sw_mgmt_gw_mac_list = [dev_net_sw_mgmt_gw_mac,official_net_sw_mgmt_gw_mac] ''' 这就得到交换机上行口的获取方法（交换机管理IP的网关的MAC，必然是从上行口学来的。） ''' # print(sw_mgmt_gw_mac_list) swS_ip_cam = sw_ip_mac_port() # 这里调用了会改变PWD，要注意下 # pprint(swS_ip_cam) sw_upLink_port = {} for k, v in swS_ip_cam.items(): # print(v) upLink_port_list = [] for m, n in v.items(): # print(n) if n['mac'] in sw_mgmt_gw_mac_list: # print(k, n) upLink_port_list.append(n['port']) # print(upLink_port_list) upLink_port_list.sort() upLink_port_list = list(set(upLink_port_list)) sw_upLink_port[k] = upLink_port_list ''' 得到了交换机的上行口 ''' # pprint(sw_upLink_port) # 将所有swS_ip_cam中涉及上行口的信息单元删除 swS_ip_cam_copy = swS_ip_cam.copy() del_list = [] for k, v in swS_ip_cam.items(): for m, n in v.items(): if n['port'] in sw_upLink_port[k]: # print(n['port']) # print(k, m) del_list.append((k, m)) # swS_ip_cam_copy[k].pop(m) # 遍历字典不能删除自己或别人操作，就是遍历的时候不能有这种动作 # pprint(swS_ip_cam[k]) # print(del_list) for i in del_list: swS_ip_cam_copy[i[0]].pop(i[1]) # pprint(swS_ip_cam_copy) ''' 这个得到了所有下行口的mac，也就是用户mac ''' swS_ip_downlink_cam_copy = swS_ip_cam_copy.copy() # pprint(swS_ip_downlink_cam_copy) client_ip_mac_sw_port_vlan_list = [] for client_ip, mac_vlan in client_ip_mac.items(): # print(v['mac'],mac) # 在比较arp里的mac和cam里的mac之前，先做mac的格式化 arp_mac = re.sub('[\\.\\-\\:]', '', mac_vlan[0]).lower() actual_vlan = mac_vlan[1] # print(cam_mac, arp_mac) client_ip_mac_sw_port_vlan = {} # 每次for循环都需要一个空的字典初始化一下。 for sw, info in swS_ip_downlink_cam_copy.items(): for seq, v in info.items(): cam_mac = re.sub('[\\.\\-\\:]', '', v['mac']).lower() # print(v) if cam_mac == arp_mac: cam_mac_format = cam_mac[0:4] + '.' + cam_mac[4:8] + '.' + cam_mac[8:12] # print(client_ip,mac,sw,v['port'],v['vlan']) client_ip_mac_sw_port_vlan[\"client\"] = client_ip client_ip_mac_sw_port_vlan[\"mac\"] = cam_mac_format client_ip_mac_sw_port_vlan[\"sw\"] = sw client_ip_mac_sw_port_vlan[\"port\"] = v['port'] # client_ip_mac_sw_port_vlan[\"vlan\"] = v['vlan'] # 存在一个mac 多个vlan 比如不同wifi等 client_ip_mac_sw_port_vlan[\"vlan\"] = actual_vlan client_ip_mac_sw_port_vlan_list.append(client_ip_mac_sw_port_vlan.copy()) # pprint(client_ip_mac_sw_port_vlan_list) pprint(len(client_ip_mac_sw_port_vlan_list)) todayis = datetime.now().strftime(\"%F\") # 将client_ip_mac_sw_port_vlan_seat_list写道文件中，后期写道库中，前端调用 with open(f'/root/yuan_gong/log/yuangong_ip_zuowei_{todayis}.json', 'w', encoding='utf-8') as z: json.dump(client_ip_mac_sw_port_vlan_list, z, ensure_ascii=False) if __name__ == '__main__': pass 综上所述： python3 /pycharm_project_418/switch/getData/getMAC/ssh_netmiko_get_mac.py python3 /pycharm_project_418/switch/getData/getArp/ssh_netmiko_get_arp.py python3 /pycharm_project_418/switch/pickInfo/ipMacPort/client_ip_cam.py 三个脚本跑一遍就会得到当前的用户ip、mac、sw、port、vlan的信息 如下图👇 此时就有了 上文开头处提到的 \" >> 此时就需要写一个脚本来实现基于mac、ip、交换机的端口的信息查询。\" 这么一个脚本 那么基于web页面收集上来的username、seat、ip就可以利用上面的结果进行查询了 当前材料：infocollection表有了、查询ip mac 端口信息的json文件也有了。 还需要再写两个脚本： 1、将infocollection表的信息通过上面的json文件进行查找，得到这么一个信息，并落库👇 2、然后将user_mac拆出来落入user_mac表；也将seat sw port落入seat_sw_port表 3、从infocollection得到user_mac和seat_sw_port两个固定信息表 的这么一个动作一定不能持续执行，一般5天完成批量众人信息的收集后就要停止该动作了。 4、最后就是基于user_mac，去查ip，去查端口，然后利用端口去索引seat_sw_port得到seat座位号。于是就有了最终的一个结果👇 上图生成的脚本就是每天都要运行一次的了，来保证ip变了，座位变了都能及时发现。 最终呈现的效果如下👇 最后，再具体的详细讲解，我会放到我的课程里去细聊。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:17:18 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/3.Dockerfile常见指令ENTRYPOINT用法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/3.Dockerfile常见指令ENTRYPOINT用法.html","title":"3.Dockerfile常见指令ENTRYPOINT用法.md","keywords":"","body":"第3节 Dockerfile常见指令ENTRYPOINT用法 概述 我把cli就是手动敲的命令行叫做cli 我把CMD ENTRYPOINT就是说的Dockerfile里的配置 我把cli entrypoint就是手动命令行里的输入的东西 下文的沟通如上所示 CLI里的追加和覆盖 CMD是直接全部覆盖掉 CMD在Dockerfile里也是最有一个生效，前面都会被覆盖。 docker run 最后跟的参数也就是最后的那些个cli，会覆盖Dockerfile里的CMD 同样之前实验的CMD是一串CLI，也被直接覆盖了 但是ENTRYPOINT就不会了 CLI追加到Dockerfilie里的ENTRYPOINT docker run 最后的那些个cli就会作为Dockerfile里ENTRYPOINT的参数 没变化，再看其实是有变化的，只是docker ps 看不到太长了 上图可见docker run 的cli也就是cat 123 全部都当作ENTRYPOINT的参数了。 这一点inspect看还是不明朗的👇，只能自己知道有这回事，哦cmd的所有参数都是合并进去的。 查看合并后的CLI emmm，工具真多啊 CLI里替换entrypoint及追加cli docker run最后的那些个参数就会追加到ENTRYPOINT里，但是此时的Dockerfile里的ENTRYPOINT已经被docker run --entrypoint替换了。 就是好像cli里的选项--entyrpoint 这种写法没有达到Dockerfile的/bin/sh -c 这种效果 这样就勉强使用--entrypoing选项实现了Dockerfiler里的\"ENTRYPOINT cd;ls\"一样的效果。 Dockerfile里的CMD和ENTRYPOINT合并 这里就不再去docker run的时候写cmd和entrypoint了， 其实可以这么沟通：CMD ENTRYPOINT就代表了Dockerfile；而cmd和entyrpoint小写就代表了docker run的参数选项。 CMD推荐使用CMD [\"\",\"\",\"\"]的方式👇这样追加到ENTRYPOINT就不会有上图CMD方式里的默认/bin/sh -c 总结一下： 1、就是cli手动敲命令行的方式，基本都是所见即所得，就是不会给你自动补一个默认 /bin/sh -c 2、然后Dockerfile方式里的CMD如果是非列表格式的，就会自动补齐/bin/sh -c，所以这种方式追加到ENTYRPPINT也会把默认的/bin/sh -c补上去。 3、CMD其实就是作为ENTYRPPING运行的参数而存在的，或者说运行的选项。 exec 内部cli用help cli获取帮助 妈的敲错激起了，敲到dns上去了，好险好险...... 在shell里面敲sleep就是在当前SHELL1192下面输入的sleep，这是父子进程👇 而exec sleep 就会将当前的1192这个SHELL替换成sleep直接运行👇所以1192就从bash变成了sleep 体会下这个exec的作用 1、先了解变量在子进程中的一个local特性 一样的变量随程序跑完而消失 2、然后观察取消exec注释后的效果 进一步理解 上图也不是全对，主要就知道exec bash为什么将变量赋值保存了下来，原因很简单 1、继承了程序运行时(也即是子进程)里的环境，所以变量就有了， 2、然后程序运行本来时退出子进程的，但是exec bash就会用一个新的子进程来替代本来退出的程序的子进程，并且不会退出了。 3、将程序脚本的子进程得以变相的保留的下来，PID不变，但是换了个BASH。 环境变量如何在程序脚本执行完后得以保存 1、那么使用exec bash确实可以将程序脚本的环境(主要是变量)保存下来，那么进一步的使用场景是什么呢？ 2、第二种实现程序脚本运行完变量不会消失的方法就是source或者.执行文件，这样就是在当前SHELL执行，所以不是和bash file那样是开启子SHELL执行，不存在程序脚本执行完退出的动作，执行前后都是在同一个SHELL中，变量自然一直都在。 3、简单讲就是脚本跑完环境变量得以留存的需求：一个是开启子进程就是bash file这种运行方式或者./file(文件内容顶行#!/bin/bash)一样也是开启子SHELL，这种开启子进程的就通过ecex bash替换子进程将子进程得以不退出，环境得以保留；一个就是source或. file不开启子进程来运行，环境都没变，变量什么的自然都在。 # 总结：方法①开启子进程exec bash和方法②不开启子进程 上图注意逻辑 1、source和. file都是在当前SHELL里执行file的 2、exec bash确实执行了，也生效了，就是用新的BASH 在PID不变的情况下，替换了之前的BASH，所以exit就一层直接推出了 3、sleep 30由于被切换了bash，所以不会被执行。 梳理一下 1、exec bash 改成exec sleep或者其他的试试，应该不一定要exec bash才能继承环境，不是这么个意思，是只有用bash才能保住本来要退出来的子进程。而用exec sleep或起的，自然是可以继承环境，但是不能保住子进程不退出啊。 2、看看nginx容器官方的dockerfile的脚本里是否有有exec CMD的列表形式，里的所有参数变成了ENTYRPOINT命令的所有参数，又变成了exec \"$@\"的所有参数 3、ENTRYPOINT和CMD是docker run的时候生效的，所以-e是可以往里面传参的。 4、然后把下图理解一下 echo 'helo e5 ri 8 dai' > index.html --------------------------------- vim Dockerfile FROM nginx:1.26-alpine LABEL maintainer=\"oneyearice \" ENV DOC_ROOT='/data/website/' RUN makedir -p ${DOC_ROOT} COPY nginx.conf /apps/nginx/conf/ ADD index.html ${DOC_ROOT} ADD entrypoint.sh /bin/ EXPOSE 80/TCP 8080 #HEALTCHECK --start-period=3s CMD wget -O - -q http://${IP:=0.0.0.0}:{PORT:-80}/ ENTRYPOINT [\"/bin/entrypoint.sh\"] #CMD指令采用列表方式，其所有内容都将成为ENTYRPOINT的参数 CMD [\"/usr/sbin/nginx\",\"-g\",\"daemon off;\"] ---------------------------- cat entrypoint.sh #!/bin/sh #注意，alpine只有sh没有bash，此处要用sh cat > /etc/nginx/conf/conf.d/www.conf 👆关键是：CMD和ENTYRPOINT以及-e之间的组合 ENTRYPOINT是docker run的时候执行，然后ENTRYPOINT的脚本内容是： 1、生成nginx的配置文件。 2、结尾exec \"$@\"，这个$@就是脚本entrypoint.sh \"usr/sbin/nginx -g daemon off \" 这些所有位置参数了。 3、所以就是1、2合起来就是生成配置文件，并且运行nginx 4、再结合docker run -e的传参，就实现了动态的设定配置文件里的server_name和listen以及root的功能 5、确实比我上面的要高端一点，更加然别人肃然起敬，哈哈哈，至少我被这种用法唬住了~ 好，后面讲上述重新整理成实现截图。 执行一个脚本，然后运行程序的常规玩法 ENTRYPOINT执行一个脚本(一个环境初始化的脚本)，然后脚本最后一行写上exec \"$@\"，CMD写一行命令，这种套路就是ENTYPOINT的脚本先执行，然后再将执行权交给CMD。 再次实现动态传参的web定制效果 1、首先是原材料 通过Dockerfile可知build好了以后/data/website/里存在一个index.html页面。 然后entrypint.sh的脚本又是默认使用的/apps/nginx/html/下的index.html页面，由于ENV在之前设置$DOC_ROOT为/data/website/所以server块的root其实就是指向了/data/website/的。除非后面docker run -e DOC_ROOT=/apps/nginx/html 指回去，所以这里是一个神经病一样的配置了，需要优化的，优化的措施就是在Dockerfile里删掉DOC_ROOT相关： 实验暂时不改作为测试对比 ENV DOC_ROOT='/data/website/' mkdir -p ${DOC_ROOT} && 还有不要写$HOSTNAME这个只会是容器的ID，改成 server_name ${HOST:-\"www.ming.org\"}; 还有修改脚本执行路径为/，如果不ADD到/usr/sbin这些PATH路径下 2、然后build 3、然后run一下 发现没有UP，进一步排查发现是$@里是空值，理由如下 如果在脚本中添加一个行echo可知，脚本确实执行了，只不过参数没有拿到。 我们改变ENTRYPOINT的书写方式 发现此时$@确实拿到了参数，这样就可以让exec执行了，报错的问题先不着急处理，先梳理以上两张图的结论 1、CMD和ENTRYPOINT的结合没有问题，不管是ENTRYPOITN 用不用列表形式，其合并的逻辑是一样的 只不过非列表有一个默认/bin/sh 然后统统加上-c攒成列表的行为👆，而列表形式就比较干净👇 所以$@位置参数 我怀疑前一个$@里是否一点东西都没有，👇验证果然是的 2、但是$@的传递必须使用script.sh arg1 arg2 arg3的方式，而不支持bash script.sh arg1 arg2 arg3这种 但是$@的传参现在看下来只能用列表的方式才能规范化下得到想要的效果。这是在容器build的场景中，而在宿主的SHELL下script.sh arg1 arg2 arg3 和bash script.sh arg1 arg2 arg3 的$@倒是一样的 这一点容器没有宿主的SHELL灵活👆。 好下面继续处理之前的分号报错 原因就是👆CMD里daemon off;不要单引号 最终重来一遍： 1、原材料修改为 2、build 3、run 这里有个细节，就是你ngin -g daemon off; 上图是容器里这么执行确实时ok的👆，但是手动执行其实是会报错的👇 看到没，这也是容器里里代码逻辑的第二点细节， ①第一个就是我上面将的$@，容器build的时候必须使用列表格式，本质上也就是不认sh -c './script.sh arg1 arg2 arg3'，这种脚本里面读不出来$@，必须是script.sh arg1 arg2 arg3 ②第二个就是CMD里列表单元其实不用加引号，虽然手动的时候需要引号，也就是nginx -g 'daemon off;'手动不加引号不行，但是CMD里加了引号才不行👇下图就是一开始的配置结果报错反而 然后发现有出错了 因为这一次我用的是官方镜像，而官方镜像的配置文件压根不在/apps/nginx/conf这个下面，我操作的都是这个目录，压根就是错误的 不相信exec -it进去可见 所以再次重新修改buid的原材料 在此之前探明人家nginx里的目录是否存在，已经server块是否有子配置文件 有一个default，根据之前所学，default.conf首字母为d很容器就会抢先，这个注意下，后面测试 1、原材料 进入到官方的nginx，run起来看看index.html在上图的default.conf里明确制定了，所以我们脚本也要修改 去掉nginx.conf主配置文件，将www.conf生成到/etc/nginx/conf/conf.d/下 可预判优先级抢不过default.conf。实验继续 2、run 3、进去curl测试下 实测就是两个子配置文件优先级方面，系统还是都会去看一遍的，port > server_name 这些都是所有子配置文件合并起来看的，如果大家都一样，就是一直到server_name都一样才会去说看排序第一个子配置文件，也就是字母排序第一个的default.conf文件了。 以下就是调整www.conf名称抢先default.conf的测试过程👇 了解了这些细节后，下面测试完整走一遍 1、原材料 2、build 3、run 先不带-e参数run一次 curl IP就是走的default.conf，域名就是走的www.conf 进去调整www.conf重命名为a.conf，IP也会走www.conf了👇 然后再带-e run一次 一般不会写死IP，这里就是实验测试而已👆 然后上图的web03容器run起来测试如下👇 补一个dns解析如下 所以实验ok，到此结束。 确实可以实现server_name和port以及index.html文件的自定义，看效果 1、原始材料，build的时候修改index.html 2、run的时候修改域名和端口，测试如下 再来一个测试手法：-H修改主机头 上图👆的grep -Ev没有去掉空行，可以优化为 sh -c 'cat /etc/.......' 就行了。 无需本地写host 看一个例子脚本里的东西：高级表达式 注意：表达式只是表达式，并不是变量赋值 这图就是说，不存在变量赋值的情况下，你不要瞎搞👆 echo $HOST,s是查看HOST变量的值， echo ${HOST:-\"www.mong.org\"} 是查看HOST变量的值，如果HOST变量没有值，这个表达式的结果就是www.mong.org。不是说HOST变量的结果， 这两行从头到尾都没有说HOST的变量存在赋值的情况哦！ 同理看下面的例子，一个意思 上图echo加个提示，否则看不清 纠错DOC_ROOT在Dockerfile里的EVN是赋值了的，所以有的，但是HOST确实是没有赋值的。 规范化 1、CMD就是最后一个命令挂前台的 2、ENTRYPOINT就是初始化的环境配置的 虽然你可以将CMD的命令合并到ENTRYPOINT的脚本里(比如将nginx -g \"daemon off;\"放到entrypoint.sh的最后一行，并注释exec \"$@\"，但是不会这么做，不清楚，属于大家都这么用的规范问题。 什么 mysql、nginx都是这么玩的👇 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:17:35 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/4.Dockerfile常见指令用法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/4.Dockerfile常见指令用法.html","title":"4.Dockerfile常见指令用法.md","keywords":"","body":"第4节 Dockerfile常见指令用法 ARG，变量 类似ENV，不过ENV是build阶段定义的变量，docker run阶段也同样可以使用 ARG只能在build阶段生效。 使用场景： 1、ENV就是build阶段定义的变量赋值，然后docker run阶段可以使用-e修改掉 2、ARG就是build阶段定义用的，docker run起来就没有这些变量了。 3、ARG可以写到FROM前面 ARG常用来定义版本号👇 ARG VERSION=3.20.0 FROM alpine-base:$VERSION LABEL maintainer=\"oneyearice \" ARG NGINX_VERSION=1.26.0 ADD nginx-$NGINX_VERSION.tar.gz /usr/local/src/ RUN cd /usr/local/src/nginx-$NGINX_VERSION && ./configure --prefix=/apps/nginx && make && make install && ln -s /apps/nginx/sbin/nginx /usr/bin && addgroup -g 2024 -S nginx && adduser -s /sbin/nologin -S -D -u 2024 -G nginx nginx && mkdir /apps/nginx/conf/conf.d COPY nginx.conf /apps/nginx/conf/nginx.conf ADD index.html /apps/nginx/html/index.html RUN chown -R nginx.nginx /apps/nginx/ #EXPOSE 80 443 #CMD nginx && tail -f /apps/nginx/logs/* CMD [\"nginx\",\"-g\",\"daemon off;\"] VOLUME，持久化 volume的目录可以自动创建的，无需事先创建 相当于同时：①实现数据持久化②也顺便创建了目录 build成功就说明VOLUME创建了目录了，因为entrypoint.sh里有一句echo $HOST > ${DOC_ROOT}index.html没报错，说明目录OK的。 目录exec进去可见 而且整个VOLUME创建的目录是持久化的，所以是独立于容器的目录的 这些LowerDir、MergerDir、UppereDir、WorkDir目录，容器删了，这些目录就没了 但是volume是独立的文件持久化的 测试 而且整个映射出来的宿主上的文件内容改了，容器里也同步的 容器里的所有文件都是在宿主机上存放的其实~ mysql的数据就很需要volume持久化一下 Dockerfile里的VOLUME遗憾 但是宿主机上的目录名称没有任何识别度，就是自动生成的这些volume在宿主上的目录是无法维护的，你都不知道谁是谁👇 容器如果删了，还可以docker inspect 查看谁是谁的谁，容器删了可就没法查了，只能编列cat里面的内容去一个个找了。 还有一个命令可以看volume的 总之Dockerfile里的VOLUME就是没法指定宿主的映射目录的，只能这样，也算是一个缺点。怎么解决，docker run -v可以做宿主和容器里的目录映射，docker compose的yml也可以实现。 也可以使用VOLUME挂载多个容器里的目录出来 EXPOSE仅仅是一个申明 申明的是-P要映射的端口是啥而已。至于是不是容器里监听的，也不一定，没有直接关系。 1、暴露端口的时候，可以用docker run -p 8080:80来 将容器里的80暴露到宿主的8080。 2、也可以用Dockerfile里的EXPOSE来说明容器里暴露的是80或443，但是宿主用哪个端口来对接就是在docker run -P来随机指定了，如果不像随机，那么还是用-p再次指定，此时Dockerfile里的EXPOSE就没意义了。 3、EXPOSE和VOLUME一个德行，都是仅仅申明容器里要暴露的端口和目录。至于暴露成宿主的哪个端口和路径，Dockerfile里没有解决，都是通过docker run -p和-v来解决，然而既然用了-p和-v，那么容器里的端口和目录都是直接写了，无需Dockerfile重复写了。 # 但是EXPOSE和VOLUME的区别是EXPOSE不管容器里是否监听某个端口，只是申明供-P暴露出来；而VOLME是容器里没有这个目录就给你自动创建出来(而且是mkdir -p 的方式来创建多层目录的)，然后暴露成宿主的目录。 4、然后EXPOSE就是配合docker -P，大P来使用的，告诉大P暴露容器里的什么端口为宿主的随机端口。 5、凡是EXPOSE写的端口不是容器里监听的端口的，都是坏人~，哪家好人干这种事啊，对不对，不写也是不对的，总不能让别人去猜你nginx或其他app监听的啥吧，不猜就去找脚本，或者你要是默认编译的基本就是80了。 以下截图就看EXPOSE就行了，其他前面的内容 443只是暴露出来，并不代表容器里有监听443，而实际情况是容器里只监听了80，443没有开，暴露是80和443都暴露的，所以👇 WORKDIR，切换工作目录 类似cd，切换文件夹的，确实切掉了， 但是如果在Dockerfile里 RUN cd xxx/xx/xx 也仅仅是在这一条RUN里切换了路径，下一行又回到容器里的/根了。实验测试下👇 1、原材料 2、build 2、run看效果👇 所以RUN cd和WORKDIR是不一样的，前置式只在RUN cd的哪一行临时切过去，后面又回到了默认的/路径，而WORKDIR就是后面都切过去了。 至于RUN一行里用&& 还是; 都行，一行都是临时切过去了。 ONBUILD， 父镜像build指定的CMD在当时不执行，在子镜像build的时候才执行。 举例：rm -rf /，就是说父镜像ONBUILD制定了rm -rf /，一旦子镜像继承了就把子镜像的根删掉。 # 你要继承我，我就把你删了。。。 这里只是一个没人这个干的例子，呵呵。 也许你不想让别人用的镜像作为父镜像，就可以这么做，ONBUILD RUN rm -rf /* 呵呵。 那么问题来了： WORKDIR /bin ONBUILD touch helo.log 这个touch在子镜像build的时候创建的，那么是在子镜像的/下还是下/bin下呢？是子镜像的/bin下的， 因为子镜像FROM xxx的时候已经继承了WORKDIR的默认工作目录已经是/bin了，然后ONBUILD又是子镜像才执行显然是在FROM以后的。 1、首先父镜像里是没有touch helo.log文件的👇 顺便看一个视频中的典型错误案例👇 图中错误就是，entrypoint.sh不会被执行，因为找不到路径， 理由很简单：你看下图是不是一样 解释给你听哦：①你要让光秃秃的xxx.sh，这种不带绝对/相对路径的脚本直接执行，那就要让xxx.sh放到$PATH变量里的路径里去；②结果你上面COPY是放到/根③你WORDIR /切到/根，人家也不会去/当前目录根里找这个xxx.sh的。哈哈，不要学到了K8S还在这里闹小学一年级的笑话~还说什么WORKDIR没切过去，这是WORKDIR的事嘛~，基础不牢地动山摇，人心不稳王座倾塌。 下面就是Dockerfile制作一个父镜像，然后再Dockerfile.son做一个子镜像，其中父镜像里的是ONBUILD touch，哦错了，是ONBUILD RUN touch才对👇 由于父镜像里已经WORKDIR /bin了，所以父镜像docker run起来进去就是/bin下的； 同样子镜像是继承的父镜像，所以子镜像的工作目录就是/bin的，所以再执行ONBUILD RUN touch helo.log就生成了文件了👇 判断下ONBUILD的 echo的动作是在WORKDIR前还是后 判读是在后，因为WORKDIR是父镜像build的时候就会执行，一旦执行，子镜像继承了就是工作目录默认就是/bin了，然后ONBUILD echo是CMD还是ENTRYPOINT啊，我的意思是在build阶段还是在docker run阶段生效啊？ 就算ONBUILD是build生效，那也是子镜像执行，而子镜像FROM xxx继承父镜像这个FROM就是父镜像的WORKDIR已经完成了，所以ONBUILD是肯定是在WORKDIR的目录里的了。 ​ 然后ONBUILD RUN或ONBUILD ENV、ONBUILD ARG、ONBUILD ENTRYPOINT 再看 buid一下，注意build的时候，镜像不要用之前的镜像名称，会有问题，待会单独研究。 验证结论的时候到了， 验证如下👇 看案例，就是父镜像没再次build导致子镜像还是用的之前的父镜像的Dockerfile👇，总之看CACHE比较明朗就： 然后为什么会有之前的影响，继续研究 还是还原故障 还原个屁，是自己搞错了，父镜像没有build覆盖原来的tag导致的 这里图中打叉的地方-t写错了，本来是覆盖nginx_env_web:1.5这个父镜像的，结果写成了_son了，所以原来的父镜像没变，也就是说ONBUILD echo '123321' > helo.log本来就在，所以子镜像继承了，人家CACHE既然看到了就说明利用了之前的分层了，就说明父镜像没有变了。 USER，指定后续指令的执行者 HEALTHCHECK,健康检查 注意👆上图IP:=哪里写错了，PORT也写错了，复习变量高级用法去，nnd。 starting就是正在检查中 过一会就发现检查失败了 失败的原因是HEALTHCHECK里的CMD里的变量高级用法写错了， 修改后OK👇 上图👆就是默认的时间是30s timeout，30s检查ok了就判定为healthy。如果30s检查不OK，就retries 3次也就是90s才会判定unhealthy。 然后研究下curl 和 wget的静默用法👇 FROM nginx # 添加健康检查脚本 COPY healthcheck.sh /usr/bin/healthcheck.sh RUN chmod +x /usr/bin/healthcheck.sh # 定义健康检查命令 HEALTHCHECK --interval=30s --timeout=3s \\ CMD /usr/bin/healthcheck.sh # 这里为啥不能直接写curl和wget，肯定可以的啊，脚本结果还导致少了一个人家健康检查无法执行的返回值2 if curl -I http://localhost:80/health >/dev/null 2>&1; then exit 0 else exit 1 fi #!/bin/sh if wget -q --spider http://localhost:80/health; then exit 0 else exit 1 fi 所以这样写就行了 #HEALTHCHECK --start-period=3s CMD wget -q --spider http://${IP:-0.0.0.0}:${PORT:-80}/ HEALTHCHECK --start-period=3s CMD curl -I http://${IP:-0.0.0.0}:${PORT:-80}/ &> /dev/null #这样做的好处，就是docker logs里就没有多余的curl wget信息了，不过这些系也是timeout到了才会出现，比如默认30s就是要到最后一秒才会出现curl或wget日志,不太能理解，因为--start-perod=3s默认就是3秒就执行了啊，我以为日志就容器起来3s就curl就会有日志了，结果docker logs查看发现要等timeout30秒到了才会有日志出来。 HEALTHCHECK 后面的CMD只是HEALTHCHECK里的一个固定参数，不要理解成Dockerfile 的CMD哦 如果健康检查失败还需要进一步去处理故障的，GPT给了一些思路，不过有待后面学到docker-compose的时候验证： 工作案例-抓某个软件的包 1、背景 公司打不开discord的客户端软件 软件还不是浏览器，无法通过F12直接看到那些URL打不开 2、需求 需要定位某个软件访问了那些域名，然后进一步写hosts，然后放行 3、classwire只能看到已经正常运行了的软件，对于打不开得，就是比如discord还在update，以及update好了还在starting的状态下是看不到 discord这个app的流量统计的。至少我的免费版看不到 4、落地方案，如下 参考两个链接 一个cmd里findstr如何写or 👇 https://blog.csdn.net/zhigang0529/article/details/86240577 一个是如何抓包抓软件👇 https://blog.csdn.net/weixin_51309915/article/details/122382555 方法1：有白名单限制的情况下进行 1、打开wireshark软件开始抓包 2、打开软件discord，让其update转圈圈，后面update好了starting一样的处理思路 这个已经没法复现了，我已经解决了，所以仅文字描述了 3、打开任务管理器 找到pid， 4、打开cmd 手动不断回车，配合discord update界面出现的时间，下图findstr 后面空格是或的关系，表示同时抓这些PID。 已经ESTABLISHED基本是OK的，不用管 找到SYN SENT 5、wireshark过滤该ip 提取第一个该ip出现的时间 然后取消过滤，再全部信息里，找到时间点的这条抓包，往上看到DNS解析对应的域名👇 这就知道了该软件目前需要访问的域名了，重复多次，找到需要访问的所有域名 然后①域名解析本地dns固定成一个IP②路由指向vpn③vpn节点放行该ip。这就是海外白名单管理的思路。 以上就是app如何抓包的思路，要抓到域名嘛 除此之外，还有一个方法 方法2：取消白名单限制去做统计 ★放开本地的海外白名单，针对自己的IP，然后打开GlassWire软件，此时由于海外全通，所以discord可以打开，包括update，start都行， 这样等软件正常运行了，GlassWire就可以正常显示该软件的域名访问了。 由于已经升级和能够打开了，所以还需要手动升级来让classwire看到流量 操作①需要退出软件②打开③手动升级 发现不全少了一个dl.discordapp.net，这个域名。可见上面的手动方法还是要掌握的。至少可以先用方法2，然后再用方法1查漏补缺。 说明 这种情况就是PID3118一开始不断和130.221.15.150去TCP CONN，但是dns解析写道104.18.52.172后，软件方面也延迟了一小会然后IP解析更正过来了以后，就可以了。 上图同样是通过wireshark过滤IP，找到时间，然后在全部信息里找到时间，看上面的A记录里的域名。 一开始是SYN还是和130.211.15.150建立，后面我解析固定后，discord这个APP的连接请求也就自动更正了 然后通过上面的实验发现 1、wireshark抓了很多很多很的包后，截图软件存在差距 2、微信截图依然顺序好用，但是PixPin截图异迟钝，慢的很咯，关闭wireshark后PinxPin截图反映速度恢复正常。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:17:50 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/5.Docker镜像制作优化和多阶段构建.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/5.Docker镜像制作优化和多阶段构建.html","title":"5.Docker镜像制作优化和多阶段构建.md","keywords":"","body":"第5节 Docker镜像制作优化和多阶段构建 镜像精简，便于迁移，因为后面都是K8S上跑容器，这些容器是可能在不同的宿主机之间迁移的。 写Dockerfile的时候应该尽可能合并层，就是合并行。 但是很多命令都看不全，可以加个选项查看,但是效果依然不行 优化1：变化的写在后面 Dockerfile里有变化的内容就是会频繁修改的，写到文件内容的后面 说明： 1、如果在Dockerfile内容的前面新增一个指令，也就是之前没有build过的分层指令，那么下面所有的分层都会重新build，不会利用CACHE的 重复build，也不是说重复build 然后这些CACHED就利用不起来了，感觉下面所有的分层就统统重做了，也不会说之前有过就能利用的。 这里面很难讲，但是也是逻辑清晰的，就是分几种情况 1、在Dockerfile的前面，新增一条RUN echo 123，那么下面什么ADD WORKDIR COPY这些就要重新执行，利用不到之前的CACHED了。 2、在Dockerfile的前面，删除RUN echo 123，那么下面的ADD WORKDIR COPY可以利用CACHED 3、在Dockerfile的前面，重新写上RUN echo 123，那么下面的ADD WORKID COPY可以利用CACHED 4、在Dockerfile的前面，将RUN echo 123改成RUN echo 122，那么下面的ADD WORKDIR COPY重新执行，利用不了CACHED。 所i有发生变化的内容，要写到Dockerfile的后面，否则就会导致build的时候很多之前的缓存无法利用。 优化2：COPY太多和遗漏 COPY * 和COPY .的区别 1、隐藏文件不管是*还是.都会复制进去 2、COPY * 的时候，文件夹，外层会剥离掉， # 无法理解的骚bug~ ​ 如果docker build目录里只有一个空dir1，那么就不会被COPY * / 复制进去； ​ 如果是dir1/f1，那么COPY * /就只会复制f1进去； ​ 如果dir1/f1,dir1/dir2/f2，那么就是COPY /就会复制f1和dir2/f2到容器里的/里，说白了就是进去的，而且是包含隐藏文件夹的。 针对COPY复制的文件太多了的问题 需要引入.dockerignore文件 .dockerignore忽略文件 .dockerignore里写的就是忽略的文件名，a.conf就是忽略，而!b.conf就是排除掉 不忽略的意思，也就是脱裤子放屁，其实就是强调一下咯。所以a.conf和b.conf就会被复制进去。 FROM补充 别名AS的作用，一般FROM写好就用一次就没了，就是build的时候，用别名无非是以后方便使用。 所以AS别名一定是有重复使用的场景，这个就是多阶段构建。 案例：多阶段构建 vim hello.go package main import \"fmt\" func main() { fmt.Println(\"hello,world\") } vim build.sh #!/bin/sh docker build -t go-hello:$1 . --------- vim Dockerfile FROM golang:1.xx-alpine COPY hello.go /opt WORKDIR /opt RUN go build hello.go CMD \"./hello\" -------------- bash build.sh v1.0 docker run --name hello go-hello:v1.0 现在宿主机上编译一遍看看 1、准备go语言的代码，呵 2、编译的工具有： ​ go是golang ​ c是gcc ​ java是javac 我是RockyLinux，就yum -y install golang就行了 编译后运行 然后制作image 找一个golang基础镜像，去hub.docker.com找到：golang:1.22.3 写Dockerfile来创建go的项目 1、原材料准备 👆注意：上图CMD没有用列表形式来写，实际上就是有一个默认的/bin/sh -c 在里面，而./hello此时是go语言不是shell，所以应该是执行不了的。需要改写为CMD [\"./hello\"]这样就没有/bin/sh的干扰了。不过后面测试发现/bin/sh -c 也行的... 2、build run 发现用的/bin/sh -c \"./hello\" 实际上是错误的cli，但是这种方式也可以，有点奇怪，为什么说它是错的，因为①理论上go就不能用/bin/sh去执行，②实际上下图所测就是不行 错误1 报错2 正确的 错误 错误 好像CMD里只能写一条CLI，多了就要用/bin/sh -c 'cli1;cli2;cl3'这种方式包装成一个来执行了。 ok了 ok了 ok了 多阶段构建正式开始-适合静态编译go 不适合java和python咯。 一个hello的go代码也就是1.9M，但是镜像就822MB大小，太夸张了。 所以如何进一步缩减镜像大小呢 FROM golang:1.22-alpine as builder COPY hello.go /opt/ WORKDIR /opt/ RUN go build hello.go FROM alpine:3.19.1 COPY --from=builder /opt/hello /opt/hello CMD [\"/opt/hello\"] 首先go是静态编译的，编译完了以后，就不依赖go编译环境了。就是RUN go build hello.go的go环境其实用完就不需要了。 所以就可以把基础镜像换一个更小的alpine或者busybox，加上之前编译的hello就可以运行了。 build一下 此时image就从851M缩小到9.27MB了~~ run一下ok 同样改成busybox来做二次构建 然后ok了 busybox的镜像会比alpine再少3MB左右，其实说白了就是go是静态编译的，哪个image小就用哪个就行了。 所以go的优势还是比较明显的 1、并发的优势，写出来就是并发的； 2、docker构建的优势，可以二次构建改成小的基础镜像，大大节省空间。 3、java、python、c都不行，这些都有大量的依赖库共享的动态的。 然后将上面的alpine或者busybox进一步优化，用scratch这个祖先镜像就是空镜像，因为go是静态编译，所有东西自带了 一个hello world的go编译后 用scrapt打包后也就是1.89MB，相当给力👇 补充docker images 查看的SIZE单位是MB，如何看到KB的精确值呢，用inspect 为什么go程序能够在scratch空镜像上跑，其原因有2：、 1、Go 是静态编译的：Go 编译器默认会将所有依赖项静态链接到可执行文件中，这意味着生成的二进制文件包含了运行所需的所有库。这样做的好处是可以确保可执行文件在任何环境中都能运行，而不需要依赖系统上预安装的库。 2、文件系统方面： 2.1、内核提供的文件系统支持： ​ 当容器启动时，它与宿主机的 Linux 内核共享同一个内核实例。Linux 内核本身提供了文件系统的支持，包括对文件操作、文件描述符管理等基本功能的支持。 ​ 容器的文件系统视图是通过 Linux 内核的 chroot 和 mount 等机制实现的，使得每个容器看起来像是有自己独立的文件系统。 2.2、容器镜像层： ​ 虽然 scratch 镜像是一个空镜像，但容器在运行时仍然有一个最基本的文件系统布局。这些文件系统布局由 Docker 容器运行时环境提供。 ​ 当你使用 COPY 指令将文件从构建阶段复制到 scratch 镜像时，这些文件被放置在容器的文件系统中，容器运行时将这些文件系统布局合并起来，使它们在容器内部可见。 2.3、构建阶段包含必要的文件： ​ 在你的 Dockerfile 中，你使用了 COPY --from=builder /opt/hello /opt/hello 将编译好的 Go 应用程序从构建阶段复制到最终的 scratch 镜像中。虽然 scratch 镜像本身是空的，但你复制进去的文件会成为容器文件系统的一部分。 ​ 因为 Go 应用程序是静态编译的，所以它不需要依赖额外的共享库或运行时环境，这使得它可以在 scratch 镜像中运行。 2.4、简而言之，scratch 镜像通过 Docker 提供的基础文件系统布局和内核支持，结合在构建阶段复制进去的必要文件，能够满足应用程序的文件系统需求。以下是简要的示意图： images如何查看当初FROM哪里的 docker inspect查看就行，不过要递归到最初的那个 docker history也行，不过同样从要找到最初的那个 scratch打出来的镜像ls没有如何查看文件 因为连ls这个命令都没有，如何查看呢 现在看不到了👆，以前可以看 然后一些案例可以补充到这里 1、基于Alpine的微服务Apollo配置中心 https://github.com/apolloconfig/apollo?tab=readme-ov-file git clone项目拉下来 编写Dockerfile FROM openjdk:8-jre-alpine3.9 RUN \\ echo \"http://mirrors.aliyun.com/alpine/v3.8/main\" > /etc/apk/repositories && \\ echo \"http://mirrors.aliyun.com/alpine/v3.8/community\" >> /etc/apk/repositories && \\ apk update upgrade && \\ apk add --no-cache procps curl bash tzdata && \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \\ echo \"Asia/Shanghai\" > /etc/timezone && \\ mkdir -p /apollo-config-server ADD apollo/ /apollo-config-server/ # apollo/.也行，apollo/*不行 ENV APOLLO_CONFIG_SERVICE_NAME=\"service-apollo-config-server.sre\" EXPOSE 8080 CMD [\"/apollo-config-server/scripts/build.sh\"] 过程排错：1 处理： 是宿主的句柄小于容器里的句柄了 这样就能跑下去了，时间比较长，明天继续 把宿主的ulimit的值改一下就继续run就行了，不过还得修改Dockerfile 安装openjdk8 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:18:02 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/6.Docker数据持久化和数据卷.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/6.Docker数据持久化和数据卷.html","title":"6.Docker数据持久化和数据卷.md","keywords":"","body":"第6节 Docker数据持久化和数据卷 之前VOLUME也可以实现数据持久化，就是容器删掉，数据还在宿主上放着，就是有点缺点，太乱了，存放没法自定义，或者要-v 命令行选项的方式来指定。 再来看下镜像和容器的分层数据存放 image的 容器的 首先容器run起来就是复制一份image 因为这个容器停掉了，所以数据释放掉了，running起来，才有merge才有复制image的那一份。 挂一个前台让他running 再看分层数据 这次就看到merged了👆 然后我将image和run起来的容器的分层目录 放一起看看 上图是容器里的meger复制了一份image里的diff，其实inspect容器可见LowerDir分层里直接写了image的diff分层， 容器生成的数据是放在diff下，比如日志、以及其他容器中生成的新的数据，这个diff就是可读可写层。 看下日志，应该就在UpperDir也就是diff里。不过我上面是tail -f /dev/null，没有日志输出，所以重新run一个有std的 然后去分层diff里看 但是显然是没有的，因为，没有落文件 落一个文件 现在diff里是空的 然后生成一个新的文件 然后UpperDir里就有了，于此同时merge作为合并层也就看得到了 这是容器起来后生成一个新的文件，如果是容器build的时候生成文件会出现在哪个目录里呢? 这里做了日志文件和生成了test文件，不过是build的时候就已经生成test了，看看 可见build阶段的文件是不会出现在UpperDir里的。而是出现在LowerDir里的diff里的 分层目录理解：结论来了 1、diff就是容器和镜像相比 diff出来的，就是多出来的 2、而UpperDir的diff就是容器的可写层 3、而LowerDIr的diff就是基于某个FROM基础镜像创建新的镜像的时候的底层。是可写的，因为不管是test还是access.log都是可以写的。 其实是不可写的，就是build的时候打进去的，后面可写都是UpperDir里的diff才是，LowerDirr的diff只是 区别于 FROM基础镜像的diff，同样也是只读层。 3、LowerDir里的每个路径之间用冒号分隔，代表一层层build的时候的封装。 4、merged就是联合文件整体的呈现。 下面开始学习持久化保存 1、session这些就是有状态的东西，没必要持久化。 2、mysql数据需要持久化，需要持久化。 持久化的的方法就是： 1、容器删除，/var/lib/doker/overlay2/xxxxxxxxx..xxx这个对应的文件夹就自动没了 2、对于重要的数据，单独存放到宿主的其他路径就行了。 具体的技术： 1、数据卷 2、数据集容器就是数据卷的变种，不常用 使用方法 docker run 命令的以下格式可以实现数据卷 -v, --volume=[host-src:]container-dest[:] ro 从容器内对此数据卷是只读，不写此项默认为可读可写 rw 从容器内对此数据卷可读可写，此为默认值 host-src 宿主机目录如果不存在，会自动创建 不管什么方法，肯定都不在/var/lib/docker/overlay2/下了，不会随容器删除而删除 方法1：手动指定宿主目录 #指定宿主机目录或文件格式： —v :[:ro] # 将宿主机目录挂载容器目录，两个目录都可以自动创建 方法2：卷ID # 匿名卷，只指定容器内路径，没有指定宿主机路径信息，宿主机自动生成/var/lib/docker/volumes//_data目录，并挂载至容器指定路径 -v # 示例： docker run --name nginx -v /etc/nginx nginx 方法3：卷名比卷ID识别度好，最通用的方式。 #命名卷将固定的存放在/var/lib/docker/volumes//_data -v : # 可以通过以下命令事先创建，如果没有事先创建卷名，docker run时也会自动创建。 # 里面外面都能自动创建文件夹，但是文件不行！ #示例： docker volume create vol1 # 也可以事先不创建，就是这句可以不写的。 docker run -d -p 80:80 --name -v vol1:/usr/share/nginx/html nginx docker rm 的-v选项可以删除容器的时候，同时删除相关联的匿名卷，只能是匿名卷 这种删除就会删掉①docker run -v 和Dockerfile里的VOLUME都是匿名，都会被删掉。 -v,--volumes Remove the volumes associated with the container 这是之前Dockerfile里的VOLUME也有类似的效果，好像就是方法2的效果一样。 这种就是随机字符串的目录名，没有可读性好的名字，所以一般就称之为匿名卷。没有名字 实验 把之前nginx的容器的Dockerfile来做挂载VOLUME实验 Dockerfile.son本次实验不要太关注，就是测试ONBUILD 指令的。 方法2 先跑一个不用方法3的，因为Dockerfile理由指定VOLUME所以也就是方法2的效果 先清一下none的镜像docker images -f \"Dangling=true\" 不run一下是没有VOLUME卷的是吧...应该是的 run起来看看 然后重点关注VOLUME 因为HOST变量没有值，所以文件也是空的👇 然后在宿主机上往里面写点东西玩玩哈哈 这不就有了吗，哈哈，其实应该去entrypoint.sh脚本里，给HOST赋值才对哦~~ 顺便也瞟一眼健康检查的log 方法1 ​ 先看下方法2的匿名卷(也就是VOLUME名称是随机一长串字符串)的效果 我想通过runlike看到VOLUME的值，看来build阶段的VOLUME，run阶段是看不到的，runlike就是看docker run的选项的。 ​ 只能通过inspect去看咯 可见这个VOLUME名称是不友好的，改之， docker run -d --name web2-run -P -v /dirtest/html:/dirtest/html -e DO C_ROOT=/dirtest/html/ -e HOST=ttttttt web1 👇图中-e DOC_ROOT 和-e HOST是传参，跳转页面转发路径，以及HOST这里赋值修复脚本里没给HOST赋值的情况。 上图👆curl的时候HOST要注意，已经被你改成tttttt了。 容器里面也是自动创建了这个目录 好了然后这个VOLUME也是自定义的就比较好维护了 方法3才是最棒的 docker run -d --name web3-run -P -v vol1:/dirtest/html -e DOC_ROOT=/dirtest/html/ -e HOST=www.test.tk web1 不要自己定义宿主的挂载目录，就用统一的/var/lib/docker/volumes/起个名字/_data/就行。 然后要注意就是Dockerfile里的VOLUME你也没删，docker run 的时候又-v vol1:/dirtest/html 所以就是创建了两个挂载卷了👆。 插播docker起不来排错思路 docker build ok，run的时候没有达到预期的排错 1、正向检查，Dockerfile是否ok 2、docker ps -a看看里面的信息，特别是CMD，不过看不全，没事用 看的全 3、docker run去掉-d，不挂后台，run着看看 4、docker logs看看现象 5、docker exec 进去手动其服务或则CMD的命令，起不来无非是CMD那条问题，起来还有别的问题才是进一步的配置设置之类的，用ENTRYPOINT和CMD的组合来解释，通常是初始化脚本没初始化好等等。或者RUN里的的apk add漏安装包啦之类的。 下图说明如果挂载的宿主卷里有文件，然后Dockerfile里有COPY了同名这个文件进去，哪一个优先，宿主的优先会覆盖掉的，如果vol1里是空的，就不会覆盖同名的文件了👇 -v只能自动删除 匿名卷 ①docker run -v dir xxx 这种自动创建的匿名卷 ②Dockerfile里的VOLUME配置后run出来的匿名卷 查看卷 单独创建卷 方法1指定宿主机的目录就不是能在docker volume ls里看的到的了 看不到就不叫数据卷，方法1直接挂宿主机目录，这个目录就不是数据卷。 方法2，方法3，一个是匿名卷，一个是命名卷，都是有名字的数据卷。 只读，指的是容器里只读 容器里是只读 宿主里是可写的 此时容器里面就看到了 工作案例分享 linux如何拨v2ray 1、搭建桥接client，server利旧 1、安装docker https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/ 2、pull镜像 https://hub.docker.com/r/v2fly/v2fly-core/tags 3、run容器 https://github.com/v2fly/docker 修改配置文件为👇根据你server那边的进行配置就行。 不行，进去看， # 其实就是修改配置文件后没有重启容器 换下国内源 https://mirrors.tuna.tsinghua.edu.cn/help/alpine/ 安装curl测试，在容器里 然后正式实施的时候加上--restart always和安装runlike docker run -d --name v2ray --restart always -v vol1:/etc/v2ray -p 10086:1080 v2fly/v2fly-core run -c /etc/v2ray/config.json yum -y install python3-pip pip install runlike 2、搭建nginx反代，不需要搞错了 将sockt反指到本地监听端口 别搭了了，需求搞错了，目前正确的需求是将所有流入进来的流量，转发到本地的socks5:10086上去。 而不是再配置一个nginx去暴露一个端口 2、构建本地流量转发 1、使用ssh试试 不行，ssh也是要指定端口的 2、iptables 可行 3、redsocks 可行 明天继续 https://tttang.com/archive/1878/ https://www.cnblogs.com/zhenyuyaodidiao/p/5494569.html 而且要构建三层代理的转发，但是通过docker exec进去发现监听的是tcp/udp，也就是说只支持4层代理，就是socks代理。不是我需要的东西，该实验暂停~ 还是用softehter就行~ 然后如何设置全局代理，配置变量就行了，不管是本地还是给鄙人用 1、linux 2、windows 然后 浏览器可以代理，cmd不行，也就是说明了 socks不管是5还是4也无法代理cmd，全软件代理还得依靠APP本身指向socks。 cmd 里没看到proxy变量设置，所有没有走代理，手动配置一个，就像linux一样 cmd里还不用用socks变量，的用linux一样的socks5 概念梳理： 全协议：http的代理、socks4代理tcp、5就是tcp/udp都支持，这种需要APP自己走socks套接字才行的。默认浏览器就会走socks，但是cmd这些就需要手动配置代理变量为socks5，其他APPS同理。这是两个概念，全协议是隧道本身的属性，APPS走隧道时APPS自己的配置。 有一种就是无需APPS配置，直接所有软件 浏览器、cmd、等等 统统走隧道，就是全软件代理。这就要虚拟网卡将默认网关全部打到隧道里去： 全软件：最新的v2ray tun模式是由虚拟网卡的，电脑上所有软件的流量就虚拟网卡就是全软件代理了。包括cmd都代理了。 # 这种就是要虚拟网卡的。 全局：沟通要统一，我习惯全局就是这里，是全部流量都走代理，就是不分流的意思。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:09:51 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/7.Docker数据持久化和数据卷容器.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/7.Docker数据持久化和数据卷容器.html","title":"7.Docker数据持久化和数据卷容器.md","keywords":"","body":"第7节 Docker数据持久化和数据卷容器 用持久化做一下wordpress docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d --restart=always -v /data/mysql:/var/lib/mysql mariadb:11.3.2 docker run -d -p 8080:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php8.2-apache 也就是生成wp-config.php文件 👆上图localhost是错误的，前文就说过了，要写宿主机的IP。 总之，wordpress容器化就是 ①wordpress服务是一个容器，你这个配置界面是wordpress连接mysql的配置界面，其实是wordpress里面配置的localhost，所以是容器里的，而不是宿主的。要注意的。 ②mysql，又是另一个容器了 上面的数据配置提交后，实际上就是生成了wp_config.php文件 这个配置完后，mysq里的相关表信息就来了 然后登入进去后，新建文章，上传图片 就会在wordpress的目录下看到 一张图片会 根据 手机、平板、电脑，分辨率，来生成多张，将来好匹配不同屏幕。 然后发布文章 然后删除上面的两个容器，由于做了数据持久化，所以文章得以保存 删除容器docker rm -f xxxx， 如果你用docker -rm -v -f xxx，就要小心了，会不会-v一下子也就把持久化的数据也给删了，但是我们的这个实验不会，因为，-v只能删除匿名卷。 所以胆子不妨大一点~，追女孩子的时候脸皮不妨厚一点~，啥~ 跑题了~~~ 删除后，由于持久化的数据还在，所以docker run起来就还能看得见 然后重新run一下就恢复 检查页面已经恢复下 数据卷容器 就是用容器来实现数据持久化 再跑一个wordpress2 发现会自动将9090跳转到8080去 因为数据库没有发开 分开来一遍 这里又会涉及初始化mysql的配置 默认就是3306，也不知道我强行写成3406行不行 可以！wordpress初始化mysql可以带端口👆 你看这就一个宿主两个wordpress了 这是英文的还是👆 这是👇之前中文的 再来一个例子，两个容器共享数据的效果 用之前的自定义的image来弄 那么问题来了，过了这么久，我应该用哪个images去run呢，或者说我的这个image是怎么build出来的，用的哪个Dockerfile呢，对吧，简单，上次的研究就没有白费，👇 寻找熟悉的镜像的准备 ①inspect看看From的谁 ②hisotry看看build的过程 开始寻找 可见这几个镜像又276MB的还有48.3MB，看看为啥大小差这么多， 再看看 就这看到了为什么0.1版大在哪里👆然后用--no-truc展开看详细CLI 然后找找哪一个Dockerfile里又这个命令的👆 发现不再这个层级目录下 通过当前的Dockerfile的就会看到FROM alpine_3.19_self:v1.0 这就找到了👇 那么276MB的原因找到了，48.3MB为什么这么小呢 找到了 最后实验的时候，FROM的镜像改用了官方镜像了 也许可以这么验证，验证什么呢，就是 这个FROM，build出来了web001_entrypoint0.3 而web001_entrypoint:0.1是自定义的， run 进去看apk list 就知道了，比如如果是自定义的，那么apk list里是由iotop这个包的 好了，目前就找到这么个方法来定位自己大量的image不知道从哪个Dockerfile来的， 思路就是：找到Dockerfile里的特点，然后基于这个特点，比如这个Dockerfile里安装了某个与众不同的软件，就将对比的images分别run起来去看有没有，有就是FROM的这个，没有就是别的或者直接是官方的。 所以还是docker-compose好啊，哎，赶紧学到那里去看看有没有更好的解决方案~~ 下面继续实验，基于定位出来的这个镜像 然后这个镜像的Dockerfile和entrypoint看看 上图，但其实你会发现，TMD，这个其实web001_entrypoint:0.3当初build那会儿，entrypoint脚本里的 echo \"$HOST\" > ${DOC_ROOT}index.html 这一行是打开的，没有被注释掉的 实验思路： 两个容器共享一个持久化 有一个问题 命名卷和匿名卷，都是可以docker volume ls查到的，此时尽管docker ps -a也没了，目录也删掉了，但是docker volume ls可见，就会导致run的时候-v 指定命名卷就会报错 所以要用docker volume rm清理 结果不消息清掉了匿名卷，哈哈哈 重启一下就好了 下次记得删除volume不要用rm，要用docker volume rm 然后宿主机的路径，就是手动创建的目录，不算逻辑卷,，会自动创建，docker volume ls是看不到的 只要docker volume ls 看不到，就可以自动创建，并run的时候挂载过去 只要docker volume ls 看到 且 文件确实存在，就可以重复run 的时候挂载过去了 继续做2个容器挂载一个目录吧 第一个容器ok 第二个 也OK，而且是共用的一个命名卷mydata 数据卷容器的产生背景 问题来了 如果10个容器都挂同样的目录，比如👇 docker1 -v mydata1 -v mydata2 -v mydata3 docker2 -v mydata1 -v mydata2 -v mydata3 。。。 。。。 docker10 -v mydata1 -v mydata2 -v mydata3 就要写10遍 -v mydata1 -v mydata2 -v mydata3，配置太繁琐，于是就有了一下的解决方案数据卷容器 容器0正常使用-v -v -v去挂载，其他容器1 2 3 4 .. 参考容器0直接挂过去--就是说你容器0的持久化方案就是我们其他容器的持久化方案 这里所谓的容器0就是图中的Data container。 1、先创建一个服务器容器，这个服务器容器就正常-v -v -v 挂载好目录，不管是逻辑卷(①匿名卷②命名卷)，还是直接使用宿主的文件夹。 2、其他容器就好比client端，都是服务器容器的挂载方案进行复制挂载就行了。 上面两个图其实有一个理解误区，其实就是画图的不专业，是什么呢，就是箭头不是数据卷挂载的动作流，而是-v复制参考的动作流，你这个图搞的好像我删掉容器A或者前一张图的Data Container就会导致容器B C ；Container 1 2 3都断开连接了一样，其实不存在连接，不存在绕行，这个容器B C 绕行容器A的箭头只是复制了-v xx -v xxx -v xxx的配置而已，一旦复制过了，那么容器B C都是直接挂载访问到宿主机的。下面有截图实验证的。 实际配置很简单 docker run --volumes-from Mount volumes from the specified container(s) 实际配置开始-多个容器共享某部分数据的方案 1、配置数据卷容器，server啦，因为只是提供别的容器，来复制挂载目录的。所以就只需要写好-v就信了； docker run -d -v mydata:/data/website --name volume-server -e HOST='www.dalao.tk' web001_entrypoint:0.3 这里留了一个-e HOST=xxx来测试是否可以复制，和-v一样被其他容器复制。 而且容器都不要UP的，也不要用比较大的镜像去run，直接将上面的优化成 docker run -d -v mydata:/data/website --name volume-server -e HOST='www.dalao.tk' busybox 2、配置使用\"数据卷容器\"的容器，clients啦 docker run -d --volumes-from volume-server -p 81:80 --name web01 web001_entrypoint:0.3 docker run -d --volumes-from volume-server -p 82:80 --name web02 -e HOST=\"web02\" web001_entrypoint:0.3 docker run -d --volumes-from volume-server -p 83:80 --name web03 -e HOST=\"web03\" web001_entrypoint:0.3 看效果👇 但是要注意到上图的最后两行curl结果是一样的，别没有想当然的出现web02。理由很简单，就是当你上面敲了最后一行docker run的时候，此时就已经传参-e HOST=web03进去了，而entrypoint就是docker run的时候执行的。加上逻辑卷又是挂载同样的一个，所以最后一行的run就把外面逻辑卷里的Index.html的内容改成了web03。 所以此时下图这个的index.htm其实就是web03 然后server块里的东西依然还是web02的，所以才可以host:web01进行转发 以上就配置验证理解ok了 然后注意是复制的数据卷容器的-v，而不要傻傻理解为挂到某个容器里哦，所以★即使你把volume-server这个参考容器给删了，也不会影响其他容器，因为只是抄了它的配置而已。 任何一个有挂载西信息都可以作为别人的参考也就是数据卷容器--volumes-from xxx 补充：这些run的时候使用的镜像可以不同，只是--volumes-from一样使用一样的数据卷容器的-v挂载情况 上图注意： 1、一般就用命名卷就好啦，上图用的宿主机的目录挂载的 2、数据卷容器--就是用来被引用持久化容器，就不要run起来up的，也不要用大镜像busybox就不错；run完以后不删了就，当然删掉也没事，只要有一个容器引用了，后面就可以让别人引用它。 3、虽然宿主机的目录挂载方式docker volume ls看不到，所以一般就会说这个严格以上不叫逻辑卷，虽然不叫对吧，但是依然可以被--volumes-from来引用。 如果C1容器跑在A机器上，持久化自然也是做在A上，将来C1跑到B机器上运行，持久化的数据怎么同步到B呢，你说手动~~~，好的， 不过有更完善的解决方案-K8S，跨主机的方案。docker是单机。 多个容器共享某部分数据的方案的典型应用场景 nginx + php 以前是跑在一个宿主上的，现在容器化，如果跑在两个容器里(虽然一般是跑在一个容器)，所以可以这两个容器共享一个持久化目录。 nginx和php的程序 应该是共享一个目录，要在一起的，，我去复习之前的动静分离的文章 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:18:25 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/8.Docker的默认网络和容器间通信.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker镜像制作及存储和网络管理/8.Docker的默认网络和容器间通信.html","title":"8.Docker的默认网络和容器间通信.md","keywords":"","body":"第8节 Docker的默认网络和容器间通信 网络简述 容器启动都会获取docker0一个网段的IP，默认都是172.17.0.0/16 同主机下不同容器互访，因为IP是动态获取的，存在变动的可能 不同主机下容器之间互访，首当其冲的就是IP段默认一样，出都出不来。听不懂啊，什么叫出不来，就是路由啊，同网段不会找GW，不走GW怎么出本地局域网呢。 启动一个容器，就会生成一个虚拟网卡，该网卡在宿主机表现为vethxxxx，在容器里表现为eth0#ifxxx; 一体两面 就这么理解就行了，官方称之为veth pair。 docker0相当于一个交换机，eth0--veth 都桥接上去。这样容器之间沟通，走docker0，容器出宿主的流量也走docker0不过还得过一层SNAT出去。 确认某个veth虚拟网卡所属容器 1、进入容器 看编号对应关系就行了，宿主上看9对应到容器的8，就是编号对应的。 也要注意这个veth-pair，一体两面的 MAC 是不一样的； 然后docker0和veth是桥接关系。 2、不进容器 但要注意：会导致 进入多层 bash，如果操作不当 所以这个方法 其实就是进入了命名空间了 其他工具看看docker0挂了几个容器或者几个veth 这个可以直观看到docker里挂的容器的NAME和容器里的IP和MAC，还不错👇 这个就是个排版看的舒服而已，ip a 看到的docker0自然是桥接到所有的vethxxx的。 同宿主容器之间的ping就是默认通的👇 解决容器IP不固定的问题 背景： 1、容器run起来了，IP不确定，run起来了IP是不是也会受dhcp的释放周期影响的咯。 2、如果你公司内网有172.17段，那么容器如果要和这个段互通，就不行了。 需求： 解决这个不固定的问题咯，然后就可以更好的自动化咯。以及更换容器的网段。 落实： 修改docker0的dhcp的地址段 方法1： vim /etc/docker/daemon.json { \"bip\": \"172.19.0.0/16\" # 这里写错啦，这写host和掩码而不是子网，改成0.1就行 } 改完以后，哪怕删掉重启电脑，这个ip就是你改后的样子了，不会恢复到出厂默认值的。 可能猜测是，考虑到以前已经有容器在了，不管是up还是exited的，哪怕你删除了，也给你留存上一次修改的记录。 如果你想恢复出厂IP，那么就手动写成172.17.0.1/16，重启docker，顶多在删掉配置...不过无所谓了。 方法2： vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --bip=172.19.100.0/24 方法2没有成功 桥接到自定义的网桥 默认是docker的内外网卡都是桥接到docker0这个虚拟网桥的，现在改掉。 1、首先创建网桥 yum -y install bridge-utils brctl addbr br0 ip a a 172.20.100.1/24 dev br0 brctl show 发现docker100还没有IP地址，然后docker0和docker100都是UP的。 2、修改服务启动文件 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -b docker 100 3、重载和重启 systemctl daemon-reload systemctl restart docker 4、查看确认 ps -ef |grep dockerd docker run --rm busybox hostname -i 再创建容器，就会桥接到docker100上了 在检查下容器里面是否可以ping通外面 可以的，通过抓docker100的包也可以看到 尝试抓容器的宿主虚拟接口的包 先定位这个容器id的内外IP是多少，当然我们通过上面抓包已经知道是172.20.100.2了。 所以这一次通过这个命令docker network inspect得知什么容器的里面的ip是什么后 再检查这个容器里的接口名称 如何知道exec 容器的接口名称 1、比如我要使用这个bd98e2b01c59容器来做抓包 就可以通过exec -it xxxx ip a 知道接口对，就知道抓外面的哪个接口的包了 2、然后还可以通过， docker network inspect 你要操作容器的id去确认待会抓包看到的内网IP是啥 3、然后开始tcpdump 那个接口 然后就看到包了 此时修了容器里的ip，也能够给通外面来，说明你修改了docker100为所有容器桥接的虚拟网桥，那么SNAT是自动给你做了的，无需配置的 可以通过iptables -vnL -t nat确认的 删除自定义的网桥 ip link down掉后，网卡信息还是少了很多东西的，上下两图docker1的信息都是全的。 此时再brctl delbr docker1就可以了 ip link set docker1 down brctl delbr docker1 使用nmcli创建网桥才能存得住 上面的btctl创建的网桥，重启就没了，这里改写为nmcli来实现 nmcli conn add type bridge ifname docker100 con-name docker100 nmcli conn modify docker100 ipv4.address 172.16.100.254/24 ipv4.method manual ipv4.gateway xxx ipv4.dns 192.168.10.2 由于之前创建过导致冲突 这个在/var/log/message里也看得到，需要删除/var/lib/docker/network里的db ​ 上图这个故障是：之前brctl创建了docker100，重启后docker100没了，由于docker100没了docker服务也就起不来了；此时我又用nmcli 创建了新的brige同样也叫docker100，我试图重启docker服务，结果就报了上图的错误，说明docker自己里面有一个docker100的名字的网桥了，一定是从之前的brctl的docker100对等过的，解决方法就是，rm -rf /var/lib/docker/netwokr/删掉这个网络的配置，其实里面有一个db文件的。 ​ 此时再重启docker就OK了。自此就完成了bridge的nmlcli的配置，以便重启不丢失。 同样改回去，删除 改回去， 再删掉bridge网卡 问题来了，此时再新建一个docker100，docker再改成-b docker100能否起来。预判起不来，理由如上，就是要删掉docker的network里的db。 其实理由就是，del可以删，但是此时docker服务不能重启，可以stop也可以不动它就让他active着，就是不能restart！，否则network数据在/var/lib/docker/network/files/local-kv.db里面产生；导致再次创建的docker100和里面的生成的docker100数据同名，但其实不是一个东西了。 解决这种冲突的方法就是： rm -rf /var/lib/docker/network/files/local-kv.db systemctl restart docker 但是有风险，就是docker的所有的网络信息就没了应该 删除虚拟网桥docker100的正确操作就是 systemctl stop docker nmcli conn del docker100 systemctl stop docker nmcli conn del docker100 nmcli conn add type bridge ifname docker100 con-name docker100 nmcli conn modify docker100 ipv4.address 172.16.100.254/24 ipv4.method manual nmcli conn up docker100 systemctl restart docker 这种操作网桥删掉再创建，docker的/var/lib/docker/network/下无残留，还会起得来的。 以下是一个完整的全过程演示截图 1、此时docker服务ok 2、直接del docker100 3、再补一个docker100回去 4、重启docker，是起的来的 故障点：docker100不在的情况下不能启动dockerf，否则docker会自动生成docker100的数据，导致你nmlic再次创建docker100的时候产生冲突。倒是起不来。 使用busybox来测试 通过help可见： 1、-f是要用的，用来保证容器里是前台，从而宿主上run -d 不会Exited 2、Home directory default . 表示 index.html要创建在运行httpd程序的工作目录就是当前文件夹 然后宿主curl看看，可见内容就是test index。 再用别的容器测试 OK 其实没有curl，用wget也行：wget模拟curl busybox的httpd -v就是带访问日志 但是问题是这个web的容器里的IP不是固定的，哇靠终于回到正题了，我哭哦~~~~ 解决容器IP不固定的问题 就是不用IP，用名字... 1、首先server运行的时候要带--name的，有名字就是固定的信息了 2、然后其他容器就--link 参数run就会自动写hosts吧，不过宿主怎么弄呢？ 实验开始 所谓server其实就是被访问的那一方了。 docker run -d --name web001 busybox httpd -v -f 显然还需要补一个端口暴露 再补一个index.html文件 这样就可以了 合成一个docker run试试，慢慢来嘛，就是玩，不要捉急实验本身，因为实验本身不代表你工作干活做事情的那个场景，你遇到的问题解决得越多，你就越发得蜜汁自省，哦是自省哦，就会更加能hold猪，加油~ docker run -d --name web001 -p 8080:80 busybox sh -c 'echo 12345 > index.html;httpd -v -f' link选项就是自动添加host了👇 添加了web001这个容器名 和 ip的解析。 从0开始计数的👆，所有就是丢了16个。 不知道为啥，我感觉这是坑。 初步定位了：就是使用默认的docker0不会卡这么久，而使用自建的docker100，我是用nmcli创建的这个网桥，就不行，ping就会卡30多个包才通这么久。。。 举例，wordpress的运行就可以采用link的方式了 docker run --name db001 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 docker run -d -p 80:80 --name wordpress --link db001 -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache 版本匹配注意下 👇图中MYSQL_PASSWORD写成了MSYQL_XXX，导致排查了半天。。。 换了个公司内部源，终于pull下来了 DB没有暴露端口，因为就是让wordpress访问的， 然后打开网页报错 因为之前实验有残留信息，删除之前的持久化路径，删除所有容器，重新run就好了 --link db001就是run 的时候就自动给你添加了 hosts解析，是db001这个主机名解析到db001这个容器name的IP。 这样就是实现了db容器里的IP如果变了，也不会影响业务了。而且mysql不对外暴露端口，你想攻击都攻击不了。 但是此时如果db001的容器里的IP地址手动改变，wordpress将无法访问。 但是如果db001的容器IP是DHCP重新获取而改变，则wordpress可以自动更新/etc/hosts文件，也就可以继续访问。所以一般不会有人手动👇修改，所以--link也就是可以解决容器里ip变动的问题的。 docker exec -it --privileged db001 sh ip a add 172.17.0.100/16 dev eth0 ip a del 172.17.0.2/16 dev eth0 此时wordpress无法连接db 而且只要容器1stop了，然后新开一个容器2就会抢了容器1的IP了 此时都不用重启wordpress，hosts文件就会自动更新，所以业务也一定是OK的（已测过了）。 容器别名 本质就是run的时候hosts里一个ip对应多个容器名称 实验过程，和别名使用场景不怎么相干了感觉，但是可以看到hosts的效果。 由于db001重新run，改名字，wordpress里的hosts文件还是写的db001，所以找不到了。 但是此时业务倒是正常的👇前提是db002要up的，虽然docker exec -it wordpress ls报错，但是业务出奇的正常 但是问题依然要处理，别名来解决 docker run --name db001 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 docker run -d -p 80:80 --name wordpress --link db001:\"db002 db003 db004\" -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache 使用场景： 1、程序里写死了是访问db004 2、容器run的时候就不一定非得要叫db004，可以叫db001， 3、然后run这个程序容器的时候可以使用 --lnk db001:db004，就可以将db004解析为db001的IP了。 大概可以这么用。 总结： 工作案例-linux的ping测 linux常ping脚本， 方法1： ping 10.100.8.29 | awk '{ print $0 \" \" strftime(\"%F %H:%M:%S\",systime()) }' >> test.ping 注意这种直接 >> 重定向到 文件，大概是60个ping包一起写入文件的。不是每个包写一个的，相当于默认的有一个I/O优化吧 方法2： ping 223.5.5.5 |& while read -r line;do echo \"$(date) $line\";done >> ping.log ping 223.5.5.5 |& while read -r line;do echo \"$(date) $line\" >> /tmp/123.ping;done 这种方式写入文件很及时，一个ping包就是一个。 时间格式优化下 ping 223.5.5.5 |& while read -r line;do echo \"$(date +%F_%H:%M:%S) $line\" >> /tmp/ping.test ;done 然后改成脚本，做成服务，可以实现异常停止后的服务自动起来，也包括重启机器自动起来。 做成服务 vim /etc/systemd/system/pingtest.service [Unit] Description=Ping Test Service After=network.target [Service] User=root Type=simple ExecStart=/path/to/pingtest.sh 8.8.8.8 Restart=always RestartSec=10 [Install] WantedBy=multi-user.target systemctl daemon-reload systemctl start pingtest.service systemctl enable pingtest.service systemctl status pingtest.service 测试kill 后自动起来 找到main pid kill -9会自动起来 kill 也会起来 stop纯人工停止自然不会起来咯，否则你怎么停服务~ 注意status里可以看到一些服务器启动日志的含时间。 系统message也可以看 以上就是简单的一个做法，或者不写服务用screen去做也不错， 不过正规还是要上监控的，比如用zabbix-agent去实现，这个前文也有。 案例2，如何做个远端节点的PING测监控 其实和斗数一样，也是看三方四正 举例，你要监控从上海到洛杉矶的云主机的ping测。就需要出5张图，也是一个三方四正 ①公司zabbix-----洛杉矶节点 ②proxy1代理----到洛杉矶节点 ③proxy2代理----到洛杉矶节点 ④公司zabbix----到proxy1节点 ⑤公司zabbix----到proxy2节点 看图 有了这个总结，以后做监控，基本上如果重要的节点，就可以一步到位全面监控住。 案例-ipsecvpn 云上的VPN的网关其实是PAT出来的，要IDC的SSG开启NAT-T来解封的。 https://cshihong.github.io/2019/04/17/IPSec%20VPN%E7%9A%84NAT%E7%A9%BF%E8%B6%8A-NAT-T-%E5%8E%9F%E7%90%86/ 文章中的这个传输模式同样不能转换端口的 还有一个问题就是rekey的问题， 理由：云上86400协商，云下28800协商，这是IKE阶段，也就是第一个阶段的周期，于是就会协商成28800也就是8小时，结果发现8小时就会断一次业务，排查发现必须是云下往云上ping一下或者发包来触发，否则要等15分钟左右才能重新拉起隧道。 因为，云上发起的rekey，云下ssg没有开启rekey功能，所以无法重协商SPA，所以就断了，解决方法有2： 1、云下做一个常ping放到screen后台运行就行了 2、最好是SSG上在ipsec阶段开启rekey，注意要和monitor一起，否则开不了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:10:35 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/Docker网络管理和docker-compose编排和仓库管理.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/Docker网络管理和docker-compose编排和仓库管理.html","title":"Docker网络管理和docker-compose编排和仓库管理","keywords":"","body":"Docker网络管理和docker-compose编排和仓库管理 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/1.Docker四种网络模式.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/1.Docker四种网络模式.html","title":"1.Docker四种网络模式.md","keywords":"","body":"第1节 Docker四种网络模式 docker的5种网络模式 1、none：没有网络 2、bridge：桥接 3、container：容器 4、host：主机 5、network-name，这一种依赖于前4种 1、Bridge桥接模式 也是默认模式，类似Vmware的NAT模式 docker安装了默认给你优化了ip-forward=1，如果是k8s的containerd也许早期是没有的，可能就需要手动调整这些内核参数。 docker network ls docker inspect bridge # 可以查看网关 iptables -vnL -t nat # 配合docker port xxx 可见PREROUTING里的端口暴露 一些网段配置👇 vim /etc/docker/daemon.json { \"registry-mirrors\":[ \"https://docker-hub.iwgame.com\", \"https://ustc-edu-cn.mirror.aliyuncs.com\" ], \"hosts\":[\"tcp://0.0.0.0:2375\",\"fd://\"], # /lib/systemd/system/docker.service里需要删掉和这里的重复配置 \"bip\":\"192.168.100.1/24\", \"fixed-cidr\":\"192.168.100.128/30\", \"fixed-cidr-v6\":\"2001:db8::/64\", \"mtu\": 1500, \"default-gateway\":\"192.168.100.2\", \"default-gateway-v6\":\"2001:db8:abcd::89\", \"dns\":[\"1.1.1.1\",\"8.8.8.8\"] } 开始创建容器，顶多4个咯，因为/30嘛 第五个果然报错了 理解下两个GW 这个DefaultGatewayIPv4是默认网关，这里改成100.2，就不会走100.1了，要注意。 Host模式 类似于Vmware的桥接模式 容器里的网卡就是宿主机的网卡了，多个容器之间需要区分端口了。 也不存在端口暴露一说，因为直接就是宿主IP的端口了。 然后你想让容器性能非常好的时候，可以考虑使用host模式，因为没有NAT转发的步骤。 看看这个，完全就是宿主上所有网卡都看的到了 同样可以inspect看看，不过host这是宿主的网络，所以没啥可看的 None模式 none不是说一个网卡都没有，还是有一个lo口的 自己访问自己还是可以的 Container模式 实验开始 1、run一个正常的容器，也就是bridge 网络情况： 2、再run一个Container网络模式的，容器 不能说一摸一样，应该说就是同一个。能理解我这句话的意思吗O(∩_∩)O，一摸一样是两个东西 一样~，哈哈哈 然后，只是网络共用，其他文件都是隔离，也就是其他命名空间都是独立隔离的。 scp 才是基于ip的，docker cp是基于容器名的。 因为docker run --network container:c1 --name c2 这样c1和c2就是一个网卡，所以端口监听要错开，而且只能由被引用方c1来暴露。c2 c3这些引用方不能暴露端口的，run -P直接报错： 然后因为是共用的一个网卡，所以只要container:c1挂上后，c2也一样可以修改网卡的 c1修改网卡 此时c2容器里自然被改了 然后此时作用引用c1的c2创建可以影响c1，但是c2退出不会影响c1，c3引用c1同样之前c2创建的ip还在 如果c1这个被引用的源头没了（exit了，stop了）；注意即使start up了也没用了，c2 c3这些引用的也不会自动出现c1的接口的。 引用c1的其他容器的网卡也就没了 案例：wordpress 正常 wordpress和mysql，一般先启用mysql，后启用wordpress，如果反了，就要会wordpress网页打不开，等mysql后起来，才能打开。 所以是无所谓的，大不了等等就行了。 但是如果使用container模式的网络，就要先启用wordpress，我来试一下啊 docker run -d -p 80:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache docker run --name db001 --network container:wordpress -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 先运行wordpress： 然后就不知道填啥了都： 然后运行DB 因为运行的是--network contaner:wordpress，所以就是共用的wordpress容器的网卡，所以下图直接填写127.0.0.1就行了： 但是如果先启动db再启动wordpress就不行了 因为作为引用方，是不可以暴露端口的。 其实可以这样 这样wordpress和db在一个网卡上，虽然是两个容器。这样通信效率更高了。 如果遇到报错： 👆这是由之前实验数据导致的，它这个报错都是establishing，tcp连接了都，都是由db信息了。 以上就是docker网络的四种模式： closed就是none模式；就是单机不与外界沟通的模式。 bridege是默认模式；就是桥接到默认的比如docker0网桥上的，自然同一个做SNAT的。 joined就是container模式；就是jion别的容器的网卡。 opencontainer就是host模式，桥接道网卡的；host就是主机一个网段的意思，所以叫host。 有感： 用神之法：就是群星拱之，神冷静观之，不受群星直接影响，要过一个空白地（a space），这个空白地就是自由所在，也是成长所在。廉贞+武官入命的人一定要学会此法；否则容易被其强烈影响，该影响若被某个化忌的大运结合就很危险。或者某宫杀星落陷再结命主廉贞+武官，同样会很强烈。或者孤星坐命的人一样。 “Between stimulus and response there is a space. In that space is our power to choose our response. In our response lies our growth and our freedom.” — Often attributed to Steven Covey or Viktor E. Frankl 七杀临身的解法：就是无为而治，用习惯来实现成果，化为习惯也就是终身制的努力了。 关于无为：其实是润物细无声的意思，就是将行动化为细小的执行动作，正所谓日积跬步。 有句话叫：弱者道之用，就是这个道理：其意思就是要徐徐图之，不可用力过猛，弱者就是无为，就是慢慢来的意思，很多感情上、事业上、生活上的用力过猛的开始往往都没有能够达到最终的一个好的结局。无为不是真的不作为，而是表面上看起来没有特别大的动作，润物细无声，其实它一直都在，不曾放弃，只是以柔弱的方式存在，这样不会激发起反作用力。正所谓反者道之动，事物都是走向它的对立面的，你想要一个好的结果，只有弱者道之用才能不激发这个反向趋势。 地空用法：化煞为用，空灵之性，弱化群星对神的影响。要知道吉星也好杀星也罢，凶吉正如太极两仪，正如64卦我想高人应该选择的去用的。物尽其用也是这个道理吧。 神在哪：两眼之间其后，菩提树(大脑)下。 心神：心和神，正如心性和神，感性和理性的关系。心代表命盘和群星；神的强大就代表能否化盘为用，而不是被盘掌控。以神控盘，神清，神清则自在。以心牵神，神弱，神弱则堕入红尘欲望。但也要尊重心，自重就是尊重自己，不可被过度的理性变得僵硬固化，难....啊。 炼神之法：是不是如呼吸一样，长长久久，绵绵不绝，去感受其所在空间。 实操之法：当心里坠坠难安时，将心神聚集收敛，也就是将意念收集在两眼之间，菩提树下。也可以在心脏之处，但是菩提树下为佳。 大概是先看心，后看神的然后持续在神，能够体会热感。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:20:33 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/2.Docker的自定义网络和网络间通信.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/2.Docker的自定义网络和网络间通信.html","title":"2.Docker的自定义网络和网络间通信.md","keywords":"","body":"第2节 Docker的自定义网络和网络间通信 docker默认的网络环境，不管是什么模式都是自带的 哦，这个已经被我改掉了，还原一下 bridge就是对应docker0网卡所在的虚拟网桥 host就是对应eth0网卡所在的宿主机的真实网络 none就是不与外界通信的容器单机游戏了 这个docker network ls看到的三个网络(bridge、host、none)是默认自带一套的。如果不想用这套网络，可以自建。 自定义容器网络 不用原来的三个bridge、host、none，但是类型也就是DRIVER还是bridge、host、null [root@realserver2 ~]# docker network --help Usage: docker network COMMAND Manage networks Commands: connect Connect a container to a network create Create a network disconnect Disconnect a container from a network inspect Display detailed information on one or more networks ls List networks prune Remove all unused networks rm Remove one or more networks Run 'docker network COMMAND --help' for more information on a command. [root@realserver2 ~]# docker network create --help Usage: docker network create [OPTIONS] NETWORK Create a network Options: --attachable Enable manual container attachment --aux-address map Auxiliary IPv4 or IPv6 addresses used by Network driver (default map[]) --config-from string The network from which to copy the configuration --config-only Create a configuration only network -d, --driver string Driver to manage the Network (default \"bridge\") --gateway strings IPv4 or IPv6 Gateway for the master subnet --ingress Create swarm routing-mesh network --internal Restrict external access to the network --ip-range strings Allocate container ip from a sub-range --ipam-driver string IP Address Management Driver (default \"default\") --ipam-opt map Set IPAM driver specific options (default map[]) --ipv6 Enable IPv6 networking --label list Set metadata on a network -o, --opt map Set driver specific options (default map[]) --scope string Control the network's scope --subnet strings Subnet in CIDR format that represents a network segment [root@realserver2 ~]# docker network create -d --subnet --gateway # 注意mode不支持host和none，默认是bridge模式 -d 写了也白写，主要是有个其他swan不怎么用的技术模式在厘面。整体来说-d可以忽略。 docker network create -d bridge --subnet 172.27.0.0/16 --gateway 172.27.0.1 test-net 视频里老师用的是ubuntu的，可以table补齐看得到其他的network类型的 除了上面自定义的test-net类型，还有overlay、macvlan这些没见过的。 创建容器的时候使用新建的network，此时ip段就是172.27.0.0/16的了。 然后容器的网卡也是和物理网卡有一个eth-pair的就是一半在外面的， 里面是6：7，外面是7：6 同样这个eth-pair就不是桥接到docker0这个默认网桥了，而是自定义那个test-net 使用场景，10来个容器在独立的网段里，就可以用上面讲的自定义的虚拟网桥了。 然后容器里的IP确实是可以固定 自定义虚拟交换机直接ping容器名 还有一个：这种自定义网络可以直接ping容器名，存在自动解析的效果 使用默认的docker0就不行：什么不行，就是直接ping c1不会给你自动解析 只能使用--link来完善一下 所以在代码层面，解决容器IP不固定的问题 方法如下：3个方法都无需担心容器里的IP发生变化，都会自动更新，除非你人为进容器里修改IP。 1、使用自定义的虚拟交换机，ip可固定 2、是用自定义的虚拟交换机，无需IP固定，直接使用对方的容器名； 3、使用默认的docker0虚拟交换机，采用--link 来实现hosts解析 测试方法2的IP是否自动更新---要知道方法3--link c1的IP如果变了只要是dhcp的，机会自动更新的。 OK，没问题，就是自动更新的👇 还是用wordpress来实验，这次就使用自定义虚拟网桥来做吧 docker network create --subnet 172.27.0.0/16 --gateway 172.27.0.1 wordpress_net docker run -d --name db001 --net wordpress_net -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -v /data/mysql:/var/lib/mysql --restart=always mariadb:11.4.2 docker run -d --name wordpress -p 80:80 --net wordpress_net -v /data/wordpress:/var/www/html --restart=always wordpress:php8.3-apache 看到这两个cli就要知道这个版本的适配要去wordpress的相关官方信息去找 https://github.com/WordPress/WordPress 实验记录 1、创建虚拟交换机 2、全部删掉，重新来一遍 由于是新建的虚拟交换机下的wordpress和db，所以数据库主机那项直接填写容器名db001就可以了，会自动解析的。 举例 如果容器里没法查看路由、网关信息，可以在外面通过Inspect容器去查看的 虚拟网桥之间的互通 1、默认就是不通的，因为有DOCKER-ISOLATION-STAGE-2这个隔离链 iptables的规则查看-S比较清晰 导出来再修改 iptables -S #只能看看 iptables-save > iptables.rule iptables-restore 添加ACCEPT 但这样就怕原来一些规则被抢了，这些规则如果仅仅是ACCEPT还好，或者是DROP你这里实验大不了就不DROP了，就怕是一些修改的动作被直接ACCEPT了。好像这里不涉及。 此时不同虚拟网桥下的容器就通了 但是这仅仅实验，不会这么修改docker自动生成的iptables 恢复iptables直接重启docker就行了 此时不通虚拟交换机下的容器就恢复到不通了，我们用另外一种方式来实现 docker network connect 虚拟交换机 容器 要让c1能够给和wordpress和db001互通，就把c1 连接到对方的虚拟交换机上。 本质上docker network connect 虚拟网络B 容器A 就是将容器A挂到虚拟网络B下，拿到了B网络的一个IP（当然会加一个接口）。所以此时容器A就可以ping通网络B下的容器了，因为在一个网段了。 当然hostname -i有时候也看不全 其实是hosts解析丢了 删掉重来看看 然后梳理一下bridge和docker0的对应，以及wordpress_net和ip a 上面的对应关系怎么查看找的 首先 ip a上关注docker0和br-xxxx这种格式就是代表虚拟网桥了 然后 这就找到了 同时inspect 网桥里还可以看到哪个容器挂载网桥里 然后看看正常情况hostsname -i 可以看到ip的，因为/etc/hosts里有解析就能看 然后故障就复现了👇 原来network connect和disconnect会破坏容器里的/etc/hosts文件里的解析 wordrpess的docker里太精简了，给他安装一下工具 https://mirrors.tuna.tsinghua.edu.cn/help/debian/ sed也行，这里直接cat > xxx 可见： hostname -i只是看看解析的地址，并不一定是真实的ip地址👆 hostname -I是准的 总之，你使用docker network connect / disconnect 要小心/etc/hosts的内容变化的，同时要小心不要踩到hostname -i的坑。 慢慢来吧👆 工作案例-openvpn容器化 1、准备材料 checkpsw.sh #!/bin/sh PASSFILE=\"/etc/ppp/chap-secrets\" LOG_FILE=\"/etc/openvpn/logs/openvpn-password.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \\\"${PASSFILE}\\\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $3;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\\\"${username}\\\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\\\"${username}\\\", password=\\\"${password}\\\".\" >> ${LOG_FILE} exit 1 chmod +x checkpsw.sh connect #!/bin/sh day=`date +%F` if [ -f /data/logs/openvpn/$day ] then echo \"`date '+%F %H:%M:%S'` User $common_name IP $trusted_ip is logged $1\" >>/data/logs/openvpn/$day else mkdir -p /data/logs/openvpn/ touch /data/logs/openvpn/$day echo \"`date '+%F %H:%M:%S'` User $common_name IP $trusted_ip is logged $1\" >>/data/logs/openvpn/$day fi chmod +x connect entrypoint.sh #!/bin/sh P_PATH=/etc/openvpn/easy-rsa W_PATH=/etc/openvpn sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk add openvpn easy-rsa mkdir -p $P_PATH cp -a /usr/share/easy-rsa/* $P_PATH cd /etc/openvpn/easy-rsa/ echo yes |./easyrsa init-pki cat > /etc/openvpn/easy-rsa/pki/vars > server.conf user-static-ip/user1@test.com chmod +x connect mkdir -p /etc/ppp cat >> /etc/ppp/chap-secrets > ${HOST_IP}.ovpn EOF sed -ir \"//r ${P_PATH}/pki/ca.crt\" ${HOST_IP}.ovpn sed -ir \"//r ${P_PATH}/pki/issued/client.crt\" ${HOST_IP}.ovpn sed -ir \"//r ${P_PATH}/pki/private/client.key\" ${HOST_IP}.ovpn sed -ir \"//r ${P_PATH}/ta.key\" ${HOST_IP}.ovpn mkdir -p /dev/net mknod /dev/net/tun c 10 200 chmod 666 /dev/net/tun exec \"$@\" chmod +x entrypoint.sh Dockerfile FROM alpine:3.20.0 LABEL maintainer=\"oneyearice \" ADD connect checkpsw.sh /etc/openvpn/ ADD /entrypoint.sh /entrypoint.sh ENTRYPOINT [\"/entrypoint.sh\"] CMD [\"/usr/sbin/openvpn\",\"--config\",\"/etc/openvpn/server.conf\"] 2、制作镜像 docker build -t openvpn:240606 . 3、启动容器 docker run的时候加上--cap-add NET_ADMIN docker run -d --name openvpnser --cap-add NET_ADMIN -p 1194:1194/udp openvpn:240606 然后可以通过logs查看日志 2048长度的时间较长 ./easyrsa gen-dh 耗时较长，会卡在Generating DH parameters, 2048 bit long safe prime 提示处，因为底层是：openssl dhparam -out /etc/openvpn/easy-rsa/pki/b03e75eb/temp.1.1 2048 能拨上去了，但是网络不通 这里有一个简单点的案例参考 https://blog.51cto.com/fengwan/1896431 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:20:54 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/3.Docker实现跨宿主机通信和Docker-compose介绍.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/3.Docker实现跨宿主机通信和Docker-compose介绍.html","title":"3.Docker实现跨宿主机通信和Docker-compose介绍.md","keywords":"","body":"第3节 Docker实现跨宿主机通信和Docker-compose介绍 跨宿主机通信 利用桥接实现跨宿主机的容器间通信 上图有个点：就是br0和eth0默认是没有桥接在一起的，是需要手动connect的。 上图的veth是画在br0上的，下图的veth是画在宿主机上的和eth0同层级的。两者画法存在冲突其实就是必然有一个是错误的理解。我TM都不用懂什么docker就能判断这里一定是错误的一个。通过查看网络文章以及参考GPT得知，下图画法是错误的，应该将veth画在docker0上才对。 恢复默认的网卡吧，什么docker100、wordpress_net网桥 都删掉吧 方法1：手动桥接二层打通 将eth0桥接到docker0 也是属于手动的方法，而且会带来eth0与外界通信中断。 1、制作两个测试的容器C1 和 C3 ，要求两个容器的IP不一样，要求两个容器分别在两个宿主机上。所以C1在192.168.126.132上，C3在192.168.126.133上，C2也在133上先于C3启动抢走172.17.0.2这个IP，专业昂C1就是172.17.0.2，C3就是172.17.0.3，ip区分开来了就。 2、把物理网卡桥接到br0也就是docker0 然后 132宿主上 brctl addif docker0 eth0 133宿主上 brctl addif docker0 eth0 此时C1和C3就通了 但是ssh却断了，初步原因就是因为物理网卡桥接到docker0上了。 估计eth0变成了docker0网络里的ip，docker0是172.17.0.0/16的子网，eth0是192.168.126.132的IP。目前没有找到解决的方法，继续吧。 但是这不是常规手段，仅仅是个测试。 方法2：手动打通3层 将两端的ip子网区分开来，走三层互通 这种方法也不太好，需要修改docker0的网段，需要配置路由--2个宿主就要互指路由，10个宿主你岂不是要full mesh就是90条路由了。 1、修改图中左侧172.17.0.0为172.27.0.0段先 记得重启docker服务 2、互指路由 这就通了 也不用管NAT的事情和iptables的规则，就直接通了。不会说被iptables给挡了，也不存在做什么DNAT。因为docker0的IP本身就是铺在宿主机物理网卡的那一层的。 然后iptables隔离链也是隔离的docker0访问非docker0这个方向的流量，而非docker0访问docker0的是不隔离的，简单来讲就是docker0这些虚拟交换机出去的拒绝，进来的放行的。 看图：docker0 --> !docker0 也即是从docker0里出去的才会命中，才会走下面的DROP，然后下面的DROP反而是进入docker0的拒绝来着。就是说连起来看就是docker0出去回来的包被DROP了。 所以本段的实验是外界直接进入docker0的流量未曾匹配到这个策略 ( 哪个策略呢，就是调用DOCKER-ISOLATION-STAGE-2的父策略--就是上面的红框的策略 ) 所以不会被DROP。 方法3：使用专业的第三方软件解决跨主机通信 主要是K8S里的组件：flannel、calico 也有脱离K8S单独使用的软件。 Docker Compose 本质上就是docker做了二次封装 比如这么一个场景：多个容器先后启动，每个容器还有很多启动参数，你说有runlike知道当时run了那些选项，但是runlike并不是十分好用，里面很多是默认无需care的选项也一并呈现了，然后多个容器的先后启动也无从得知。所以需要一个文本记录这些启动措施。 这些措施涉及：启动顺序、run的参数、这些参数就多了还涉及 网络、存储等。 compse就是解决方案，类似ansbile ansbile也分：①如果临时执行 就是一条条命令就完了；②如果稍微复杂的任务就不推荐cli的方式了，而是用playbook来写，完了用ansible-playbook调用就行了。 也好比iptables：①iptables 命令一条条加也是临时的；②也可以写到iptables配置文件里，然后统一重启服务就行，或者iptables-save > iptables.rules ; vim iptables.rules; iptables-restore docker也是如此：①docker 命令就是之前的docker run这些命令；②docker compose就是配置文件，类似剧本的功能。 为了实现在单机上的综合的复杂项目，于是产生了docker compose 类似Dockerfile一样，有一个默认文件名，compose的默认文件名就是docker-compose.yml或docker-compose.yaml，只要在当前工作目录下就不用指定。 下面学习如何些docker-compose.yml，本质上就是将docker run的命令写到compose里。 docker-compose不是独立的工具，底层是依赖docker的。 docker-compose就是docker命令的脚本化。 docker-compose是单机的解决方案。单机上跑多个容器需要有编排--限执行谁后执行谁。 https://docs.docker.com/compose/ https://docs.docker.com/compose/compose-file/07-volumes/ docker-compose应用场景：①单机咯②大规模还是K8S。主要还是比如办公网的一些服务：vpn、confluence jira、等就可以用独立的宿主+docker+docker-compose来时间，这样简单方便啊，比如公司的gitlab也可以这样玩，没必要说一定要K8S啊。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:25:33 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/4.Docker-compose单机编排工具安装和实战案例.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/4.Docker-compose单机编排工具安装和实战案例.html","title":"4.Docker-compose单机编排工具安装和实战案例.md","keywords":"","body":"第4节 Docker-compose单机编排工具安装和实战案例 docker-compose也是docker官方制作的 关键字编排，就是根据业务来管理，就是docker ps 看到的就是所有容器的陈列，不清楚所属的项目，但是docker compose就是业务层次清晰。也可以之启动某个业务的容器。 安装docker-compose 方法1： 方法2：直接yum ubuntu查看版本 yum安装的就是版本低 方法3：直接github下载 其实就是官网上走一波install步骤就行了啊 wget https://github.com/docker/compose/releases/download/v2.27.1/docker-compose-linux-x86_64 mv docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 👇可见确实是可执行文件 也没有任何依赖 因为完全就是go写的，静态编译的。 通过help可见docker-compose也和docker有些像的 但是这种👆副本的效果没啥意义，因为docker-compose是单机游戏，你再多的副本都是在一个机器上的，HA并不给力。 SERVERS也是要写在yml文件里的，这里是CLI格式咯，就好像ansbile的cli和ansible的playbook，就好像iptables的cli和iptalbes的配置文件。 写在yml里，然后docker-compose up就完了。 yml是按业务分文件夹存放的，根Dockerfile一个逻辑。 在这个yaml文件下去执行docker-compose up就会自动读取该yaml文件，就和Dockerfile的操作一样的。 也可叫docker-compose.yml，后缀yaml，yml都行。 docker-compose语法 和版本还有关系，需要注意；如果发现up不起来，比如之前的，比如别人的yml，你up不起来，可能就是版本问题，格式变了。 其实有工具可以将docker run的cli自动转成docker-compose的yaml的。 https://www.composerize.com/ 该网站也有cli的方式 https://linux.cn/article-14970-1.html composerize的cli 👆cli的方式，两个docker run 好像不会给你自动合并出来，可能要手动两次 自己合并，不如web上方便。 composerize的web ui 将之前的wordpress的两个run 改写成 一个 compose 不过这个网站只能转换docker run的cli。网络、创建存储，就不支持了。所以上图的wordpress_net这个虚拟交换机是没有的。 试试 mariadb写在前面就先执行的意思，虽然这里无所谓 name: services: mariadb: ports: - 3306:3306 environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=123456 container_name: mysql restart: always volumes: - /data/mysql:/var/lib/mysql image: mariadb:11.3.2 wordpress: ports: - 8080:80 container_name: wordpress volumes: - /data/wordpress:/var/www/html restart: always image: wordpress:php8.2-apache 然后正常设置wordpress就行了 不过可惜上面的compose是前台的 退出容器也就是推出了 不过start一下也就ok了 想后台，-d就行 不过没有定义网络，就会自动生成一个网络 这个自动生成的虚拟网络命名就是用 yaml文件里的name_default来起名字的。 正好👆就是一个业务一个网络，妥妥的~~但是不同业务就是不同项目之间就默认隔离了。 docker ps ，docker-compose ps就需要在yaml下才能看到 images在compose里看到的就是compose这条线索的images 总之docker images看的是全的，docker-compose images看的就是它自己的 而且还得在yaml文件的所在目录下，不同的yaml的路径看到的结果自然是不同的。 看本项目容器里的各自的进程 docker-compose up # 创建+启动 docker-compose down # 停止+删除 扩容和缩容，0就是删除 不过扩容有前提的， ①yaml文件里的container_name不要写 ②端口冲突也就起不来了 复杂一点的yml version: '3' services: db: image: mariadb:11.3.2 container_name: db restart: unless-stopped environment: - MYSQL_DATABASE=wordpress - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=wordpress - MYSQL_PASSWORD=123456 volumes: - dbdata:/var/lib/mysql networks: - wordpress-network wordpress: depends_on: - db image: wordpress:php8.2-apache container_name: wordpress restart: unless-stopped ports: - \"80:80\" environment: - WORDPRESS_DB_HOST=db:3306 - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=123456 - WORDPRESS_DB_NAME=wordpress volumes: - wordpress:/var/www/html networks: - wordpress-network volumes: wordpress: dbdata: networks: wordpress-network: driver: bridge ipam: config: - subnet: 172.30.0.0/16 由于yaml里wordpress里已经配置了DB的，打开网页，设置站点标题和管理员就进去了。 再来一个yml name: spug-001 services: db: image: mariadb:11.3.2 container_name: spug-db restart: always command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci volumes: - /data/spug/mysql:/var/lib/mysql environment: - MYSQL_DATABASE=spug - MYSQL_USER=spug - MYSQL_PASSWORD=spug.cc - MYSQL_ROOT_PASSWORD=spug.cc spug: image: openspug/spug-service #image: registry.aliyuncs.com/openspug/spug # 这是国内镜像 container_name: spug privileged: true restart: always volumes: - /data/spug/service:/data/spug - /data/spug/repos:/data/repos ports: - \"80:80\" environment: - SPUG_DOCKER_VERSION=v3.2.1 - MYSQL_DATABASE=spug - MYSQL_USER=spug - MYSQL_PASSWORD=spug.cc - MYSQL_HOST=db - MYSQL_PORT=3306 depends_on: - db docker-compose config -q # 语法检查 然后就docker-compose up -d，结果报错了 这里面有CF的CDN啊 emmm，改路由吧，从海外线路出去，或者image换成国内的：registry.aliyuncs.com/openspug/spug 好了👆 初始化登入密码 工作案例 简单加载小脚本 #!/bin/sh char=('\\' '|' '/' '—') while true;do for i in \"${char[@]}\";do echo -ne \"\\r$i\" sleep 0.2 done;done # 根据需要将while改成loading的实际进度执行时间就行了，不过要记得最后清除掉符号。 To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video 优化：使用斜杠转圈圈来表示加载的效果 #!/bin/sh char=('\\' '|' '/' '—') end=$((SECONDS+10)) echo \"转$end秒\" while [ $SECONDS -lt $end ] do for i in \"${char[@]}\" do echo -ne \"\\r$i\" sleep 0.2 done done echo -ne \"\\r\" # 清除最后的加载字符 优化2：使用贪吃蛇小点点来做加载的显示。 [root@realserver2 loading2]# cat loading_gpt.sh #!/bin/sh #char=('\\' '|' '/' '—') char=('⠋' '⠙' '⠹' '⠸' '⠼' '⠴' '⠦' '⠧' '⠇' '⠏') end=$((SECONDS+10)) #echo \"转$end秒\" while [ $SECONDS -lt $end ] do for i in \"${char[@]}\" do echo -ne \"\\r$i\" sleep 0.2 done done echo -ne \"\\r\" # 清除最后的加载字符 printf \"\\033[32m✔\\033[0m\\n\" [root@realserver2 loading2]# [root@realserver2 loading2]# [root@realserver2 loading2]# bash loading_gpt.sh ✔ 优化3：单任务转圈圈，汇总任务也准圈圈 先看几个shell基础 运算符里RANDOM可不加变量符号 kill和wait的是start_task 这个函数的pid，就是行首的加载贪吃蛇效果。 最终要这种效果👇 [root@realserver2 loading3]# cat loading.sh #!/bin/bash chars=('⠋' '⠙' '⠹' '⠸' '⠼' '⠴' '⠦' '⠧' '⠇' '⠏') tasks=(\"任务1\" \"任务2\" \"任务3\") # 开始任务 function start_task { local task_index=$1 local char_index=0 echo -ne \"\\n\\r\" while true; do echo -ne \"${chars[$char_index]} ${tasks[$task_index]} \\r\" char_index=$(( (char_index + 1) % ${#chars[@]} )) sleep 0.1 done } # 完成任务 function finish_task { local task_index=$1 echo -ne \"√ ${tasks[$task_index]}\" #echo -ne \"\\n\\r\" } # 定义更新进度汇总行的函数 update_progress() { echo -ne \"任务进度: \" for status in \"${task_status[@]}\"; do #echo -n \"$status \" #printf \"\\033[32m$status\\033[0m\" echo -ne \"\\033[32m$status\\033[0m\" done #echo -ne \"\\n\\r\" # 保持在第一行 } update_progress # 主程序 function main { for ((i=0; i /dev/null 2>&1 wait $task_pid finish_task $i # 顶行汇总 task_status[$((i))]=\"✔\" echo -ne \"\\033[$(( i + 1 ))A\\r\" # 移动光标到第一行第一列 update_progress echo -ne \"\\033[$(( i + 1 ))B\\r\" done echo } # 运行主程序 main 优化为带框框的 [root@realserver2 loading3]# cat loading002.sh #!/bin/bash chars=('⠋' '⠙' '⠹' '⠸' '⠼' '⠴' '⠦' '⠧' '⠇' '⠏') tasks=(\"任务1\" \"任务2\" \"任务3\") # 开始任务 function start_task { local task_index=$1 local char_index=0 echo -ne \"\\n\\r\" while true; do echo -ne \"${chars[$char_index]} ${tasks[$task_index]} \\r\" char_index=$(( (char_index + 1) % ${#chars[@]} )) sleep 0.1 done } # 完成任务 function finish_task { local task_index=$1 echo -ne \"√ ${tasks[$task_index]}\" #echo -ne \"\\n\\r\" } task_status=() # 定义更新进度汇总行的函数 proress_intial() { echo -ne \"任务进度: [\" for ((i=0; i /dev/null 2>&1 wait $task_pid finish_task $i # 顶行汇总 task_status[$((i))]=\"✔\" echo -ne \"\\033[$(( i + 1 ))A\\r\" # 移动光标到第一行第一列 update_progress echo -ne \"\\033[$(( i + 1 ))B\\r\" done echo } # 运行主程序 main 优化：任务进度汇总为，柱状增长形态 不会了.... 优化：并发状态 .... docker-compose up -d 这条cli如果敲击多遍之间，某个容器配置改变了，那么就会重建这个容器以及依赖于这个容器给i的容器。 对就是敲击怎么滴~ 如果修改docker-compose.yml，然后运行docker-compos up -d，只会影响修改的那个容器，也就是说只是重新创建修改以及受到修改影响的容器。举例 1、Harbor的容器都依赖log 所以只要修改了log，所有容器都会重建 修改log的监听端口 为0.0.0.0 因为所有都依赖log 所以所有都重建 所有都依赖log的证据 2、修改jobservice容器的target路径 改为变量测试，也就是仅仅重启了受影响的容器 同样修改变量的值，一样会重启该容器 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:25:55 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/5.Docker私有仓库Harbor的部署和上传下载镜像.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/5.Docker私有仓库Harbor的部署和上传下载镜像.html","title":"5.Docker私有仓库Harbor的部署和上传下载镜像.md","keywords":"","body":"第5节 Docker私有仓库Harbor的部署和上传下载镜像 简述 官方的hub.docker.com的上传 比如自己的一个image推上去 docker tag openvpn:v001 oneyearice/openvpn:v001 # 打标签就是hub.docker.com的账号/镜像+tag 1、注册账号 2、打标签，打的是仓库的ID和镜像信息，待会登入哪个仓库就用哪个仓库的ID。 3、登入指定仓库，不指定就是官方hub.docker.com 密码本地是存起来了，而且是文明的👇，就是base64格式的而已，可以转 果然echo xxx |base64 -d就转回来了 4、push 报错了，好像是两个账号冲突了，就是，oneyearice/这个路径，的用oneyearice这个账号来登入，结果上面用的是oneyearice@gmail.com登入的，这个账号的主目录是oneyeariceXXX是今天刚刚注册的，deactive这个账号就行了。 其实可以不必用hub.docker.com的账号密码，可以用专门的access token Harbor Vmware公司做的 https://goharbor.io/ https://goharbor.io/docs/2.11.0/ https://github.com/vmware/ https://github.com/vmware/harbor 组成： harbor是十几个容器组成的： 1、nginx：nginx做反代，这是一个容器；给后端各种服务提供反代 2、AdminServer：对应启动组件harbor-admin server，是系统的配置管理中心附带检查存储用量，ui和jobserver启动时需要加载adminserver配置。 3、UI：以UI为前端的core service：涉及UI、API、Auth，而Auth 4、API： 5、Auth：对接AD/LDAP 6、AD/LDAP 7、LogCollector 8、DB 9、ReplicationService 10、Basic Registry 官方提供了这些容器组件启动的配置文件，也就是docker-compolse的yml文件。 安装Harbor 1、安装docker 2、安装docker-comose 然后docker-compose又是依赖docker服务的，所以harbor的启动就需要docker+docker-compose 3、下载harbor离线包 4、解压 harbor内部所有功能组件都是用容器提供的，而容器的镜像都在harbor.v2.11.0.tar.gz这个包里了，这就是所有images的打包文件。相当于docker save tar tvf预览一下，可见都是分层镜像文件👇 然后理论上就是docker load导出images，但是人家提供了脚本给你了👇，脚本里面就有docker load 命令 这个还不是docker-compose.yml文件哦，后缀名也要改成yml👇，这个文件是初始化文件，将来跑完install.sh后，会生成docker-comose.yml文件的。 5、修改yml文件 修改部分内容👇 ①harbor的访问域名或者IP ②ssl证书可以关掉，实验用无所谓 ③默认的admin账号密码 其他不用修改 metric是Prometheus监控用👇 6、运行install.sh进行安装 早期需要安装python环境，现在python都是自带了，版本也不低。 加载镜像👇 其后就会创建docker-compose.yml文件，然后就会使用docker-compose up -d啦👇 可见proxy就是nginx啦，然后访问一下就OK了 关机下班~~ 开机发现harbor的这个几个容器就没有起来几个 原因可以看logs，但是看不出个所以然来 但查看docker-compose.yml里可见很多都是有依赖关系的，猜测原因如下 1、开机启动了docker，systemctl enable docker 2、docker起来了以后，这些容器就立马restart always了 但是有依赖的 看👆图，可以得出两点结论 ①十有八九是因为存在依赖启动关系，才导致一窝蜂的restart always没有启动得起来 ★ 看来光有restart always不够，还需要daemon.json里的live-strore true来辅助！ ②笔者具有深厚的sed技能理解能力，此乃通才的潜质~~~，解释下，这里的sed 里的分号的逻辑其实是存在两种动态的逻辑的：1、就是或者 2、就是并且，哈哈哈不相信？一个sed里的多个语句用分号隔开，笔者也就是本大爷竟然扯犊子说既有或者 也有 并且的逻辑，哈哈哈，不信你看 早在一年前就梳理出来了 再次验证说明 1、如上上图的sed取出docker-compose.yml里的关键词 container_name、restart、以及depends_on的子模块内容。 这个就是典型的或者 2、然后上图之前的sed专栏里，同样使用 来说明了此时 sed 里 多个;分号间隔的 或者 关系；是不等价于多个 管道符 递归的。 1和2就是 或者逻辑 存在于 挑选内容的时候 3、在删除内容的时候，分号代表的逻辑就是并且 4、所以sed 里的 多个动作用分号隔离，多个动作之间的关系 是并且，此时等价于多个grep；并且的逻辑存在于取东西 是或者，此时不等价于多个grep；或者的逻辑存在于去东西。 5、方法论：当你用sed 取东西的时候，一个sed里的分号之间的动作 就是 或者 当你用sed 去 东西的时候，一个sed里的分号之间的动作就 并且递进 怎么理解到生活中来呢，就是上图生活化的例子，这里再次梳理 一个A4纸，好比你要用sed处理的对象；你有两种动作， 第一种动作 从A4纸上 取两个小形状，此时就是 或者的关系 取出第一个小圆片(^_^) 作为结果是不可能被grep出一个三角形的，就好比你取出字母A，然后A里grepB出来是不可能的。 第二种动作 将A4纸 去掉 两个小圆片，此时就是 并且的关系 去掉一个小圆片的结果是背景的A4纸，此时自然可以交给grep再次去掉另一个小三角的。这就是并且递进的关系。 那就起个名字吧，就叫做蒙面侠效应~ 上面的GIF最终像不像眼罩，那不就是蒙面侠嘛~ 所以哥们下次和人吹牛逼，就会说sed有一个蒙面侠效应~，然后观察对方表情，酌情进一步解释，当然对方在努力听明白后会恍然大悟 原来如此，然后就忘记了蒙面侠这个起因词汇，当然脑子快的会问一句蒙面侠是什么鬼~~~ 好了继续Harbor 打开harbor管理页面 公开是指 下载的时候是否需要login。 上传怎么着都需要login的。 项目归属成员 然后用user1推一个镜像上来 域名是当初这里配置的 docker tag SOURCE_IMAGE[:TAG] harbor.oneyearice.org/project001/REPOSITORY[:TAG] docker push harbor.oneyearice.org/project001/REPOSITORY[:TAG] 直接推是推不上去的，不仅仅是因为没有login，而且默认是走的443👇 所以要login，还要解决默认走443的问题 1、解决443问题 写域名写IP都行 这个refused就是80端口实际上没了 重新up一下就对了 此时就是没有login的事了 就好了 此时网页上就看到了👆 发现镜像点进去一些操作都是灰的的 唉~~笨呐，勾选一下就好了 可以拉的 用复制的CLI就是一串编码 一样好用的 效果一样的👇，其实就是版本tag往往也就是哈希值来做一样的效果。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:26:12 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/6.Docker私有仓库Harbor高可用和双向复制.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/6.Docker私有仓库Harbor高可用和双向复制.html","title":"6.Docker私有仓库Harbor高可用和双向复制.md","keywords":"","body":"第6节 Docker私有仓库Harbor高可用和双向复制 Harbor的自动启动 前一篇发现 reboot宿主后，Harbor的9个吧 容器没有全部起得来，当时处理方式就是 1、docker的daemon.json里要有live-restore:true的配置 2、reboot后由于Harbor的docker-compose.yml里的除了log容器以外，其他都是有depends_on的，所以启动次序没有 这就是我要的效果了👆 到这可以判断下 1、从上往下启动来着 2、harhor-log 首先启动 3、registry，其次启动，但是log启动如果速度慢了，registry也许就启动不了了，因为查看docker-comose.yml里没有延迟设置 4、reboot看看哪些起不来 这次就一个jobservice没起来 再reboot 这次就一个log起来了，你看不稳定的 就是log启动慢了，依赖它的其他服务启动的时候log没起来呢，就gg了。 然后restart docker一下就基本都好了，这是因为有daemon.json里的live-restore和compose里的restart always结合的效果吧。 Harbor安装后启动问题 1、部分启动OK，不稳定 ​ 如上所述 2、数据存放默认定义在了/data下 ​ 不过openvpn的image只找到找个名称的文件，但是image文件大小对不上，可能还在其他地方 可以自定义harbor的数据持久化目录 然后运行./prepare脚本 就自定了harbor的数据卷的地方，但是需要重新docker-compose down / up了 而且新的数据持久化目录里是空的， 注意down up才行，restart不行 然后就发现之前上传的镜像没了 =============寻找harbor上传的image到底放在哪了👇====================== 原来的openvpn还在旧的路径里，当然图中的openpvn只是名称不是image image在registry里不过名字是hash值了，而且不太好找，比如我上传一个500MB的镜像，他会压缩到141MB👇 然后就是找找个文件到底在什么地方 关注layer层文件 通过搜索，du -sh *观察文件大小，发现141MB也被拆分成分层镜像了 但实际文件确不在这些路径里，在这里👇，上传的镜像整体是在regsitry大文件夹里面 找到一个 所以结论来了：你上传一个500MB的image，harbor首先给你拆分 分层，然后给你压缩分别存放。页面上看到的就是141MB的压缩后大小。传统观念是我上传一个镜像，我可以从后台也就是底层linux去找到这个镜像然后导出来，对不起这种方式harbor不支持了，因为image被拆分了，只能通过cli去pull下来了。 =============寻找harbor上传的image到底放在哪了👆====================== 改回去上传的镜像就回来了 可以看出来，down是从yml文件里 ，从下往上down的，up就是从上往下up的。 言归正传开启自启动咋解决 方法1，写个service文件 [root@harbor ~]#vim /lib/systemd/system/harbor.service [unit] Description=Harbor After=docker.service systemd-networkd.service systemd-resolved.service Requires=docker.service Documentation=http://github.com/vmware/harbor [Service] Type=simple Restart=on-failure RestartSec=5 ExecStart=/usr//bin/docker-compose -f /apps/harbor/docker-compose.yml up ExecStop=/usr/bin/docker-compose -f /apps/harbor/docker-compose.yml down [Install] WantedBy=multi-user.target [root@harbor ~]#systemctl daemon-reload [root@harbor ~]#systemctl enable harbor 如果将上面的up down改成start和stop反而不灵了--就是起不来。 reboot试试 方法2，写rc.loal咯 vim /etc/rc.local #! /bin/bash cd /apps/harbor /usr/bin/docker-compose up 保存退出👇修改执行权限 chmod +x /etc/rc.local Harbor的HA Harbor最关键还是镜像别丢了就行。 1、不太完美的共享存储 前面顶一个LB，比如nginx，lvs、httpproxy 后面是几个Harobr做实例， 但是数据不放在Harbor上了，单独放到后面，供几个Harbor共享存储。 数据用NAS，NFS来做；然后NFS要做好备份，比如rsync的实时备份...不知道会不会降低IO降低业务效率啊。 总之这个方案不太优秀~ 就是这个data_volume目录是远程挂载的 2、基于镜像复制 就是存储时单独的，但是互相复制的；类似mysql 双主A/A模型。 前端还是LB，比如nginx， 后面是几个Harbor做实例，数据也是跟着Harbor走的。不用共享数据存储。 第一种方案就是共享存储NFS NAS的搭建了，重点时第二种方案的学习 Harbor之间的数据存储的复制，其粒度更精细--就是多个项目，指定哪几个做HA，是可以这样的。 有些测试的可能就没必要做HA。 实验开始 搞第二台Harbor 当然监听端口2台都得改改，第一台页不用改，到时候LB那边顶前面就行了 1、docker 的配置同步下 记得重启服务 systemctl enable docker systemctl start docker 2、安装docker-compose wget https://github.com/docker/compose/releases/download/v2.27.1/docker-compose-linux-x86_64 mv docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 3、然后就开整harbor #下载 wget https://github.com/goharbor/harbor/releases/download/v2.11.0/harbor-offline-installer-v2.11.0.tgz #解压 tar xf harbor-offline-installer-v2.11.0.tgz #修改 cd harbor mv harbor.yml.tmpl harbor.yml vim harbor.yml 注释下👇： #https: # # https port for harbor, default is 443 # port: 443 # # The path of cert and key files for nginx # certificate: /your/certificate/path # private_key: /your/private/key/path # # enable strong ssl ciphers (default: false) # # strong_ssl_ciphers: false ... 修改 hostname: harbor.oneyearice.org # 据说是HA的时候两个Harbor必须用IP，我先不用 data_volume: /data/harbor ... ... #初始化 ./install.sh # 此时docker-compose ps就可以看到harbor已经起来了，web打开看看是ok的就行 admin/Harbor12345 默认账号密码 #写service实现开启和异常退出时自启动 [root@server ~]# cat /lib/systemd/system/harbor.service [unit] Description=Harbor After=docker.service systemd-networkd.service systemd-resolved.service Requires=docker.service Documentation=http://github.com/vmware/harbor [Service] Type=simple Restart=on-failure RestartSec=5 ExecStart=/usr/local/bin/docker-compose -f /apps/harbor/docker-compose.yml up ExecStop=/usr/local/bin/docker-compose -f /apps/harbor/docker-compose.yml down [Install] WantedBy=multi-user.target #加载 systemctl daemon-reload systemctl enable harbor docker-compose down # 先停掉之前的docker-compose手动up的服务，再统一使用s systemctl restart harbor # 再统一使用service来启动和停止，否则service还管不了之前手动启动的服务。 #这样就就ok了 4、配置HA 注意，可能需要将2台Harbor的初始化yml里的hostname改成IP地址，当然我觉得没必要，先不改。 如果改就需要./prepare后再systemctl restart harbor的。 现在有两台Harbor 一台192.168.126.132 ；一台192.168.126.133 1、先新建需要同步的项目：两边都要创建，最好同名 注意第二台上的项目名称少写了8，但是不要经，待会132直接可以把K888s连项目带镜像一起推过来👆 2、其次就要做两遍单向复制，实现双向 关键词：仓库管理--什么项目进行同步 、 复制管理--怎么个同步法 可见👆不仅仅支持harbor的同步，还支持从其他地方比如AWS，比如gitlab之间的同步。 也支持docker官方的registry同步--这个是指registry这个私有方案吧，还是说hub.docker.com啊，应该不是说hub.docker.com 其实我倒没希望有一个类似缓存代理的功能，就是从这个harbor下载的公共镜像都缓存，下次其他人下载也能够利用缓存。nexus倒是用的比较多。 目标名就是同步的对方Harbor的项目名 肯定写IP了，否则域名一样就出问题，不过我上面harbor.yml初始化的配置里 都用的一个harbor.oneyearice.org域名我觉得不碍事，所以没改，继续看看呗~~ 访问ID和密码，严格来讲，是对方仓库项目里的成员ID和密码来着，不过admin是都有了。 因为没有启动SSL，所以去掉勾选 报错了，回头去修改hostname为IP吧 两台除了IP不同，操作都来一遍👆 你看，192.168.126.133一改为IP，👇132去连接就好了，我估计这个hostname十有八九写到nginx的server块了，你写域名的结果就是你要用域名去访问。 查一下nginx的配置呗 没找到..... 纠错: 算了 两边都改成IP吧，就是说HA的时候两台harbor.yml里都要用IP 现在两遍就都OK了，👇图的项目名称不必事先存在，不过最好不要瞎折腾，这里仅仅顺便讲清楚而已。 好像SSL不去掉也行。。。 还是去掉吧，万一后面复制的时候验证SSL就麻烦了 3、继续配置怎么同步 也就是复制管理 新版是有pull和push的，老版的只有push 还是用push，推，用推可以做到实时同步，因为pull拉应该不能实时的去知道什么时候去拉。就是本地镜像一变就push过去，但是要实时拉，就得知道对端镜像的实时变化了不方便可能。理论上是可以让对端将镜像变化的信息发过来的，但是实际应用就不好说，所以还是朱永push吧。 如图👆可见pull拉是没有触发事件的，没法做到实时 简单配一下 此时已经实现了192.168.126.132--->192.168.126.133的单向复制。 测试下 ①再开一个机器192.168.126.134，并修改docker配置文件，加一下没有ssl的忽略配置 这里临时👆配置了两个Harbor的ip地址，后面加了nginx后就删掉2个ip，改成1个nginx的代理ip就行了。 ②两个Harbor都登入一下👇 PS：密码是明文存放的 推一个busybox上去 此时133那台harbor也就自动复制过去了 注意133上的K888S是事先不存在的。上文页提到了，而且log也看得到👇，就是仓库管理那里写啥就推了啥估计。 此时由于133没有push到132的 复制规则 ，所以 这个busybox:v002只会存在于133上，但是由于哈希值一样的，只是改了个tag，所以harbor给你合并了 再推一个alpine上到133去 然后将133-->132的反向复制给做了 然后发现此时132上没有立马出现alpine和busybox:v002的两个镜像。 应该是触发动作没有，所以133上push一个再 随便推一个存在都行 此时132就拿到133所有的多出来的镜像了，不对，还少了一个busybox:v002👇 再推一个nginx到133看看 好nginx已经复制到132了 再看看busybox:v002 还是没有 所以复制的事件触发规则是👇 push哪个image就复制哪个image的，不会说一个imageA触发所有的差异化的images 所以再推一次需要同步的那个image到133 此时133就会触发同步到133了 删除，在132上删除镜像 此时133上镜像的名称文件夹还在，但是里面的东西确实没了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:26:28 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/7.Docker私有仓库的安全加密HTTPS实现.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker网络管理和docker-compose编排和仓库管理/7.Docker私有仓库的安全加密HTTPS实现.html","title":"7.Docker私有仓库的安全加密HTTPS实现.md","keywords":"","body":"第7节 Docker私有仓库的安全加密HTTPS实现 书接上文，再开一个135的IP机器作为 nginx反代 1、当前拓扑 132和133两个harbor的双向复制上一篇已经搞定，基本简单的实现了就。 下面134就是client访问135这个nginx。 2、docker跑一个nginx services: nginx: image: nginx:1.27.0 container_name: nginx_server ports: - \"80:80\" - \"443:443\" volumes: - ./nginx/conf.d:/etc/nginx/conf.d - ./nginx/html:/usr/share/nginx/html - ./nginx/logs:/var/log/nginx restart: always # 说明： # - 使用最新的 nginx 镜像。 # - 映射容器的 80 和 443 端口到主机。 # - 映射本地的 nginx 配置文件目录到容器的配置文件目录。 # - 映射本地的 html 文件目录到容器的 html 文件目录。 # - 映射本地的日志文件目录到容器的日志文件目录。 # - 设置容器重启策略为 always。 up一下 编写配置文件 同样做成服务 vim /lib/systemd/system/nginxd.service [unit] Description=nginxd After=docker.service systemd-networkd.service systemd-resolved.service Requires=docker.service [Service] Type=simple Restart=on-failure RestartSec=5 ExecStart=/usr/local/bin/docker-compose -f /apps/nginx/docker-compose.yml up ExecStop=/usr/local/bin/docker-compose -f /apps/nginx/docker-compose.yml down [Install] WantedBy=multi-user.target 此时就实现了反代了 但是此时登入有问题的，因为轮询调度，ID密码输入，再点就调度到另一台Harbor上去了。 验证就用同样的密码 分别登入两台Harbor就行了，不用测了没问题的，不放心可以测的。 就是nginx调度轮询，没有session导致的。估计连三次握手都握不起来，更遑论验证密码呢！ 所以改成session调度 1、会话绑定，也就是nginx的session调度：①基于cookie的(harbor的cookie是多少也不知道)②基于源IP的(PAT环境就不适合了) 2、session复制，tomcat里有这个功能可以做，harbor没有做不了。 3、redis，理论上可以，但是harbor不知道怎么配置redis，让harbor去redis读session。 下面基于session 1、cookie好像是sid👇 但是好像不能确定 所以还是用👇 2、源IP来做会话保持吧 ip_hash说是源IP，其实是源IP的/24，不是/32的 所以得用hash👇 应该是做成服务了，所以出现问题了， ①docker-compose管理的就用docker-compose来管理，不要用docker； ②做成服务的，就要用服务来start，restart，stop，不要用docker来弄。 systemctl restart nginxd 就好了 上传镜像要需要注意https 因为现在是nginx代理的，所以insecure-registries里要写nginx的地址或域名 再次登入OK 由于没做dns，直接改tag为IP👇，但是报错了 这是因为当初登入的是IP地址，不是域名👆，👇 这样，报错就和IP PUSH的时候一致了 报错写明：Entity Too Large 再次push ok HTTPS实现 HTTPS的SSL证书可以做在nginx上 也可以做在单机版的Harbor上 新版Harbor本身的SSL证书比老版的要复杂些 https://goharbor.io/docs/2.11.0/install-config/configure-https/ 以下是对上面链接内容的简要说明👇 1、CA的私钥 2、CA用私钥自签名 3、CA给他用户也就是server颁发证书 ​ 3.1 server的私钥 ​ 3.2 server的申请文件 ​ 3.3 CA给server颁发证书 cat > v3.ext 比老版多了一个文件👆，然哥们我一看，这不就是让下面PC浏览器显示安全的关键操作嘛，就是subectAltname就是我之前折腾的那东西啊👇 ​ 其实不是什么 比老板 多了了，而是时代变了(屎大便了)，浏览器要人为安全就得这么干，不仅仅是Harbor的新老版本之分，而是所有的都变了。 然后还有一个nginx的ssl生成的便捷脚本也是一样 两处地方都是用到了一个知识点👇 然后这里Harbor的ssl解决方案里自然跑不掉这个新的政策subject Alternative name的意义。 啊~~~~ 继续学习 继续说明https://goharbor.io/docs/2.11.0/install-config/configure-https/这里的内容 👇这个其实好像是无用功，理论上是转换个，其实就是内容都一样，就是改了个后缀名称 其实官方这么些，也是对的，①改后缀②用这种方式改后缀也是大而全的cli ①因为docker deamon会认为.crt是CA的，所以要改成.cert作为client证书，client就是CA的用户，而CA的用户也就是 PC--SERVER ，的server服务器证书了。上图client就指的是server 链接里同样有不完善的地方 这里点进去也不是很明朗，就是yml文件的ssl配置。 以下是实验 # 创建证书相关数据的目录 mkdir -p /data/harbor/certs cd /data/harbor/certs # 生成CA的私钥 openssl genrsa -out ca.key 4096 # 生成CA的自签名证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=ca.ming.org\" \\ -key ca.key \\ -out ca.crt # 生成harbor主机的私钥 openssl genrsa -out harbor.ming.org.key 4096 # 生成harbor主机的证书申请 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=harbor.ming.org\" \\ -key harbor.ming.org.key \\ -out harbor.ming.org.csr # 创建x509 v3扩展文件(时代变了新增内容) cat > v3.ext 然后 mkdir -p /data/harbor/certs/ cp harbor.ming.org.crt harbor.ming.org.key /data/harbor/certs/ vim /apps/harbor/harbor.yml ...... hostname: harbor.ming.org # 注意：此行必须是网站的域名，而且harbor主机的/etc/hosts可以不解析此域名，不能是IP地址，否则登入时会报如下错苏 Error response from daemon: Get \"https://harbor.ming.org/v2/\": Get \"https://192.168.126.132/service/token?account=admin&client_id=docker&offline_token=true&service=harbor-registry\": x509: cannot validate certificate for 192.168.126.132 because it doesn't contain any IP SANs # 而且 之前Harbor HA那会也对hostname有写法要求，好像写IP就得用IP去做HA。在👇这一篇里有提到 ，不过当时没有测试域名写个解析得效果可惜了：https://oneyearice.github.io/1-%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E5%92%8C%E5%A0%A1%E5%9E%92%E6%9C%BAJumpServer%E5%AE%9E%E6%88%98/4-Docker%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%92%8Cdocker-compose%E7%BC%96%E6%8E%92%E5%92%8C%E4%BB%93%E5%BA%93%E7%AE%A1%E7%90%86/6-Docker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93Harbor%E9%AB%98%E5%8F%AF%E7%94%A8%E5%92%8C%E5%8F%8C%E5%90%91%E5%A4%8D%E5%88%B6.html # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /data/harbor/certs/harbor.ming.org.crt private_key: /data/harbor/certs/harbor.ming.org.key ...... # 使上面的配置生效 cd /apps/harbor/ ./prepare docker-compose down -v docker-compose up -d 以下使过程截图 生成证书文件 1、生成ca的key，用这个key生成ca的自签名 # key就是私钥 看看时间到是否10年 2、生成用户的key，用这个key 生成csr证书申请文件 3、生成openssl v3 扩展文件 4、利用ca.crt和ca.key和用户的csr文件生成用户的证书文件，并生成serial字符串 这里上图少了一个换行符，导致CLI卡住不动 查看生成的用户证书，确认时限10年 将上面的证书文件配到Harbor上 就使用，用户key和crt文件。 然后重新启动harbor的dockers，也就是 docker-compose down -v docker-compose up -d 不过之前使写了service的 所以直接restart就行 慢慢就重新创建了👆 一定要做好数据卷的持久保存。 1分钟大概 访问就看到鸟 然后导入证书，试试域名访问，已经IP访问的效果 然后要测试是否可以上传下载镜像，因为开了ssl 然后还要测试HA，因为之前是用的IP还互指HA的，当时用域名还不行咧。 好一个个来 1、导入证书 双击导入 IP访问不行，换域名试试 发现 少了通配符，去试试，重新颁发crt，不麻烦，因为Harbor调用路径没发生变化，文件名称不会改变，# 但是要./prepare的因为crt文件名称没变，但是内容变了，容器不是链接而是内容的复制进去到容器里的。 重启harbor也就是docker-compose down 然后docker-compose up -d 发现PC浏览器还是老的证书 于是怀疑要./prepare一下， 果然 然后导出来，再导进去 不行 可惜了，先试试FQDN 继续改 删除老的证书 导出导入 FQDN写错了 改 重启harbor试试 导出导入，谁让你这么玩的，你可真牛逼，受信任的根证书颁发机构，这个是要导入CA的证书，FUCK 终于可以了 回到过去，通配符的设置 FUCK 然后修正DNS 所以结论来了 只要 证书主题背景的备用名称 里的通配符包含哪个就行了呢？ 👇这个无所谓，你和域名不一样无所谓， 所以结论 就是证书主体背景的备用名称 包含你访问的域名就行了，其他无所谓 然后就是要PC导入ca.crt CA的证书到受信任的根证书颁发机构 √ 最后修正为规范的统一的配置 # 创建证书相关数据的目录 mkdir -p /data/harbor/certs cd /data/harbor/certs # 生成CA的私钥 openssl genrsa -out ca.key 4096 # 生成CA的自签名证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=ca.ming.org\" \\ -key ca.key \\ -out ca.crt # 生成harbor主机的私钥 openssl genrsa -out harbor.ming.org.key 4096 # 生成harbor主机的证书申请 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=example/OU=Personal/CN=harbor.ming.org\" \\ -key harbor.ming.org.key \\ -out harbor.ming.org.csr # 创建x509 v3扩展文件(时代变了新增内容) cat > v3.ext 然后要测试是否可以上传下载镜像，因为开了ssl 然后还要测试HA，因为之前是用的IP还互指HA的，当时用域名还不行咧。 2、检查HA 由于132这台改成域名+ssl，果然 同样要 安装ca.crt吧 yum install -y ca-certificates # 一般就有 # 1√ 根证书到/etc/pki/ca-trust/source/anchors目录中远程就scp啦 cp ca.crt /etc/pki/ca-trust/source/anchors/ # 2√ 执行以下命令来更新 CA 证书存储，以便系统识别新的 CA 证书。 update-ca-trust # 如果根证书是以*.pem结尾，需要转换成crt，然后再执行上述步骤。命令如下： openssl x509 -in ca.pem -inform PEM -out ca.crt ============ # 你可以使用 openssl 命令来验证 CA 证书是否已经成功安装。运行以下命令并检查输出是否包含你刚才添加的 CA 证书。 # 这个也可以不做，直接用curl -L 域名 测试效果就行了，排查故障倒是分布检查需要的 openssl verify -CAfile /etc/pki/tls/certs/ca-bundle.crt your_cert.pem #确保系统信任新的 CA 证书：在安装和更新 CA 证书后，系统应该信任由这个 CA 证书签署的所有证书。可以使用以下命令测试： # 其实没必要，直接curl -L 域名就行了，就能测试了，这个👇是没有做上面步骤的时候测试的吧，不管了。 curl --cacert /etc/pki/tls/certs/ca-bundle.crt https://your-secured-site.com 测试 算了改回去，将132的harbor.yml里hostname改成IP地址，重新./prepare 再restart harbor 不对啊， 1、如果要做HA，那么就是nginx顶前面，就nginx自己做HTTPS就行了，HA之间就用IP。没毛病。 2、如果不做HA，那么就是单台，那么也就无需这里测试什么连接性了，对吧。 3、其实可以做到132 133 134 两台harbor自带ssl，一个nginx也带ssl的，就是hostname用IP就行了，具体如下👇测试OK： 试试看吧，改回IP ​ 然后去看132，harhor.yml里的hostname改为IP后的HTTPS是否OK吧。。 没问题啊 谁说一定要域名的 然后要测试是否可以上传下载镜像，因为开了ssl 3、测试上传下载image 目前上传ok，但也要知道没有走https上传 删除insecure-registries里的相关站点👆，让其走ssl。 这个被拒绝了，要登入一下看看 1、登入是要登入的，就是/root/.docker/config.json里有这个域名的信息 2、重启了docker，harbor服务估计也有问题，80没了 3、导入ca解决证书不信任问题，但是只是停留在curl层面就像PC打开浏览器的效果 4、即使curl层面判断安全了，但是docker push login 还是不认。 5、所以OS系统层面有系统层面的信任途径，docker有docker层面的信任途径 OS就是上面3里面操作的 docker层面的来搞一下👇 # 转换harbor的crt证书为cert后缀，docker识别crt文件为CA证书，cert为客户端证书，这里的客户端应该说的是CA颁发给客户端也就是server服务器的证书，而非docker的客户端。而且全文也只有ca证书和server的证书。 # 不过其实这里是仅仅只是改了后缀而已，不存在转换一说 openssl x509 -inform PEM -in harbor.ming.org.crt -out harbor.ming.org.cert # 所以仅仅等价于👇 cp -a harbor.ming.org.crt harbor.ming.org.cert # 不相信可以对比两文件 diff harbor.ming.org.crt harbor.ming.org.cert md5sum harbor.ming.org.crt harbor.ming.org.cert # 默认的配置文件路径里创建 # 如果是docker就👇 mkdir -pv /etc/docker/certs.d/harbor.ming.org/ # 如果四containerd就👇 mkdir -pv /etc/containerd/certs.d/harobr.ming.org/ # 在docker客户端使用上面的证书文件 # 注意，官方介绍还需要同时复制harbor.ming.org.key和ca.crt，实际不需要 cp harbor.ming.org.cert 或 harbor.ming.org.crt harbor.ming.org.key ca.crt /etc/docker/certs.d/harbor.ming.org/ # 注意，实际只需要复制一个文件即可 # ①实际操作1 cp harbor.ming.org.crt /etc/docker/cert.d/harbor.ming.org/ # 无需重启服务，docker客户端即可上传下载镜像 # 新版如果无法登入，就重启docker服务 # ②实际操作2 就行了 systemctl restart docker 确实转的没意义👆 就是要重启docker的，但是docker重启后，harbor估计就不正常了 harbor是做成服务的，状态倒是OK，但是监听端口80没了 不得已重启harbor吧 然后就看到一开始提到的报错了 [root@realserver2 harbor]# docker login -u admin -p Harbor12345 harbor.ming.org WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error response from daemon: Get \"https://harbor.ming.org/v2/\": Get \"https://192.168.126.132/service/token?account=admin&client_id=docker&offline_token=true&service=harbor-registry\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.126.132 because it doesn't contain any IP SANs 前文解决方法就是hostname写域名，但是这方法被我否掉了，因为HA没法做了。 当然你说做了HA，SSL就配置在nginx就行了，单机不做SSL。也行~ 单机就hostname改为域名，然后不做HA，SSL也就OK了。 但是我就要既做HA--hostname就得IP，然后就要ngnix和后面得nodes都做ssl呢，你说神经病。我呸 就是要hostname写IP，正面解决这个报错，这个报错的本意还是证书里的信息选项不全。 cannot validate certificate for 192.168.126.132 because it doesn't contain any IP SANs 因为这个服务器证书里没有IP SANs信息，什么叫IP SANs Subject Alternative Name (SAN) 这个熟悉吧，所以加一行就行了 [ alt_names ] IP.1 = 192.168.126.132 DNS.1 = *.ming.org DNS.2 = ming.org 将v3.ext优化为 authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=*.ming.org DNS.2=ming.org 重新颁发server的证书，服务器的证书啊，一些文章称之为客户端证书，其实是指CA的客户端，CA为其客户颁发的证书。哦，这是一个意思。 然后重新配置docker信任的ssl证书 再看最终效果 好了 搞定👆 搞定啥了？搞定了 1、HA 2、nginx的ssl证书不要讲了 3、HA环境下harbor.yml 的 hostname 必须写IP的情况下，每个单台harbor的ssl证书也做好了 4、操作逻辑如下 1、curl 的 ssl 信任路径和加载 2、docker的ssl信任路径和重启docker的必要性 3、重启docker对harbor的服务起来没影响，但是对可怜的监听端口有影响，需要重启harbor 4、harbor.yml这个初始化配置yml文件里hostname以后就记住必须写ip了 5、报错如果是refuse拒绝，一般可能就是监听端口没了 6、新版ssl要让浏览器也就是curl认可，需要SAN(subjectAltName)里有DNS1的域名， 7、新版ssl要让docker push的时候不报错IP SAN，就要让SAN(subjectAltName)里有IP1信息，就是配置为ssl服务器的IP就行了 差不多了，其他就是删改你乱七八糟但是又一个过程不拉的讨人厌的繁琐的操作记录了 tomcat那篇弄完再来👇 构建tomcat镜像运行jpress 1、ubuntu基础镜像: ubuntu22.04 2、jdk镜像: jdk-ubuntu22.04 3、tomcat镜像: tomcat-jdk-ubuntu22.04 4、最后一个jpress镜像也就是业务镜像: jpress-tomcat-jdk-ubuntu22.04 ​ 1 2 3 就是都属于基础镜像，4就是业务镜像会存在改动，所以变动就做4步就行了。 5、然后利用多阶段构建新开一个ubuntu然后二次构建，这样整体镜像就会小很多。如果是go不是java就直接用busybox来构建甚至用scratch空基础镜像来弄。 这步感觉要考虑插入上面哪一步里面，最好是第四步了。 # 就是能够脱离原环境到一个新的基础环境里运行得情况下就可以进行多阶段构建。还需要打几个常会用得检查工具 如 ping curl 7、上传到harbor上 8、从另外一台主机下载 docker run jpress 上传的jdk源码包，可以删掉，或者直接用ADD解压缩进去，但是还是有过程文件残留还是会占用空间，不一定能释放得掉，即使删掉一些，镜像可能也不会缩减。 案例 openssh升级高版本 以下未验证👇，思路应该使对的 用telnet上去编译安装openssl openssl升级 直接换rocky9 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:26:42 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/Docker资源限制和堡垒机JumpServer.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/Docker资源限制和堡垒机JumpServer.html","title":"Docker资源限制和堡垒机JumpServer","keywords":"","body":"Docker资源限制和堡垒机JumpServer Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/1.Docker基于Cgroup实现内存和CPU资源限制.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/1.Docker基于Cgroup实现内存和CPU资源限制.html","title":"1.Docker基于Cgroup实现内存和CPU资源限制.md","keywords":"","body":"第1节 Docker基于Cgroup实现内存和CPU资源限制 传统限制手段PAM PAM里的limit模块 可以控制某一个用户的资源使用 1、控制CPU运行时间 # cpu 2、控制使用的进程个数 # nproc 3、打开文件个数 # nofile 4、物理内存 # rss docker种的限制方式 java服务很可能会占用较大的资源 tomcat里梳理举例不做限制oom的情况 涉及堆内存 早期docker版本有一个swap开启的告警，现在没了 WARNING: No swap limit support # 没有启用swap 限制功能会出现次提示报警，这是早期的docker才会 不过即使是老版本这个告警也没啥，不处理也没关系。仅仅是一个warning而已。 看一下oom_socre_adj的内容 虽然可以手动改，但是通过修改为-1000来实现进程保护--防止被杀掉，这种操作也是比较少见的。而且这个文件的存放路径是PID的，举例nginx的PID你每次启动的PID也是不同的，也就是说不是一个路径存放了的。实际操作也不是靠谱的。而且这只是防止这个PID被杀掉，并不是说限制资源的使用的角度出发的。 oom_adj就是oom_score_adj的老版本，并且自动兼容的一个文件，可以不用管 而oom_socre是系统给你算出来的值，不可调 压力测试Stress-ng https://wiki.ubuntu.com/Kernel/Reference/stress-ng 安装也有docker版 https://hub.docker.com/r/polinux/stress-ng 可以用来判断docker是否限制住了某个容器应用的内存；或者看其是否会OOM。 yum 安装下 stress-ng -h查看 stress-ng --cpu 8 --iomix 4 --vm 2 --vm-bytes 128M --fork 4 --timeout 10s # 开启进程，占8个cpu，占用4个io，开2个vm就是256*2=512MB内存，但是vm-bytes=128，所以还是256MB内存的占用，--fork就是开启子进程4个，timeout就是压力测试时间是10s。 测试下 CPU 估计就是每个CPU做大量运算，比如加解密之类的计算 结果就是1颗CPU的占用 就是占用了4个CPU 内存 内存就是看RSS物理内存就行，VIRT是进程跟OS申请的内存--虚拟内存，不是实际占用的RSS内存。 确实是256MB，但是CPU也是跟着占用了1颗了，差不多占用0.7的使用率 开3个vm就是256*3=768MB的内存占用，不过同样会消耗3个CPU 开3个vm就是256*3=768MB的内存占用，不过同样会消耗3个CPU 下面开始测试容器 测试是否能够做到docker限制资源的使用 思路：就是docker run 的时候 ①限制资源；②同时用stree-ng撑爆资源，③观察结果如果没有撑爆说明就限制住了。 docker pull polinux/stress-ng # 一般关注的是CPU和内存的资源占用，以及磁盘IO 确实是512MB内存占用👆 也可以通过docker stats xxx来看，不过就是动态的 目前由于docker run 的时候没有限制内存的使用，所以上图👆LIMIT里显示的就是物理内存的最大值3.6GiB。 docker 限制资源 Docker可以强制执行硬性内存限制，即只允许容器使用给定的内存大小。 Docker也可以执行非硬性内存限制，即容器可以使用尽可能多的内存，除非内核检测主机上的内存不够用了。 https://docs.docker.com/config/containers/resource_constraints/ -m 和 --oom-kill-disable 一起使用的场景就是：这个容器很重要不能被oom杀掉，所以自身限制内存使用的同时，也不要让别人杀掉自己。但是不能单独使用--oom-kill-disable这样会造成自己无节制的使用内存，然后把系统宿主的进程杀掉从而导致业务同样出问题。 swap就别研究了，k8s好像也不推荐使用swap的，swap这种就别用了。 ​ 限制内存 docker rum --name c1 -m 100m polinux/stress-ng -m 2 # 限制内存100MB，但是容器里是开到了512MB 以前版本可能存在这么一个文件 这个值就是限制的内存的大小 限制CPU --cpu period喝--cpu-quota都不用了。 测试1.分配CPU个数 docker run --rm --name c2 --cpus 1.5 polinux/stress-ng -c 2 # 压测开到2个cpu，但是限制--cpus指定为1.5 这样限制0.5个CPU，即使压测给到2个CPU，也不会涨上去。 而且这个0.5个CPU的占用不是说就盯着一个CPU干的，而是均摊到所有CPU上的 测试2-容器之间按比例分担CPU docker run --rm --name c2 -c 1024 polinux/stress-ng -c 4 docker run --rm --name c3 -c 1024 polinux/stress-ng -c 4 # c2 c3两个容器都压测开到4个CPU，然后run的时候给到1024的值也就是1024/2048的占比使用 然后输入docker stats看所有的容器的占用情况 再开个容器同样也是1024的占比 此时差不多300%的占比就变成均分了 再开一个2048占比 此时2048就占了一般，其他两个1024就各占剩下一半的一半 比例是一目了然的，但是压测-c 4 明明是4个CPU，但是实际是3个CPU的使用情况--从第一张图上可见不超过300%其实。 测试3-CPU绑定 查看CPU绑定关系 ps axo pid,cmd,psr 开了三个压测，发现CPU没有固定在某一颗上，是会飘的 为什么明明开了3个压测容器，为什么这么多个，因为是父子进程 父进程确实就是3个，没错👆 docker run -d --name c1 --cpuset-cpus 3 -c 1024 polinux/stress-ng -c 4 # 绑定到第四块CPU上 下图选中的父子进程是多出来的，也就是c1压测容器的进程👇 这些就固定绑定到3号CPU了 一般限制也就是上面几个资源 内存、cpu的各种参数 docker run --cpuset-cpus 3 -c 1024 --cpus 1.5 -m 100m # cpu绑定、cpu分配比例、cpu分配个数，内存 超分超卖 cpu可以 然后CPU是可以多个容器绑一颗的，属于可压缩资源 内存不行 第一种oom-宿主卡爆 内存超分就会产生oom咯 docker stats 观察如果oom就会 此时宿主ssh操作就会卡顿 第二种oom-宿主没事 人工计算好了再分配，此时宿主不会卡，但是容器里受限，一样会有oom 这种分配没问题，但是也要知道stress-ng -m 10就是256*10*4=10G的内存压下去了，但是你有限制了 200m*4=800m的内存才，所以stress-ng这个容器里会认为oom了 所以在console控制台里同样会看到oom-kill的 但是容器日志里是看不到的 还是人为限定就可以了 这样就stress-ng容器压测也不会有oom了 工作案例 1、自有机房入伏前一个月 最好准备好空调故障应急措施 ①是否在保 ②在保内的维修、投诉电话、上门点在哪、维修师傅电话、厂家调货流程周期--以为配件调货AUX是师傅直接联系厂家，总台都看不到的 ③第三方上门30块起步，KFR-120W这一个压缩机主控板就要1000多，一天给你修好但是费用要1600大洋。 ④基于维护等待配件的漫长时间里，机房温度过高如何处理 ​ 小机房，先上5 6 个电风扇(这招其实真不错，对着机柜吹就能降10度)，但是电风扇直吹的区域，周围会被吹出来的热气导致气温上去，大概上去个3度。(￣▽￣)\" ​ 然后L3的空调可以关掉了，开也行，就是开30分钟就必须关掉否则就开始吹热风了，温度比室内环境还高！ ​ 买些冰块放机房里吧 ​ 打开机房门，用空调导风管，将大厅里的空调风导进机房里。 2、抱脸模型下载 1、首先你的能够从浏览器直接下载 2、然后复制连接到迅雷，这样就可以利用迅雷从最佳的源下载了，速度噶快 3、对比哈希值，防止被种码 3、谷歌企业邮箱 workspace里的第二档收费才支持群发的功能，同样支持群发单显 1、首先你的有一个域名，推荐godaddy 2、然后就大致按如下走一遍就行了，结合下一步操作操作 https://gnfob.com/google-email/ 3、邮件的跟踪，比如收件人的已读于否，是要插件来支持的。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:27:08 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/2.JumpServer介绍和部署.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/2.JumpServer介绍和部署.html","title":"2.JumpServer介绍和部署.md","keywords":"","body":"第2节 JumpServer介绍和部署 1、概述 看看企业版，然后就弄社区版吧~~O(∩_∩)O 2、实验-直接用docker部署 选择有很多，分开来各个组件也有独立的容器化，这里选择单机直接安装，就用jms-all这个容器 安装方法1：一键安装 https://docs.jumpserver.org/zh/v4/quick_start/ 官方一键安装脚本也是容器安装的 其中2222端口是CLI窗口界面👇，登入需要先页面使用默认密码登入下也就是改一下密码，才能从cli登入。 一键安装也可以维护的 启动停止升级cli在这里👇 安装方法2：手动分步安装 意义就是，学会分步安装，听不懂？即使jumpserver可以不用这种分步安装，其他的服务可能也会用的，其他服务即使用不到，分步安装过程中的问题解决也会出现在其他使用场景里，这是必然的。所以学东西 眼睛放亮一点，格局打开~ 基于容器部署 1、docker环境安装 2、mysql 3、redis 4、jumpserver docker安装 参考前文，这里写ubuntu意思下 apt update && apt -y install docker.io systemctl restart docker mysql的安装 1、版本现在可能不讲究了，之前是高版本的mysql，jumpserver不适配 ​ jumpserver-v2.28.7之前版本不支持mysql8.0需要选择mysql5.7。 2、配置文件要改 ​ 默认字符集要改，但是镜像是官方做的，就需要自定义配置文件了。具体在容器里的配置文件里添加，characer-set-server=utf8 # 好歹用utf8mb4啊 # 所以要做持久化 mkdir -p /etc/mysql/mysql.conf.d/ mkdir -p /etc/mysql/conf.d/ # 生成服务器配置文件，指定字符集 tee /etc/mysql/mysql.conf.d/mysqld.conf 安装redis 启动 docker run -d -p 6379:6379 --name redis --restart always redis:7.2.5 连接 yum -y install redis redis-cli -h a.b.c.d 进入redis交互界面 a.b.c.d>info 回车后显示👇 # server redis_version:x.x.x redis_git_sha1:000000 redis_git_dirty:0 部署jumpserver 需要先生成key和token 迁移和更新升级就要检测SECRET_KEY是否与之前设置一致，不能随机生成，否则数据库所有加密的字段均无法解密。BOOTSTRAP_TOKEN也一样 https://docs.jumpserver.org/zh/v3/installation/migration/?h=secret_key # key.sh文件内容 #!/bin/bash if [ ! \"$SECRET_KEY\" ];then SECRET_KEY=`cat /dev/urandom |tr -dc A-Za-z0-9 | head -c 50` echo \"SECRET_KEY=$SECRET_KEY\" >> ~/.bashrc; echo SECRET_KEY=$SECRET_KEY; else echo SECRET_KEY=$SECRET_KEY; fi if [ ! \"$BOOTSTRAP_TOKEN\" ];then BOOTSTRAP_TOKEN=`cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 16`; echo \"BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN\" >> ~/.bashrc; echo BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN; else echo BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN; fi 然后运行 bash key.sh 最后检测 tail -n2 .bashrc #获取两个变量的值 SECRET_KEY=DiShVgRRBMEqM7kJ41ArYdvfVQ216amp3FUVxiHdxJNxTObkNU BOOTSTRAP_TOKEN=QZ54bEJphGpRhP1R docker run --name jms_all -d --restart always \\ -v /opt/jumpserver/core/data:/opt/jumpserver/data \\ -v /opt/jumpserver/koko/data:/opt/koko/data \\ -v /opt/jumpserver/lion/data:/opt/lion/data \\ -p 80:80 \\ -p 2222:2222 \\ -e SECRET_KEY=DiShVgRRBMEqM7kJ41ArYdvfVQ216amp3FUVxiHdxJNxTObkNU \\ -e BOOTSTRAP_TOKEN=QZ54bEJphGpRhP1R \\ -e LOG_LEVEL=ERROR \\ -e DB_HOST=192.168.126.133 \\ -e DB_PORT=3306 \\ -e DB_USER=jumpserver \\ -e DB_PASSWORD=123456 \\ -e DB_NAME=jumpserver \\ -e REDIS_HOST=192.168.126.133 \\ -e REDIS_PORT=6379 \\ -e REDIS_PASSWORD='' \\ --privileged=true \\ jumpserver/jms_all:v3.10.12 登入就行了，用户名密码 都是admin，这个和一键安装不一样，一键安装的密码是ChangeMe 尝试复现一键安装的故障： 1、首先停止本地的harbor systemctl stop harbor 2、然后复制官方的一键安装 安装一如既往正常的，然后reboot看看是否会出现故障👆 此时不出意外就会出现ssh登不上的情况，然后就可以找一下具体故障点了。 不过可惜没有复现，猜测是harbor和jumpserver容器网络冲突了，导致eth0物理口的IP没有起来，当时是手动ifconfig down eth0 ifconfig up eth0后才能够ssh上去。 明天复现看看吧，也可能是192.168.126.133那台docker run的时候run成了132了？不对啊，早就ctrl c了 工作案例 1、关于comfui，往往需要高配显卡 但是现在随着时间的推进，很多云上也有此类机器，而且SD比如会成为服务直接对外提供，无需人工搭建，下面就是成本问题。 2、自己折腾的必要性，在于自己动手丰衣足食。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:27:21 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/3.JumpServer用户管理和添加服务器资产.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/3.JumpServer用户管理和添加服务器资产.html","title":"3.JumpServer用户管理和添加服务器资产.md","keywords":"","body":"第3节 JumpServer用户管理和添加服务器资产 呃呃呃，我的目的就是减少沙箱这种收费软件的LIC的使用，而期待Jumpserver来做安全审计的远程操作。 然后邮箱是要的，因为一般涉及告警发送，钉钉后面再看，api也要的。 架构上，肯定不会说先拨vpn再连jumpserver，直接公网加端口怼出去，反正有审计，关键还要做风险操作的及时告警，这个学完后自己折腾下。 邮箱设置 其实也没啥好说的，就是常规配置，倒是这里有一个extmail的常规操作 我的extmail邮箱满了，web点击要么都是无标题，要么就直接页面报错了，所以需要清空一下邮件 1、删除收件箱、Junk(垃圾邮件)、Trash(垃圾箱)的邮件 2、删除用户Maildir下的extmail-curcache.db文件 然后刷新web就好了 然后jumpserver的测试一下啊邮件就发过来了👆 这里如果配置SSL 465发送邮箱会报错 你还在为^H而困恼嘛 stty erase ^H 关于容器里(其实其他地方也可能遇到)较多遇到Backspace往前删除，删不掉的清空，这个时候简单操作一下就行了 其实backspace就是^H，所以最原初的你敲backspace，屏幕上显示的就是^H，所以需要优化下👆 其实正确的方法是安装bash 因为你就算解决backspace，你的上下左右键还是没办法弄，而切换到bash就都有了，如果没有bash就安装一下 然后继续解决邮件465SSL发送报错DH长度的问题 查文档 1、这是python 的处理方式 https://blog.csdn.net/weixin_47383889/article/details/125019751 2、这是openssl，linux里的处理方式 https://www.cnblogs.com/testzcy/p/17425364.html 3、这个好像才是正解 https://stackoverflow.com/questions/61626206/what-could-cause-dh-key-too-small-error 4、都不对，jumpserver的邮件发送好像是django里的模块实现，所以要从这里入手 进入容器的时候用的是sh，所以还得切bash，没有就安装 然后安装vim sed -i 's|http://deb.debian.org/debian|https://mirrors.tuna.tsinghua.edu.cn/debian|g' /etc/apt/sources.list.d/debian.sources sed -i 's|http://deb.debian.org/debian-security|https://mirrors.tuna.tsinghua.edu.cn/debian-security|g' /etc/apt/sources.list.d/debian.sources apt update apt install vim 4、都不对，jumpserver的邮件发送好像是django里的模块实现，所以要从这里入手 https://kb.fit2cloud.com/?p=77 再安装postfix yum -y install postfix systemctl enable postfix systemctl start postfix 再设置s-nail set smtp=smtps://smtp.gmail.com:465 # 这行删掉就是25的smtp了 set smtp-auth=login set smtp-auth-user=your-email@gmail.com set smtp-auth-password=your-password set from=your-email@gmail.com set ssl-verify=ignore s-nail好像要配置一个真实的邮箱用户和密码了，但是老早的centos的postfix+mailx哪里就不需要，直接匿名发送了就。 上面就测试了jumpserver所在宿主是可以465发送邮件的，问题还是在jumpserver容器里的DH问题 jms_all容器里测试 使用mailutils提供mail命令好比s-nail，ssmtp提供postfix的邮件client端但是比postfix轻量级。 apt install ssmtp apt install mailutils ---------------- root@f8fcc4d9b3a8:/opt# cat /etc/ssmtp/ssmtp.conf |grep -Ev '^(#|$)' root=postmaster mailhub=mail.iwgame.com:25 hostname=f8fcc4d9b3a8 AuthUser=shenyiming@iwgame.com AuthPass=Cisc0@123 UseTLS=NO UseSTARTTLS=NO FromLineOverride=YES --------------- root@f8fcc4d9b3a8:/opt# cat /etc/ssmtp/revaliases root:shenyiming@iwgame.com:mail.iwgame.com ubuntu:shenyiming@iwgame.com:mail.iwgame.com OK的，所以定是jumpserver使用邮件发送模块里的DH的问题，可能就是django里发送的 不搞了，一键安装方式v4.0.0的版本也一样不行 docker安装的版本是 一键脚本安装的版本是 不搞了，一键安装方式v4.0.0的版本也一样不行 操作 当前用户是admin登入的， 该角色身兼三职①管理员，操作jumpserver，②审计员，安全log查看的人，③使用jumpserver提供的跳转功能干活的，走工作台了就。 创建账号 管理员走控制台 1、分组，开发，测试、运维等 2、建用户 新版本反而没有钉钉认证了，老版本有的 这个密码选项图中的也能用，不过发给用户那边的链接其实就是127.0.0.1的地址，因为是容器里的嘛，也没有优化好，所以不推荐用这种方式。 然后测试了重置密码(会有邮件的交互)也都是ok的，就是同样要注意127.0.0.1改为真实的宿主IP的问题，要是能找到容器里的这个127.0.0.1的设置就好了。 不过还是不推荐邮件参与进来，直接给一个默认密码，然后让用户首次登入修改就很好。 注意点编辑进去的系统角色显示有问题，记住你选啥，就是啥，外面确认是对的就行 先看看审计员的界面 也没啥好看的，就是一堆日志，不过也有工作台就是没有配置任何机器给他用。主要还是看日志。 此时再开一个浏览使用别的账号登入一下，啥，为啥要别的浏览器，因为有cookie啊，chrome用的admin，chrom无痕用的审计员账号，所以再开一个firefox登入一个普通用户。 此时审计员可以看到记录，不管是在线记录，还是登入记录 连管理员的操作日志都能看到 审计专员还是挺牛逼的，哈哈哈 普通人员只有工作台 下面就开始添加资产 这里有一个底层做事情的逻辑 1、用户---通过跳板机---链接业务机器 ​ 用户通过账号进入跳板机，而跳板机分配该用户可以链接哪些机器 ​ 所以跳板机就需要事先连接好后端的很多机器 2、ansible就是如此：ansible要管理机器ABC，就要配置一个主机清单文件，然会为了方便就要做key验证。 3、jumpserver亦是如此：把后端需要远程访问的机器配置到jumpserver资产里，后面分配给不同的用户。 ​ 这些jumpserver关联的后端资产可以有(一般就是linux和windows)：服务器(linux\\windows\\Unix)、数据库、网络设备、应用、K8S。 ​ 涉及到另外的账号概念：早期jumpserver称之为，现在统一叫系统用户里的特权和普通👇 jumpserver里的三种用户 前面梳理了 管理员、审计员、用户，现在 关于账号的理解，这里有一个宇宙法则：谁的，什么系统的账号，就TMD从这个角度去梳理就一目了然。就问你屌不屌~一念之间，天地翻转，地天泰啦~ 1、登录用户 ​ ① 管理员 ②审计员 ③用户 ​ 说白了就是jumpserver自身的账号。 2、系统用户 里的 特权用户 ，早期jumpsever版本叫 管理用户 ​ 对后端服务器具有管理权限的账号root或administrator以及sudo ALL权限的用户，用于管理后端服务器 ​ 说白了就是后端的root账号 3、系统用户 里的 普通用户，早期jumpserver版本叫 系统用户 ​ 给登录用户使用ssh连接后端服务时对应的系统用户，一般是后端服务器的普通的系统用户账号 ​ 说白了就是后端的普通账号 jumpserver内置了ansible，拿后端的root账号是可以批量去后端机器上创建账号的 然后用户通过jumpsever的账号 登录 jumpserver后，就可以使用批量的各个账号去登入 后端机器了。 xiaoming 图中的xiaoming哦，看图👆，通过xiaoming账号登入jumpserver，jumpserver通过不同机器的各自的root账号+ansible批量创建下发后端机器的所需用户ID，就是哪些linux机器的用户，windows的也有ansible？回头看看 用户管理：就是jumpserver自己的登入账号 账号管理：就是后端服务器的账号，root和master以及其他的 新版本，root还是普通就一个选项就搞定了👇 往前的版本如下👇就是系统用户里的 普通 和 特权 之分， 开始创建账号 1、直接创建账号是要有前置条件的，需要关联资产也就是这个账号登入的后端机器，所以要去新建资产 2、可以先新建账号模板，将来创建账号的时候调用就行了 3、新建资产-也就是后端的机器 实验开始 先准备两台机器作为跳板机的资产 就133 和 134吧， 1、先创建账号模板，不同后端机器使用不同的管理员账号，一般来讲偷懒就是dev环境就用一个哈哈。 图中的自动推送，肯定是在这个图中是不能选的啦，因为推送本质上jumpserver应该就是用的ansible，所以要先有root权限才能推的，或者做了key登入理论上分析这样的没错的。 顺便看看自动推送 然后看 这里并没有，所以相关性有待继续学习 2、然后新建账号推送 图中的无需再次输入密码有待验证，因为少了一个老版本里的托管密码按钮。 老版本的逻辑也是类似，就是一个密码的自动生成，自动托管，什么意思，就是上图红色字表达的，你不用二次输入密码，不管是不是图中的按钮具体意思，总之这里的开发意图都是如此。因为我也是没有学完，这里直接下结论也是为了更好的梳理，要敢于下结论，不管他的具体作用是什么，一定是为了什么。要有这个态度和眼光，就好比机房精密空调，不管怎么个精密，一定是机房的需求--送风一定要到位，到什么位置，到机柜底部送来冷风，对吧。从最终结果同样也是最初需求出发，准没错。咦~怎么有点像 我不管过程，我就要结果，我日TMD。。。 再来一个账号推送，这个是测试账号 1和2 两步动作，相当于 普通账号有了，特权账号也有了，下面就开始关联后端资产了。 所以前文的操作，就是 新建账号模板-且都是特权；新建账号推送-且都是普通。思路倒是OK的，也是从需求出发的，特权的用来连接后端机器，普通账号就是推送过去，挺好的，就不知道界面里的 账号模板里的 普通账号 自动推送怎么玩的，估计也是类似的逻辑吧。 3、新建资产 两台机器 192.168.126.133和134，分别作为开发环境 和 测试环境的机器 我再搞一个windows吧，恩，放到实验最后工作案例吧，也是我的初衷 然后就搞两个文件夹出来 好 能推送啥哦，不就是后端机器的普通账号咯。总感觉该产品的这里逻辑设计还不够简单。 选择立即推送后的界面无明显变化 联想到要推送的就是普通账号，于是去 自动化-账号推送里看看，应该是后面要补充下资产来明确往什么机器推什么帐号了。 下一篇继续~~休息下 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:27:33 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/4.JumpServer实现管理服务器资产.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/4.JumpServer实现管理服务器资产.html","title":"4.JumpServer实现管理服务器资产.md","keywords":"","body":"第4节 JumpServer实现管理服务器资产 书接上回，为什么其他章节不怎么出现 书接上回，因为，此片和前一篇实乃一篇也~~ 强调初心 1、让xiaoming这个开发，通过xiaoming账号登入 jumpserver 2、然后再通过dev这个账号，去登入后端机器进行操作 当前133 和 134 后端机器上只有root，没有普通账号呢还 是需要jumpsever讲这两个账号分别下发到不同的机器上的 账号下发下去后，就涉及 “资产授权”了。就是好比网络ACL一样。 重点看这张图，我认为jumpserver的管理层次理解的线索就是这张图了👇 账号信息： 后端机器的账号 1、所有账号：事先在这些后端机器上创建好，比如windows的就只能如此，所以上图针对windows就要选择\"所有账号\"； 2、指定账号：后端机器上没有事先创建，这是事先在jumpserver里创建的普通账号模板，然后推下去的。而其是和网络访问权限一起配置的，这就很搓的一个开发思路，作者肯定是纯开发，没有深入干过网络工程师，没有这种网络上的清晰逻辑。我来开发，肯定拆开来弄。 而且 \"账号管理\"-\"自动化\"-\"账号推送\"，现在回过头来看，应该就是一个独立的模块，用来单独下发普通账号的。 其实如果我开发，只做一个功能，只有一个用户账号入口，就是 ①配置后端节点root账号：这是人家后端机器自带的，写进jumpserver里来； ②新建普通用：不管后端节点里有没有，这是你jumpserver层面来看的，都是新建；然后不要开放任何自动创建的按钮给用户，就是建好以后就会自动下发的，如果后端有这个用户，忽略报错，如果没有就真的新建。但是密码就不要说了，所以如果后端节点有，你要提示冲突，让人工介入（①要么改后端密码②要么jumpserver这里换用户名） ③SSO：只选择用 各人登入jumpserver的用户，来登入后端机器。 3、虚拟账号：唉，别管了，就是跳板机而已，花里胡哨的。 所以他这个产品，推送后端机器账号出现在 ①独立的推送功能 ②账号模板里的自带推送 点击上图的同步更新账号信息后👇，其实我也不知道他在同步个啥，root密码给人家改掉？可以的。 这个更多是模板修改后同步到调用模板的账号里去。关键是上图的那个\"自动推送\" 明天再点开看看。感觉是所有调用该模板的账号的关联后端资产上账号的同步了。应该是的。这些东西我应该不会去一个个实际测试了，我是有多闲啊~~~ ③开通跳板机上用户权限的时候的推送 ④ 推你马勒戈壁的推送 所以你是一个球，任何球面都能进入到球心对吧，你就是个球。 继续操作 选择开发权限里的推送吧，所以需要在用户模板里去创建普通用户，不过那里好像关联资产，哦，如果关联资产了倒是可以在用户模板里直接自动推送了。 表达不准确，账号模板只是用来创建账号， ①账号模板里的\"同步更新账号信息\"，就是会讲所有用该模板创建的账号，所关联的后端节点的密码更新推送了。见上图第二张图也就是②账号模板里的自带推送段落里的图。 ②账号里的\"立即推送 \"到了后端机器。 ③账号模板里不点击\"同步更新账号信息\"，也可以让开通权限的时候选择账号模板，那里也可以自动推送，见上图咯就是上面的最近的一张图。 下图👇是上面的②账号里的 立即推送。 这样应该就是说，👆账号界面自带的推送了。 1、创建账号模板的普通用户 在此之前，已经完成了 ①jumpserver的用户组和用户的创建 ①特权用户模板 ②资产的创建 ③注意：没有进入到\"账号列表\"去创建账号，这里的账号都是自动生成的。反正本次实验是的。 继续在创建一个test，这样两个普通账号模板就好了 2、资产授权 选择 文件夹 或者 单个资产 ，然后 创建，这就是对文件夹或者单个机器 授权 然后编辑：针对什么组成员，访问 什么 资产组 ，使用什么账号连接后端资产，账号是模板里的还会涉及自动下发。 点击确认，立刻就会把这个 dev 👆 也就是 调用的账号模板里的账号，当然我这里是普通账号，下发到上图的资产节点里，所谓节点就是文件夹，就是dev账号下发到所有文件夹里的机器上。于是👇之前id dev还看不到，现就就有该帐号了 同样把test用户针对test资产的授权给做了 3、测试不同用户的操作 在此之前总结下前面的操作步骤 ①jumpserver的用户组和用户的创建 ②特权用户模板 ③资产的创建 ④注意：没有进入到\"账号列表\"去创建账号，这里的账号都是自动生成的。反正本次实验是的。 当然后期维护，也是可以从任何地方新建的，这个就是你对jump理解到位了，就很灵活了。是好事，就是有个新手适应期了，呵呵，我不太爱这种表达方式，不过大家都讲，比较好沟通。 授权的就是dev和test组，里面成员情况如下 使用王麻子这个dev成员登入jumpserver发 发现操作权限有问题 点击连接进去发现只有SFTP 授权没问题， 找到了，是资产那里没有给出来，就是说资产本身就没有定义ssh这些东西，只定义了sftp，所以轮不到授权说好话。 直接改成ssh就行 再次登入wanglin账号dev组的 点击连接 进去就是看到SSH 这就OK了 同样看看test组的，也就是134机器的 也OK 4、审计功能 这些操作，审计员，或者管理员，反正就是有审计权限的账号，就可以进入审计台 就能看到操作录屏了，这就是你要的了👆 上面是录屏，下面是cli的记录 操作录屏下载后， 下载后需要jumpserver提供的播放器 exe下载安装 我就不装了，TMD还报错风险提示 5、发现风险剔除动作 1、先模拟一个正在操作的状态 王麻子这个dev进来正在操作 优化下 好看点 中午休息下~~~ 此时在审计平台里就可以看到在线会话， 此时审计这边可以点击终断，操作者那边就断开了 但是在连又能连上了 这他喵的终断有个什么意义。 后期需要API介入直接断开和取消其用户的控制权限 6、事先过滤掉风险CLI 1、创建风险命令组 去验证下alias一个mv=rm能否跳过这里的禁用措施 哪些是风险CLI呢 1、常规众所周知类：rm、reboot 2、跳板动作类：ssh、telnet、mstsc远程rdp、vnc？ ssh这种也是高危哦，因为可以再次跳到其他机器上 rm reboot shutdown ssh telnet alias 未完未补充.... 2、调用上面的CLI组，进行命令过滤 生效了没呢，在线用户需要踢下线，让其重新登入才能保证生效 此时用户的ssh连接被终端，重新连接ssh就行了，这里其实就是用户与后端机器的连接断一下就行了。 通过alias来变动也不行，直接交互式的alias mv='rm'是不行的，但是vi进去是可以的👇 诺，所以alias也是高危CLI 但是你说去禁止掉alias，其实一样，下图就是jumpserver禁止掉了alias，但是人家vi进去，然后运行. .bashrc，这样就同样实现了alias，但是又没有运行alias，所以还是可以删除文件。 所以只是说帮助使用者协助他不让其删除，这是站在使用者也不想删东西的角度的，如果他就想删，vi总不能不给吧，所以录屏时候追责就行了。 ​ 此外，除了bashrc里的alias，还有vi进去写脚本，同样限制不住。 然后再看看cli方式的跳板机操作 jumpserver的cli管理台是2222端口来着 使用jumpserver的登入用户王麻子登入，就用cmd来吧 输入Enter后再输入1就连接到那个序号的host了 打问号？显示帮助信息 下一篇，就梳理管理DB之类，最后案例再弄管理windows，别最后，这里直接弄windows吧，正好工作上用得到 工作案例-jumpserver-跳windows 1、首先需求 数据不能落家里去 访问内网就用RDP windows的密码在用户自己手里，也就是堡垒机登一次，后端windows再登一次 2、开搞 1、创建账号 先创建组 再创建用户，并归属上面的组 3、配置后端资产 在资产列表里，DEFUALT里右键新建节点 内网RDP 其实就是个文件夹 仔细👆看图，有个细分的windows-RDP，大概率就直接用它了。 因为资产添加的时候没有指定jumpserver上的账号，所以\"账号列表\"里\"资产树\"里看不到的 只有带上账号信息的资产，才会出现在账号列表里👆 4、配置资产授权 针对整个内网RDP文件夹 进行 授权 其实我觉得这两个都可以，反正都是内网机器上的账号 测试 首次登入修改密码 再次登入后 试下 一直灰的啊👆 注意看图 所以回过头去修改吧，上面的猜测是错的 修改为👇 此时界面发生了变化 但是一直转圈圈啊.... 去容器里面telnet 3389试试 选择web整个模块进去👆 然后修改apt为国内源 root@jms_web:/opt# sed -i 's|http://deb.debian.org/debian|https://mirrors.tuna.tsinghua.edu.cn/debian|g' /etc/apt/sources.list root@jms_web:/opt# sed -i 's|http://deb.debian.org/debian-security|https://mirrors.tuna.tsinghua.edu.cn/debian-security|g' /etc/apt/sources.list root@jms_web:/opt# 通的啊 为啥半天连不上呢 妈的好像是账号输错了，不是这个原因，账号错误应该报错账号的，结果账号改对了也不行。 是平台选错了，全部改成windows是可以的，否则192.168.25.70选择windows-rdp和windows-tls都不行，只能是windows，然后全部改成windows也是可以的。 好了，三台不同windows都好了👇 其他零碎的: 这个是window版本不同导致RDP过去的报错，资产里面 平台改成windows或者windows-TLS就行了👇 插播GPTCF验证的地址 内网放行GPT，不断跳转CF验证需要放行这个URL，这个URL可以说是闪现了，所以一般抓不到 针对这种，完整的方案，好像之前也有👇可以参考下 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:27:45 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/5.JumpServer管理数据库资产和批量导入导出.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/5.JumpServer管理数据库资产和批量导入导出.html","title":"5.JumpServer管理数据库资产和批量导入导出.md","keywords":"","body":"第5节 JumpServer管理数据库资产和批量导入导出 1、先准备一个mysql作为后端资产 1、ubuntu sed apt update apt -y install mysql-server vim /etc/mysql/mysql.conf.d/mysqld.cnf #bind-address =127.0.0.1 #mysqlx-bind-address = 127.0.0.1 # 也可以用sed 直接注释掉127.0.0.1的行 sed -i '/127.0.0.1/s/^/#/' /etc/mysql.conf.d/mysqld.cnf systemctl restart mysql # 检查端口监听是不是不再是127了 ss -tlnup # 看到*:3306 就可以了 mysql mysql>create database wordpress; mysql>create user wordpress@'192.168.%.%' identified by '123456'; mysql>grant all on wordpress.* to wordpress@'192.168.%.%'; 2、红帽系统 sed yum -y install mysql-server systemctl enable --now mariadb mysql mysql>create database wordpress; mysql>create user wordpress@'192.168.%.%' identified by '123456'; mysql>grant all on wordpress.* to wordpress@'192.168.%.%'; mysql>create table wordpress.t1(id int); mysql>exit 2、jumpserver上配置DB 然后授权，可以利旧的 不过要改一下里面的 授权账号 上图👆二选一，要么所有账号，要么精确到wordpress账号 然后用 王麻子测试 不好意思mysql的IP地址写错了。。。 改掉就好了 这里可见一个命名规范👇 去改掉 因为这个连接jumpserver是一进来就是wordpress这个数据库里的了👆。 反正就是mariadb的提示符不太友好，CLI界面也就是2222端口连接jumpserver也是一样 然后已经可以操作了，但是连接性还是显示错误，显然界面刷新或者显示判定有问题，其实是OK的。 大概知道了，可能要👆手动点一下三个点里面的 测试 一下就可以了。 看看mysql提示符一样，不咋地👇，不过连接性都是能过够正确显示了。 资产的批量添加 手动新建一台windows后端资产 然后导出来当作模板 再ctrl+c ctrl+v 写多个机器 然后上传就行了 就是节点也就是文件夹的id注意下 同步的按钮的作用 看下 1、模板的信息修改 2、然后调用模板的所有地方不会自动同步的，需要手动的 3、修改和同步操作备忘 此时 资产列表 和 账号列表 里的 用户都不会自动更新的 手动进去模板里进行同步👇 此时 资产列表 和 账号列表 里就更新了 推送账号了解下 我推1-自动推送账号一般就是普通账号-看下 使用场景：用在单独下发账号的时候 用root去ansible推的 这里就是要给独立的推送账号的模块，不过前提是 root 这个特权账号要先配到到推送的机器上👇 我推2-还可以这里推 使用场景：首推这种使用方式，就是从源头就定义了要推下去的。 创建模板的时候就定义号自动推，就是谁用我这个模板，谁就自动推， 谁是谁，谁就是资产咯 确认后，等10s钟 就退下去了，这是模板里自动退了，谁用了这个模板，谁就自动得到了 我推3-就是上一篇讲的 资产授权里 也可以推 使用场景：这个就是从碗底吃饭的人爱用的方式了，我上一篇也是这么用的。 这个上一篇写过了，这里就简单看看上图就行了 我推4-账号列表里推 使用场景：也是修改的时候推的，就是不是一个从无到有的过程，而是不走模板，直接创建账号的时候，关联资产了，确认了，然后在外面的界面里也可以推 10s不到，也推下来了 网域列表 就是你的资产在互联网上，很多个地方，你不希望搭建jumpserver 1 2 3 那么多台，于是可以直接利用网域网关的逻辑去实现一个jumpserver搞定多地的资产跳登。 https://docs.jumpserver.org/zh/v4/guide/admin/asset/domain_list/?h=%E7%BD%91%E5%9F%9F 好像也没说啥，自己走一遍看看玩 一 、弄网域 1、创建网域 2、创建网关列表 点击进到上面创建的网域里，创建网关列表 找了一个我的QQ云上的机器的公网IP 此时netArea001这个网域里就有个一个网关， 1、当然这里可以创建多个GW，所以才叫网关列表，好比腾讯云这个网域里自然有多个VPC，一个VPC一个GW接入； 2、然后将需要跳板连接的机器也就是资产 去 \"资产列表\" 里创建好， 3、最后关联到这些资产到对应的网关里就行了 好实操开始 二、补资产 1、创建文件夹归类 2、创建资产 这里变通下，就用网关本身的内网IP充当后端资产就行咯👇 上图👆的账号就新增一个root测试。 三、回到网域里 1、关联资产 👆上图连接性错误，也好理解，是QQ云上的内网IP，资产那边怎么可能是OK呢。 测试下， 也不知道是通还是不通。。。因为如果走GW，就会通的。。。 四、资产授权 五、测试-OK~ 看图👇是说gw不可用 报错鸟，我的是2208，GW的SSH那里改下👇 顺便测一下GW 再次测试，看图👇是说gw dial 去连接资产失败 哦，因为GW的内网IP，实验代替了后端资产，所以一样要改SSH为2208，👇改掉 搞定~~ 掌声在哪里~~ 不过连接性 还是显示不对，不管了 跳板机的登入都好了，显示问题，或者这里的联通测试 没有走GW去测 不管了。 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 11:28:02 "},"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/6.Haproxy介绍和多种安装方法.html":{"url":"42-Docker容器技术和堡垒机JumpServer实战/Docker资源限制和堡垒机JumpServer/6.Haproxy介绍和多种安装方法.html","title":"6.Haproxy介绍和多种安装方法.md","keywords":"","body":"第6节 Haproxy介绍和多种安装方法 概述 lvs 四层代理，转发性能强 nginx 4层和7层，主要还是7层，且转发性能一般；nginx应该说是专业的web服务器。 HAproxy都行，都是高性能，还有WEB界面。 HAproxy的HA还得靠keepalive那么HAproxy的HA是啥呢，是后端调度的冗余--是后端节点的HA调度。 HAProxy本身的HA还是用keepalive来做的。LVS的HA同样也是keepalive，nginx的ha也还是keepalive。 图中HAProxy可以用LVS代替，就是用来做4层转发；图中KA就是keepalive缩写。 右边的nginx 套nginx/tomcat，前面的nginx就是做7层转发，后面的nginx就是web了。 这图就是参考一下 HAProxy简介 LVS： 4层转发效率最高，因为c---lvs----ser, lvs在中间是不产生两个段tcp的。 HAProxy：4层转发效率虽然比nginx高，但是比lvs低，因为c---HAproxy----ser，HAProxy在中间是拆包后重新与server建立tcp了，前后是两段连接。 也就是说HAPorxy转发的流量，不管是4层还是7层，都是改变了数据包的源IP地址的。所以和nginx一样还要做真实IP的透传。 四层 LVS: Linux Virtual Server Nginx: 1.9版本之后 通过 stream模块实现 HAProxy：High Availability Proxy mode tcp 不支持udp? 七层 HAProxy 通过 mode http 指令 Nginx 直接写在 http模块 应用场景 四层：Redis mysql rabbitmq memcached等 七层：nginx tomcat apache php 图片、动静分离、api等 其实就是一个GUI带来的好处。 官网 1、免费社区版 https://www.haproxy.org/ 用TLS，stable别用，号称稳定也别用。 2、收费企业版 https://www.haproxy.com 安装HAProxy 红帽系列 yum直接就可以安装，但是版本较低 容器也可以安装，但是这是做边缘网络转发的，不推荐使用容器，容器本身就是有性能损耗的。 新版本的安装 debian/ubuntu有直接的安装包，其他的要么是容器，要么就是自己编译了 编译安装 1、前置软件Lua https://www.lua.org/start.html lua -v yum install -y gcc readline-devel curl -L -R -O https://www.lua.org/ftp/lua-5.4.7.tar.gz tar zxf lua-5.4.7.tar.gz -C /usr/local/src cd lua-5.4.7 make all test make install # 官网上竟然没有这一条... # -R就是时间设置为remote-time就是往上文件的时间，而不是当前本地的时间 [root@realserver2 ~]# curl --help all |grep -E '^ .*-R' -R, --remote-time Set the remote file's time on the local output 下面就是有R没有R的区别 这就安装好了 2、编译安装HAProxy Rockylinux: # HAProxy 1.8及1.9版本编译参数： make ARCH=x86_64 TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 PREFIX=/usr/local/haproxy # HAProxy 2.0以上版本编译参数： yum -y install gcc openssl-devel pcre-devel systemd-devel zlib-devel curl -LRO http://git.haproxy.org/?p=haproxy-3.0.git tar xvf haproxy-3.x.x.tar.gz -C /usr/local/src cd /usr/local/src/haproxy-3.x.x/ # 查看安装方法 ll Makefile cat READEME CAT INSTALL # 参考INSTALL文件进行编译安装 make clean # 注意如果将来需要被Prometheus监控的话就要加上USE_PROMEX=1 make -j 4 ARCH=X86_64 TARGET=linux-glibc USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_PROMEX=1 USE_LUA=1 LUA_INC=/usr/local/src/lua-5.4.7/src/ LUA_LIB=/usr/local/src/lua-5.4.7/src/ make install PREFIX=/apps/haproxy ln -s /apps/haproxy/sbin/haproxy /usr/sbin/ tree /apps/haproxy/ # 源码包安装，还需要弄一个services文件，以及起送services的cfg配置文件 vim /usr/lib/systemd/system/haproxy.service [Unit] Description=HAProxy Load Balancer After=syslog.target network.target # ExecStarPre是与启动，就是启动前的检查，如果不行就不会启动了 # USR2是自定义的信号，haproxy拿来reload了应该 # 这里service文件里加了haproxy.pid文件所以haproxy.cfg配置文件里就不要再指定haproxy.pid了 [Service] ExecStartPre=/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -c -q ExecStart=/usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /var/lib/haproxy/haproxy.pid ExecReload=/bin/kill -USR2 $MAINPID LimitNOFILE=100000 [Install] WantedBy=multi-user.target ------------------------- # 编写haproxy的配置文件 mkdir /etc/haproxy vim /etc/haproxy/haproxy.cfg global maxconn 100000 chroot /apps/haproxy stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin #uid 99 #gid 99 user haproxy group haproxy daemon #nbproc 4 #cpu-map 1 0 #cpu-map 2 1 #cpu-map 3 2 #cpu-map 4 3 #pidfile /var/lib/haproxy/haproxy.pid 不用配置，配置反而提示冲突 log 127.0.0.1 local2 info defaults option http-keep-alive option forwardfor maxconn 100000 mode http timeout connect 300000ms timeout client 300000ms timeout server 300000ms listen stats mode http bind 0.0.0.0:9999 stats enable log global stats uri /haproxy-status stats auth haadmin:123456 #listen web_port # bind 0.0.0.0:80 # mode http # log global # server web1 127.0.0.1:8080 check inter 3000 fall 2 rise 5 # 后端服务器的检查 ------------------------------------------------ # 准备socket文件目录 # mkdir /var/lib/haproxy # 下面useradd的时候带上-m就行了，而且用户和用户组还精确到了haproxy，这里就不要敲了 # 创建用户和用户组 useradd -r -s /sbin/nologin haproxy -d /var/lib/haproxy -m # -d /var/haproxy本来是多此一举，但是-m加上去就很完美了，啊，你说不用-r ，也不用-m，那不行，那样就会自动创建邮箱了。这也算一个固定用法了吧。 # 检查配置文件语法 haproxy -c -f /etc/haproxy/haproxy.cfg # 加载service文件 systemctl daemon-reload # 启动 systemctl enable --now haproxy 过程记录 报错处理，make的报错，一般就是要安装xxx-dev 发现并不是dev而是zlib-devel 最后安装oK，👇但是没有配置文件的。 就一个二进制文件，所以写个软连接就行。如果文件夹下很多个二进制，则将这个文件夹的目录加入PATH变量。 PATH变量 然后配置文件在源码包里的example目录下有案例的 纠错 -r -d就是多此一举？是的，不管是家目录还是mail都不会创建，所以没必要-d👇 kill USR2是啥 有点小问题好像 注释掉pid文件，已经后端健康检查的那段就行 然后打开haproxy的管理界面 URL哪来的？cfg里配置的👇 进入网页后： ubuntu: # 安装基础命令及编译依赖环境 apt update && apt -y install gcc make libssl-dev libpcre3 libpcre3-dev zlib1g-dev libreadline-dev libsystemd-dev # 安装Lua方法1：包安装Lua apt update && apt -y install liblua5.x-dev # 安装Lua方法2：编译安装Lua cd /usr/local/src wget https://www.lua.org/ftp/lua-5.4.7.tar.gz tar zxf lua-5.4.7.tar.gz -C /usr/local/src cd lua-5.4.7 make all test make install # 官网上竟然没有这一条... # 或安装系统自带的Lua #👇 https://haproxy.debian.net/#distribution=Ubuntu&release=jammy&version=3.0 apt update # apt-get install --no-install-recommends software-properties-common # add-apt-repository ppa:vbernat/haproxy-3.0 # apt-get install haproxy=3.0.\\* # HAProxy 1.8及1.9版本编译参数： make ARCH=x86_64 TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 PREFIX=/usr/local/haproxy # HAProxy 2.0以上版本编译参数： yum -y intall gcc openssl-devel pcre-devel systemd-devel tar xvf haproxy-3.x.x.tar.gz -C /usr/local/src cd /usr/local/src/haproxy-3.x.x/ # 查看安装方法 ll Makefile cat READEME CAT INSTALL # 参考INSTALL文件进行编译安装 make clean make -j 4 ARCH=X86_64 TARGET=linux-glibc USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_PROMEX=1 USE_LUA=1 LUA_INC=/usr/local/src/lua-5.4.7/src/ LUA_LIB=/usr/local/src/lua-5.4.7/src/ make install PREFIX=/apps/haproxy ln -s /apps/haproxy/sbin/haproxy /usr/sbin/ tree /apps/haproxy/ #!/bin/bash #只支持离线下载源码安装，不支持在线下载安装 HAPROXY_VERSION=3.0.3 HAPROXY_INSTALL_DIR=/apps/haproxy STATS_AUTH_USER=admin STATS_AUTH_PASSWORD=123456 HAPROXY_FILE=haproxy-${HAPROXY_VERSION}.tar.gz LUA_VERSION=5.4.7 LUA_FILE=lua-${LUA_VERSION}.tar.gz SRC_DIR=/usr/local/src CWD=`pwd` CPUS=`lscpu |awk '/^CPU\\(s\\)/{print $2}'` LOCAL_IP=$(hostname -I|awk '{print $1}') VIP=192.168.126.100 MASTER1=192.168.126.132 MASTER2=192.168.126.133 MASTER3=192.168.126.134 . /etc/os-release color () { RES_COL=60 MOVE_TO_COL=\"echo -en \\\\033[${RES_COL}G\" SETCOLOR_SUCCESS=\"echo -en \\\\033[1;32m\" SETCOLOR_FAILURE=\"echo -en \\\\033[1;31m\" SETCOLOR_WARNING=\"echo -en \\\\033[1;33m\" SETCOLOR_NORMAL=\"echo -en \\E[0m\" echo -n \"$1\" && $MOVE_TO_COL echo -n \"[\" if [ $2 = \"success\" -o $2 = \"0\" ] ;then ${SETCOLOR_SUCCESS} echo -n $\" OK \" elif [ $2 = \"failure\" -o $2 = \"1\" ] ;then ${SETCOLOR_FAILURE} echo -n $\"FAILED\" else ${SETCOLOR_WARNING} echo -n $\"WARNING\" fi ${SETCOLOR_NORMAL} echo -n \"]\" echo } check_lua_file (){ if [ ! -e ${LUA_FILE} ];then color \"缺少${LUA_FILE}文件!\" 1 curl -L -R -O https://www.lua.org/ftp/lua-${LUA_VERSION}.tar.gz color \"${LUA_FILE}文件已下载\" 0 else color \"相关文件已准备!\" 0 fi } check_haproxy_file (){ if [ ! -e ${HAPROXY_FILE} ];then color \"缺少${HAPROXY_FILE}文件!\" 1 exit else color \"相关文件已准备!\" 0 fi } install_packs () { if [ $ID = \"centos\" -o $ID = \"rocky\" ];then yum -y install gcc make gcc-c++ glibc glibc-devel pcre pcre-devel openssl openssl-devel systemd-devel ncurses-devel libevent-devel readline-devel elif [ $ID = \"ubuntu\" ];then apt update apt -y install gcc make openssl libssl-dev libpcre3 libpcre3-dev zlib1g-dev libreadline-dev libsystemd-dev liblua5.3-dev lua-devel else color \"不支持此操作系统!\" 1 fi [ $? -eq 0 ] || { color '安装软件包失败,退出!' 1; exit; } } install_lua () { cd ${CWD} check_lua_file tar xf ${LUA_FILE} -C ${SRC_DIR} LUA_DIR=${LUA_FILE%.tar*} cd ${SRC_DIR}/${LUA_DIR} make all test make install } install_haproxy(){ check_haproxy_file cd ${CWD} tar xf ${HAPROXY_FILE} -C ${SRC_DIR} HAPROXY_DIR=${HAPROXY_FILE%.tar*} cd ${SRC_DIR}/${HAPROXY_DIR} if [[ ${VERSION_ID} =~ ^(7|8) ]] ;then install_lua cd ${SRC_DIR}/${HAPROXY_DIR} make -j ${CPUS} ARCH=x86_64 TARGET=linux-glibc USE_PROMEX=1 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 USE_LUA=1 LUA_INC=${SRC_DIR}/${LUA_DIR}/src/ LUA_LIB=${SRC_DIR}/${LUA_DIR}/src/ PREFIX=${HAPROXY_INSTALL_DIR} else install_lua cd ${SRC_DIR}/${HAPROXY_DIR} color \"HAPROXY编译安装开始...\" 0 make -j ${CPUS} ARCH=x86_64 TARGET=linux-glibc USE_PROMEX=1 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1 USE_CPU_AFFINITY=1 USE_LUA=1 fi # mkdir -p ${HAPROXY_INSTALL_DIR} make install PREFIX=${HAPROXY_INSTALL_DIR} [ $? -eq 0 ] && color \"HAPROXY编译安装成功\" 0 || { color \"HAPROXY编译安装失败,退出!\" 1;exit; } [ -L /usr/sbin/haproxy ] || ln -s ${HAPROXY_INSTALL_DIR}/sbin/haproxy /usr/sbin/ [ -d /etc/haproxy ] || mkdir /etc/haproxy #[ -d /var/lib/haproxy/ ] || mkdir -p /var/lib/haproxy/ cat > /etc/haproxy/haproxy.cfg /lib/systemd/system/haproxy.service /dev/null && color 'HAPROXY安装完成!' 0 || { color 'HAPROXY 启动失败,退出!' 1; exit; } echo \"-------------------------------------------------------------------\" echo -e \"请访问链接: \\E[32;1mhttp://${LOCAL_IP}:9999/haproxy-status\\E[0m\" echo -e \"用户和密码: \\E[32;1m${STATS_AUTH_USER}/${STATS_AUTH_PASSWORD}\\E[0m\" } install_packs install_haproxy start_haproxy 以上脚本已调整，并检查ok 工作案例-CMD脚本-CDN地址观察 检查CDN的IP是否稳定下来了，就是不再出现有问题的那个IP 1、站点A，ping测丢包严重，发现43打头的哪个ip地址 2、发现是cdn地址， 3、dig xx.xx.xx +short 4、ipconfig /flushdns 刷新 5、再次dig xx.xx.xx +short 后发现没有43开头的那个ip地址了 6、使用cmd循环测试下，确实没有43.开头的那个ping丢包验证的IP了。 for /L %i in (1,1,10) do @dig hd.zxxxxxx.com +short |findstr \"43\" && timeout /t 1 >nul 工作案例-jumpserver使用注意 我是直接把jumpserver挂外网的，直接访问是jumpserver做了安全限制，报错如下 处理方法如下 再重启就行了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-06 12:26:41 "},"43-HAProxy/43-HAProxy.html":{"url":"43-HAProxy/43-HAProxy.html","title":"43-HAProxy","keywords":"","body":"43-HAProxy Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"43-HAProxy/反向代理服务Haproxy/反向代理服务Haproxy.html":{"url":"43-HAProxy/反向代理服务Haproxy/反向代理服务Haproxy.html","title":"反向代理服务Haproxy","keywords":"","body":"反向代理服务Haproxy Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"43-HAProxy/反向代理服务Haproxy/1.Haproxy的全局配置和性能优化及日志管理.html":{"url":"43-HAProxy/反向代理服务Haproxy/1.Haproxy的全局配置和性能优化及日志管理.html","title":"1.Haproxy的全局配置和性能优化及日志管理.md","keywords":"","body":"1.Haproxy的全局配置和性能优化及日志管理 全局配置 而这个配置文件的指定也是自己写的service文件 然后配置文件里整体分为两大块：全局和代理 chroot有点类似os.chdir吧，就是修改工作目录的，不过change root就是修改工作目录的根了，ls /实际就是chroot后面的目录了，主要是安全。 deamon 后台运行，如果将来制作为容器，就要拿掉这行，前台运行。 stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin process 1 1、首先mysql也好，docker也罢，都是通过socket和服务进程通信的，socket可以是tcp/ip套接字，也可以是套接字文件。 2、文件就是600权限 3、level admin，是运行通过这个socket文件对其服务进行admin级别的管理 4、process 1就是，只存在和主进程的通信或者主进程下的第一个子进程通信？ 5、还可以通过相关指令 把后端服务器上线、下线。 uid gid 这个应该是创建haproxy这个用户的时候指定的id了，不过我没有指定id，而是指定用户也行的。 ，貌似是用来执行worker进程--我理解就是和nginx一样的进程咯： 不指定就是root运行了 看看多线程和CPU绑定 nbthread vs nbproc nbproc就是和nginx一样多个子进程了 nbthread就是进程下的多线程了。 2.5之前的haproxy用nbproc开启多个wokers进程，这个worker和nginx一个意思 之后的版本就用nbthread来做多个worker，不过没有看到多个worker，因为多个worker也是跑在一个子进程下(33740 pid)的，所以ps aux只会看到一个root 跑起来的haproxy主进程和一个haproxy用户跑起来的一个子进程 而多个线程可以通过pstree -p去观察到，一般就是N个cpu，就开启N-1个线程来着 3个线程的配置nbthread 3，就是开启2个线程，同样是一个子进程下开启的 即使CPU实际就2个，同样可以开到8个线程，反正就是绑在一个子进程下的 这是nginx的worker进程，不过id应该是发生变化了，导致找不到用户了，这个不用管，此图就是看下nginx也是多个worker来着。 不是找不到用户，是docker运行的 不是找不到用户，是docker运行的就这样 haproxy好像是花括号，也就是说haproxy是 线程在跑的，和主线程享用同样的内存空间。 而nginx是子进程再跑的，不是和父进程一个内存空间。 这还有一个区别 而haproxy因为是用的多线程，所以看到了3个线程开在了一个74172下，而74173和74174和74175看不到，应该是开在74172这个子进程里的内存空间了。是共用的内存空间，所以进程ID不会再次显示出来了。 haproxy就是一个总入口，硬盘要求不高不存在存数据的关键要求除非你开log？，正常就是CPU内存要给多谢，转发数据代理这块。 然后由于是多线程的，不知道CPU的使用率会不会出现盯着一个CPU干的情况。。。 所以就有了线程绑定cpu的配置 cpu-map auto 原来2.5之前的版本是进程，就用cpu-map 1 0 (# 1 woker 进程 和cpu0进行绑定，多个就写多行 ) 这种方式来绑定进程和cpu； 现在2.5之后的版本只能是线程，所以的用cpu-map auto: 1/1-8 0-7 (# 1-8的worker 线程和0-7的cpu分别绑定 ) 来绑线程和cpu了 上图是2核，开了8个线程，然后都是在一个子进程下的。尝试绑到cpu上去 通过systemctl status haproxy可见，看不全？键盘左右键可以看咯👇 找了半天没找到找个日志文件，但是除了上图的方法看全，还可以这么导出来看全 👆这是journalctl -u haproxy看的，和systemctl status haproxy一样看不全， 但是可以导出来 journalctl -u haproxy |tail -10 > xxxxx 人家说要么帮帮好，要么不绑，那么问题来了，不绑的话你会自动分担到多个cpu核上吗? 观察CPU绑定的情况 ps axo pid,cmd,psr 和 ps axo pid,cmd,psr -L -L Show threads, possibly with LWP and NLWP columns. -L 用来显示线程信息的 还可以带上user 突然发现子进程没有用haproxy跑啊 看看配置文件 果然👆 改改 重启服务后就好了👇 即使不手动绑定CPU，也是会自动分担的 就是效率肯定没有绑过去稳定吧，因为要利用L1和L2 缓存还是要绑的，否则CPU一旦飘走就无法利用之前那个CPU的1级和2级的缓存了。 奇葩👇我只能称之为未知渲染shell 没用的，grep不出来，lscpu |tee >> xxx ，标题内容也会丢掉的。 lscpu |awk '{print $0}' 一样看不到标题，分析....肯定有其他程序参与进来了，lscpu的输出一定是经过了其他处理工序了， 呵呵呵，解释来了，小飞棍来咯~ 1、在rocky里 敲lscpu看到的东西 为什么 比 lscpu > xxxx ，然后看xxxx的东西多出标题行 2、因为lscpu的本质是，用户通过shell和系统调用交互的，通过系统调用拿到的结果再返给shell后，shell再次 渲染 拿到的东西 3、而lscpu > xxxx ，是用通过shell和系统调用交互其结果直接再系统调用层面重定向到了xxxx里，并没有返回经过shell 渲染，所以结果就不同了 4、画图意思意思：lscpu是有去有回，terminal--->shell--->sys.call--->内核，完了再返回给termnal lscpu > xxxx，是terminal-->shell-->sys.call-->内核--->硬盘，返回不经过shell，也就没有渲染了 lscpu |tee ，同理返回的路径也不同了，走的tee，没有走lscpu的shell的渲染了。 也就是shell针对不同的cli肯定有不同的渲染了。 同理 lscpu |awk '{print $0}' 也没有lscpu本身的标题栏，应该也是shell只针对lscpu做了输出渲染。 我就这么分析了 怎么滴，要不你给一个合理的解释，我反正没有资料，也不会查，我就爱用渲染这个词怎么地。fuck 做个压力测试知道pstree少一个线程 while true;do ps axo user,pid,cmd,psr -L |grep haproxy |grep -v color;echo ;sleep 1;done for i in {1..10};do ps axo user,pid,cmd,psr -L |grep haproxy |grep -v color ;echo ;sleep 1;done ab 压力测试 如果不绑cpu就会飘的 L1 L2缓存利用不上了基本上，绑一下就不会飘了 有个疑问，pstree -p看到线程好像少一个，不对，就是2个，ps axo -L看到的一个主进程，然后一个pid子进程下的三个线程？对不上啊。 权威点的查看进程ID下的线程👇可见就是3个,pstree -p看的竟然不全啊--不是不全就是少一个，估计算到进程头上去了。 还可以用pidstat命令去查看 yum -y install sysstat pidstat -p 41185 -t 然后curl 可以带密码的 [root@realserver2 ~]# curl http://haadmin:123456@192.168.126.132:9999/haproxy-status maxconn 每个haproxy进程的最大并发连接数 maxssiconn 每个haproxy进程ssl最大连接数，用于haproxy配置了证书的场景下 maxconnrate 每个进程每秒创建的最大连接数，这属于新增的速度限制 spread-checks 后端server状态check随机提前或延迟百分比时间，建议2-5(20%-50%)之间，默认值0 nginx默认的后端server检查叫做被动检查，就是请求来了，转发的当口之前吧，看看后面的server提供的服务是否正常，如果NG，就不转过去了，或者转到其他server上。当然也可以用第三方插件实现主动检查。 而haproxy是主动检查，比如3s一次检查，那么问题来了，如果后端100台机器，那么3s一次，就会100个请求一次性发出去了，流量存在一个高峰现象，所以要随机差值来错开检查动作时间点，不能扎堆~ ​ 这个值建议设置成3，就是实际发送检查的时间区域再2.7--3.3之间。考虑一个问题 ​ 第一次检查 和 第二次 检查 相隔3秒，然后 有错开时间，可能导致第二次检查往前错开和第一次检查的往后错开重叠，那么3s的 50%最大了就是下图黄线代表50%的随机错开时间。50%已经就会出问题了，所20%是保证错开一点，45%才是最大值了。下图绿线框框代表30%的错开时间 pidfile 指定pid文件路径 log 127.0.0.1 local2 info 定义全局的syslog服务器，日志服务器需要开启UDP协议，最多可以定义两个。 info应该是日志级别对应常识里的debug, info, notice, warn, err, crit, alert emerg这些。 local2是啥？ local2是基础设置，rsyslog里也有的；可以参考👇，总体就是什么\"设施\"--local2的什么日志，发送到127.，然后再用rsyslog接收进一步处理？估计是的，不是估计，就是，haproxy的local2这个自定义设备记入info日志然后，发送127的514端口，然后rsyslog从514接收这个日志，再通过local2.info来取出日志。 ==================== 不过一般haproxy作为总入口，也不推荐记录日志，减轻压力主要是I/O消耗，然后日志记录，坐在后端服务器上，后端服务器如果是nginx、tomcat这些各自记录日志。 ​ 其实实际应用如果压力不大，也可以开启日志，没啥问题。 开启配置方法 # 本机记录日志 log 127.0.0.1 local2 info # 远端机器也记录日志 log 192.168.11.117 local2 info #然后在本地的以及远端机器上的rsyslog里配置，打开下面两行 module(load=\"imudp\") # needs to be done just once input(type=\"imudp\" port=\"514\") # tcp应该不用开了吧 module(load=\"imtcp\") # needs to be done just once input(type=\"imtcp\" port=\"514\") .... # 再加一个行 local2.info /var/log/haproxy/haproxy.log #然后重启haproxy和rsyslog [root@realserver2 ~]# systemctl restart haproxy rsyslog #远端一样操作夏 # 最后别忘了chmod 下 mkdir /var/log/haproxy/ chmod a:haproxy:w /var/log/haproxy 然后ubuntu配置是单独放置的 最后效果如下，能看到日志就行了，然后要注意/var/log/message下也是有的，理由就是cfg里他在前面也能接收到。 然后抓以下udp的包，问：tcpdump可以抓udp吗？阔以的 [root@realserver2 ~]# tcpdump -nni lo udp [root@realserver2 ~]# tcpdump -nni lo udp port 514 # 都行 工作案例 1、办公网络的IP冲突，如何知晓， 首先dhcp日志先弄出来 linux的dhcpd的日志处理 1、配置dhcpd的日志 vi /etc/dhcp/dhcpd.conf log-facility local6; #为local6自定义设备，local1-xxx都是设备名称 2、配置rsyslog vi /etc/rsyslog.conf # Save dhcpd messages also to dhcpd.log local6.* /var/log/dhcpd.log # 以下配置是移除操作，local6不再往message文件发送， # Don't log private authentication messages! *.info;mail.none;authpriv.none;cron.none;local6.none /var/log/messages 3、重启dhcpd和rsyslog服务 systemctl restart dhcpd systemctl restart rsyslog Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-30 18:01:56 "},"43-HAProxy/反向代理服务Haproxy/2.Haproxy实现defaults-Listen-frontend-backend代理配置.html":{"url":"43-HAProxy/反向代理服务Haproxy/2.Haproxy实现defaults-Listen-frontend-backend代理配置.html","title":"2.Haproxy实现defaults-Listen-frontend-backend代理配置.md","keywords":"","body":"2.Haproxy实现defaults-Listen-frontend-backend代理配置 前面的小结介绍了haproxy的全局的一些配置，下面正式接收proxy本身的代理配置 Proxies配置 defaults就是默认配置，可以写明对哪些组name，生效为default配置。 default配置 redispath比较重要的，但是判断服务器挂掉是否靠谱都带进一步研究。 abortonclose这个，服务其负载高了，断掉当前队列处理比较久的连接，这个比较久的判断又是啥？可能不是太靠谱的参数机制。 http-keep-alive，有人说是会话保持；应该不对，我理解：是长连接的意思，不是会话保持；长连接主要是为了节省tcp建连三次握手的开销。 同样连接建立在那，一直不用也有问题，于是就有了下面的timeout http-keep-alive 120s的老化时间。就是引入一个措施，必然也会带来一个新的问题，于是还要解决那个带来的问题才能差不多做好事情。这是宇宙法则； forwardfor，保留客户端真实IP，这个提到保留一词，基本上就是因为proxy做了两头欺骗，就是两段TCP连接了，所以才有打上真实IP字段的说法。 mode http | tcp ，指定默认是哪层代理。如果默认使用http，那么配置文件下文的具体的比如mysql的代理就要特别写上mode tcp来代理3306了。 ​ 比如安装后的haproxy的状态页👇的语句块里的mode http就可以删掉，因为可以用default里的7层代理。 timeout connect 120s 客户端请求从haproxy到后端server最长tcp连接超时时间(tcp连接之前的等待时间)，默认单位ms timeout server 600s 客户端请求从haproxy到后端server请求处理超时时长(tcp连接之后的数据处理响应超时时间)，如果超时会出现502错误，建议设置大点，防止502报错。默认单位ms timeout client 600s 设置haproxy与客户端的最长非活动时间，建议与timeout server相同。默认单位ms。 timeout check 5s 检测后端服务器的超时时间。 default-server inter 1000 weight 3 看着像一个整体性的设置，1000ms一次的健康检查，语句块里的服务器们都是权重3。 查看帮助去官网就行 https://docs.haproxy.org/3.0/configuration.html#3 然后安装后倒是有一个man帮助，不过看要这么看 haproxy里的frontend就是nginx和http里的vip和端口；Backend就是Realserver后端服务器。 配置文件里2中写法 1、frontend和Backend分开来写，实现多对多的效果 2、如果一个前端就对应一个后端，那么就写成listen的格式也就是前后端写在一个语句块里。 https://docs.haproxy.org/3.0/configuration.html#4.1 这就是一些指令只能放在一些地方👆。 listen的写法，就是前端后端写一起 vim /etc/haproxy/haproxy.cfg # 文末加上 listen www.ming.com bind 0.0.0.0:80 server web1 192.168.126.133:80 server web2 192.168.126.134:80 # 保存后reload systemctl reload nginx haproxy可以reload是因为当初写了service文件 nginx也有reload的👇 haproxy配置bind后就监听相应的端口了 LVS那边是内核监听，ss -tlnup是看不到的；而haproxy是七层APP，监听的端口，ss 是可见的。 此时就可以测试了 可见调度默认还是轮询的，1：1的。 然后观察后端日志也可以看到是两个tcp 然后ss -nt看看长连接，不过配合大文件+限速看最佳，还要注意ss -nt可见即使wget文件完成了，最终tcp断开要继续晚个20s左右的。 wget的cli限速和nginx那头限速👇 [roo@realserver2 haproxy]# wget --limit-rate=1k 192.168.126.132:80/test 此时haproxy和后端服务器的长连接就一直都在的，什么时候client下载完了，什么时候haproxy和后端的长连接就没了 这点和nginx是不一样的。nginx是快速从后端拉下资源(应该是缓存在本地，不过这个缓存不是每次都能利用的，好像每次client发起都要来一遍，关于缓存参见nginx缓存章节38章第一节里有详细讲解)，然后慢慢的发给client的。 将haproxy改成nginx，同样下载测试 nginx的相关配置 然后测试下 负载调度OK 也是默认1：1 下载测试 1、client自己限速 2、观察nginx和后端realserver的tcp，发现很快就没有tcp的连接了，然后client还在那慢慢下着。 3、结论： nginx和haproxy存在 下载缓存的区别 ①client wget --limit-rate 限速下载 ②如果是nginx，那么限速只发生在client---nginx代理之间，后端到nginx代理是飞速的也就是不限速的很快就下完了。 ​ 然后，如果是nginx 代理配置里做限速，是client---nginx-----realserver，nginx中间代理上做的限速，效果一样的，后端到nginx的资源下载都是飞速就下完了。 ​ 这是实测的结论：就是client----nginx----realserver, client自己限速 或者 nginx配置server块里配置了limit_rate来限速，都是限制的client到nginx之间的速度，而nginx从realserver上的资源是不限速的； ​ 然后nginx有缓存功能吗？默认是没有的，需要配置参见38章第一节内容，client再次发起下载，同样nginx代理要往后端再次重新请求资源的，时间都是一样的。 ③如果是haproxy，client---haproxy----后端服务器，client 限速请求资源，haproxy和后端的资源请求一样是慢慢下载的，不会先飞速下载来然后慢慢发给client，这点和nginx不一样。 ④所以表现形式就是，nginx代理，你client发送一个请求，然后在后端上看tcp连接，可能就看不到了，因为很快就完成了nginx和后端服务器的交互了。而haproxy是一直可以在后端服务上看到tcp连接的，因为什么时候client下完，什么时候haproxy和后端服务器的tcp连接什么时候才会断开。 ubuntu加网卡，激活操作 [root@realserver2 docker]# haproxy -c -f /etc/haproxy/haproxy.cfg frontend和backend分开来写 业务复杂，多对多就需要用frontend和backend，简单一对一就用上面的listen配置块就行了 frontend www.ming.com bind 192.168.126.132:80 use_backend web_servers backend web_servers server web1 192.168.126.133:80 server web2 192.168.126.134:80 报错修改👇 看后端日志可见 行首的IP是haproxy的ip，行尾的ip是真实用户的ip，为什么后面几行不显示了，因为haproxy那边的配置取消了转发真实IP的语句 测试四层代理 #搞两台mariadb yum -y install mariadb systemctl enable --now mariadb # 有的os会只监听127，所以要注释掉 sed -i '/127.0.0.1/s/^/#/' /etc/mysql/xxxx/xx/xx mysql -e \"create user test@'192.168.126.%' identified by '123456'\" # 检查下是否创建 mysql -e 'select user,host from mysql.user' #在修改haproxy的相关配置 listen msyql bind 192.168.126.132:3306 mode tcp server mysql1 192.168.126.133:3306 server mysql2 192.168.126.134:3306 然后连上去了，要知道通过haproxy连的是哪个后端机器 首先修改后端机器的hostname，哈哈 先修改，再重启mysql 然后再通过haproxy连mysql， 此时就可以知道是连的那一台了👆。 还可以通过server_id来得知 [root@mysql-1 ~]# mysql -e 'set global server_id=101' [root@mysql-2 ~]# mysql -e 'set global server_id=102' [root@realserver2 ~]# for i in {1..10}; do mysql -e 'select @@server_id' -utest -p123456 -h192.168.126.132;sleep 1;echo ;done 说明4层负载也ok了，同时也是轮询的👆。 改写成forntend和backend写法 frontend mysql bind 192.168.126.132:3306 mode tcp use_backend mysql_servers backend mysql_servers mode tcp server mysql1 192.168.126.133:3306 server mysql2 192.168.126.134:3306 前后端都要加mode tcp 测试ok Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-30 18:03:13 "},"43-HAProxy/反向代理服务Haproxy/3.Haproxy代理的健康性检查和子配置等基础配置.html":{"url":"43-HAProxy/反向代理服务Haproxy/3.Haproxy代理的健康性检查和子配置等基础配置.html","title":"3.Haproxy代理的健康性检查和子配置等基础配置.md","keywords":"","body":"3.Haproxy代理的健康性检查和子配置等基础配置 haproxy的健康检查 如果不做健康检查，调度还是会往down掉的后端调度 502 网关不能访问 504超时 503服务挂了 backend web_servers server web1 192.168.126.133:80 check server web2 192.168.126.134:80 check 重新加载配置，此时就会调度到down掉的后端server上了 于此同时，我打开了所有后端的check，所以页面上可以看到一些检查状况 通过打开后端的对应服务，就可以修复啦 不过由于这些都是4层检查，所以很容易就被欺骗 比如关闭nginx服务，用nc -l 80打开监听80，此时haproxy的健康检查依然判断为正常状态。 默认check的周期是 默认检查周期是2s一次 backend web_servers server web1 192.168.126.133:80 check inter 5000 server web2 192.168.126.134:80 check inter 5000 backend web_servers server web1 192.168.126.133:80 check inter 5000 fail 5 rise 5 server web2 192.168.126.134:80 check inter 5000 fail 5 rise 5 # 这TM就要25s才能判定出来故障，就灰常不好的参数配置了 # rise 5 次 inter 5000ms，也就是 25s才判定起立，这也不好，优化为 backend web_servers server web1 192.168.126.133:80 check inter 1000 fail 2 rise 10 server web2 192.168.126.134:80 check inter 1000 fail 2 rise 10 #1秒一次检查 #2秒钟故障可切换到好的服务 #10次检查ok才判定故障服务节点恢复了，这个要花20s了。 加上权重 backend web_servers server web1 192.168.126.133:80 check inter 1000 fall 2 rise 10 weight 3 server web2 192.168.126.134:80 check inter 1000 fall 2 rise 10 页面上显示，3:1的权重显示效果如下👇 backup备用 就是所有后端server都挂了，就让backup标识的机器上线 这里就让haproxy自己作为一个backup备用nginx服务，以备不测 注意：由于haproxy占用了80，所以自己做backup站点的nginx服务就改成8080好了。 这个backup的使用场景是啥呢，可以是道歉服务器，就是正常业务都挂了，这个页面就写上sorry xxxx backend web_servers server web1 192.168.126.133:80 check inter 1000 fall 2 rise 10 weight 3 server web2 192.168.126.134:80 check inter 1000 fall 2 rise 10 server web_backup 127.0.0.1:8080 check backup 测试，当后端server都挂了后，sorry server(也就是backup server)就出来啦 disable禁用后端服务 backend web_servers server web1 192.168.126.133:80 check inter 1000 fall 2 rise 10 server web2 192.168.126.134:80 check inter 1000 fall 2 rise 10 disabled server web_backup 127.0.0.1:8080 check 这样第二台后端节点就不会被调度了 maxconn并发的数值 全局数值 只找到一个10000并发的，看来这是针对的frontend前端的设置。后端的1000没找到，估计是默认值，是前端➗10的关系。 注释掉后，就看到默认值了是一个奇怪的49973，然后后端并发自动变化4998，好像是10:1的关系。 单台数值 重定向redir backend web_servers server web1 192.168.126.133:80 check inter 1000 fall 2 rise 10 server web2 192.168.126.134:80 check inter 1000 fall 2 rise 10 maxconn 20000 redir http://www.bing.com server web_backup 127.0.0.1:8080 check 当调度到第二台的时候就会redirect bing.com了 上面是单个的，下面全局的 backend web_servers redirect prefix http://www.bing.com server web1 192.168.126.133:80 check inter 1000 fall 2 rise 10 server web2 192.168.126.134:80 check inter 1000 fall 2 rise 10 maxconn 20000 server web_backup 127.0.0.1:8080 check 这样全部后端节点都被重定向了。 https也是可以的 网站配置放到子文件里去 一个维护思路，就是 主配置文件放一些全局的配置就行了；站点配置放到独立的子配置文件，一个站点放一个。 然后主配置文件里删掉frontend、backend、listen的站点，当然除了初始的listen stats可以留着。 然后要让haproxy识别到自己定义的conf.d这个子配置文件夹，CLI里有类似的方法👇 不过我希望能有像nginx那样的include xxx/xx/xx/就好了。 还是按haproxy自带的来吧，在service文件里加上-f /etc/haproxy/conf.d/这个文件夹路径 然后 [root@realserver2 conf.d]# systemctl daemon-reload [root@realserver2 conf.d]# systemctl restart haproxy 关于配置文件后缀只能用cfg 观察状态页发现msyql前后端没了 改回去就恢复了 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-30 18:03:22 "},"43-HAProxy/反向代理服务Haproxy/4.Haproxy使用socat工具管理和静态调度算法.html":{"url":"43-HAProxy/反向代理服务Haproxy/4.Haproxy使用socat工具管理和静态调度算法.html","title":"4.Haproxy使用socat工具管理和静态调度算法.md","keywords":"","body":"4.Haproxy使用socat工具管理和静态调度算法 socat工具 用来发送消息给socket文件 这里help看到是仅仅haproxy.sock支持的指令，不是所有socket文件支持这种方式。 通过haproxy的socket查看和配置 [root@realserver2 conf.d]# echo help | socat stdio /var/lib/haproxy/haproxy.sock The following commands are valid at this level: abort ssl ca-file : abort a transaction for a CA file abort ssl cert : abort a transaction for a certificate file abort ssl crl-file : abort a transaction for a CRL file 。。。 。。。 set severity-output [none|number|string]: set presence of severity level in feedback information 。。。 show backend : list backends in the current running config 通过这个比如可以查看连接数 1、wget --limit-rate 限速查看长连接 2、ab 并发来制造连接数 查看后端服务器的状态 查看调度权重 注意：3(initial 3) 就是现在是3，最初也是3的权重。说明可能会变化的。比如通过socket修改掉。 echo \"get weight web_servers/web1\" |socat stdio /var/lib/haproxy/haproxy.sock echo \"set weight web_servers/web1 10\" |socat stdio /var/lib/haproxy/haproxy.sock 此时就通过sock修改了调度的比例 测试如下，确实是10比1的调度👇 这个haproxy就是可以通过sock去修改，而不用修改配置文件再reload， 而实际操作：reload不生效的时候也不报错，接着restart就报错； 如果配置文件修改ok，直接reload就很nice 下线上线：weight 0 就是停机维护了 这种方式的切换都不会出现503报错 1、如果是后端服务节点挂了一个，此时健康检查一般设置也就是1秒 2次 就判定为故障也差不多了 2、而用socat 传cli给sock的方式，业务切换丝滑流畅 不过也对，故障检测本来就是有一个判断过程，而socat属于强制下线切换自然无缝。 然后将所有后端节点都设置为0的权重，此时就会调度到backup节点了 这是主配置文件 这是子配置文件 这是services文件，要利用起来子配置文件，就需要再service里修改i的 但是haproxy -c -f 这个cli只能检查主配置文件的语法了就 多线程haproxy下socat发送的情况 ​ haproxy本来是多进程的不像现在是多线程的，所以以前存在这么个情况： ​ 不同的链接请求也是落在不同的进程上的。所以socat发送信号实现需求的时候不能直接👇这么发送了就。 ​ 需要修改配置文件，将sock和进程绑定，此后socat发给哪个socket就是哪个进程了。 ​ 而现在haproxy优化为多线程了，一个socket就是对应一个进程，然后多个线程都是在这个进程下的，所以你往一个socket发送就能发送到全部线程了。其实本质还是一个进程下的所有线程共享内存空间的。 静态算法和动态算法 cli： balance https://docs.haproxy.org/3.0/configuration.html#4.1 静态算法：不关心后端服务器负载状态，什么连接数，资源是否富裕，不管统统，就定死了怎么调度； 动态算法：会根据后端服务器的负载情况，动态跳转调度机制。 haproxy里的静态算法机制，不支持 上面提到的echo \"get weight web_servers/web1 0\" socat stdio /var/lib/haproxy/haproxy.sock这种用法的。 也不是一点都不支持，可以改成0，就是仅支持上线下线。 看来默认就是动态算法，支持set 其他数值。 cli的补充： 小实验：上下线后端服务器的脚本： 两台后端服务器改为docker 132本身也是haproxy，简单模拟用户curl下 简单的web升级脚本👇 #!/bin/bash WEB_SERVERS=\" 192.168.126.133 192.168.126.134 \" BACKEND=web_servers APP=nginx IMAGE=$APP:latest DELAY=15 for i in $WEB_SERVERS;do echo \"set server $BACKEND/$i state maint\" |socat stdio /var/lib/haproxy/haproxy.sock ssh $i docker rm -f $APP ssh $i \"echo Docker $i WEBSITE $i v1.0 > /data/www/index.html\" ssh $i docker run -d -p 80:80 -v /data/www:/usr/share/nginx/html --name $APP $IMAGE sleep $DELAY echo \"set server $BACKEND/$i state ready\" |socat stdio /var/lib/haproxy/haproxy.sock done 还得做一下ssh的key登入 ssh-keygen ssh-copy-id 192.168.126.133 ssh-copy-id 192.168.126.134 验证key登入 再修改以下配置文件 将名称改为ip，这样上面的切换脚本就可以用了 关于平滑升级 nginx本身的平滑升级 关键字：-USE2，-WINCH 其实官方早就针对Nginx平滑升级做足了功夫，基本原理就是，启动新的Nginx(master+worker)进程，之后给旧的master进程发送-USER2指令，这样就能同时让新版和旧版本进程同时接收处理请求。之后我们再发送-WINCH给旧进程，让它停止工作服务(关闭所有旧worker进程，但是旧的master进程没关，防止后面你遇到问题回滚). 如果确认新Nginx没问题，那么再手动Kill旧的master进程即可完成平滑升级. https://mojun.me/2022/09/20/nginx-reload-in-prod/ https://bbs.huaweicloud.com/forum/thread-0245111487078556005-1-1.html 开始测试实验 1、模拟用户长连接 2、查看旧版本1.24的进程信息 主进程ID是1682，子进程ID是1683和1684，两个子进程也是从配置文件里里worker_processess auto;来的，因为2核CPU，所以auto成了2个worker进程。 查看旧版本的编译参数，当然我这里是yum的，不过也有编译的残留，有点尴尬 可能是用的yum，编译的没用 这就能说明是用的yum的了吧，rpm查到这个bin文件是来自包的。 这段无所谓，是我自己的环境不干净导致的，不过查确认就这么查 yum的nginx的编译参数就多了 3、二进制文件备份 mv nginx nginx.old 4、准备好新版本 编译参数就用常规的就好，如果是老的nginx是编译安装就直接nginx -V那里复制过来就行了，yum的没必要，就是用常规的参数就行。 cd /tmp/ curl -LO https://nginx.org/download/nginx-1.26.1.tar.gz tar xvf nginx-1.26.1.tar.gz -C /usr/local/share/ cd /usr/local/share/ ll cd nginx-1.26.1/ # 这里的prefix可能要修改，第一次我没改也成了奇怪。。 重启服务的时候 nginx二进制里只会认/apps/nginx这个根目录，会找不到很多log和cfg文件的 ./configure --prefix=/apps/nginx \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-pcre \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module make # make install没必要，只需要一个二进制文件 # 只需要一个obj/nginx 二进制文件其实 echo $? # 以上初步安装好了新版1.26的nginx 5、拷贝新的二进制nginx，覆盖之前旧版本的nginx 之前已经备份了第三步，所以直接用新的nginx二进制放到老的nginx路径下，这里用软连接 上图备份了老版本的nginx二进制文件，然后从仅仅make出来的objs/下复制过去nginx新版本的二进制文件。 此时环境变量已经ok了 已经是新的nginx了 6、发送关键信号-USR2 kill -USR2 1682 # 既可以升级，也可以降级，还可以同级别新建主进程 # 这条cli敲下去，会负载均衡到两个版本的nginx的，只不过编译的二进制nginx的prefix就是设置的老版本的nginx的主目录，所以这里负载均衡的效果就看不清了，因为页面都是一样的，这也是为什么不用make install的原因，只是用了二进制，如果测试就要看负载均衡的到两个版本的效果，可以make的prefix到别的路径，然后在make install一下看看，明儿弄吧。 好像不是，配置文件路径 不会变。。。 发送kill -USR2 xxx(旧版nginx主进程)进行替换，此时会多出一个nginx主进程和2个子进程 而且从图上看，新的nginx高版本的主进程貌似挂载老版本呢的主进程下的。 此时 client 用户的下载长连接没有断 下载进度还在 测试都是访问的老的nginx的页面，肯定的，启动的配置文件路径都是老的，内容也一样。 7、发送-winch信号，停止旧版本接收新的请求 确认升级成功，-WINCH 信号 停止旧版master接收新的请求(此时旧版本nginx master进程没死，只是停止接收新的请求) 8、回退 9、quit 主进程支持的信号 TERM,INT 立刻退出 QUIT 等待工作进程结束在退出 KILL 强子终止进程 HUP 重新加载配置文件，使用新的的配置启动工作进程，并逐步关闭旧进程 USR1 重新打开日志文件 USR2 启动新的主进程，实现热升级 WINCH 逐步关闭工作进程 工作进程支持的信号： TERM，INT 立刻退出 QUIT 等待请求处理结束后再退出 USR1 重新打开日志文件 下面是高版本做了makeinstall的实验 这里有个反代的缓存开了，所以进程利厄多了两个cache。 然后准好备好make install的nginx 1.26.0版本 然后用户做一个长连接也就是限速下载，再做一个循环curl 注意编译的新版本的nginx的prefix是 --prefix=/apps/nginx ，所以一定和现在的老版本的nginx的工作路径是不一样的， 所以配置文件首当其冲的不同，于是首页也不同了，借此可以观察kill -USR2后的新老两个版本nginx同时工作的效果--负载分担 然后： 备份老版本的nginx二进制 复制新版本的nginx二进制进去 在kill -usr2前先看下进程 发现 cache loader process没了，不管 敲入kill -USR2后发现新版本的主进程挂载了老版本的主进程下👇但是不知道为什么配置文件的路径没有变为/apps/nginx， 此时用户那边看看 老样子？？？是啥，因为版本升上去了，而且配置文件的路径还是老的，自然不会出现期待的负载分担了。 再降级降回去 再升回去 此时 原来的长连接的端口都没变过，就是没断过了 加上此时长连接的用户完成了数据交互，老的进程就会自动退出了 这么看难道编译的nginx二进制里是不带那个什么prefix=/apps/nginx的？还是被systemctl给接管了 对的，就是这个意思。去掉-c /etc/nginx/nginx.conf，用老版本nginx二进制还能systemctl restart nginx起得来，新版本就起不来了。 新版本的nginx启动后，默认就是走的/apps/nginx的 重新编译为老版本的prefix 还是有一些问题 不过有进一步进程，说明就是编译的时候没有指对配置文件路径👆仔细看/etc/nginx/conf/nginx.conf是不存在， 虽然--prefix一样，但是yum的nginx的cfg是在/etc/nginx/nginx.conf比你编译的一层级的。 再来，不烦了，直接复制 报错了精简下，把后面的--with-cc-op全部删掉试试 所以总结下 1、nginx的平滑热升级注意事项 ①只需要二进制nginx文件， ②编译的时候要用原来nginx -V看到的参数，可适当删减，比如yum的nginx -V那个你编译可能会卡很久，上图删掉最后一大段单引号的选项就行了。测试OK； ②如果原来也是编译安装的，那么正好，就用原来的-V的编译参数全盘复制过来。 ②如果编译的时候参数不同，也会被systemd的service文件接管，-c就会改了你的配置文件，所以也不会遇到问题 ③如果两个新老版本都是cli启动的，那么问题就来了，-c 不指定，就是默认编译的时候的配置文件了，此时就会导致问题。 ④你肯定希望版本升级，业务文件配置文件数据文件不变，所以就是要用老的版本呢的编译选项 ⑤但是如果你就是要两个版本呢不同的工作路径，做出来复制分担的效果，下午继续 试下用cli启动nginx，两版本不同页面的热升级负载效果 1、先启动nginx，这是新版本了 目录在/apps/nginx下，页面就是 2、覆盖掉nginx二进制，用老版本 此时长连接挂起来，while curl继续 又出现了kill 不出来效果了 好像必须是systemctl启动的方式才会接收kill -USR2信号 那就这样，用修改后的service文件来启动吧 这样就是nginx二进制里当初编译的prefix是啥就用啥了，也能接收USR2信息了 然后关闭所有nginx进程，再用systemctl start nginx启动， 此时是nginx 1.24版本 下面准备好1.26版的二进制， 此时终于看到了 新 老 两个 nginx的负载分担效果 整体看来还是新的nginx进程多点 长连接也不会断 然后发送winch信号 不工作的worker子进程都直接关闭，还在长连接的就shutting down 此时ss -nt 观察 长连接还是不变的 quit敲一下 关闭client端的长连接，就当下完了吧 结果nginx 所有进程没了。。。哈哈哈，惨败~~每次都能做出不一样的bug哦~ 再来 发送USR2后，两个版本同时工作，负载不均衡分担 同时长连接不断端口孩子啊 发送WINCH给主进程，此时长连接依然不变，但是请求调度不在发给被切换的版本了，这里是降级操作了。从1.26切到1.24的。 关闭client的下载，观察nginx进程，发现存在两个主进程， 关闭被切换的主进程，希望这一次不会全部关掉 失败。。。。 看看哪里出问题了。 用人家本来的也就是yum的service文件没问题 研究下 1、上面为啥那么-QUIT全部kill掉了 2、如何curl的时候看到当前版本呢？就是测试不用不同工作目录，而是用同一个service里面指定的配置文件去看nginx的版本 其实我还可以这样 就是中间插入一个手动修改service文件的过程 估计就是PIDFile的问题 注释掉PIDFile哪一行 再试试 果然👆 但是一旦没有这个PIDFile，就没法实现平滑切换，就会报错，就是QUIT发过去，全部就kill了。 解决1、上面为啥那么-QUIT全部kill掉了这个问题 就是PIDFile两个新老nginx版本要一直，才好USR2吧，我来试试 希望这是最后一次测试，不过最开始的已经可以实现需求，只不过涉及yum安装cli安装，kill -USR2没效果，两个nginx的根目录以及配置文件和页面不共用看到负载分担，等等这些问题都要解决才折腾了这么久，各位看官 你觉的我多此一举，NONONO，各位看官~ 你细听分说 暗杠： 这江山风雨 岁月山河 刀光剑影 美了多少世间传说 且看他口若悬河 衣上有风尘 却原来是一位江湖说书人 那天山女子 独守枯城 也只是为了曾经的那一个人 好了，继续。。。 找到了👆 修改service文件 进入源码路径，make clean， 这样还是新建根目录，但是pid文件和老版本共用👆 make一下，不要make install 这样两个版本的nginx二进制就准备ok了， 长链接挂起来 curl 循环起来 所以这次实验就是关键PIDFile 切换nginx二进制文件 发送USR2 此时长连接不会断，循环curl可见负载分担 发送WINCH 是循环curl就已经全部负载到新的nginx版本上了 最后一步 发送QUIT 失败。。。 我要离开地球。。。。 改成这样试试 奇怪没有去我以为的地方生成PID文件啊，而是配置文件抢先了。。。 好继续，老子又有信心了 注销点/apps/nginx/conf/nginx.conf里的pid那行 好了这下 新老nginx的pid文件一致了 确认下service文件配置，还是改成这样试试👇 好了，继续测试 长连接和curl 二进制文件的处理 USR2发送 长连接不断👆，负载分担观察👇 发送WINCH curl的不在负载到被切换之前的版本，以及长连接不断 发送QUIT 终于搞定了~地球我还是回来吧~~~ 再切回1.26.0去，同理了就~~~， 以上就是切回1.26的擦欧总 总之： 1、生产中，就不要这么麻烦，直接 编译的时候 复制老版本nginx -V的所有参数，可能要略作删减，就是报错了就删掉没必要的一些，特别是老版本是yum的参数会过多 然后处理二进制文件 然后usr2 然后winch 最后quit 就搞定了 由于新版本编译也是老的编译参数，所以负载分担看不到效果，其实只是我不会看，而已，实际就是usr2发过去，两个版本同时工作， winch发过去，老版本nginx 不处理新的请求， quit发过去，处理完老版本的请求连接后，就退出了，就将新版本的master进程独立出来了。 2、我做成两个版本独立配置就是看看而已的， 本来两个版本独立是 最后一步quit发过去，就所有nginx进程都退掉了， 所以才需要修改service文件，而修改的关键就是去掉-c /etc/nginx/nginx.conf配置文件，同时将新版本的pidfile 和 老版本的pidfile 保持一致，才保证最后quit信号不会杀掉所有的nginx进程的。 3、研究下如何实现正儿八经生产中，也就是不修改service直接第1点中 如何观察新老版本的负载效果 开始操作 1、当前service，当前nginx版本如下 2、测试手法 长连接挂起来 3、开始降级，当然是热降级 处理二进制 其实不是上图说的\"不看看到1.24.0出来\"，可以看到，观察久一点，还是可以看见的 他是一阵子全部转给1.26，一阵子全部转给1.24，观察久点就看到了，试过两遍了，确实要等很久才会看到负载分担地效果，这个比不同地页面来的慢很多。 长连接不会断 回退操作 一样啊，回退就当作降级操作，一样的，不过好像有HUP的操作，不管了。 长连接挂着，从升级到回退都没断 明天研究下 升级 回退 长连接始终不断地操作，上面只是研究了 升级 降级 长连接不断地操作。 HUP好像只能回滚 新开地子master进程。明天再弄 回滚好像又可以了， 分析是QUIT以后就无法使用HUP回滚了 有个长连接挂着，所以有个shutting down👆 一旦quit了确实就不能hup回滚 就只能等用户长连接自己结束了，再去做平滑降级了。 因为quit了就不会再HUP打开子woker进程了，没法回退了，只能做回降，不过回降，也不会导致用户长连接断啊，明天继续 结论：所以热升级最后一个QUIT是不要敲地，可以保持时刻HUP回滚。 如果敲了HUP，是不是可以再在子master下挂master？试试，不行，不能多层挂下去， 就是一个maser挂再挂一个master，所以quit不能乱敲，要保证随时可以hup回去。 工作案例 https://docs.github.com/en/copilot/managing-copilot/managing-github-copilot-in-your-organization/configuring-your-proxy-server-or-firewall-for-copilot 开发内网copilot放行和自动关闭git push的措施 nginx后端节点提供服务的平滑升级 haporxy后端节点的平滑升级 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-30 18:03:35 "},"43-HAProxy/反向代理服务Haproxy/5.Haproxy动态其它调度算法和状态页.html":{"url":"43-HAProxy/反向代理服务Haproxy/5.Haproxy动态其它调度算法和状态页.html","title":"5.Haproxy动态其它调度算法和状态页.md","keywords":"","body":"5.Haproxy动态其它调度算法和状态页 动态算法 roundbin算法 默认的调度算法 leastconn算法 类似LVS的WLC算法 random算法 其他算法： source算法 source : map-base取模法 这是source下的默认的算法(也就是配置了balance source，hash-type不配置也是map-based了)，反而不好 不能修改权重的，所以不能通过sockt修改👆，而下面的一致性哈希算法 是动态的，就可以修改weight值。 图中A就是源IP地址👆 ​ 但是随着后端服务器的增减，权重就会增减，会导致map-base取模的结果就不一样了，比如 上图 两台后端服务的权重是1和2，当前调度是hash(SRC_IP) 对3 取模，也就是0 1 2 三个结果，0就调度到后端服务器1，1 2 就调度到后端服务器2上去，如果新增一台服务器且权重是2，那么就是对5取模，结果就是 原来hash(SRC_IP) 对3 取模的结果比如是0，现在可能不再是0可能是3 那么对其调度就变了，而且此时整体调度都会发送变化，会话保持就保持不住了。 ​ 这里可以联系nginx的一致性哈希算法调度，是一回事。不管是分布式站点的调度，还是缓存穿透的防止都是一回事，都是用$request_uri consistent来解决的。 source : 一致性哈希 这是动态算法，就可以通过sockt修改权重了👇 测试source 一致性哈希算法调度 开始源ip不同，没有测出来还 再换别的IP来测 然后就出来了👇--不同的IP不一定调度到不同的服务器上，但是很多不同的IP一定可以得到效果，源IP 经过hash后再对 后端服务器的权重之后 取模，然后再调度，大概要了解成这样吧。 uri算法 只支持HTTP协议，不支持TCP，想来也是uri就是http报文里的内容了。TCP协议通常就是TCP头部去掉看TCP的DATA自然是看不到uri也就是链接文件目录的。 同样对于uri一样有一致性哈希算法调度。 就是uri访问的固定，就会调度到那一台服务器，也就和source 一致性哈希算法调赴 一样实现了 会话保持，或者叫缓存利用，或者叫防止缓存穿透，或者叫分布式站点的会话保持。 uri取模算法配置 uri一致性hash算法配置 测试uri一致性哈希算法调度 然后换一个uri 去curl 这就实现了不同uri，调度的效果，当然同src调度一样，不同的uri不一定就调度到不同的后端节点，但是大量的不同的uri一定会调度开来的。底层逻辑就是个2^32的环👇 再测试下 两个后端节点做10个页面出来 调度就是基于uri的了， 大家都是一样，只有第一个uri调度到134上，因为都是基于uri了，不管什么源IP都一样了。 也奇怪的，不过随意了 不对，uri要改成mode http来着，我没改！改之 没关系，我有default就是mode http，所以上面实验没有体现，但是实际也是http的，在主配置文件里设置了 正好去掉这个mode观察下 此时调度虽然写的是 但是由于uri必须是mode http，所以不会生效，就又变成了默认的roundbin算法了👇 url_param算法 uri里一般是一个xxx.php后面跟一个问好?key=value就好比sql语句里的where key=value查询一样 同理uri的东西肯定是要mode http的 url_param取模算法配置 这个userid就是uri里的key=value的那个key 如果key的值也就是value一样，那么调度到同一台机器，如果key的值不一样，那么调度到别的机器--当然可能还是同一台，但是key的量起来就能分开来了。底层还是hash(value)取模权重。 url_param一致性hash算法配置 上图userid就是key，不过可以是其他的key咯，总之你页面里？问号 问的是谁就写谁了。 实验要写php去查DB，所以不弄了，大概意思就是 测一下 不对，mode http没加 再测 改变username=测试效果，多改几次就出效果了，无非是hash和权重的鬼👇可能不同的uri_param也调度到一台机器的。 不过要具体做出?username=xxx的sql查询效果，就要做php和db库了吧，这就不折腾了。 hdr算法 复习下head头里的东西，curl 和 F12都可以看到 hdr取模算法配置实例 balance hdr(这里就写请求头 http head里的字段名)，比如user-agent(浏览器类型+版本信息)这些👇 测试下，实验都是配置了hash-type consistent哦，区别就是开启了一致性哈希算法调度，同时也支持调整weight值。其实如果后端服务器以及各自权重没有变化，开不开consistent没有区别，但是生产中肯定会有机器的增加的。所以是要开的。 发现测不出来 好说，改成循环测 出来了吧👆哈哈，发现其实是不均衡的👇 这个就可以实现手机和pc用户分流。 hdr一致性哈希算法调度配置实例 rdp-cookie算法 rdp-cookie取模算法配置实例 rdp-cookie一致性hash算法配置实例 算法总结 设置的时候10和10%的写法都可以，区别就是10是数值，10%是initial 100的10% 静态还是动态的意思就是 是否支持上面👆的方法：socat通过sock动态的设置weight值为非上下线的值。 静态：仅可以设置0%和100%。 动态：任何值，当然最大256咯。 然后支持tcp的可以尽可能配置成mode tcp，这样转发效率肯定高的 静态： static-rr # tcp/http first # tcp/http 动态： roundrobin # tcp/http leastconn # tcp/http random # tcp/http 以下动态还是静态取决于是否加上consistent关键字 source # tcp/http uri # http url_param # http hdr # http rdp-cookie # tcp 各种算法的使用场景 first # 使用较少，典型的QoS流量队列的饿死机制 static-rr # 做了session共享的集群 roundronbin random leastconn # 数据库 source # 基于客户端公网IP的会话保持 uri mode http # 缓存服务器，CDN服务商，腾讯云，阿里云 url_param mode http # 可以实现session保持 hdr # 基于客户端请求报文头部做下一步处理 rdp-cookie # 基于windows主机，还不错，就是说3台堡垒机，供远程使用。 以上lvs不能实现http协议的调度算法，比如lvs不支持uri nginx不支持url_param的算法，因为，nginx的hash key👇取不出来url里的?key=xxx；除非写代码把这个url里的?key=xxx转换成变量 这样nginx可能就能识别进行调度了。 然后还有nginx的ip_hash也是只取ip地址的前24位，和source不同，要实现source ip 要用nginx的hash或者haproxy的source。 HAProxy的高级功能 基于cookie的会话保持 会话保持的方法：①session绑定；②集群之间session复制；③session服务器 这里是cookie还是用的sesson绑定思路，包括上文的source ip也是session绑定的思路，就是同样的会话都往同一个服务器转发，这个同样的会话的判定就是①source ip②cookie。 配置选项 cookie本质就是一个键值对key,value，所以cookie WEBSRV insert nocache indirect的 WEBSRV就是key，而这个key对应的value，每个后端服务器都不同 在每行server 语句里定义，比如👇的cookie we1 cookie web2 配置示例 cookie调度的逻辑👇 1、client--->haproxy---->web1；与此同时haproxy打上cookie，然后通过set-cookie字段，返回给client，client收到set-cookie关键字，就会打上报文携带的cookie信息也就是key:value，也就是WEBSRV=web1。 2、下次client再次访问就会携带cookie，haproxy看到这个cookie就会转发对应的server。 前面的章节也有学过cookie，不过这个cookie是server自己给用户发的，显然不是这里的haproxy代理打的标签。使用场景不一样。 nocache👇 1和2就是用户，A和B是服务器 前面是haproxy CDN会缓存页面，包括cookie； 于是cdn发过来的请求统统都是cookie(WEBSRV=web1)了，也就会导致1 和 2 已经该CDN其实就是该第地区比如一个城市下的所有对应站点请求都转发到后端A服务器去了。 所以需要nocache。 哦，道理是这么个道理，但是我的问题来了，haproxy打的cookie为啥不是发给1 和 2 ，等等，然后确实是发给1 和 2，只是CDN也能看到也会缓存，对吧？好像是。 实验加深下理解 此时curl -Iv 可见cookie值了 循环测一下，看看是不是session绑定住了，也就是这个client都发往同一个后端了。 测试手法问题，client要携带这个cookie才能命中的啊，不然client都不携带，自然haproxy就会不断地set-cookie了。 该选项要生效，你client得携带~ curl 模拟保持 和 携带 cookie的方法 curl -c cookie.txt 192.168.126.132 curl -b cookie.txt 192.168.126.132 这是👇直接curl -b 自定义个cookie的方法： 携带cookie，haproxy就不会再发送set-cookie了这是携带的cookie，而不是haproxy发送的set-cookie 测试携带不同的cookie，绑定到不同的server，这就完成了session绑定 同样测试手法还可以这样，不是-b指定，而是-c 保持haproxy下发下来的，然后-b利用保持的cookie文件来测试 这样测试过程更加完整 改一下cookie看看效果 先是haproxy通过set-cookie发送给client 然后client就会带上cookie 这个然后的时间，就是下次client请求这个url的时候 自然就是刷新页面的时候啦 HAproxy状态页 状态页配置项 启用状态页示例 登录状态页说明 vim /etc/haproxy/conf.d/status.cfg listen status bind 0.0.0.0:8888 stats enable stats hide-version stats uri /status # 没有写status页的具体路径，所哟就是默认值，而默认值就是haproxy?stats stats refresh 10 stats admin if TRUE 如果用脚本处理curl请求地址需要用引号 引用起来 curl \"http://192.168.126.132:8888/status;json;norefresh\" curl \"http://192.168.126.132:8888/status;json;norefresh\" |python -m json.tool Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-09-03 10:01:48 "},"43-HAProxy/反向代理服务Haproxy/6.Haproxy常见功能压缩报文修改健康性检查.html":{"url":"43-HAProxy/反向代理服务Haproxy/6.Haproxy常见功能压缩报文修改健康性检查.html","title":"6.Haproxy常见功能压缩报文修改健康性检查.md","keywords":"","body":"6.Haproxy常见功能压缩报文修改健康性检查 haproxy如何携带用户信息 正常后端服务器看到的IP都是haproxy的IP地址 client IP就是192.168.126.133👇 后端server看到的就是haproxy中过来的IP地址👇 haproxy透传 1、option forwardfor 实现7层透传 一般写到主配置文件的里默认值语块就行了 请求 可见x-forward-for 后端nginx需要修改log格式才能看到这个参数 log_format的格式有个规律：就是比如，要抓取抓包里看到的x-forwarded-for字段，写到log里就要 1、http头部标识$http， 2、然后-换成_下划线， 3、全部小写 此时后端就可以看到真实的用户IP了👇 总结 需要注意的是，header 大小写，还有减号-，在后端nginx日志里都转成 小写，和下划线。 后端nginx日志里转小写和下划线👇 总结，补充： 2、四层透传-不推荐的方式，其实是mode tcp而不是lvs那种四层透传 #haproxy配置： listen web_http_nodes bind 192.168.126.132:80 mode tcp balance roundrobin server web1 www.testtttt.com:80 send-proxy check inter 1000 fall 2 rise 10 # 比常规的转发cli多一个send-proxy,会造成大量PROXY TCP4 a.b.c.d a.b.c.d xxxxx 报文 -------------------------------------------- #后端服务器的nginx配置：在访问日志中通过变量$proxy_protocol_addr 记录透传过来的客户端IP http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" \"$proxy_protocol_addr\"' server { listen 80 proxy_protocol; # 启动此项，将无法直接访问此网站，只能通过四层代理访问 server_name www.testtttt.com; } } 实操过程处理截图，含故障注意点： ​ 就是send-proxy的配置，要么server xxx 都配置上，要么都不配，否则会导致send-proxy配置的那个后端请求失败，可能是初始实验理解不到位，其实就是后端nginx没有配置proxy_protocol。继续往下看就行了。不要太纠结。纠结的话，实验敲一下就行了。 比如错误案例 haproxy的配置👇 对应的后端126.134的nginx的配置👇 不过好像无法复现了，现在上面配置也不会报错了， 可以复现，就是上图的listen 80后的proxy_protocol去掉，就能看到了(看到这些大量的haproxy 发过来的PROXY TCP4)，不仅如此，而且此时如果观察log可见，其实也已经可以携带了client IP了。👇只不过，就是页面还不正常。 同样抓包可见，需要知道的是上图除了192.168.126.1高亮的那一行是HTTP包👇，其他的都是TCP包--这些192.168.126.132频繁发给192.168.126.134的包是没有像下图一样携带什么IP信息来着。 比较好奇的是continuation关键字 然后，再在后端nginx服务，加了proxy_protocol后就不能直接访问了 只能通过proxy进行访问了，此时页面正常，但是log里就看不到这个真实的IP了，以及的频繁的PROXY TCP4 也看不到了--但是这个频繁的报文只是看不到，其实抓包可见还是一直再发送的 但是log和抓包都看不到continuation这个报文，以及真实的客户端IP也看不到了，不过只需要再来一步 就是log日志里写上$proxy_protocol变量就行了 此时log里有了 抓包依旧没有，抓包看来就是后端nginx配置正确了，抓包反而看不到了，哈哈，应该是有的，否则log里不可能看到。果然，抓包工具显示问题，再敲一下回车就出来了 但这种方式其实也是2个TCP，haproxy越是说做4层透传，LVS才是。 也可以两个方式都用上，就是mode tcp及支持x-forward-for，也支持proxy_protocol_addr 然后去后端nginx配置 看看log就两种携带真实客户IP的方式都生效了 报文修改 理论 代理可以修改的报文一般就是自己发出去的报文，也就是②和④两个包 http-request就是②的改包 http-response就是④的改包 老版本了解下就行： 实验 1、先看未修改之前的head 👆大段大段每秒一个的都是健康检测的，而拼手速抓到的就是192.168.126.1 去curl出来的包，这个包是可以看到当前的header有哪些，其中client-real-ip是自定义的x-forward-for👇 命令参数配置位置说明： PS：√! 和 √的区别，就是能用但是有限制，需要点进去看详情的意思。然后就点进去瞧一瞧 这段话就不太好看懂，拆分开来意思大概如下： 1、default默认的就是不带名字的，就是http-request这个指令不能配置在匿名的default下 这就是匿名的👇defaults 而这是命名的default，既然是命名的就需要调用的 1、命名方法，和调用方法 2、所以http-request，如果配置在defaults里，是要用命名的defaults的， 3、而命名的defaults xxx，如何被listen，以及frontend和backend调用 就是用这种方式，或者写在前后端或listen语句块前面就行 4、但是这个http-request写到default xxx里，是不能同时被frontend和backend调用的，也就是不能被listen调用。 5、但是http-requst却可以直接写到listen里，好奇怪....不研究了 实验，实验 给后端转发的时候添加header字段 写到frontend里 此时后端抓包就能看见了👇 写到backend里 同样生效 写到listen里 生效的 给后端转发的时候修改header字段 删除hosts信息，一样不影响转发和用户请求。 给后端转发的时候删除header字段 除了真实的用户IP地址信息，其他确实可以删除 不过删的太多会报错 写到backend里去看看x-forward-for能否被删除 还是不能删除x-forward-for 不过此时用户以及curl报错了 说明x-forward-for是送出去的时候打上真实用户IP的动作实在http-request之后的。正要想删这个字段，就别打上就行了，本来默认就不携带，哈哈。 给用户回包的时候增删改字段 比如正常的响应头里会暴露真实的web服务器的信息，如上图的server:nginx/1.26.1 隐藏掉 自定义日志格式 log指令适配的语法块，就是哪都可以👇 然后看到(*) 其实就是表格前文会有解释，我跟你说，这种一般就是表头，表尾，点进去蓝色就是子链接，三个地方找找ctrl f 搜索下，一般就能找到了。 实验 修改为httplog 上图关键字段解释：这个可以用来排除网络故障哦 PS：主要www.ming.com不要以为是什么IP反解哦，其实只是Haproxy配置文件里的frontend xxx起的名字 改一下就看到了👇 压缩功能-节省带宽 压缩-耗时CPU，节省网络传输空间，CPU一般富裕的都是，所以还是推荐压缩的，这样就是用时间换空间，用CPU的计算时间 来换取 网络传输的小空间占用。其实也提升了传输的时间。也是用时间换时间。哈哈，有点哲学了哦，不过我不太喜欢话术~ 实验测试压缩的好处 准备大一点的文件测试用 mess文件补一个后缀.html否则浏览器不识别就会直接下载处理了 之前nginx里也提到过 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Basics_of_HTTP/MIME_types 这个默认行为的资料说明如下 好，回到测试压缩效果，276M的文本耗时情况如下👇 压缩前耗时 稍微等等吧，文件还是给大了哈哈👆 然后 OOM了，，，，卧槽，直接我笔记本报警 内存爆了。。。一个chrome内存占用96%，紧接着就上图报错后，内存才降下来。不过第一次会OOM，后面就不会，可能还是有缓存机制的。也许可以通过清理APPLICATION进一步复现OOM，好像无痕可以复现👇 再等会就oom了👇 看来我踩了一个前端的坑 分页体验没有流式的好，所谓流式其实就是滚轮滚下去才加载到内存里，这样内存在接收大文本的时候可以得到释放。分页不是分页面的意思，可能是区别于流式的一行一行，而是一屏一屏也就是一页一页的加载。 ​ 然后浏览器的看到东西都是再内存里的，是不会放到本地磁盘上的。所以浏览器经常占用大量内存 好像系统本身也有一些优化，记得第一次是系统报警内存96%了，后来系统内存就没超过80%，所以还是这个机制自动生效了看来，不过这个机制保护了整体系统，但是对于浏览器的内存一样是限制了的，所以浏览器层面的OOM还是在的。 好 没有启用haproxy压缩的时候一次OOM出来要5分钟👇 同时看看响应头 此时是没有压缩的 启动后端nginx压缩再试试 开启nginx的压缩试试，4分就抵达了OOM了确实加速了， 不过OOM毕竟是故障，不适合用来对比测试。减少加载文本重新对比测试。 还是太大了 find -name \"mess_parta[^a]*\" -exec rm -rf {} \\; mv mess_partaa mess.html 后端服务器nginx直接压缩，测试多次 5-7s左右的样子。 后端nginx不压缩测试 3-6s的样子，就是nginx不压缩，我TM还快一点。就是用户PC浏览器解压耗时了。。。 等等，调大测试文本，拉开后端nginx启用和不启用压缩的耗时差距 不压缩 测多几次6次的样子，发现稳定再33s的样子，那个下图19s神经病不管他。 开启后端nginx的压缩测多几次，发现25s-1min都有。整体耗时50s的样子，压缩反而变长了。 发现①首先后端nginx开启压缩后，waitting for server response时间变成好多，就是后端要压缩所以耗时变成了。 ②其次，整体上content download时间也是加载文件时间，也TMD变长了， 所以还是那句话，还不如不压缩呢。 再通过对比后端nginx 压缩 和没压缩的 log里的字节数看看 结论1：压缩可能带来不好的用户体验 压缩后👆网络传输的文本大小确实小了很多👇417663 : 21032194 的效果，节省了网络带宽，加重了用户加载，用户体验变差。 也有可能是我服务的CPU也就是nginx那台CPU太差了。也不是我对比了TOP查看压缩和不压缩的CPU，都占用很小。那就不知道了。。。 启用haproxy的压缩测试 haproxy的配置示例： compression alog gzip raw-deflate # alog是算法的缩写，就是用gzip和raw-deflate压缩算法。 compresssion type text/html text/css text/plain # 这是针对哪些mime类型进行压缩 此时nginx日志是没有压缩的，因为c---haprox---nginx , haproxy和nginx之间是没有压缩的， 压缩是haproxy做的，是发给c的时候压缩的，所以日志要开哈proxy的发送文件大小 haproxy压缩的效果，在用户浏览器里可以看到，haproxy日志是压缩前的字节数 同样结论2：压缩可能带来不好的用户体验 content download 时间大差不差，整体是压缩后反而时间变长。 haproxy转发了一下多了210字节 观察用户那边吧 haproxy开启压缩：后端nginx未开启压缩 haproxy关闭压缩：后端nginx未开启压缩 client---haproxy---nginxServer，建议压缩在nginx上做，减轻haproxy的压力。 后端服务器的健康性检查 概述 默认也是有健康检查的，不过是L4的，不是http应用的检查 option http就是开启后端的http应用层的检查，望文生义，后端服务的健康检查自然是写到backend、以及合二为一的listen里的，是不能写到frontend里的 实验 此时status页面是报错的👇检查失败，也就是说haproxy的健康检查开启option http默认就是是失败的，要处理 PS：上图由于两个server都是down，所以最后一样的Backend就是DOWN 红色的了。 然后tcpdump抓包也是报错的 -vv也一样 改成option http GET /index.html 此时就可以了 抓包可见 然后nginx的log也可以看到，不过实验遇到点问题 192.168.126.133和134两个http检查都是OK的 但是两台nginx的log，133的tcpdump明明是有的，但是log就是看不到，134 是正常的。 研究下为什么133的log看不到健康检查过来的http请求。 手动curl，两台nginx的log倒是都可以看到 妈的，出BUG了，删掉a.conf让default.conf抢先，haproxy的option http有日志了. 不管了，反正有了，操作过程就是复制了一个134的a.conf过来抢了，然后删掉只剩default.conf就好了。 再一个error.log里的notice可能级别看不全，不过和这里的实验没关系，就是OPTIONS发过去也是在nginx的access日志里而不是在error日志里 修改index.html造成故障 造成：健康检查option httpchk GET /index.html的故障 鼠标悬浮那边就是 好玩的是192.168.126.132的access和error日志都看的到 减轻7层健康检查的负担 如果默认是访问首页，首页很大会导致每次健康检查都要打开很大的页面。 所以 自定义一个简单页面用来检测就行了 后端服务器弄一个状态页👇 echo ready > /usr/share/nginx/html/check.html 也许我可以弱化实验，因为 实验过程繁复，我已经有一些基础应对，所以记录操作和关键，就会凸显步骤，脉络更为清晰，而不必每个实验，甚至小实验都开几台模拟器敲一遍。 错！！！ 模拟器打开要1分钟，敲实验也就几分钟，不要为懒找借口。正所谓道理怎么讲都是通的，不讲道理的坚守往往更为宝贵。 不过倒是提醒了我，实验要敲，梳理脉络也要清晰，不要什么都往笔记里扔。 工作案例 1、gitlab拆库，用户无感 2、我的环境是gitlab虚拟机，所以直接克隆3份出来，每份只留1个不同的库 3、不同库的url使用location转到不同的后端VM 4、针对登录这些同样域名的处理就用会话保持来做 浏览器没问题可以做出来，但是git clone由于不支持重定向所以做不了。而gitlab主要就是给研发用的git clone不支持就没戏了。所以gitlab拆库 无感 做不了。 附详情： proxy_set_header Host $host;作用就是让后端gitlab认定为你设定的host头，也就是域名咯，如果不改就是： 不重定向分库的难点：client请求的nginx代理的域名，会作为后端gitlab 认证登入页面的域名处理，这样nginx的一个域名两个库的登入url都是一样的，就不太可能实现路由的区分； 重定向分库的缺点：如果repo1和repo2分别改为proxy_set_header Host www.repo1.com和www.repo2.com就会让后后端gitlab基于这个host生成login的url，返还给用户，用户就跳转到这两个域名去了，所以分库实现，不过这是浏览器支持了，git clone不支持的。 工作案例-service里的环境变量问题 1、cli运行没问题 [root@network001 log]# nohup /root/bin/python /pythonProject/netPolicyAuto/ssgAuto/devIpPermitTemporarily/transApi/api002.py & 2、改成service运行的起来，貌似也没问题 [root@network001 log]# cat /usr/lib/systemd/system/policytmp.service [Unit] Description=API 002 Python Service After=network.target [Service] ExecStart=/root/bin/python /pythonProject/netPolicyAuto/ssgAuto/devIpPermitTemporarily/transApi/api002.py Restart=always RestartSec=5 [Install] WantedBy=multi-user.target [root@network001 log]# 但是api002这个其实是api咯，调用的时候通过status 看到报错的👇 58行就是ssh登入的时候用了os调用变量 而变量值通过os.environ.get这句取出来的，service里就取不出来，修改为实际的密码后，测试ok，说明就是这里os的调用在service服务里实现不了。 进一步研究发现可以取出来的，通过service里设置环境变量的方式，但是即使取出来还是报错一样的。 工作案例-DNS 53丢包 1、现象 192.168.10.2的tcp 53丢包，这是tcping测试可见的， 同样dig解析也就是udp 53 卡顿 ssh端口都是不丢包，但是存在ssh登入卡几秒的情况，偶发 2、处理： https://www.cnblogs.com/lemon-flm/p/7975812.html Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-10-23 16:37:56 "},"43-HAProxy/反向代理服务Haproxy/7.Haproxy实现ACL和HTTPS安全.html":{"url":"43-HAProxy/反向代理服务Haproxy/7.Haproxy实现ACL和HTTPS安全.html","title":"7.Haproxy实现ACL和HTTPS安全.md","keywords":"","body":"7.Haproxy实现ACL和HTTPS安全.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/44-KeepAlived.html":{"url":"44-KeepAlived/44-KeepAlived.html","title":"44-KeepAlived","keywords":"","body":"44-KeepAlived Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/高可用集群Keepalived.html":{"url":"44-KeepAlived/高可用集群Keepalived/高可用集群Keepalived.html","title":"高可用集群Keepalived","keywords":"","body":"高可用集群Keepalived Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/1.高可用集群和VRRP协议.html":{"url":"44-KeepAlived/高可用集群Keepalived/1.高可用集群和VRRP协议.html","title":"1.高可用集群和VRRP协议.md","keywords":"","body":"1.高可用集群和VRRP协议.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/2.Keepalived编译安装和Global全局配置.html":{"url":"44-KeepAlived/高可用集群Keepalived/2.Keepalived编译安装和Global全局配置.html","title":"2.Keepalived编译安装和Global全局配置.md","keywords":"","body":"2.Keepalived编译安装和Global全局配置.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/3.Keepalived实现VRRP协议配置和日志及独立子配置文件.html":{"url":"44-KeepAlived/高可用集群Keepalived/3.Keepalived实现VRRP协议配置和日志及独立子配置文件.html","title":"3.Keepalived实现VRRP协议配置和日志及独立子配置文件.md","keywords":"","body":"3.Keepalived实现VRRP协议配置和日志及独立子配置文件.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/4.Keepalived脑裂现像单播通信非抢占式及邮件通知实现.html":{"url":"44-KeepAlived/高可用集群Keepalived/4.Keepalived脑裂现像单播通信非抢占式及邮件通知实现.html","title":"4.Keepalived脑裂现像单播通信非抢占式及邮件通知实现.md","keywords":"","body":"4.Keepalived脑裂现像单播通信非抢占式及邮件通知实现.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/5.Keepalived多主模式和实现LVS.html":{"url":"44-KeepAlived/高可用集群Keepalived/5.Keepalived多主模式和实现LVS.html","title":"5.Keepalived多主模式和实现LVS.md","keywords":"","body":"5.Keepalived多主模式和实现LVS.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/6.Keepalived实现多主模式的LVS和其它服务高可的vrrp_script实现说明.html":{"url":"44-KeepAlived/高可用集群Keepalived/6.Keepalived实现多主模式的LVS和其它服务高可的vrrp_script实现说明.html","title":"6.Keepalived实现多主模式的LVS和其它服务高可的vrrp_script实现说明.md","keywords":"","body":"6.Keepalived实现多主模式的LVS和其它服务高可的vrrp_script实现说明.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"44-KeepAlived/高可用集群Keepalived/7.Keepalived实现Haproxy和Nginx其它服务的高可用.html":{"url":"44-KeepAlived/高可用集群Keepalived/7.Keepalived实现Haproxy和Nginx其它服务的高可用.html","title":"7.Keepalived实现Haproxy和Nginx其它服务的高可用.md","keywords":"","body":"7.Keepalived实现Haproxy和Nginx其它服务的高可用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/45-Redis.html":{"url":"45-Redis/45-Redis.html","title":"45-Redis","keywords":"","body":"45-Redis Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/NoSQL数据库Redis.html":{"url":"45-Redis/NoSQL数据库Redis/NoSQL数据库Redis.html","title":"NoSQL数据库Redis","keywords":"","body":"NoSQL数据库Redis Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/1.NoSQL和Redis特性.html":{"url":"45-Redis/NoSQL数据库Redis/1.NoSQL和Redis特性.html","title":"1.NoSQL和Redis特性.md","keywords":"","body":"1.NoSQL和Redis特性.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/2.Redis工作机制和编译安装.html":{"url":"45-Redis/NoSQL数据库Redis/2.Redis工作机制和编译安装.html","title":"2.Redis工作机制和编译安装.md","keywords":"","body":"2.Redis工作机制和编译安装.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/3.Redis性能优化和客户端访问.html":{"url":"45-Redis/NoSQL数据库Redis/3.Redis性能优化和客户端访问.html","title":"3.Redis性能优化和客户端访问.md","keywords":"","body":"3.Redis性能优化和客户端访问.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/4.Redis程序访问和多实例.html":{"url":"45-Redis/NoSQL数据库Redis/4.Redis程序访问和多实例.html","title":"4.Redis程序访问和多实例.md","keywords":"","body":"4.Redis程序访问和多实例.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/5.Redis动态修改配置和慢查询及数据持久化RDB机制.html":{"url":"45-Redis/NoSQL数据库Redis/5.Redis动态修改配置和慢查询及数据持久化RDB机制.html","title":"5.Redis动态修改配置和慢查询及数据持久化RDB机制.md","keywords":"","body":"5.Redis动态修改配置和慢查询及数据持久化RDB机制.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/6.Redis的持久化RDB和AOF实现.html":{"url":"45-Redis/NoSQL数据库Redis/6.Redis的持久化RDB和AOF实现.html","title":"6.Redis的持久化RDB和AOF实现.md","keywords":"","body":"6.Redis的持久化RDB和AOF实现.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/NoSQL数据库Redis/7.Redis常见命令用法.html":{"url":"45-Redis/NoSQL数据库Redis/7.Redis常见命令用法.html","title":"7.Redis常见命令用法.md","keywords":"","body":"7.Redis常见命令用法.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/Redis数据类型和主从复制哨兵机制.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/Redis数据类型和主从复制哨兵机制.html","title":"Redis数据类型和主从复制哨兵机制","keywords":"","body":"Redis数据类型和主从复制哨兵机制 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/1.Redis的五种数据类型.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/1.Redis的五种数据类型.html","title":"1.Redis的五种数据类型.md","keywords":"","body":"1.Redis的五种数据类型.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/2.Redis的五种数据类型.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/2.Redis的五种数据类型.html","title":"2.Redis的五种数据类型.md","keywords":"","body":"2.Redis的五种数据类型.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/3.Redis消息队列和集群简介.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/3.Redis消息队列和集群简介.html","title":"3.Redis消息队列和集群简介.md","keywords":"","body":"3.Redis消息队列和集群简介.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/4.Redis集群模式类型和主从复制实现原理.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/4.Redis集群模式类型和主从复制实现原理.html","title":"4.Redis集群模式类型和主从复制实现原理.md","keywords":"","body":"4.Redis集群模式类型和主从复制实现原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/5.Redis主从复制实现和故障转换.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/5.Redis主从复制实现和故障转换.html","title":"5.Redis主从复制实现和故障转换.md","keywords":"","body":"5.Redis主从复制实现和故障转换.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/6.Redis主从复制优化和故障排错.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/6.Redis主从复制优化和故障排错.html","title":"6.Redis主从复制优化和故障排错.md","keywords":"","body":"6.Redis主从复制优化和故障排错.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/7.Redis的哨兵sentinel机制工作原理.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/7.Redis的哨兵sentinel机制工作原理.html","title":"7.Redis的哨兵sentinel机制工作原理.md","keywords":"","body":"7.Redis的哨兵sentinel机制工作原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis数据类型和主从复制哨兵机制/8.Redis的哨兵sentinel实现案例和客户端访问.html":{"url":"45-Redis/Redis数据类型和主从复制哨兵机制/8.Redis的哨兵sentinel实现案例和客户端访问.html","title":"8.Redis的哨兵sentinel实现案例和客户端访问.md","keywords":"","body":"8.Redis的哨兵sentinel实现案例和客户端访问.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis集群和微服务/Redis集群和微服务.html":{"url":"45-Redis/Redis集群和微服务/Redis集群和微服务.html","title":"Redis集群和微服务","keywords":"","body":"Redis集群和微服务 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis集群和微服务/1.Redis集群的工作原理.html":{"url":"45-Redis/Redis集群和微服务/1.Redis集群的工作原理.html","title":"1.Redis集群的工作原理.md","keywords":"","body":"1.Redis集群的工作原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis集群和微服务/2.Redis集群架构和创建.html":{"url":"45-Redis/Redis集群和微服务/2.Redis集群架构和创建.html","title":"2.Redis集群架构和创建.md","keywords":"","body":"2.Redis集群架构和创建.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis集群和微服务/3.Redis的集群访问和故障转移.html":{"url":"45-Redis/Redis集群和微服务/3.Redis的集群访问和故障转移.html","title":"3.Redis的集群访问和故障转移.md","keywords":"","body":"3.Redis的集群访问和故障转移.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis集群和微服务/4.Redis扩容机制和实现.html":{"url":"45-Redis/Redis集群和微服务/4.Redis扩容机制和实现.html","title":"4.Redis扩容机制和实现.md","keywords":"","body":"4.Redis扩容机制和实现.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"45-Redis/Redis集群和微服务/5.Redis集群动态缩容和维护.html":{"url":"45-Redis/Redis集群和微服务/5.Redis集群动态缩容和维护.html","title":"5.Redis集群动态缩容和维护.md","keywords":"","body":"5.Redis集群动态缩容和维护.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/46-文档数据库MongoDB.html":{"url":"46-文档数据库MongoDB/46-文档数据库MongoDB.html","title":"46-文档数据库MongoDB","keywords":"","body":"46-文档数据库MongoDB Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/1MongoDB介绍和安装.html":{"url":"46-文档数据库MongoDB/1MongoDB介绍和安装.html","title":"1MongoDB介绍和安装.md","keywords":"","body":"1MongoDB介绍和安装.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/2MongoDB相关工具基本使用.html":{"url":"46-文档数据库MongoDB/2MongoDB相关工具基本使用.html","title":"2MongoDB相关工具基本使用.md","keywords":"","body":"2MongoDB相关工具基本使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/3MongDB用户和权限管理.html":{"url":"46-文档数据库MongoDB/3MongDB用户和权限管理.html","title":"3MongDB用户和权限管理.md","keywords":"","body":"3MongDB用户和权限管理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/4MongoDB复制集实现和管理.html":{"url":"46-文档数据库MongoDB/4MongoDB复制集实现和管理.html","title":"4MongoDB复制集实现和管理.md","keywords":"","body":"4MongoDB复制集实现和管理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/5MongoDB分片集群架构.html":{"url":"46-文档数据库MongoDB/5MongoDB分片集群架构.html","title":"5MongoDB分片集群架构.md","keywords":"","body":"5MongoDB分片集群架构.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/6MongoDB分片集群实现.html":{"url":"46-文档数据库MongoDB/6MongoDB分片集群实现.html","title":"6MongoDB分片集群实现.md","keywords":"","body":"6MongoDB分片集群实现.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"46-文档数据库MongoDB/7MongoDB的备份和还原.html":{"url":"46-文档数据库MongoDB/7MongoDB的备份和还原.html","title":"7MongoDB的备份和还原.md","keywords":"","body":"7MongoDB的备份和还原.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/47-消息队列和微服务.html":{"url":"47-消息队列和微服务/47-消息队列和微服务.html","title":"47-消息队列和微服务","keywords":"","body":"47-消息队列和微服务 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务/微服务.html":{"url":"47-消息队列和微服务/微服务/微服务.html","title":"微服务","keywords":"","body":"微服务 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务/1.单体服务和微服务特性说明与比较.html":{"url":"47-消息队列和微服务/微服务/1.单体服务和微服务特性说明与比较.html","title":"1.单体服务和微服务特性说明与比较.md","keywords":"","body":"1.单体服务和微服务特性说明与比较.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务/2.Zookeeper功能说明和单节点二进制部署.html":{"url":"47-消息队列和微服务/微服务/2.Zookeeper功能说明和单节点二进制部署.html","title":"2.Zookeeper功能说明和单节点二进制部署.md","keywords":"","body":"2.Zookeeper功能说明和单节点二进制部署.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/微服务和消息队列.html":{"url":"47-消息队列和微服务/微服务和消息队列/微服务和消息队列.html","title":"微服务和消息队列","keywords":"","body":"微服务和消息队列 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/1.Zookeeper集群工作原理.html":{"url":"47-消息队列和微服务/微服务和消息队列/1.Zookeeper集群工作原理.html","title":"1.Zookeeper集群工作原理.md","keywords":"","body":"1.Zookeeper集群工作原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/2.Zookeeper集群实现和故障转移.html":{"url":"47-消息队列和微服务/微服务和消息队列/2.Zookeeper集群实现和故障转移.html","title":"2.Zookeeper集群实现和故障转移.md","keywords":"","body":"2.Zookeeper集群实现和故障转移.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/3.Zookeeper的客户端访问.html":{"url":"47-消息队列和微服务/微服务和消息队列/3.Zookeeper的客户端访问.html","title":"3.Zookeeper的客户端访问.md","keywords":"","body":"3.Zookeeper的客户端访问.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/4.消息队列功能和常见的MQ产品.html":{"url":"47-消息队列和微服务/微服务和消息队列/4.消息队列功能和常见的MQ产品.html","title":"4.消息队列功能和常见的MQ产品.md","keywords":"","body":"4.消息队列功能和常见的MQ产品.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/5.Kafka节点和数据结构相关概念和原理.html":{"url":"47-消息队列和微服务/微服务和消息队列/5.Kafka节点和数据结构相关概念和原理.html","title":"5.Kafka节点和数据结构相关概念和原理.md","keywords":"","body":"5.Kafka节点和数据结构相关概念和原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/6.Kafka集群部署和客户端访问.html":{"url":"47-消息队列和微服务/微服务和消息队列/6.Kafka集群部署和客户端访问.html","title":"6.Kafka集群部署和客户端访问.md","keywords":"","body":"6.Kafka集群部署和客户端访问.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/7.kafka相关工具访问.html":{"url":"47-消息队列和微服务/微服务和消息队列/7.kafka相关工具访问.html","title":"7.kafka相关工具访问.md","keywords":"","body":"7.kafka相关工具访问.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和消息队列/8.Dubbo微服务框架和实战项目.html":{"url":"47-消息队列和微服务/微服务和消息队列/8.Dubbo微服务框架和实战项目.html","title":"8.Dubbo微服务框架和实战项目.md","keywords":"","body":"8.Dubbo微服务框架和实战项目.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和Elasticsearch基础/微服务和Elasticsearch基础.html":{"url":"47-消息队列和微服务/微服务和Elasticsearch基础/微服务和Elasticsearch基础.html","title":"微服务和Elasticsearch基础","keywords":"","body":"微服务和Elasticsearch基础 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和Elasticsearch基础/1.微服务之Dubbo编译安装.html":{"url":"47-消息队列和微服务/微服务和Elasticsearch基础/1.微服务之Dubbo编译安装.html","title":"1.微服务之Dubbo编译安装.md","keywords":"","body":"1.微服务之Dubbo编译安装.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和Elasticsearch基础/2.微服务之Dubbo编译安装.html":{"url":"47-消息队列和微服务/微服务和Elasticsearch基础/2.微服务之Dubbo编译安装.html","title":"2.微服务之Dubbo编译安装.md","keywords":"","body":"2.微服务之Dubbo编译安装.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和Elasticsearch基础/3.Dubbo-admin编译安装和Nacos介绍和单机部署（无声）.html":{"url":"47-消息队列和微服务/微服务和Elasticsearch基础/3.Dubbo-admin编译安装和Nacos介绍和单机部署（无声）.html","title":"3.Dubbo-admin编译安装和Nacos介绍和单机部署（无声）.md","keywords":"","body":"3.Dubbo-admin编译安装和Nacos介绍和单机部署（无声）.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和Elasticsearch基础/4.Nacos单机使用外置MySQL数据库和安全加固.html":{"url":"47-消息队列和微服务/微服务和Elasticsearch基础/4.Nacos单机使用外置MySQL数据库和安全加固.html","title":"4.Nacos单机使用外置MySQL数据库和安全加固.md","keywords":"","body":"4.Nacos单机使用外置MySQL数据库和安全加固.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"47-消息队列和微服务/微服务和Elasticsearch基础/5.Nacos基于二进制和Docker-compose实现集群模式.html":{"url":"47-消息队列和微服务/微服务和Elasticsearch基础/5.Nacos基于二进制和Docker-compose实现集群模式.html","title":"5.Nacos基于二进制和Docker-compose实现集群模式.md","keywords":"","body":"5.Nacos基于二进制和Docker-compose实现集群模式.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/48-ELK.html":{"url":"48-ELK/48-ELK.html","title":"48-ELK","keywords":"","body":"48-ELK Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/微服务和Elasticsearch基础/微服务和Elasticsearch基础.html":{"url":"48-ELK/微服务和Elasticsearch基础/微服务和Elasticsearch基础.html","title":"微服务和Elasticsearch基础","keywords":"","body":"微服务和Elasticsearch基础 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/微服务和Elasticsearch基础/5.ELK各种组件功能和架构.html":{"url":"48-ELK/微服务和Elasticsearch基础/5.ELK各种组件功能和架构.html","title":"5.ELK各种组件功能和架构.md","keywords":"","body":"5.ELK各种组件功能和架构.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/微服务和Elasticsearch基础/6.Elasticsearch基于单机部署.html":{"url":"48-ELK/微服务和Elasticsearch基础/6.Elasticsearch基于单机部署.html","title":"6.Elasticsearch基于单机部署.md","keywords":"","body":"6.Elasticsearch基于单机部署.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/微服务和Elasticsearch基础/7.Elasticsearch集群部署和性能优化.html":{"url":"48-ELK/微服务和Elasticsearch基础/7.Elasticsearch集群部署和性能优化.html","title":"7.Elasticsearch集群部署和性能优化.md","keywords":"","body":"7.Elasticsearch集群部署和性能优化.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/Elasticsearch索引管理和日志收集.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/Elasticsearch索引管理和日志收集.html","title":"Elasticsearch索引管理和日志收集","keywords":"","body":"Elasticsearch索引管理和日志收集 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/1.Elasticsearch索引管理及利用shell和python数据访问.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/1.Elasticsearch索引管理及利用shell和python数据访问.html","title":"1.Elasticsearch索引管理及利用shell和python数据访问.md","keywords":"","body":"1.Elasticsearch索引管理及利用shell和python数据访问.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/2.Elasticsearch常见插件安装和使用.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/2.Elasticsearch常见插件安装和使用.html","title":"2.Elasticsearch常见插件安装和使用.md","keywords":"","body":"2.Elasticsearch常见插件安装和使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/3.Elasticsearch集群数据访问和故障转移工作原理.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/3.Elasticsearch集群数据访问和故障转移工作原理.html","title":"3.Elasticsearch集群数据访问和故障转移工作原理.md","keywords":"","body":"3.Elasticsearch集群数据访问和故障转移工作原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/4.Elasticsearch集群扩容缩容和Kibana安装及beats介绍.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/4.Elasticsearch集群扩容缩容和Kibana安装及beats介绍.html","title":"4.Elasticsearch集群扩容缩容和Kibana安装及beats介绍.md","keywords":"","body":"4.Elasticsearch集群扩容缩容和Kibana安装及beats介绍.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/5.Metricbeat和Heartbeat安装和使用实现监控性能和可用性.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/5.Metricbeat和Heartbeat安装和使用实现监控性能和可用性.html","title":"5.Metricbeat和Heartbeat安装和使用实现监控性能和可用性.md","keywords":"","body":"5.Metricbeat和Heartbeat安装和使用实现监控性能和可用性.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/6.Filebeat安装和日志收集.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/6.Filebeat安装和日志收集.html","title":"6.Filebeat安装和日志收集.md","keywords":"","body":"6.Filebeat安装和日志收集.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/Elasticsearch索引管理和日志收集/7.Filebeat日志收集到Elasticsearch.html":{"url":"48-ELK/Elasticsearch索引管理和日志收集/7.Filebeat日志收集到Elasticsearch.html","title":"7.Filebeat日志收集到Elasticsearch.md","keywords":"","body":"7.Filebeat日志收集到Elasticsearch.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/ELK日志收集.html":{"url":"48-ELK/ELK日志收集/ELK日志收集.html","title":"ELK日志收集","keywords":"","body":"ELK日志收集 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/1.Filebeat收集系统日志到ES.html":{"url":"48-ELK/ELK日志收集/1.Filebeat收集系统日志到ES.html","title":"1.Filebeat收集系统日志到ES.md","keywords":"","body":"1.Filebeat收集系统日志到ES.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/2.Filebeat收集nginx的json格式的访问日志和错误日志到ES.html":{"url":"48-ELK/ELK日志收集/2.Filebeat收集nginx的json格式的访问日志和错误日志到ES.html","title":"2.Filebeat收集nginx的json格式的访问日志和错误日志到ES.md","keywords":"","body":"2.Filebeat收集nginx的json格式的访问日志和错误日志到ES.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/3.Filebeat收集nginx的json格式的访问日志和错误日志到ES.html":{"url":"48-ELK/ELK日志收集/3.Filebeat收集nginx的json格式的访问日志和错误日志到ES.html","title":"3.Filebeat收集nginx的json格式的访问日志和错误日志到ES.md","keywords":"","body":"3.Filebeat收集nginx的json格式的访问日志和错误日志到ES.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/4.Filebeat收集tomcat的json格式的访问日志和多行错误日志合并到ES.html":{"url":"48-ELK/ELK日志收集/4.Filebeat收集tomcat的json格式的访问日志和多行错误日志合并到ES.html","title":"4.Filebeat收集tomcat的json格式的访问日志和多行错误日志合并到ES.md","keywords":"","body":"4.Filebeat收集tomcat的json格式的访问日志和多行错误日志合并到ES.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/5.Filebeat收集日志到Redis和Kafka集群avi.html":{"url":"48-ELK/ELK日志收集/5.Filebeat收集日志到Redis和Kafka集群avi.html","title":"5.Filebeat收集日志到Redis和Kafka集群avi.md","keywords":"","body":"5.Filebeat收集日志到Redis和Kafka集群avi.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/6.Logstash安装和基本使用.html":{"url":"48-ELK/ELK日志收集/6.Logstash安装和基本使用.html","title":"6.Logstash安装和基本使用.md","keywords":"","body":"6.Logstash安装和基本使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/7.Logstash各种输入插件使用.html":{"url":"48-ELK/ELK日志收集/7.Logstash各种输入插件使用.html","title":"7.Logstash各种输入插件使用.md","keywords":"","body":"7.Logstash各种输入插件使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/8.Logstash的filter插件grok和geoip使用.html":{"url":"48-ELK/ELK日志收集/8.Logstash的filter插件grok和geoip使用.html","title":"8.Logstash的filter插件grok和geoip使用.md","keywords":"","body":"8.Logstash的filter插件grok和geoip使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集/9.Logstash的filter插件date和mutate等使用.html":{"url":"48-ELK/ELK日志收集/9.Logstash的filter插件date和mutate等使用.html","title":"9.Logstash的filter插件date和mutate等使用.md","keywords":"","body":"9.Logstash的filter插件date和mutate等使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集和展示分析/ELK日志收集和展示分析.html":{"url":"48-ELK/ELK日志收集和展示分析/ELK日志收集和展示分析.html","title":"ELK日志收集和展示分析","keywords":"","body":"ELK日志收集和展示分析 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集和展示分析/1.Logstash输出插件和实战案例.html":{"url":"48-ELK/ELK日志收集和展示分析/1.Logstash输出插件和实战案例.html","title":"1.Logstash输出插件和实战案例.md","keywords":"","body":"1.Logstash输出插件和实战案例.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集和展示分析/2.Logstash收集rsyslog的hapryxy日志发送到ES.html":{"url":"48-ELK/ELK日志收集和展示分析/2.Logstash收集rsyslog的hapryxy日志发送到ES.html","title":"2.Logstash收集rsyslog的hapryxy日志发送到ES.md","keywords":"","body":"2.Logstash收集rsyslog的hapryxy日志发送到ES.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集和展示分析/3.Logstash采集定制日志格式并用kibana展示.html":{"url":"48-ELK/ELK日志收集和展示分析/3.Logstash采集定制日志格式并用kibana展示.html","title":"3.Logstash采集定制日志格式并用kibana展示.md","keywords":"","body":"3.Logstash采集定制日志格式并用kibana展示.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集和展示分析/4.Logstash综合案例采集web访问日志kibana展示地图.html":{"url":"48-ELK/ELK日志收集和展示分析/4.Logstash综合案例采集web访问日志kibana展示地图.html","title":"4.Logstash综合案例采集web访问日志kibana展示地图.md","keywords":"","body":"4.Logstash综合案例采集web访问日志kibana展示地图.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集和展示分析/5.Logstash综合案例采集web访问日志到MySQL.html":{"url":"48-ELK/ELK日志收集和展示分析/5.Logstash综合案例采集web访问日志到MySQL.html","title":"5.Logstash综合案例采集web访问日志到MySQL.md","keywords":"","body":"5.Logstash综合案例采集web访问日志到MySQL.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"48-ELK/ELK日志收集和展示分析/6,DevOps和CICD理论基础.html":{"url":"48-ELK/ELK日志收集和展示分析/6,DevOps和CICD理论基础.html","title":"6,DevOps和CICD理论基础.md","keywords":"","body":"6,DevOps和CICD理论基础.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/49-Kubernetes和Prometheus进阶实战.html":{"url":"49-Kubernetes和Prometheus进阶实战/49-Kubernetes和Prometheus进阶实战.html","title":"49-Kubernetes和Prometheus进阶实战","keywords":"","body":"49-Kubernetes和Prometheus进阶实战 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/Kubernetes入门与进阶01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/Kubernetes入门与进阶01.html","title":"Kubernetes入门与进阶01","keywords":"","body":"Kubernetes入门与进阶01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/IT技术栈的迭代01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/IT技术栈的迭代01.html","title":"IT技术栈的迭代01.md","keywords":"","body":"IT技术栈的迭代01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/Kubernetes组件架构及工作逻辑02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/Kubernetes组件架构及工作逻辑02.html","title":"Kubernetes组件架构及工作逻辑02.md","keywords":"","body":"Kubernetes组件架构及工作逻辑02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/Kubernetes编排运行应用的基础逻辑03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/Kubernetes编排运行应用的基础逻辑03.html","title":"Kubernetes编排运行应用的基础逻辑03.md","keywords":"","body":"Kubernetes编排运行应用的基础逻辑03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/部署Kuberntes集群v1.27.104.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/部署Kuberntes集群v1.27.104.html","title":"部署Kuberntes集群v1.27.104.md","keywords":"","body":"部署Kuberntes集群v1.27.104.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/部署Kuberntes集群v1.27.105.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶01/部署Kuberntes集群v1.27.105.html","title":"部署Kuberntes集群v1.27.105.md","keywords":"","body":"部署Kuberntes集群v1.27.105.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes入门与进阶02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes入门与进阶02.html","title":"Kubernetes入门与进阶02","keywords":"","body":"Kubernetes入门与进阶02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes的名称空间03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes的名称空间03.html","title":"Kubernetes的名称空间03.md","keywords":"","body":"Kubernetes的名称空间03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes系统组件及运行逻辑回顾与扩展01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes系统组件及运行逻辑回顾与扩展01.html","title":"Kubernetes系统组件及运行逻辑回顾与扩展01.md","keywords":"","body":"Kubernetes系统组件及运行逻辑回顾与扩展01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes资源管理基础02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/Kubernetes资源管理基础02.html","title":"Kubernetes资源管理基础02.md","keywords":"","body":"Kubernetes资源管理基础02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象04.html","title":"管理Pod资源对象04.md","keywords":"","body":"管理Pod资源对象04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象之多容器Pod和Pod错误示例07.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象之多容器Pod和Pod错误示例07.html","title":"管理Pod资源对象之多容器Pod和Pod错误示例07.md","keywords":"","body":"管理Pod资源对象之多容器Pod和Pod错误示例07.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象之安全上下文和资源边界06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象之安全上下文和资源边界06.html","title":"管理Pod资源对象之安全上下文和资源边界06.md","keywords":"","body":"管理Pod资源对象之安全上下文和资源边界06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象之容器探针05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶02/管理Pod资源对象之容器探针05.html","title":"管理Pod资源对象之容器探针05.md","keywords":"","body":"管理Pod资源对象之容器探针05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Kubernetes入门与进阶03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Kubernetes入门与进阶03.html","title":"Kubernetes入门与进阶03","keywords":"","body":"Kubernetes入门与进阶03 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/ConfigMap和Secret04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/ConfigMap和Secret04.html","title":"ConfigMap和Secret04.md","keywords":"","body":"ConfigMap和Secret04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/ConfigMap和Secret05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/ConfigMap和Secret05.html","title":"ConfigMap和Secret05.md","keywords":"","body":"ConfigMap和Secret05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:47:59 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Kubernetes卷使用基础01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Kubernetes卷使用基础01.html","title":"Kubernetes卷使用基础01.md","keywords":"","body":"Kubernetes卷使用基础01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Kubernetes网络卷和持久卷使用02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Kubernetes网络卷和持久卷使用02.html","title":"Kubernetes网络卷和持久卷使用02.md","keywords":"","body":"Kubernetes网络卷和持久卷使用02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Service的模式、类型及工作机制06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Service的模式、类型及工作机制06.html","title":"Service的模式、类型及工作机制06.md","keywords":"","body":"Service的模式、类型及工作机制06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Service类型及其应用07.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/Service类型及其应用07.html","title":"Service类型及其应用07.md","keywords":"","body":"Service类型及其应用07.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/StorageClass和CSI存储扩展03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶03/StorageClass和CSI存储扩展03.html","title":"StorageClass和CSI存储扩展03.md","keywords":"","body":"StorageClass和CSI存储扩展03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Kubernetes入门与进阶04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Kubernetes入门与进阶04.html","title":"Kubernetes入门与进阶04","keywords":"","body":"Kubernetes入门与进阶04 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/DaemonSet和StatefulSet05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/DaemonSet和StatefulSet05.html","title":"DaemonSet和StatefulSet05.md","keywords":"","body":"DaemonSet和StatefulSet05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Operator、Job和CronJob06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Operator、Job和CronJob06.html","title":"Operator、Job和CronJob06.md","keywords":"","body":"Operator、Job和CronJob06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Service与服务发现01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Service与服务发现01.html","title":"Service与服务发现01.md","keywords":"","body":"Service与服务发现01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Service名称解析与CoreDNS配置02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Service名称解析与CoreDNS配置02.html","title":"Service名称解析与CoreDNS配置02.md","keywords":"","body":"Service名称解析与CoreDNS配置02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Service名称解析与CoreDNS配置03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/Service名称解析与CoreDNS配置03.html","title":"Service名称解析与CoreDNS配置03.md","keywords":"","body":"Service名称解析与CoreDNS配置03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/应用编排与Deployments04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶04/应用编排与Deployments04.html","title":"应用编排与Deployments04.md","keywords":"","body":"应用编排与Deployments04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Kubernetes入门与进阶05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Kubernetes入门与进阶05.html","title":"Kubernetes入门与进阶05","keywords":"","body":"Kubernetes入门与进阶05 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Ingress资源格式及使用案例06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Ingress资源格式及使用案例06.html","title":"Ingress资源格式及使用案例06.md","keywords":"","body":"Ingress资源格式及使用案例06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/kubeconfig和ServiceAccount02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/kubeconfig和ServiceAccount02.html","title":"kubeconfig和ServiceAccount02.md","keywords":"","body":"kubeconfig和ServiceAccount02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Kubernetes Ingress及其工作机制05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Kubernetes Ingress及其工作机制05.html","title":"Kubernetes Ingress及其工作机制05.md","keywords":"","body":"Kubernetes Ingress及其工作机制05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Kubernetes安全体系及认证功能01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/Kubernetes安全体系及认证功能01.html","title":"Kubernetes安全体系及认证功能01.md","keywords":"","body":"Kubernetes安全体系及认证功能01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/RBAC应用示例04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/RBAC应用示例04.html","title":"RBAC应用示例04.md","keywords":"","body":"RBAC应用示例04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/鉴权体系与RBAC03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶05/鉴权体系与RBAC03.html","title":"鉴权体系与RBAC03.md","keywords":"","body":"鉴权体系与RBAC03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/Kubernetes入门与进阶06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/Kubernetes入门与进阶06.html","title":"Kubernetes入门与进阶06","keywords":"","body":"Kubernetes入门与进阶06 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/calico网络插件部署及功能验证05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/calico网络插件部署及功能验证05.html","title":"calico网络插件部署及功能验证05.md","keywords":"","body":"calico网络插件部署及功能验证05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/flannel网络插件及不同后端的功能验证04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/flannel网络插件及不同后端的功能验证04.html","title":"flannel网络插件及不同后端的功能验证04.md","keywords":"","body":"flannel网络插件及不同后端的功能验证04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/helm程序包管理器06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/helm程序包管理器06.html","title":"helm程序包管理器06.md","keywords":"","body":"helm程序包管理器06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/Kubernetes网络插件模型及工作逻辑02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/Kubernetes网络插件模型及工作逻辑02.html","title":"Kubernetes网络插件模型及工作逻辑02.md","keywords":"","body":"Kubernetes网络插件模型及工作逻辑02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/Kubernetes网络插件模型及工作逻辑03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/Kubernetes网络插件模型及工作逻辑03.html","title":"Kubernetes网络插件模型及工作逻辑03.md","keywords":"","body":"Kubernetes网络插件模型及工作逻辑03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/基于Ingress的灰度发布策略01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Kubernetes入门与进阶06/基于Ingress的灰度发布策略01.html","title":"基于Ingress的灰度发布策略01.md","keywords":"","body":"基于Ingress的灰度发布策略01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Prometheus和Kubernetes09.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Prometheus和Kubernetes09.html","title":"Prometheus和Kubernetes09","keywords":"","body":"Prometheus和Kubernetes09 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kube-Prometheu监控系统05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kube-Prometheu监控系统05.html","title":"Kube-Prometheu监控系统05.md","keywords":"","body":"Kube-Prometheu监控系统05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes指标系统及关联的组件03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes指标系统及关联的组件03.html","title":"Kubernetes指标系统及关联的组件03.md","keywords":"","body":"Kubernetes指标系统及关联的组件03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes的日志收集系统06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes的日志收集系统06.html","title":"Kubernetes的日志收集系统06.md","keywords":"","body":"Kubernetes的日志收集系统06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes自定义指标和HPA04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes自定义指标和HPA04.html","title":"Kubernetes自定义指标和HPA04.md","keywords":"","body":"Kubernetes自定义指标和HPA04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes调度器调度逻辑介绍07.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Kubernetes调度器调度逻辑介绍07.html","title":"Kubernetes调度器调度逻辑介绍07.md","keywords":"","body":"Kubernetes调度器调度逻辑介绍07.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Prometheus的存储系统01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/Prometheus的存储系统01.html","title":"Prometheus的存储系统01.md","keywords":"","body":"Prometheus的存储系统01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/VictoriaMetrics和Prometheus扩展02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus和Kubernetes09/VictoriaMetrics和Prometheus扩展02.html","title":"VictoriaMetrics和Prometheus扩展02.md","keywords":"","body":"VictoriaMetrics和Prometheus扩展02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/Prometheus监控系统07.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/Prometheus监控系统07.html","title":"Prometheus监控系统07","keywords":"","body":"Prometheus监控系统07 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/Prometheus快速入门02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/Prometheus快速入门02.html","title":"Prometheus快速入门02.md","keywords":"","body":"Prometheus快速入门02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/Prometheus部署和配置初步03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/Prometheus部署和配置初步03.html","title":"Prometheus部署和配置初步03.md","keywords":"","body":"Prometheus部署和配置初步03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/PromQL使用入门06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/PromQL使用入门06.html","title":"PromQL使用入门06.md","keywords":"","body":"PromQL使用入门06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/PromQL使用入门与进阶07.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/PromQL使用入门与进阶07.html","title":"PromQL使用入门与进阶07.md","keywords":"","body":"PromQL使用入门与进阶07.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/监控Consul和黑盒监控blackbox_exporter05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/监控Consul和黑盒监控blackbox_exporter05.html","title":"监控Consul和黑盒监控blackbox_exporter05.md","keywords":"","body":"监控Consul和黑盒监控blackbox_exporter05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/监控主机、mysql、nginx和tomcat04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/监控主机、mysql、nginx和tomcat04.html","title":"监控主机、mysql、nginx和tomcat04.md","keywords":"","body":"监控主机、mysql、nginx和tomcat04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/监控系统基础和Promtheus快速入门01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统07/监控系统基础和Promtheus快速入门01.html","title":"监控系统基础和Promtheus快速入门01.md","keywords":"","body":"监控系统基础和Promtheus快速入门01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/Prometheus监控系统08.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/Prometheus监控系统08.html","title":"Prometheus监控系统08","keywords":"","body":"Prometheus监控系统08 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/Prometheus的服务发现02.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/Prometheus的服务发现02.html","title":"Prometheus的服务发现02.md","keywords":"","body":"Prometheus的服务发现02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/Prometheus重新打标03.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/Prometheus重新打标03.html","title":"Prometheus重新打标03.md","keywords":"","body":"Prometheus重新打标03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/PromQL使用进阶01.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/PromQL使用进阶01.html","title":"PromQL使用进阶01.md","keywords":"","body":"PromQL使用进阶01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/告警媒介和告警路由06.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/告警媒介和告警路由06.html","title":"告警媒介和告警路由06.md","keywords":"","body":"告警媒介和告警路由06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/查询持久化与告警功能基础04.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/查询持久化与告警功能基础04.html","title":"查询持久化与告警功能基础04.md","keywords":"","body":"查询持久化与告警功能基础04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/配置使用AlertManager05.html":{"url":"49-Kubernetes和Prometheus进阶实战/Prometheus监控系统08/配置使用AlertManager05.html","title":"配置使用AlertManager05.md","keywords":"","body":"配置使用AlertManager05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/50-CICDGitLab和Jenkins.html":{"url":"50-CICDGitLab和Jenkins/50-CICDGitLab和Jenkins.html","title":"50-CICDGitLab和Jenkins","keywords":"","body":"50-CICDGitLab和Jenkins Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/CICD和DevOps01/CICD和DevOps01.html":{"url":"50-CICDGitLab和Jenkins/CICD和DevOps01/CICD和DevOps01.html","title":"CICD和DevOps01","keywords":"","body":"CICD和DevOps01 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/CICD和DevOps01/CICD和DevOps基础01.html":{"url":"50-CICDGitLab和Jenkins/CICD和DevOps01/CICD和DevOps基础01.html","title":"CICD和DevOps基础01.md","keywords":"","body":"CICD和DevOps基础01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/CICD和DevOps01/Git分支模型和工作流模型06.html":{"url":"50-CICDGitLab和Jenkins/CICD和DevOps01/Git分支模型和工作流模型06.html","title":"Git分支模型和工作流模型06.md","keywords":"","body":"Git分支模型和工作流模型06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制基础应用02.html":{"url":"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制基础应用02.html","title":"git版本控制基础应用02.md","keywords":"","body":"git版本控制基础应用02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制基础应用03.html":{"url":"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制基础应用03.html","title":"git版本控制基础应用03.md","keywords":"","body":"git版本控制基础应用03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制应用04.html":{"url":"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制应用04.html","title":"git版本控制应用04.md","keywords":"","body":"git版本控制应用04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制应用进阶05.html":{"url":"50-CICDGitLab和Jenkins/CICD和DevOps01/git版本控制应用进阶05.html","title":"git版本控制应用进阶05.md","keywords":"","body":"git版本控制应用进阶05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD02/Jenkins和CICD02.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD02/Jenkins和CICD02.html","title":"Jenkins和CICD02","keywords":"","body":"Jenkins和CICD02 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD02/Freestyle流水线及使用案例03.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD02/Freestyle流水线及使用案例03.html","title":"Freestyle流水线及使用案例03.md","keywords":"","body":"Freestyle流水线及使用案例03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD02/GitLab和基于远程仓库的工作流实践01.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD02/GitLab和基于远程仓库的工作流实践01.html","title":"GitLab和基于远程仓库的工作流实践01.md","keywords":"","body":"GitLab和基于远程仓库的工作流实践01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD02/Jenkins流水线应用快速入门02.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD02/Jenkins流水线应用快速入门02.html","title":"Jenkins流水线应用快速入门02.md","keywords":"","body":"Jenkins流水线应用快速入门02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD02/参数化构建及构建Docker Image04.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD02/参数化构建及构建Docker Image04.html","title":"参数化构建及构建Docker Image04.md","keywords":"","body":"参数化构建及构建Docker Image04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD02/构建Image并部署至Kubernetes集群05.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD02/构建Image并部署至Kubernetes集群05.html","title":"构建Image并部署至Kubernetes集群05.md","keywords":"","body":"构建Image并部署至Kubernetes集群05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD02/构建触器及GitLab自动触发的生产案例06.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD02/构建触器及GitLab自动触发的生产案例06.html","title":"构建触器及GitLab自动触发的生产案例06.md","keywords":"","body":"构建触器及GitLab自动触发的生产案例06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins和CICD03.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins和CICD03.html","title":"Jenkins和CICD03","keywords":"","body":"Jenkins和CICD03 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins分布式构建及配置04.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins分布式构建及配置04.html","title":"Jenkins分布式构建及配置04.md","keywords":"","body":"Jenkins分布式构建及配置04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins流水线的通知功能02.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins流水线的通知功能02.html","title":"Jenkins流水线的通知功能02.md","keywords":"","body":"Jenkins流水线的通知功能02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins流水线调用SonarQube进行代码质量分析03.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD03/Jenkins流水线调用SonarQube进行代码质量分析03.html","title":"Jenkins流水线调用SonarQube进行代码质量分析03.md","keywords":"","body":"Jenkins流水线调用SonarQube进行代码质量分析03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD03/Pipeline任务基础应用06.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD03/Pipeline任务基础应用06.html","title":"Pipeline任务基础应用06.md","keywords":"","body":"Pipeline任务基础应用06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD03/基于docker和Kubernetes pod的动态Agent05.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD03/基于docker和Kubernetes pod的动态Agent05.html","title":"基于docker和Kubernetes pod的动态Agent05.md","keywords":"","body":"基于docker和Kubernetes pod的动态Agent05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD03/流水线触发器GitLab Trigger和GWT01.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD03/流水线触发器GitLab Trigger和GWT01.html","title":"流水线触发器GitLab Trigger和GWT01.md","keywords":"","body":"流水线触发器GitLab Trigger和GWT01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/Jenkins和CICD04.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/Jenkins和CICD04.html","title":"Jenkins和CICD04","keywords":"","body":"Jenkins和CICD04 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/Argo-Rollouts以及在Pipeline中使用rollout完成高级发布06.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/Argo-Rollouts以及在Pipeline中使用rollout完成高级发布06.html","title":"Argo-Rollouts以及在Pipeline中使用rollout完成高级发布06.md","keywords":"","body":"Argo-Rollouts以及在Pipeline中使用rollout完成高级发布06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job应用进阶之参数化构建和触发器02.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job应用进阶之参数化构建和触发器02.html","title":"Pipeline Job应用进阶之参数化构建和触发器02.md","keywords":"","body":"Pipeline Job应用进阶之参数化构建和触发器02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job应用进阶之参数化构建和触发器03-1.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job应用进阶之参数化构建和触发器03-1.html","title":"Pipeline Job应用进阶之参数化构建和触发器03-1.md","keywords":"","body":"Pipeline Job应用进阶之参数化构建和触发器03-1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job应用进阶之参数化构建和触发器03-2.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job应用进阶之参数化构建和触发器03-2.html","title":"Pipeline Job应用进阶之参数化构建和触发器03-2.md","keywords":"","body":"Pipeline Job应用进阶之参数化构建和触发器03-2.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job的分布式构建04.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job的分布式构建04.html","title":"Pipeline Job的分布式构建04.md","keywords":"","body":"Pipeline Job的分布式构建04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job语法及基础应用01.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/Pipeline Job语法及基础应用01.html","title":"Pipeline Job语法及基础应用01.md","keywords":"","body":"Pipeline Job语法及基础应用01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"50-CICDGitLab和Jenkins/Jenkins和CICD04/案例-基于Pipeline实现CICD05.html":{"url":"50-CICDGitLab和Jenkins/Jenkins和CICD04/案例-基于Pipeline实现CICD05.html","title":"案例-基于Pipeline实现CICD05.md","keywords":"","body":"案例-基于Pipeline实现CICD05.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"51-SRE/51-SRE.html":{"url":"51-SRE/51-SRE.html","title":"51-SRE","keywords":"","body":"51-SRE Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"51-SRE/SRE基础概念01.html":{"url":"51-SRE/SRE基础概念01.html","title":"SRE基础概念01.md","keywords":"","body":"SRE基础概念01.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"51-SRE/SRE灾备建设06.html":{"url":"51-SRE/SRE灾备建设06.html","title":"SRE灾备建设06.md","keywords":"","body":"SRE灾备建设06.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"51-SRE/SRE的核心理念02.html":{"url":"51-SRE/SRE的核心理念02.html","title":"SRE的核心理念02.md","keywords":"","body":"SRE的核心理念02.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"51-SRE/SRE的核心理念和SLO03.html":{"url":"51-SRE/SRE的核心理念和SLO03.html","title":"SRE的核心理念和SLO03.md","keywords":"","body":"SRE的核心理念和SLO03.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"51-SRE/应急事件处理05.html":{"url":"51-SRE/应急事件处理05.html","title":"应急事件处理05.md","keywords":"","body":"应急事件处理05.md 测试git book上次 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 10:41:55 "},"51-SRE/服务质量目前、错误预算和监控04.html":{"url":"51-SRE/服务质量目前、错误预算和监控04.html","title":"服务质量目前、错误预算和监控04.md","keywords":"","body":"服务质量目前、错误预算和监控04.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/52-选修py自动化与简单实践.html":{"url":"52-选修py自动化与简单实践/52-选修py自动化与简单实践.html","title":"52-选修py自动化与简单实践","keywords":"","body":"52-选修py自动化与简单实践 测试gitbook update 52 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-08-05 14:02:54 "},"52-选修py自动化与简单实践/1-Python基础/1-Python基础.html":{"url":"52-选修py自动化与简单实践/1-Python基础/1-Python基础.html","title":"1-Python基础","keywords":"","body":"1-Python基础 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052401.html":{"url":"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052401.html","title":"01Python基础2021052401.md","keywords":"","body":"01Python基础2021052401.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052402虚拟环境安装1.html":{"url":"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052402虚拟环境安装1.html","title":"01Python基础2021052402虚拟环境安装1.md","keywords":"","body":"01Python基础2021052402虚拟环境安装1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052403计算机基础理论1.html":{"url":"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052403计算机基础理论1.html","title":"01Python基础2021052403计算机基础理论1.md","keywords":"","body":"01Python基础2021052403计算机基础理论1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052404基础语法1.html":{"url":"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052404基础语法1.html","title":"01Python基础2021052404基础语法1.md","keywords":"","body":"01Python基础2021052404基础语法1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052405运算符和内建函数1.html":{"url":"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052405运算符和内建函数1.html","title":"01Python基础2021052405运算符和内建函数1.md","keywords":"","body":"01Python基础2021052405运算符和内建函数1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052601分支循环1.html":{"url":"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052601分支循环1.html","title":"01Python基础2021052601分支循环1.md","keywords":"","body":"01Python基础2021052601分支循环1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052602循环和三元表达式1.html":{"url":"52-选修py自动化与简单实践/1-Python基础/01Python基础2021052602循环和三元表达式1.html","title":"01Python基础2021052602循环和三元表达式1.md","keywords":"","body":"01Python基础2021052602循环和三元表达式1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/2-线性表.html":{"url":"52-选修py自动化与简单实践/2-线性表/2-线性表.html","title":"2-线性表","keywords":"","body":"2-线性表 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052603数值和内建函数1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052603数值和内建函数1.html","title":"02线性表2021052603数值和内建函数1.md","keywords":"","body":"02线性表2021052603数值和内建函数1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052604线性表原理详解1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052604线性表原理详解1.html","title":"02线性表2021052604线性表原理详解1.md","keywords":"","body":"02线性表2021052604线性表原理详解1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052605列表操作和内存模型1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052605列表操作和内存模型1.html","title":"02线性表2021052605列表操作和内存模型1.md","keywords":"","body":"02线性表2021052605列表操作和内存模型1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052801列表复制原理1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052801列表复制原理1.html","title":"02线性表2021052801列表复制原理1.md","keywords":"","body":"02线性表2021052801列表复制原理1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052802Random模块1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052802Random模块1.html","title":"02线性表2021052802Random模块1.md","keywords":"","body":"02线性表2021052802Random模块1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052803元组1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052803元组1.html","title":"02线性表2021052803元组1.md","keywords":"","body":"02线性表2021052803元组1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052804字符串构造1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052804字符串构造1.html","title":"02线性表2021052804字符串构造1.md","keywords":"","body":"02线性表2021052804字符串构造1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052805字符串分割替换移除1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052805字符串分割替换移除1.html","title":"02线性表2021052805字符串分割替换移除1.md","keywords":"","body":"02线性表2021052805字符串分割替换移除1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052806字符串判断1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052806字符串判断1.html","title":"02线性表2021052806字符串判断1.md","keywords":"","body":"02线性表2021052806字符串判断1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052807字符串格式化1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052807字符串格式化1.html","title":"02线性表2021052807字符串格式化1.md","keywords":"","body":"02线性表2021052807字符串格式化1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021052808编码的本质mp41.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021052808编码的本质mp41.html","title":"02线性表2021052808编码的本质mp41.md","keywords":"","body":"02线性表2021052808编码的本质mp41.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021053101字节类型1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021053101字节类型1.html","title":"02线性表2021053101字节类型1.md","keywords":"","body":"02线性表2021053101字节类型1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021053102切片1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021053102切片1.html","title":"02线性表2021053102切片1.md","keywords":"","body":"02线性表2021053102切片1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021053103第一章习题-九九乘法表1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021053103第一章习题-九九乘法表1.html","title":"02线性表2021053103第一章习题-九九乘法表1.md","keywords":"","body":"02线性表2021053103第一章习题-九九乘法表1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021053104第一章习题-登录1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021053104第一章习题-登录1.html","title":"02线性表2021053104第一章习题-登录1.md","keywords":"","body":"02线性表2021053104第一章习题-登录1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/2-线性表/02线性表2021053105第二章习题1.html":{"url":"52-选修py自动化与简单实践/2-线性表/02线性表2021053105第二章习题1.html","title":"02线性表2021053105第二章习题1.md","keywords":"","body":"02线性表2021053105第二章习题1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/3-哈希表/3-哈希表.html":{"url":"52-选修py自动化与简单实践/3-哈希表/3-哈希表.html","title":"3-哈希表","keywords":"","body":"3-哈希表 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/3-哈希表/03哈希表2021053106封装和解构1.html":{"url":"52-选修py自动化与简单实践/3-哈希表/03哈希表2021053106封装和解构1.html","title":"03哈希表2021053106封装和解构1.md","keywords":"","body":"03哈希表2021053106封装和解构1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/3-哈希表/03哈希表2021053107set和哈希表原理1.html":{"url":"52-选修py自动化与简单实践/3-哈希表/03哈希表2021053107set和哈希表原理1.html","title":"03哈希表2021053107set和哈希表原理1.md","keywords":"","body":"03哈希表2021053107set和哈希表原理1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/3-哈希表/03哈希表2021060201字典增删改查1.html":{"url":"52-选修py自动化与简单实践/3-哈希表/03哈希表2021060201字典增删改查1.html","title":"03哈希表2021060201字典增删改查1.md","keywords":"","body":"03哈希表2021060201字典增删改查1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/3-哈希表/03哈希表2021060202遍历和有序性1.html":{"url":"52-选修py自动化与简单实践/3-哈希表/03哈希表2021060202遍历和有序性1.html","title":"03哈希表2021060202遍历和有序性1.md","keywords":"","body":"03哈希表2021060202遍历和有序性1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/3-哈希表/03哈希表2021060203解析式和生成器表达式1.html":{"url":"52-选修py自动化与简单实践/3-哈希表/03哈希表2021060203解析式和生成器表达式1.html","title":"03哈希表2021060203解析式和生成器表达式1.md","keywords":"","body":"03哈希表2021060203解析式和生成器表达式1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/4-函数.html":{"url":"52-选修py自动化与简单实践/4-函数/4-函数.html","title":"4-函数","keywords":"","body":"4-函数 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060204函数概念1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060204函数概念1.html","title":"04函数2021060204函数概念1.md","keywords":"","body":"04函数2021060204函数概念1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060401第三章作业1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060401第三章作业1.html","title":"04函数2021060401第三章作业1.md","keywords":"","body":"04函数2021060401第三章作业1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060402传实参2种方式1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060402传实参2种方式1.html","title":"04函数2021060402传实参2种方式1.md","keywords":"","body":"04函数2021060402传实参2种方式1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060403缺省值1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060403缺省值1.html","title":"04函数2021060403缺省值1.md","keywords":"","body":"04函数2021060403缺省值1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060404可变形参1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060404可变形参1.html","title":"04函数2021060404可变形参1.md","keywords":"","body":"04函数2021060404可变形参1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060405形参和返回值1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060405形参和返回值1.html","title":"04函数2021060405形参和返回值1.md","keywords":"","body":"04函数2021060405形参和返回值1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060406作用域和嵌套函数1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060406作用域和嵌套函数1.html","title":"04函数2021060406作用域和嵌套函数1.md","keywords":"","body":"04函数2021060406作用域和嵌套函数1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060407局部变量赋值和global1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060407局部变量赋值和global1.html","title":"04函数2021060407局部变量赋值和global1.md","keywords":"","body":"04函数2021060407局部变量赋值和global1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060701闭包原理1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060701闭包原理1.html","title":"04函数2021060701闭包原理1.md","keywords":"","body":"04函数2021060701闭包原理1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060702nonlocal和LEGB1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060702nonlocal和LEGB1.html","title":"04函数2021060702nonlocal和LEGB1.md","keywords":"","body":"04函数2021060702nonlocal和LEGB1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060703匿名函数1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060703匿名函数1.html","title":"04函数2021060703匿名函数1.md","keywords":"","body":"04函数2021060703匿名函数1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060704生成器函数1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060704生成器函数1.html","title":"04函数2021060704生成器函数1.md","keywords":"","body":"04函数2021060704生成器函数1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060705冒泡排序1avi.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060705冒泡排序1avi.html","title":"04函数2021060705冒泡排序1avi.md","keywords":"","body":"04函数2021060705冒泡排序1avi.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060901函数执行原理1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060901函数执行原理1.html","title":"04函数2021060901函数执行原理1.md","keywords":"","body":"04函数2021060901函数执行原理1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060902递归函数_1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060902递归函数_1.html","title":"04函数2021060902递归函数_1.md","keywords":"","body":"04函数2021060902递归函数_1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060903作业之比较大小1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060903作业之比较大小1.html","title":"04函数2021060903作业之比较大小1.md","keywords":"","body":"04函数2021060903作业之比较大小1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060904作业上下三角打印1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060904作业上下三角打印1.html","title":"04函数2021060904作业上下三角打印1.md","keywords":"","body":"04函数2021060904作业上下三角打印1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/4-函数/04函数2021060905选择排序精讲1.html":{"url":"52-选修py自动化与简单实践/4-函数/04函数2021060905选择排序精讲1.html","title":"04函数2021060905选择排序精讲1.md","keywords":"","body":"04函数2021060905选择排序精讲1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/5-装饰器.html":{"url":"52-选修py自动化与简单实践/5-装饰器/5-装饰器.html","title":"5-装饰器","keywords":"","body":"5-装饰器 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器202106139可迭代对象和内建函数.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器202106139可迭代对象和内建函数.html","title":"05装饰器202106139可迭代对象和内建函数.md","keywords":"","body":"05装饰器202106139可迭代对象和内建函数.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器202106140高阶函数柯里化.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器202106140高阶函数柯里化.html","title":"05装饰器202106140高阶函数柯里化.md","keywords":"","body":"05装饰器202106140高阶函数柯里化.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器202106141无参装饰器.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器202106141无参装饰器.html","title":"05装饰器202106141无参装饰器.md","keywords":"","body":"05装饰器202106141无参装饰器.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器202106142带参装饰器.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器202106142带参装饰器.html","title":"05装饰器202106142带参装饰器.md","keywords":"","body":"05装饰器202106142带参装饰器.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器202106143类型注解和类型检查装饰器.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器202106143类型注解和类型检查装饰器.html","title":"05装饰器202106143类型注解和类型检查装饰器.md","keywords":"","body":"05装饰器202106143类型注解和类型检查装饰器.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器2021061601插入排序.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器2021061601插入排序.html","title":"05装饰器2021061601插入排序.md","keywords":"","body":"05装饰器2021061601插入排序.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器2021061602functools之reduce和partial.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器2021061602functools之reduce和partial.html","title":"05装饰器2021061602functools之reduce和partial.md","keywords":"","body":"05装饰器2021061602functools之reduce和partial.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/5-装饰器/05装饰器2021061603functools之lru_cache和cache原理.html":{"url":"52-选修py自动化与简单实践/5-装饰器/05装饰器2021061603functools之lru_cache和cache原理.html","title":"05装饰器2021061603functools之lru_cache和cache原理.md","keywords":"","body":"05装饰器2021061603functools之lru_cache和cache原理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/6-文件IO和序列化.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/6-文件IO和序列化.html","title":"6-文件IO和序列化","keywords":"","body":"6-文件IO和序列化 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061604open方法和主模式.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061604open方法和主模式.html","title":"06文件IO和序列化2021061604open方法和主模式.md","keywords":"","body":"06文件IO和序列化2021061604open方法和主模式.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061605附加模式和指针.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061605附加模式和指针.html","title":"06文件IO和序列化2021061605附加模式和指针.md","keywords":"","body":"06文件IO和序列化2021061605附加模式和指针.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061801递归作业详解.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061801递归作业详解.html","title":"06文件IO和序列化2021061801递归作业详解.md","keywords":"","body":"06文件IO和序列化2021061801递归作业详解.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061802文件对象读写和迭代.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061802文件对象读写和迭代.html","title":"06文件IO和序列化2021061802文件对象读写和迭代.md","keywords":"","body":"06文件IO和序列化2021061802文件对象读写和迭代.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061803os.path模块.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061803os.path模块.html","title":"06文件IO和序列化2021061803os.path模块.md","keywords":"","body":"06文件IO和序列化2021061803os.path模块.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061804Path常用操作.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061804Path常用操作.html","title":"06文件IO和序列化2021061804Path常用操作.md","keywords":"","body":"06文件IO和序列化2021061804Path常用操作.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061805Path操作2.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061805Path操作2.html","title":"06文件IO和序列化2021061805Path操作2.md","keywords":"","body":"06文件IO和序列化2021061805Path操作2.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061806shutil模块使用.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021061806shutil模块使用.html","title":"06文件IO和序列化2021061806shutil模块使用.md","keywords":"","body":"06文件IO和序列化2021061806shutil模块使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062101序列化本质和pickle.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062101序列化本质和pickle.html","title":"06文件IO和序列化2021062101序列化本质和pickle.md","keywords":"","body":"06文件IO和序列化2021062101序列化本质和pickle.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062102json和msgpack.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062102json和msgpack.html","title":"06文件IO和序列化2021062102json和msgpack.md","keywords":"","body":"06文件IO和序列化2021062102json和msgpack.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062103csv文件处理.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062103csv文件处理.html","title":"06文件IO和序列化2021062103csv文件处理.md","keywords":"","body":"06文件IO和序列化2021062103csv文件处理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062104树.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062104树.html","title":"06文件IO和序列化2021062104树.md","keywords":"","body":"06文件IO和序列化2021062104树.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062105正则表达式1.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062105正则表达式1.html","title":"06文件IO和序列化2021062105正则表达式1.md","keywords":"","body":"06文件IO和序列化2021062105正则表达式1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062106正则表达式2.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062106正则表达式2.html","title":"06文件IO和序列化2021062106正则表达式2.md","keywords":"","body":"06文件IO和序列化2021062106正则表达式2.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062301复制作业详解.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062301复制作业详解.html","title":"06文件IO和序列化2021062301复制作业详解.md","keywords":"","body":"06文件IO和序列化2021062301复制作业详解.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062302正则习题讲解1.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062302正则习题讲解1.html","title":"06文件IO和序列化2021062302正则习题讲解1.md","keywords":"","body":"06文件IO和序列化2021062302正则习题讲解1.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062302正则习题讲解2.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062302正则习题讲解2.html","title":"06文件IO和序列化2021062302正则习题讲解2.md","keywords":"","body":"06文件IO和序列化2021062302正则习题讲解2.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062303re模块使用.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062303re模块使用.html","title":"06文件IO和序列化2021062303re模块使用.md","keywords":"","body":"06文件IO和序列化2021062303re模块使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062304argparse模块使用.html":{"url":"52-选修py自动化与简单实践/6-文件IO和序列化/06文件IO和序列化2021062304argparse模块使用.html","title":"06文件IO和序列化2021062304argparse模块使用.md","keywords":"","body":"06文件IO和序列化2021062304argparse模块使用.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/7-devops/7-devops.html":{"url":"52-选修py自动化与简单实践/7-devops/7-devops.html","title":"7-devops","keywords":"","body":"7-devops Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/7-devops/DevOps现状调查/DevOps现状调查.html":{"url":"52-选修py自动化与简单实践/7-devops/DevOps现状调查/DevOps现状调查.html","title":"DevOps现状调查","keywords":"","body":"DevOps现状调查 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/7-devops/DevOps现状调查/【DevOps现状调查报告-代码管理】.html":{"url":"52-选修py自动化与简单实践/7-devops/DevOps现状调查/【DevOps现状调查报告-代码管理】.html","title":"【DevOps现状调查报告-代码管理】.md","keywords":"","body":"【DevOps现状调查报告-代码管理】.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/7-devops/web安全体系之DNS/web安全体系之DNS.html":{"url":"52-选修py自动化与简单实践/7-devops/web安全体系之DNS/web安全体系之DNS.html","title":"web安全体系之DNS","keywords":"","body":"web安全体系之DNS Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/7-devops/web安全体系之DNS/【web安全体系之DNS】.html":{"url":"52-选修py自动化与简单实践/7-devops/web安全体系之DNS/【web安全体系之DNS】.html","title":"【web安全体系之DNS】.md","keywords":"","body":"【web安全体系之DNS】.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:00 "},"52-选修py自动化与简单实践/7-devops/全球现状调查解决/全球现状调查解决.html":{"url":"52-选修py自动化与简单实践/7-devops/全球现状调查解决/全球现状调查解决.html","title":"全球现状调查解决","keywords":"","body":"全球现状调查解决 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"52-选修py自动化与简单实践/7-devops/全球现状调查解决/2021DevOps全球现状调查报告解读.html":{"url":"52-选修py自动化与简单实践/7-devops/全球现状调查解决/2021DevOps全球现状调查报告解读.html","title":"2021DevOps全球现状调查报告解读.md","keywords":"","body":"2021DevOps全球现状调查报告解读.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"52-选修py自动化与简单实践/7-devops/常见DevOps发展和学习路线/常见DevOps发展和学习路线.html":{"url":"52-选修py自动化与简单实践/7-devops/常见DevOps发展和学习路线/常见DevOps发展和学习路线.html","title":"常见DevOps发展和学习路线","keywords":"","body":"常见DevOps发展和学习路线 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"52-选修py自动化与简单实践/7-devops/常见DevOps发展和学习路线/常见DevOps发展和学习路线梳理.html":{"url":"52-选修py自动化与简单实践/7-devops/常见DevOps发展和学习路线/常见DevOps发展和学习路线梳理.html","title":"常见DevOps发展和学习路线梳理.md","keywords":"","body":"常见DevOps发展和学习路线梳理.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"52-选修py自动化与简单实践/7-devops/项目是怎样炼成的/项目是怎样炼成的.html":{"url":"52-选修py自动化与简单实践/7-devops/项目是怎样炼成的/项目是怎样炼成的.html","title":"项目是怎样炼成的","keywords":"","body":"项目是怎样炼成的 Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "},"52-选修py自动化与简单实践/7-devops/项目是怎样炼成的/项目是怎样炼成的，从项目0-1过程中，探寻软件入门和岗位高效协作方法.html":{"url":"52-选修py自动化与简单实践/7-devops/项目是怎样炼成的/项目是怎样炼成的，从项目0-1过程中，探寻软件入门和岗位高效协作方法.html","title":"项目是怎样炼成的，从项目0-1过程中，探寻软件入门和岗位高效协作方法.md","keywords":"","body":"项目是怎样炼成的，从项目0-1过程中，探寻软件入门和岗位高效协作方法.md Copyright 🌹 © oneyearice@126.com 2022 all right reserved，powered by Gitbook文档更新时间： 2024-07-28 14:48:01 "}}